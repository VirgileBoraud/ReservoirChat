Memory in linear recurrent neural networks in continuous time Michiel
Hermans∗, Benjamin Schrauwen Department of Electronics and Information
Systems, Ghent University, Sint-Pietersnieuwstraat 41, 9000 Ghent,
Belgium Abstract Reservoir Computing is a novel technique which employs
recurrent neural networks while circumventing diﬃcult training
algorithms. A very recent trend in Reservoir Computing is the use of
real physical dynamical systems as implementation platforms, rather than
the customary digital emulations. Physical systems operate in continuous
time, creating a fundamental diﬀerence with the classic discrete time
deﬁnitions of Reservoir Computing. The speciﬁc goal of this paper is to
study the memory properties of such systems, where we will limit
ourselves to linear dynamics. We develop an analytical model which
allows the calculation of the memory function for continuous time linear
dynamical systems, which can be considered as networks of linear leaky
integrator neurons. We then use this model to research memory properties
for diﬀerent types of reservoirs. We start with random connection
matrices with a shifted eigenvalue spectrum, which perform very poorly.
Next, we transform two speciﬁc reservoir types, which are known to give
good performance in discrete time, to the continuous time domain.
Reservoirs based on uniform spreading of connection matrix eigenvalues
on the unit disk in discrete time give much better memory properties
than reservoirs with random connection matrices, where reservoirs based
on orthogonal connection matrices in discrete time are very robust
against noise and their memory properties can be tuned. The overall
results found in this work yield important insights in how to design
networks for continuous time. Key words: Reservoir Computing, Continuous
time, Memory function, Linear dynamics, Recurrent neural networks 1.
Introduction A signiﬁcant body of research on neural networks fo- cuses
on recurrent neural networks. These networks have for instance been
studied for their ability to store pat- terns (the so-called Hopﬁeld
networks (Hopﬁeld, 1982)). More recently however, recurrent networks are
being used to process temporal information; due to internal feedback,
these networks have an intrinsic ability to retain informa- tion about
past input for a certain time, which allows for the processing of
signals which are explicitly coded in time. This property is often
called ‘fading memory’, ’short term memory’ or the ‘echo state
property’. Though training algorithms for recurrent neural networks
exist which can be quite successful (most notably backpropagation
through time (Rumelhart et al., 1986)), some problems like slow
convergence and limited applicability due to high compu- tational costs
remain (Hammer and Steil, 2002). An alternative approach is generally
known as Reser- voir Computing (RC), discovered independently by Jaeger
(Jaeger, 2001a) for analog hyperbolic tangent neurons, and by Maass
(Maass et al., 2002) for spiking neurons, where the networks are called
‘Echo State Networks’ and ‘Liq- uid State Machines’ respectively. In
these systems, the ∗Corresponding author. Tel.: +32 9 264 95 28. Email
address: Michiel.Hermans@ugent.be (Michiel Hermans) “reservoir” is a
randomly initiated recurrent neural net- work with ﬁxed interconnection
weights, excited by the input which needs to be processed. The readout
mech- anism consist of a single layer of linear nodes observing the
reservoir nodes, which are usually trained using linear regression. This
approach is very fast and easy to imple- ment, and does not suﬀer from
common problems in other training algorithms such as fading error
gradients or slow convergence (check Prokhorov (2005) for a broader
context of RC in schemes of training recurrent networks). Despite its
recent introduction, RC has already proven to be quite powerful: many
applications for RC have already been successfully implemented. For
instance speech recognition (Verstraeten et al., 2006), robotics
applications (Antonelo et al., 2008), and for certain tasks (predicting
chaotic time series) it outperforms all other state-of-the-art
techniques (Jaeger and Haas, 2004). Due to its generic nature, RC is not
limited to digi- tal simulations of neural networks. Any high
dimensional complex dynamical system with the right properties1 can
1These properties are still the matter of some debate, though it is
generally accepted that good reservoirs should have input separa- bility
and fading memory (Maass et al., 2005; Maass and Markram, 2004) and its
dynamics should be close to the regime shift from sta- ble to chaotic
(Bertschinger and Natschl¨ ager, 2004; Legenstein and Maass, 2007)
Preprint submitted to Elsevier August 12, 2009 serve as a reservoir. For
instance the use of photonic com- ponents (Vandoorne et al., 2008) and
the use of a liquid water surface (Fernando and Sojakka, 2003). As of
yet un- published research at TU Graz has been focusing on the
computational abilities of nonlinear spring-mass-damper systems, with
possible applications in robotics. It has even been suggested that
certain bacterial organisms use their gene regulation network to perform
RC for adapting to changing environmental conditions (Jones et al.,
2007). An important trend in RC is to use leaky integrators, which
employ a low-pass ﬁltering operation inside the neu- rons to increase
the reservoir’s temporal processing abili- ties by increasing its memory
(Jaeger et al., 2007), which has also been extended to band-pass ﬁlters
(Schrauwen et al., 2007; wyﬀels et al., 2008). One of the most im-
portant empirical results in this ﬁeld is that usually the timescale of
the reservoir (which is determined by the pass- band of the ﬁltering
operation) has to be matched to the timescale of the signal in order to
maximize performance (see for instance Schrauwen et al. (2007)). In
spiking leaky integrate-and-ﬁre neurons as well, low-pass ﬁltering is
per- formed when using exponential synapse models (Gerstner and Kistler,
2002). Here as well, the timescale of the synapses can be tuned to
optimize performance on certain tasks (Hermans et al., 2008). Compared
to ‘classic’ RC, which is implemented in dis- crete time, all of the
above examples have a continuous time element: most are physical systems
which inherently operate in continuous time. When applying low-pass ﬁl-
tering in discrete time reservoirs, the equations which de- scribe the
evolution of the reservoir states can in fact be considered as a
discrete time approximation of continuous time diﬀerential equations
(Schrauwen et al., 2007). As stated earlier, one of the most important
proper- ties a reservoir needs is fading memory. This paper will
research this property for continuous time linear systems. Although RC
in fact relies on nonlinearity for its compu- tational power, a linear
model serves as a ﬁrst-order ap- proximation which can still be
approached with analytical methods and can give valuable insights in the
dynamics of the system. Little research on continuous time RC has yet
been performed. A recent paper (Ganguli et al., 2008) studies short term
memory in discrete time neural networks by us- ing a criterion based on
Fisher information, and it brieﬂy mentions an extension of its
theoretical frame to continu- ous time reservoirs. However, the method
by which they evaluate memory does not depend on the nature of the
signal which needs to be remembered, which - as we will show - is quite
important in the criterion we use in this paper. In order to study the
temporal processing ability of linear ﬁrst order systems, one needs to
know how much information on past input is coded in the immediate
spatial state of the system. This property is represented by the memory
function (MF) (Jaeger, 2001b), which measures the ability to reconstruct
the input signal from a past time τ from the immediate state of the
system. The classical deﬁnition of the MF is usually deﬁned for discrete
time step reservoirs, where the input consist of a signal s(i), i ∈ N
that takes on a random value from a normal distribution at each time
step. The MF is then deﬁned as m(k) = ⟨s(i −k)uk(i)⟩2 i σ2(s)σ2(uk) ,
(1) in which ⟨· · · ⟩i is the mean over all values i, and uk(i) =
y(k)Ta(i), the output trained to reconstruct the signal from k timesteps
ago, with y(k) optimized output weights and a(i) the reservoir state at
the i-th time step, and σ de- notes standard deviation. The downside of
this approach is that the signal used here is not necessarily a
realistic signal that has to be processed. If we for instance deﬁne a
continuous time equivalent for this type of signal by assum- ing that
the time steps become inﬁnitesimally small, this will become a signal
which ﬂuctuates inﬁnitely fast and has an unbounded spectrum. This type
of signal does not exist in continuous time because it would require
inﬁnite power. Throughout this paper we shall therefore assume that the
continuous-time signal s(t) is of a more realistic form which has a
bounded spectrum and amplitude. This paper is structured as follows.
First, we propose a derivation for the MF which takes into account the
na- ture of the input signal and the low pass ﬁltering of the reservoir.
First we will derive an analytical expression which allows for a fast
and accurate computation of the MF. Next, we shall use this expression
to semi-empirically analyze the memory of linear leaky-integrator
reservoirs in continuous time for diﬀerent parameters such as eigen-
value distribution, state noise and the reservoir timescale. The only
limitation of our model is the condition that the connection matrix
should be diagonalizable, which is true for almost all common reservoir
setups2. 2. Analytical derivation of the memory function 2.1. Reservoir
states In this part, we shall derive the MF for any diagonal- izable
connection matrix W of size N ×N, where N is the number of neurons. To
start our discussion, we will ﬁrst consider the case where no noise is
present in the system. The reservoir state a(t) then evolves according
to ˙ a(t) = Wa(t) + vs(t), (2) in which v is a vector which describes
the weights of input connections. The general solution to equation (2)
in steady 2A notable exception is the delay line, which consists of a
chain of neurons where input is fed only to the ﬁrst neuron, and the
last one does not give output to any other neurons. The delay line has
been studied as an important academic example in White et al. (2004) and
Ganguli et al. (2008) 2 state regime (i.e., when the inﬂuence of initial
conditions has faded) is given by a(t) =  u(t)eWtv  ∗s(t), (3) where
u(t) is the Heaviside-step function. When we now assume that W is
diagonalizable, hence that W = CDC−1 with D a diagonal matrix with
eigenvalues λi. Then we can rewrite the above equation elementwise as
ai(t) =  u(t)CeDt C−1v | {z } p   i ∗s(t) = u(t) N X j=1 Cijpj
exp(λjt) ∗s(t) = [Tz(t)]i , where Tij = Cijpj and zi(t) = u(t) exp(λit)
∗s(t). This means that any linear combination of reservoir states can be
written as a linear combination of the output of ﬁlters with impulse
response exp(λit) operating on the input sig- nal. These are all well
known basic results from linear dynamical system theory as found in for
instance Sontag (1998). 2.2. Modeling noise Noise can appear in many
diﬀerent forms: one can in- ject noise into the neurons as extra input,
noise can be superposed on the input signal, or the neurons can be in-
herently noisy, superposing noise on their output etc. In this article,
we limit ourselves to the last option, which means we superpose a term
σ(a′)√ǫh(t) on the reservoir states, where σ(a′) is the mean over all
neurons of the stan- dard deviation of the states when no noise is
present, ǫ is the signal-to-noise ratio (SNR), and h(t) is the noise
sig- nal with unit standard deviation and zero mean. Scaling the noise
to the mean standard deviation of the reservoir states has the advantage
that one does not need to account for internal ampliﬁcation of the input
signal. If we further assume that this noise has a bandwidth which is
far greater than the pass-bands of the neurons, it is easy to see that
noise will propagate through the network only to a limited degree.
Low-pass ﬁltering of a signal comes down to taking its average value
over an exponential window. If the noise ﬂuctuates very fast compared to
the time scale of the low-pass ﬁltering operation, the output of this
ﬁltering will be very small in amplitude. Each neuron acts as a low pass
ﬁlter or an integrator of some sort and hence the states of the neurons
are assumed to ﬁlter out the noise on their input. We will therefore
assume that no noise propagation exists in the network, which leads to
the following reservoir states a(t) = Tz(t) + σ(a′)√ǫh(t). (4) Note that
in any realistic scenario, noise superposed on the reservoir states will
propagate through the network to some degree, which means this model
will only be ap- plicable in some situations. However, this
approximation allows us to form a direct link between eigenvalues of the
correlation matrix of the reservoir states, and noise sensi- tivity (see
3.1). Extending the analytical model used in this paper to fully account
for noise propagation is in fact not very diﬃcult, but since this
introduces extra parameters (particularly noise spectral range) we
choose this simpler model. There is another advantage to this model: if
the reservoir is a physical system, reading out the reservoir state will
have to be done by some sort of measurement, which is usually inherently
noisy. Assuming non-propagating noise obvi- ously also serves to model
limitations on readout preci- sion. When noise on the readout mechanism
is much more intense than the noise propagating through the network, the
above approximation will be valid a fortiori. It should be noted that
adding state noise which does not propagate is mathematically identical
to ridge regres- sion, a special case of Tikhonov regularisation
(Tikhonov and Arsenin, 1977), a technique to regularize readout weights,
which is commonly performed when training lin- ear regressors to avoid
overﬁtting (Wyﬀels et al., 2008). 2.3. Memory function Optimal readout
weights for reconstructing the sig- nal at a time τ in the past can be
found by solving the system Ax = y (see for instance Jaeger (2001b)),
where x(τ) = ⟨a(t)s(t −τ)⟩t and A =

a(t)aT(t)  t. This yields the following expression for the memory
function m(τ) = xT(τ)A−1x(τ) σ2(s) . (5) For simplicity, we assume σ2(s)
= 1 and a zero mean, although these are in no way necessary conditions.
For continuous time, we deﬁne the operator ⟨ ⟩t as ⟨f(t)⟩t = lim P →∞ 1
2P Z P −P f(t)dt, i.e the mean over time; we assume no time origin for
the signal and reservoir states. Furthermore we assume that the signal
is a stationary stochastic process. Using the above deﬁnitions, we can
now write x(τ) and A in a more explicit form. We start with x(τ).
[x(τ)]i =  ⟨Tz(t)s(t −τ)⟩t + σ(a′)√ǫ ⟨h(t)s(t −τ)⟩t  i = N X j=0 Tij
Z ∞ 0 dt′s(t −t′)eλjt′s(t −τ)  t = N X j=0 Tij Z ∞ 0 dt′ exp(λjt′)
⟨s(t −t′)s(t −τ)⟩t | {z } R(t′−τ) = N X j=0 Tij Z ∞ 0 dt′ exp(λjt′)R(t′
−τ) | {z } bj(τ) , 3 Figure 1: Comparison of the empirically determined
MF versus the result obtained in equation (7). Grey dashed line is
analytical predic- tion, black solid line empirical measurement. Setup
of the experiment is described in the text. where R(t) is the
autocorrelation function of the input signal. Writing this in matrix
notation gives. x(τ) = Tb(τ). Using the same reasoning, we can calculate
the correlation matrix A. Notice that, since a(t) = Tz(t) + √ǫh(t), with
a(t) real, and T and z(t) generally complex, we can write aT(t) = a†(t)
= z†(t)T† + σ(a′)√ǫh†(t), where † stands for the Hermitian transpose.
This allows us to calculate a Hermitian form for A: A = T

z(t)z†(t)  | {z } B T† + ǫσ2(a′)

h(t)h†(t)  = TBT† + ǫσ2(a′)I. Here, B is the correlation matrix for the
responses of the ﬁlters exp(λit), which can now be calculated the same
way as the elements of b(τ): Bij = Z ∞ 0 dt Z ∞ 0 dt′R(t −t′) exp(λit)
exp(λ∗ jt′). The variances of the states of the individual neurons with-
out noise are given by the diagonal elements of the corre- lation
matrix, which yields σ2(a′) = N −1tr(TBT†). This number is also equal to
the mean eigenvalue of the noise- less correlation matrix. When we
denote these as γi, we can write N −1tr(TBT†) as ¯ γ for short.
Combining these equations ﬁnally leads to a useful expression for the
MF: m(τ) = b†(τ)T†  TBT† + ¯ γǫI −1 Tb(τ). (6) This expression allows
for a quick numerical evaluation of the MF for linear dynamical systems.
When ǫ = 0, the MF reduces to m(τ) = b†(τ)B−1b(τ), (7) which means that
without noise, the MF depends solely on the eigenvalues of W and not on
the input vector or connection topology, and it remains invariant under
a sim- ilarity transformation of W. We can for instance replace W with
its diagonal form D which reduces the network to a set of decoupled
complex ﬁlters. If real elements are re- quired, each complex pair of
eigenvalues can be replaced by a 2×2 block on the diagonal of W,
resulting in a damped oscillator. Real eigenvalues simply represent
disconnected low-pass ﬁlters. To perform empirical studies on the MF, we
shall as- sume an input autocorrelation function of the form R(t) =
exp(−α|t|), which describes a signal which is limited in bandwidth by
the ﬁnite autocorrelation length α, and where we deﬁne the signal
timescale as α−1. This serves as an analogy to the signal used in
discrete networks and is quite common for many natural stochastic
processes. Using this function, we can calculate the elements of B and
b(τ): Bij = 1 (α −λi)(α −λ∗ j)

1 − 2α λi + λ∗ j ! (8) bi(τ) = (λi −α)e−ατ + 2αeτλi (α2 −λ2 i ) . (9) To
validate these expressions, we compared their result with the
empirically determined MF of a 10-neuron toy network. To approximate a
continuous-time neural net- work in the simulation, we used discrete
time steps of 1 ms in a reservoir with a timescale τR = 1 s. Generat-
ing a signal which has R(t) = exp(−α|t|) was done by creating a signal
which was sampled from a Gaussian dis- tribution each time step, and
then low pass-ﬁlter this with timescale α−1. The resulting signal can
easily be proved to approximately have the desired autocorrelation func-
tion. As parameters we chose, α = 1 Hz, ζ = 0.9 (where ζ is the spectral
radius of the matrix used to construct W, as fully explained in
logarithmic A.2), and the MF was numerically evaluated using formula
(5). Simulation was performed for a duration of 2 × 104 s (2 × 107 time
steps). No noise was imposed on the signal; the only limitation on
accuracy was the ﬁnite simulation time step and numerical precision. The
result is depicted in Figure 1, which shows a good correspondence
between theory and experiment. For practical purposes, we use the
Moore-Penrose pseu- doinverse for calculating the inverse of the
correlation ma- trix in equation (6), as is common for practical purpose
lin- ear regression problems. In some cases (see Section 3.2), this
matrix will be very ill-conditioned, and inverting it is bound to be
quite inaccurate. In fact, inverting the correlation matrix is required
to calculate optimal output weights for signal reconstruction, where one
has to solve the system Ax = y as stated before. The pseudoinverse has
the advantage that it ﬁnds an optimal solution for this problem, even
when limited by numerical precision. 4 As such, solutions given by the
pseudoinverse give results which represent not necessarily exact
solutions, but solu- tions which reﬂect practical numerical limitations
on the MF. 2.4. Further deﬁnitions 2.4.1. Reservoir timescale In this
paragraph, we will deﬁne a few useful parame- ters to qualify reservoirs
and their memory. First of all we introduce a parameter to describe the
intrinsic timescale of a reservoir, which we shall call the reservoir
timescale τR and deﬁne as τR = − tr(W) N −1 = −

1 N N X i=1 λi !−1 . (10) This deﬁnition is based on a reservoir in
which all neurons act as low-pass ﬁlters with the same timescale τR,
where all diagonal elements of W are equal to −τ−1 R . Note that this
does not imply that, for a network with a certain τR, all neurons have
to act as low pass ﬁlters with the same timescale, which would imply all
diagonal elements of W would have to be equal. It is easy to see that
the shape of the MF depends on the ratio between the signal timescale
and the reser- voir timescale. We can deﬁne normalized eigenvalues κi =
τRλi, and a normalized time τα = ατ. We can then rewrite equations (8)
and (9) as Bij = τ 2 R (ατR −κi)(ατR −κ∗ j)

1 − 2ατR κi + κ∗ j ! , bi(τα) = τR (κi −ατR)e−τα + 2ατR exp  τα κi ατR
 α2τ 2 R −κ2 i . Since ¯ γ scales with tr(TBT†), the noise term can
also be rewritten the same way as B. The factors τR and τ 2 R which
emerge after the transformation are eliminated when calculating the
actual MF, and as such one can see that m(τα) solely depends on the
normalized eigenvalue spectrum κi, and the ratio between reservoir and
signal timescale ατR. For this reason we shall throughout the remainder
of this paper assume α = 1, as if working with time τα. The results can
then easily be extended to any desireable timescale. 2.4.2. Memory
capacity and quality In discrete time reservoirs one deﬁnes the memory
ca- pacity as the sum over the MF: MC = ∞ X k=0 m(k), which can be
proven to be always smaller or equal to N (Jaeger, 2001b). For
continuous time, the obvious equiva- lent is the integral over the MF.
µc = Z ∞ 0 m(τ)dτ. (11) Note that this quantity, for real physical
systems, would have the dimension of time. If the MF would be equal to
one up to a certain time θ in the past and zero everywhere else, memory
capacity would be equal to θ. In Section 2.5 we shall suggest an upper
bound for memory capacity in continuous time which is very similar to
the bound N. We also introduce a useful quantity which we denote as the
memory quality, deﬁned as µq(x) = 1 x Z x 0 m(τ)dτ. (12) This measure is
always smaller or equal to 1 and is a num- ber which denotes the average
MF up to a time x in the past which can be chosen at a value relevant
for a certain task or type of reservoir. In this article we will mostly
take x = µc. In this case, the memory quality expresses the relative
amount of memory which is actually present in a range equal to the
memory capacity. In Section 3.4, we will study a special kind of
reservoir which has a MF which abruptly drops to a much lower value at a
certain time TR in the past. For these reservoirs we shall therefore
choose x = TR. µc as well as µq(x) can be straightforwardly calculated
from equation (6). To do this, one needs to calculate the integrals over
the crossproducts of the elements bi(τ). Re- sulting formulas become
quite complex, particularly for µq(x) and hence we shall omit these
here. 2.5. Asymptotic memory capacity An interesting limiting case for
the memory capacity is for when τR goes to inﬁnity and no noise is
present. First of all, we look more closely at the deﬁnition of the
memory capacity: µc = Z ∞ 0 m(τ)dτ = N X i=1 N X j=1 Z ∞ 0 b∗ i (τ)bj(τ)
 B−1 ij dτ = tr  B−1 Z ∞ 0 b(τ)b†(τ)dτ  . With this deﬁnition and
equations (8) and (9) we can cal- culate the asymptotic limit. To do
this, we again deﬁne normalized eigenvalues κi = τRλi, and normalize
time on the reservoir timescale: θ = τ/τR. Transforming the inte-
gration variable from τ to θ gives Z ∞ 0 bi(τ)b∗ j(τ)dτ = τR Z ∞ 0
bi(θ)b∗ j(θ)dθ. Expanding bi(θ) and taking the limit τR →∞gives lim
τR→∞bi(θ) = lim τR→∞  ( κi τR −α)e−ατRθ + 2αeθκi  (α2 −  κi τR 2 ) =
2eθκi α . 5 This yields lim τR→∞τR Z ∞ 0 bi(θ)b∗ j(θ)dθ = lim τR→∞ −4τR
α2(κi + κ∗ j). We can similarly apply the limit to the elements of B,
where we ﬁnd lim τR→∞Bij = lim τR→∞ −2τR α2(κi + κ∗ j). Finally, we can
write the the memory capacity as lim τR→∞µc = 2 αtr (I) = 2 αN. (13)
When τR →∞, the MF will stretch on to inﬁnity and consequently it will
be inﬁnitesimally close to zero, so at ﬁrst sight the above calculation
might not seem very useful. However, there are strong suggestions that
this limit might in fact be an upper bound for the memory capacity of
linear ﬁrst order networks. It seems the memory capacity always rises
monotonically with τR (see following sections), and reaches an
asymptotic upper limit for very large τR. Furthermore, in Section 3.4,
when researching a special kind of reservoirs where we can ﬁnd
approximate solutions for the memory capacity, we can also conﬁrm that
this number is the maximal value for memory capacity. So far, we have
not been able to ﬁnd mathematical proof that this in fact a true upper
bound, and for now we will leave this as a conjecture to be proven or
disproven in future research. Notice that this expression is, just like
in discrete time networks, proportional to N and links memory capacity
to signal statistics, suggesting that each neuron is capable to store a
maximal amount of “information” equal to 2/α, just like in discrete time
each single neuron is capable to store one time step of the input
signal. In the next sections, we shall investigate a few impor- tant
reservoir types, using results which have been ac- quired for discrete
time networks which we translate to the continuous time domain. 3.
Memory and noise sensitivity 3.1. Noise and the correlation matrix
Before moving to empirical testing of diﬀerent reservoir types, we
derive a general expression which connects the noise sensitivity of the
MF to a basis of orthogonal reser- voir states and the eigenvalues of
the correlation matrix A. The operations performed in this section are
highly similar to those in Principal Component Analysis (Pear- son,
1901; Jolliﬀe, 2002), where we apply it on continuous time functions
instead of the more common discrete data points. A = TBT† is a real
symmetric positive-deﬁnite ma- trix3 and has an eigendecomposition such
that TBT† = 3For random matrices, discussed in the next paragraph, some
negative eigenvalues can be found, but these are quite probably due to
errors originating from limited numerical precision. UΓU†, where U is
orthogonal, and Γ a diagonal matrix with only positive real eigenvalues
γi. Notice that, since UU† = I, we can write (TBT† + ¯ γǫI)−1 = (UΓU† +
¯ γǫUU†)−1 = U†(Γ + ¯ γǫI)−1U = U(Γ + ¯ γǫI)−1U†. This allows us to
write the MF as follows m(τ) = b†(τ)T†U | {z } ψ†(τ) (Γ + ¯ γǫI)−1
U†Tb(τ) | {z } ψ(τ) = N X i=1 ψ2 i (τ) γi + ¯ γǫ. Notice that ψ(τ) is
strictly real. Deﬁning βi(τ) = ψ2 i (τ)γ−1 i , we can write this as m(τ)
= N X i=1 βi(τ) 1 + ¯ γγ−1 i ǫ. (14) This means that we can decompose
the MF as a set of functions which are all real, positive and between
zero and one. The order of the terms in this equation follows the order
of the eigenvalues γi, with i = 1 corresponding to the largest of γi and
as such in a declining order. There is a clear interpretation of the
functions βi(τ). Suppose we wish to linearly transform the reservoir
states a(t) in the absence of noise, so as to deﬁne an base of states ˆ
a(t) which have the property that

ˆ a(t)ˆ aT(t)  = I, i.e. a set of orthogonal (uncorrelated) states with
unit standard deviation. The above procedure does in fact perform this
transformation. When we implicitly deﬁne ˆ a(t) as a(t) = UΓ1/2ˆ a(t),
(15) we can easily see that this yields the desired expression for

a(t)aT(t)  . So we can deﬁne the orthogonal base states from this
expression: ˆ a(t) = Γ−1/2U†a(t), (16) which yields the desired
correlation matrix. Looking at the original deﬁnition of the MF, one can
then easily deﬁne the MF in terms of the base states ˆ a(t), and in the
absence of noise: m(τ) = N X i=1 ⟨s(t −τ)ˆ ai(t)⟩2, (17) so that βi(τ) =
⟨s(t −τ)ˆ ai(t)⟩2. Each of the terms in (14) has a clear dependence on
its corresponding eigenvalue γi and its noise sensitivity. We can make a
very rough estimate of the MF for a certain noise intensity by
approximating ǫ¯ γ/γi as zero when ǫ < γi/¯ γ, and ǫ¯ γ/γi = ∞when ǫ >
γi/¯ γ. This means that we only add up terms in (14) up to i = k where ǫ
< γk/¯ γ and ǫ > γk+1/¯ γ. This estimation is inaccurate but gives a 6 0
10 20 30 40 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 τ m(τ) 0 10 20 30
40 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 τ cumulative sum of βi a b
Figure 2: (a) Example of the MF for diﬀerent values of ǫ of a 20-neuron
network constructed as described in 3.2 with timescale τR = 2, and ζ =
0.9 (which is a spectral radius, see logarithmic A.2). The thick black
line is for ǫ = 1, the dark grey line for ǫ = 10−3 and the light grey
line for ǫ = 10−6. Notice the strong sensitivity for noise: even for a
signal-to-noise ratio of the order 10−6, the MF is still not close to
its asymptotic convergence to its ideal for ǫ = 0 (highest line). (b)
Cumulative sum of β-functions. The k-th line from the bottom up is the
sum of the β-functions up to k. The last 4 β-functions were not added
due to the fact that the smallest of the eigenvalues γi cannot be
calculated accurately, resulting in irregular behavior graphical
interpretation of the β-functions in (14). Figure 2 gives a depiction of
the MF for diﬀerent noise values on the right, and a cumulative sum of
β-functions on the left. Notice the increasing number of oscillations on
the β-functions, which gives rise to the ﬁnal shape of the MF when no
noise is present. There is a very clear interpretation for this type of
noise sensitivity. When writing equation (15) elementwise, one can see
that the base states ˆ aj are each encoded in the reservoir state a with
a magnitude √γj: ai(t) = N X j=1 Uij√γjˆ aj(t). The smaller γj gets, the
smaller the actual contribution of ˆ aj is to the reservoir states, and
the more it will “drown” into the surrounding noise. This result reﬂects
facts which have been suggested by others as well. For instance, in
Jaeger (2001b) it is mentioned that the memory capacity of neural
networks is limited by the conditioning of the correlation matrix, where
it was found that for discrete time reservoirs, the measured memory
capacity always is slightly smaller than its theoretical value. Since
the condition number of the correlation matrix is equal to the ratio
between its highest and lowest singular values, which are equal to the
eigen- values because A is a positive semi-deﬁnite matrix, this eﬀect is
directly linked to our result. In Ozturk et al. (2006) a diﬀerent
approach is used where the goal is to maximize the entropy of the
reservoir states in order to span the highest possible range of non-
linear mappings of the input signal. It was found that - in order to do
this - the reservoir states should be as little correlated as possible.
When the reservoir states are all uncorrelated, we essentially get the
orthogonal base states ˆ ai(t), where the eigenvalues of the correlation
matrix are equal to the individual variances of the reservoir states,
which means that they will not become excessively low. 3.2. Random
reservoirs In this section we will discuss the MF for reservoirs with
connection matrices with random Gaussian elements that have unit
standard deviation and zero mean (which we shall call ‘random network’
for short). In RC in discrete time, this is the most common way to build
connection ma- trices, and here we will investigate a naive translation
of this technique to the continuous time domain. For this, we will start
from discrete time networks which employ a low-pass ﬁltering operation,
and convert this to a contin- uous time system by reducing the timestep
to zero. The full process is described in Appendix A.2, and the result-
ing connection matrix is the initial random matrix with an eigenvalue
spectrum shifted to the left half of the complex plain and scaled with τ
−1 R . The spectral radius η, as de- ﬁned in Appendix A.2, was chosen at
0.9, but the MF does not depend critically on this value and it seems
conclu- sions found here remain generally valid when 0.5 < η < 1. When η
becomes smaller than 0.5, the MF will quickly start to decrease in
quality and capacity and will become more sensitive to noise. We can use
our analytical model to study the mem- ory for these types of
reservoirs. First, we shall deﬁne a criterion to optimize the MF of a
given reservoir. For practical purposes, this will obviously depend on
the task 7 10 100 200 0.02 1 50

τR µc

N 10 20 30 40 0.02 1 50

τR µq(µc)

0.2 0.4 0.6 0.8 0.02 1 50

τR µcµq(µc)

5 10 15 10 −1 10 0 10 1 10 2 10 3 0 0.2 0.4 0.6 0.8 1 τ m(τ)

τR = 0.01 τR = 2 τR = 100 20 40 60 80 100 120 140 160 180 200 0 5 10 15
20 25 30 35 40 N µc ǫ = 0 ǫ = 10− 6 ǫ = 10− 3 ǫ = 1 Figure 3: Top three
panels: µc, µq(µc) and µcµq(µc) in function of N and τR for random
networks (τR shown on a logarithmic scale and ǫ = 0). Bottom left panel:
MF with respect to diﬀerent values of τr. The argument τ is shown on a
logarithmic scale, N = 100. Bottom right panel: memory capacity in
function of N with respect to diﬀerent noise levels. For this
experiment, τR = 2. All results in this ﬁgure have been found by
averaging over 50 reservoir initializations. which needs to be
performed, but as a general criterion, we wish to ﬁnd a balance between
memory capacity and memory quality. As an overall objective function for
op- timum memory, we multiply the memory quality by the memory capacity.
This number is equal to R µc 0 m(τ)dτ, and signiﬁes how much memory is
in fact present within the range {0 · · ·µc}. We investigate the three
measures µc, µq(µc) and µcµq(µc) as functions of the reservoir timescale
and neuron number N. Results are depicted in the top three panels of
Figure 3. The highest memory quality is found at low τR (between 0.01
and 0.1), and memory capacity rises mono- tonically with τR. Optimal
values for µcµq(µc) are found for τR ≈2 and this seems independent of
the number of neurons. The shape of the MFs corresponding to high, low,
and optimal τR are depicted in the bottom left panel of Figure 3. As one
can expect, fast reservoirs will have very good memory, but only for a
very short history. Slow reservoirs have generally very low MFs which
extend very far. The optimal value for τR as found by our criterion
tries to balance these two eﬀects by producing a MF with a reasonable
range and quality. Still, it is fairly low in a large part of its range,
until it drops to about zero at τ = 100. The bottom right panel of
Figure 3 shows the average memory capacity as a function of N for
diﬀerent noise lev- els and τR = 2. Two things can be derived from this
graph. First, it seems that µc does not grow linearly with N at all, not
even when ǫ = 0. Secondly, as we have already seen in Figure 2, random
reservoirs are very sensitive to noise. Both these eﬀects are caused by
the fact that the correlation matrices for these types of networks, are
very ill-conditioned, with condition numbers which reach the order of
1018 for N = 50, and higher still for higher N. Most of the normalized
eigenvalues γi¯ γ−1 are extremely small and cause the high noise
sensitivity. In fact, most numerical values of γi are so small compared
to the highest eigenvalue, that for N > 20 it becomes virtually impossi-
ble to calculate accurate values, resulting in negative γi (which is
deﬁnitely incorrect, since A has to be positive semi-deﬁnite). The
relative number of numerically incalculable eigen- values increases
rapidly with N. This means that - theo- retically - memory capacity may
in fact grow linearly with N, but conﬁrming this with our analytical
model would require much higher numerical precision for inverting A.
Another illustration of this fact can be found when nu- merically trying
to conﬁrm equation 13. For this, we took τR = 105 (much higher than α−1)
and calculated the ac- cording memory capacity with our analytical model
for networks of 100 neurons. Over 50 trials, the average µc was equal to
48, approximately 4 times lower than the expected 200. Obviously, such
high numerical precision requirements are undesirable and these results
lead us to conclude that random reservoirs in continuous time are
unsuitable for good memory storage. 8 3.3. Exponentially distributed
eigenvalues It has been suggested in Ozturk et al. (2006) that good
performance for a wide variety of tasks for discrete time reservoirs
with hyperbolic tangent nonlinearities, can be achieved when the
eigenvalues of the connection matrix are spread uniformly over a disk on
the complex plain with a spectral radius η < 1. The main motivation of
this is the maximization of state entropy to span the highest possible
range of nonlinear mappings of input data. Although we are not able to
generalize our analytical model to nonlinear neurons, we can still
investigate the memory of the linear continuous time equivalent of such
reservoirs. To ﬁnd such an equivalent, we apply the inverse z- transform
to the uniform distribution of eigenvalues for the discrete system. The
entire process is explained in Ap- pendix B.1. The end result of this
transformation yields an exponential distribution of eigenvalues in a
horizontal strip in the left half of the complex plain. The actual
eigen- values are selected using an algorithm (see Appendix A.3) which
makes sure they are spread evenly over their dis- tribution. This avoids
clustering of eigenvalues, or areas where no eigenvalues are present.
When using random number generators to generate eigenvalues, the
resulting reservoirs will have memories which diﬀer widely in qual- ity:
some very poor, some very good. Making sure eigen- values were spread
evenly over their distribution reduced this variance, and resulted in
overall better memory prop- erties. The actual construction of the
connection matrices is explained in detail in Appendix A.1. We performed
the same tests as in Section 3.2 to qual- ify memory capacity, where
again, α = 1. Results are depicted in Figure 4. This time, the optimal
reservoir timescale was found for τR = 6 again virtually indepen- dent
of N. Memory capacity again rises monotonically with τR. Also, the MF
for optimal τR is of a higher overall quality than that found for random
reservoirs, with a rel- atively high value over most of its range until
a drop-oﬀat τ = 100. The bottom right panel shows memory capacity at the
optimal reservoir timescale. Again, µc rises slower than linear with N,
however, we ﬁnd signiﬁcantly better values than for random reservoirs.
We can again test equa- tion (13) using 100 neurons and τR = 105,
averaged over 50 reservoir initializations. This gives an average memory
capacity of about 154, which is still below the theoretical value of
200, but the diﬀerence is not as dramatic as for random reservoirs.
Clearly, choosing exponentially distributed eigenvalues will give
acceptable memory capacity and quality for most tasks. However, looking
at the memory capacities at dif- ferent noise levels, we can again see
that they are quite sensitive to noise. Relative to the memory capacity
when no noise is present, noise sensitivity is in fact similar to that
of random reservoirs, however an absolute compar- ison shows that even
when ǫ = 1, memory capacity is comparable to that of noiseless random
reservoirs. The eigenvalue spectrum of A is indeed generally better than
that of random reservoirs: most eigenvalues can still be calculated
accurately and remain in a reasonable range. However, with increasing N,
most normalized eigenvalues slowly drop, and eventually, more and more
become incal- culable, explaining the slower-than-linear increase of µc
in function of N. 3.4. Resonator reservoirs The third type of reservoirs
we will investigate, we will similarly derive from a result which has
been achieved for discrete time. In White et al. (2004), it has been
theorized that networks with orthogonal connection matrices have memory
which is very robust under noisy conditions, and as such are optimal for
memory storage. The eigenvalues of these matrices all lie on a circle on
the complex plain. Again applying the z-transform yields eigenvalues
which all lie on a vertical line, parallel to the imaginary axis (see
Appendix B.2), which crosses the real axis at −τ −1 R . We choose the
imaginary parts of the eigenvalues equidis- tantly, with a diﬀerence in
angular frequency ω between two successive eigenvalues. We now redeﬁne
the index i for the eigenvalues going from −(N −1)/2 to (N −1)/2 rather
than from 1 to N. This gives (see Appendix B.2) λi = ωi −1 τR , (18)
where we use as the symbol for the imaginary unit to avoid confusion
with indices i or j. When no noise is present, we can replace the
reservoir by a set of disconnected ﬁlters characterized by the eigen-
values. The reservoir states are then given by ai(t) = Z ∞ 0 exp(ωit′)
exp(−t′/τR)s(t −t′) | {z } sW (t,t′) , (19) where sW (t, t′) is what we
will call the “windowed signal”, the signal at a time t−t′, multiplied
by an exponential win- dow function exp(−t′/τR). One immediately notices
that the reservoir states in the above equation are very simi- lar to
the ﬁrst N Fourier coeﬃcients of a discrete Fourier transform of sW .
Signal reconstruction can then be per- formed by constructing the
Fourier series. This kind of reservoir is basically made of a set of
damped resonators, which act as band-pass ﬁlters. Therefore, we shall
simply call them resonator reservoirs for the remainder of this article.
The nature of resonator reservoirs allows us to make good analytical
approximations for its memory properties. Although not too complicated,
the necessary derivations are quite involved. To avoid breaking up the
main text, we have placed all calculations in Appendix C, and we will
limit ourselves to mentioning the main results here. First we will
assume ǫ = 0 and describe the properties of the MF and an approximation
of its memory quality. Next we study the eﬀects of noise. 9 10 100 200
0.02 1 50

τR µc

N 50 100 150 200 0.02 1 50

τR µq(µc)

0.2 0.4 0.6 0.8 0.02 1 50

τR µcµq(µc)

10 20 30 40 50 60 70 80 10 −1 10 0 10 1 10 2 10 3 0 0.2 0.4 0.6 0.8 1 τ
m(τ)

τR = 0.01 τR = 6 τR = 100 20 40 60 80 100 120 140 160 180 200 0 20 40 60
80 100 120 140 160 180 N µc ǫ = 0 ǫ = 1 ǫ = 10− 6 ǫ = 10− 3 Figure 4:
Top three panels: µc, µq(µc) and µcµq(µc) in function of N and τR for
exponentially distributed eigenvalue networks (τR shown on a logarithmic
scale and ǫ = 0). Bottom left panel: MF with respect to diﬀerent values
of τR. The argument τ is shown on a logarithmic scale, N = 100. Bottom
right panel: memory capacity in function of N with respect to diﬀerent
noise levels. All results in this ﬁgure have been found by averaging
over 50 reservoir initializations. 3.4.1. Tunability A discrete Fourier
transform is deﬁned for a periodic function or a function within an
interval. We can deﬁne this period as the longest resonance period
present in the network, which we shall call the reservoir period TR = 2π
ω . The ﬁrst problem which becomes apparent is the fact that the
exponential window stretches on asymptotically to inﬁnity. When this
window has dropped signiﬁcantly at TR, reconstruction of the signal
further in the past is heavily limited by interference from the more
recent part of the signal. As a consequence, m(τ) will drop oﬀfast for τ
> TR and we will only have useful memory up to a time TR in the past. If
the exponential window stretches on far beyond TR, signal reconstruction
for τ < TR is in the same way hindered by interference from the signal
stretching beyond TR in the past. This result means that we can in fact
tune the reser- voir to have a MF which is concentrated in an interval τ
∈{0 · · ·TR} simply by tuning ω. Choosing the reservoir timescale τR has
to be such that interference from τ > TR is negligible, but should still
allow for good reconstruction of the signal for τ < TR. In Appendix C.1
we ﬁnd the general shape of the MF. It consists of a periodic function
with period TR, multi- plied by a factor exp(−2τ/τR). This quantiﬁes the
above statement in the sense that the MF which stretches be- yond TR is
a factor exp(−2TR/τR) smaller than the MF for τ < TR. A depiction of
this property is presented in Figure 7a. 3.4.2. Memory quality Since it
is obvious that good memory can only be achieved for up to a history TR,
we will use the memory quality µq(TR) as the main criterion to qualify
the memory of resonator reservoirs. It is easy to discern the factors
which will limit µq(TR). Most importantly, signal reconstruction is
limited by the ﬁnite number of Fourier coeﬃcients which can be extracted
from the reservoir. A full description of the signal within the
reservoir period needs an inﬁnite amount of Fourier co- eﬃcients.
However, since the signal we consider here has a spectrum which drops
asymptotically for high frequen- cies, a truncated Fourier series can
already give a good ap- proximation. The second eﬀect we have to
consider is the balance between signal interference from τ > TR, and the
reconstruction within the reservoir period. Notice that the signal that
can be reconstructed from the reservoir states is sW , which then has to
be divided by the window func- tion to retrieve s(t). If the window
function is already very low when τ < TR, division will signiﬁcantly
amplify any errors. In Appendix C.2, we derived an approximate solution
for the memory quality which takes in account the men- tioned eﬀects. We
ﬁnd µq(TR) = 2 π  1 −e−2 TR τR  arctan  πN TR(α + τ −1 R )  . (20)
10 0 10 20 30 40 50 0 0.2 0.4 0.6 0.8 1 τR m(τ) 10 −1 10 0 10 1 10 2 10
3 0 0.2 0.4 0.6 0.8 1 τ m(τ)

TR = 5 TR = 100 TR = 500 50 100 150 200 0 20 40 60 80 100 120 140 160
180 N µc ǫ = 0.01 ǫ = 0.1 ǫ = 1 a b c ǫ = 0 Figure 5: Properties of
resonator reservoirs. (a) MF with respect to diﬀerent values of TR. τ
shown on logarithmic scale, N = 100, and τR = 0.3TR. (b) Memory capacity
in function of N for diﬀerent values of noise. For this experiment, TR =
N, and again τR = 0.3TR. (c) Black lines: memory functions at diﬀerent
values of noise, from left bottom corner to the right, ǫ equals 10, 1,
0.1, 0.01, 0.001, and zero. Grey dashed lines are predicted curves as
given by equation C.4, where τs has been determined simply by visual
matching. N = 50, TR = 50, τR = 0.3TR. All results in this ﬁgure have
been found by averaging over 50 reservoir initializations. This
expression allows us to ﬁnd an optimal solution for τR when no noise is
present (either numerically or graphically, since this is a
transcendental equation for τR), which is τR ≈0.3TR. It also clearly
expresses the relation between µq, N and TR, which is depicted in Figure
7b. Basically, memory quality has to be sacriﬁced to increase TR, as one
would intuitively expect. We can now conﬁrm the limit situation from
equation (13) analytically, where τR as well as TR goes to inﬁnity. We
indeed ﬁnd (see Appendix C.2.3) that µc = 2N α . This can be easily
conﬁrmed with the numerical evaluation of the analytical model. Using
100 neurons and τR = TR = 105 indeed yields a memory capacity of almost
200. 3.4.3. Eﬀects of Noise Resonator reservoirs are particularly robust
against noise, as one can see in Figure 5b. Here, memory ca- pacity
grows in fact linearly with reservoir size. Also, the memory capacity
remains very good at a realistic SNR of 10−2. This reﬂects the fact that
a Fourier decomposition is a very eﬃcient way to decompose a signal. The
condi- tion number of the correlation matrix A is not necessarily low
and is for instance on average of the order 108 for N = 100. However,
most of the normalized eigenvalues γi¯ γ−1 are not small at all, and
usually only few of the nor- malized eigenvalues are truly small, most
are around the order 10−2. The shape of the MF under the inﬂuence of
noise is depicted in Figure 5c. The cut-oﬀat TR remains, and with
increasing noise levels the MF decays starting close to τ = TR. In
Appendix C.2.4 we derive the shape of the MF under the inﬂuence of
noise. We also ﬁnd that with increasing noise level, this function will
shift to the left within the range {0 · · · TR}, as can be clearly seen
in in the ﬁgure. Unfortunately, we have not been able to fully quantify
this phenomenon. 4. Discussion One of the key features in RC is its
intrinsic memory: it is able to store a certain amount of information on
past inputs in its immediate spatial state. This property allows it to
process sequences of data using a simple memoryless readout. This
transient memory property can be charac- terized by the memory function,
which quantiﬁes how well the input signal from a certain time in the
past can be reconstructed using only the current reservoir state. Due to
its generic character, RC is not limited to digital implementations and
can be extended to physical systems, as in Fernando and Sojakka (2003);
Jones et al. (2007); Vandoorne et al. (2008). We explored memory
properties of linear ﬁrst order systems in continuous time. We found an
expression for the MF for all networks which have a diagonalizable
connection matrix. This expression allows for fast numerical calculation
of the MF or properties such as memory capacity. We discovered that the
MF for continuous time reser- voirs is critically dependent on the
nature of the signal it needs to remember. More speciﬁcally, the MF
depends on the autocorrelation function of the input signal. We found
what we expect strongly to be an upper bound on the memory capacity of
linear ﬁrst order dynamical sys- tem, which scales simply with the
number of neurons N, which is equivalent to results in discrete time
reservoirs, and would suggest that constructing reservoirs with suﬃ-
cient memory capacity can simply be achieved by increas- ing the number
of neurons. However, upon trying to sub- stantiate this claim, it became
immediately obvious that in realistic scenarios, the memory capacity is
severely re- duced by the conditioning of the correlation matrix and
consequently it grows much slower than linear with N for reservoir types
described in Section 3.2. This is caused by the inability to invert the
correlation matrix of the reser- voir states to great numerical
accuracy. We signiﬁcantly improved the scaling of the memory 11 capacity
with N by the transformation of discrete time reservoir dynamics to
continuous time. Some important prescriptions for “good reservoirs” in
discrete time have been made in past research: one which enriches the
dy- namics of nonlinear reservoirs (Ozturk et al., 2006), and one which
makes the MF very robust against noise (White et al., 2004). For both
these types of reservoirs we deﬁned continuous time equivalents. First
of all, the reservoirs described in Section 3.3 reached far higher
memory capac- ity than reservoirs with random connection matrices, but
still suﬀer from a signiﬁcant noise sensitivity. Finally, res- onator
reservoirs (described in Section 3.4) - speciﬁcally built for good
memory - outperform the other types in noise robustness signiﬁcantly,
and indeed show a linear increase in memory capacity with increasing N.
Further- more, memory depth of resonator reservoirs can be tuned such
that memory is very good up to a certain time TR in the past. This work
sheds light on memory properties of continu- ous time dynamical systems.
Unfortunately, as is common in the ﬁeld of RC, precise analytical
modeling is restricted to the linear case, whereas RC derives its power
from non- linearities. As stated before, a linear ﬁrst order dynamical
system can only perform a linear ﬁltering operation on its input.
Therefore, future work should focus on test- ing the results found in
this paper to dynamical systems with some nonlinearity and to
investigate the computa- tional power for the diﬀerent kinds of
reservoirs discussed in this work. Especially in the design of Echo
State Net- works with leaky integrator neurons, using an eigenvalue
distribution similar to that used in Section 3.3 can be a valuable
improvement over simply using random matrices. Another very interesting
research direction is the inclusion of ﬁxed delays into the network, as
this is a common (and important) feature in many physical dynamical
systems. Acknowledgements This work was partially funded by a
Ph.D. grant of the Institute for the Promotion of Innovation through
Science and Technology in Flanders (IWT-Vlaanderen), the Pho- tonics@be
Interuniversity Attraction Poles program (IAP 6/10), FWO Flanders
project # G.0088.09, and # FP7- 231267 (ORGANIC) of the European
commission. We would also like to thank Professor Dr. Jan Van Camp-
enhout for helpful suggestions. Appendix A. Construction of connection
matrices A.1. For a given eigenvalue distribution Here we explain the
process for building connection matrices with a given eigenvalue
distribution. Though this is common knowledge, we include it here for
com- pleteness. To build a connection matrix W for a given eigenvalue
distribution we start of with a diagonal matrix D with the eigenvalues
ordered by absolute value on the diagonal. Since our resulting
connection matrix has to have real elements, all eigenvalues will be
either real, or complex conjugated pairs. In the next step, we perform
an orthogonal transformation to make this a real block- diagonal matrix.
To do this, we construct a matrix O. When Dii is real, Oii = 1. When Dii
and Di+1,i+1 are a complex conjugated pair, we take the elements of the
2 × 2 block on the diagonal at the i-th and i + 1-th row and column of O
as [O]{i,i+1} =

1 √ 2 1 √ 2  √ 2 − √ 2 ! . All other elements of O are zero. Finally,
we can trans- form D to a real block-diagonal form: Db = ODO†. The
resulting matrix is a block-diagonal matrix with the same structure as
O. All real eigenvalues remain in the same place as in D, complex pairs
of eigenvalues are replaced by a block with elements [Db]{i,i+1} = 
ℜ(λi) ℑ(λi) −ℑ(λi) ℜ(λi)  , Which means one can also directly construct
Db from the real and imaginary parts of the eigenvalues. One can already
use Db as a connection matrix, where it is clear that all single
diagonal entries simply act as dis- connected low-pass ﬁlters, and 2 × 2
blocks are associated with two interconnected neurons which together act
as a damped resonator. A more general connection topology can be
constructed by a similarity transform: W = CDbC−1, where C can be any
nonsingular matrix with the eigen- vectors of W as its columns. For the
connection matrices used for empirical testing throughout this paper we
will choose C with random Gaussian elements. A.2. Random matrices The
update equation for discrete time linear neural net- works is given by
ai+1 = W′ai + v′si, where the subindex states the time and not vector
element positions. The spectral radius of W′ has to be smaller or equal
to one to ensure stability. When we assume discrete time runs in steps
of duration ∆t, and we implement a low-pass ﬁltering operation with
timescale τR (which - we will see - approximately ﬁts its previous
deﬁnition), we can rewrite this equation as ai+1 =  1 −∆t τR  ai + ∆t
τR W′ai + ∆t τR v′si = ai + ∆t τR (W′ −I)ai + ∆t τR v′si, 12 based on
Jaeger et al. (2007). When we rewrite this equa- tion as a ﬁnite
diﬀerence approximation of equation (2), we get ai+1 −ai ∆t = 1 τR (W′
−I) | {z } W ai + 1 τR v′ | {z } v si = Wai + vsi, Obviously, when ∆t →0
this results in equation (2), which is the inverse of the Euler
approximation for integrating a diﬀerential equation. This means that we
can simply use any random matrix with a spectral radius η ≤1, subtract
the unit matrix and scale with τ−1 R . Random matrices (with elements
independently drawn from a Gaussian distribution with zero mean and unit
standard deviation) have their eigenvalue spectrum roughly spread out in
a disk centered on the origin of the complex plain with radius √ N
(known as Girko’s circu- lar law (Girko, 1984)). We assume W′ is scaled
on its spectral radius and we also assume that the mean of the
eigenvalues is roughly equal to zero. One can then ﬁnd that the
eigenvalue spectrum of W is simply that of W′, shifted to the left on
the complex plain by 1, and scaled to τ −1 R . When looking at the
eigenvalue decompositions W = CDC−1 and W′ = C′D′C′−1 we can write D = 1
τR C−1(W′ −I)C ⇕ D + 1 τR I = 1 τR C−1W′C. Since the left hand side is
diagonal, the right hand side is the unique eigenvalue decomposition of
W′ and C′ = C and D = τ−1 R (D′ −I), which means the eigenvalues of W′
are shifted to the left. Stability in continuous time for equation (2)
is guaran- teed when all eigenvalues of W lie on the left half of the
complex plain. Since all eigenvalues of W′ lie in the unit disk, which
is then shifted to the left by 1 and scaled with a positive number τ−1 R
, we indeed get a stable system. The timescale τR ﬁts its previous
deﬁnition when the mean of the eigenvalues of W′ equals zero. This is of
course gener- ally not true, and for constructing matrices for testing
in Section 3.2, we used a similar shifting of the eigenvalues of the
initial random matrix, where we subtracted its initial mean value so
that the spectrum is centered around zero, before we scale to the
spectral radius η. The above process can be summarized in one single
equation. Starting from any random matrix W0 with eigenvalues λ0 i with
mean value ¯ λ0, we can construct a connection matrix W to be used in
continuous time: W = 1 τR  η W0 −¯ λ0I max |λ0 i −¯ λ0| −I  A.3.
Evenly distributed eigenvalues One strategy to generate eigenvalues for
an exponen- tially distributed spectrum, is to use a random number
generator with an exponential distribution for the real part, and one
with a uniform distribution for the imag- inary part. The problem that
one faces with this strat- egy is that the eigenvalues will not
necessarily be evenly spread; some places will be crowded, others empty.
This gives a very large variance of the MFs; some performing very poor,
others very good. Therefore, we shall use a simple algorithm that avoids
clustering of eigenvalues. We start from the z-domain where we spread
eigenvalues more or less evenly, and later transform them to the Laplace
do- main using the z-transform (see Appendix B). The method used in
Ozturk et al. (2006) to gener- ate even distributions is based on
Erdogmus et al. (2003), which uses an iterative method with entropy
maximization as its end goal. We used a much simpler approach start- ing
from a geometric point of view. We ﬁrst deﬁne the upper half of the unit
disk. The ﬁrst eigenvalue of the dis- crete system is chosen randomly
from this circle segment. Next, we deﬁne a circle of a certain radius ρh
around this point, and make sure no other eigenvalues can be chosen
within it. We repeat the process until we have deﬁned N/2 points and
then include their complex conjugates. We also make sure no eigenvalue
is chosen with an imaginary part smaller than ρh/2, which avoids
clustering with the com- plex conjugates. ρh has to be chosen small
enough so that there will be enough space to have N/2 eigenvalues within
the given area, but large enough to avoid clustering. Since the area cut
out by each circle is proportional to ρ2 h, and the total area cut out
by the circles is proportional to N, ρh will have to be proportional to
N −1/2. Rather than meticulously working out the necessary conditions
for ρh, we eventually settled after some trial and error to choose ρh =
(1.7N)−1/2. An example of the resulting distribution of eigenvalues is
given in Figure 6. B. z-transform of eigenvalue spectrum To transform a
dynamical system in continuous time to discrete time, one has to use the
z-transform (see for instance Jury (1964)). The transformation between
the complex variable z in the z-domain and s in the Laplace domain is
deﬁned as z = esTs, where Ts is the sampling period by which the input
sig- nal is sampled. Discrete-time reservoirs can in fact be considered
as a system where the input signal is sampled from a continuous signal
at each time step. As such, we can transform this system to a continuous
time equiva- lent by an inverse z-transform. Since reservoir dynamics
are predominantly determined by the eigenvalues, we can use the above
equation to ﬁnd the Laplace-domain equiva- lent eigenvalues. Obviously,
the sample period has no well deﬁned meaning in this reasoning. However,
when apply- ing the transformation in the two following examples, one
can quickly see that it is fact proportional to the reservoir timescale.
13 −5 −4 −3 −2 −1 0 −6 −4 −2 0 2 4 6 ℜ(λ) ℑ(λ) −5 −4 −3 −2 −1 0 −6 −4 −2
0 2 4 6 ℜ(λ) ℑ(λ) Figure 6: Examples of eigenvalue distributions using
random number generators (left), or using the algorithm described in the
text (right). N = 500, τR = 1. B.1. Laplace-eigenvalue distribution for
a uniform distri- bution in the z-domain Starting from a distribution in
the z-domain, deﬁned in Ozturk et al. (2006) as uniform over a disk with
radius η, we can again use the z-transform to determine the distri-
bution of eigenvalues in the Laplace domain. Using coor- dinates σ and ̟
which denote the real and imaginary part of s, we can deﬁne an
inﬁnitesimal patch of area dσd̟. In the z-domain, we use coordinates ρ
and φ for the ra- dius and angle. A patch of area is here deﬁned as ρ dρ
dφ. The expected number of eigenvalues in this patch is pro- portional
to its surface because of the uniform distribu- tion. Using the relation
ρ = eσTs we ﬁnd dρ = TseσTsdσ. Together with dφ = d̟Ts we can ﬁnally
write that the expected number of eigenvalues in the patch dσd̟ is pro-
portional to T 2 s e2σTsdσd̟. Since φ goes from −π to π, the distribution
in function of ̟ will be uniform between ̟ ∈{−π/Ts · · · π/Ts}, and zero
outside this range. Equiv- alently, we ﬁnd that for ρ > η, the
distribution is zero, so in the Laplace-domain the distribution will be
equal to zero for σ < ln(ρ)/Ts. We can then ﬁnally write for the
distribution of eigenvalues in the Laplace domain Dλ: Dλ(σ, ̟) ∼e2σTsu
ln(ρ) Ts −σ  rect Ts̟ 2π  , where u(x) is the unit step function, and
rect(x) is the rectangle function, equal to one when x ∈{−1/2 · · ·1/2},
and zero everywhere else. Since we deﬁned τR as the inverse of the mean
real part of the eigenvalues, we can do the same here and use the above
formula to ﬁnd that τ −1 R = (2Ts)−1 + ln(η). For simplicity, we assume
η = 1. This ﬁnally yields for the distribution Dλ: Dλ(σ, ̟) ∼eστRu (−σ)
rect τR̟ 4π  . (B.1) B.2. Resonator reservoirs Here we base ourselves
on a result found in White et al. (2004). In this paper it was found
that optimal noise ro- bustness for memory storage is found for
reservoirs where the eigenvalues of the connection matrix all lie on a
circle on the complex plain centered on the origin, with a radius
smaller than 1 (i.e. an orthogonal connection matrix). We assume that
discrete-time eigenvalues, denoted by ξi can be written as ξi = η exp
(2πθi) , where we use the symbol as the imaginary unit to avoid
confusion with indices i or j. Transformation of this sys- tem to the
Laplace domain yields λi = ln(η) + 2πθi Ts , which means all
eigenvalues will lie on a line parallel to the imaginary axis which
crosses the real axis at ln(η)/Ts = −τ −1 R . We are free to choose the
values Ts and η, which means we have control over the imaginary as well
as the real part of the eigenvalues. We will choose the eigenval- ues to
lie equidistantly on this line spread between val- ues ωN/2 and −ωN/2.
This way, when choosing i as i = −(N −1)/2 · · · (N −1)/2, we can write
the eigenvalues as λi = ωi −1 τR . (B.2) C. Memory quality of resonator
reservoirs C.1. General shape of the memory function We can draw
conclusions concerning the shape of the MF of resonator reservoirs when
we look at equations (7) and (9) which state that the MF consists of a
set of cross- products of the elements bi(τ). The timescale τR deﬁnes
the exponential window as deﬁned above and we assume that τR ≫α−1,
i.e. that the reservoir timescale is much longer than the signal
ﬂuctuations. This way, for τ ≫α−1 we can neglect the term with exp(−ατ)
in (9). The cross- products of the elements of b(τ) can then be written
as bi(τ)b∗ j(τ) ≈exp  −2 τ τR  exp (ω(i −j)τ) (α2 −λ2 i )(α2 −λ∗2 j
), 14 0 100 200 300 0 0.2 0.4 0.6 0.8 1 TR µq(TR) 0 20 40 60 80 0.3 0.4
0.5 0.6 0.7 0.8 0.9 1 τR µq(TR) 0 20 40 60 80 100 10 −8 10 −6 10 −4 10
−2 10 0 τ m(τ) a b c N = 100 N = 50 N = 25 N = 10 TR = 100 TR = 50 TR =
20 Figure 7: (a) Depiction of the MF of a reservoir with 50 neurons at
diﬀerent noise levels (the thick grey line is at ǫ = 1). TR = 20 and τR
≈13.4: chosen such that exp(−2TR/τR) = 1/20. The y-axis is on a
logarithmic scale to visualize its exponential decay. Notice the sudden
drop-oﬀat each multiple of TR. (b) Depiction of µq(TR) in function of TR
for diﬀerent reservoir sizes. The grey dashed lines are the predictions
made by equation (C.3), whereas the black lines are from the full
analytical model. τR is chosen to be 0.3 times the reservoir period TR,
which is an optimal value found with equation (C.3). (c) The inﬂuence of
signal interference: µq(TR) in function of τR for diﬀerent TR, N is
chosen at 50. Grey dashed lines are the theoretical prediction found by
equation (C.3), black lines are the values found for the full analytical
model. which means that the MF consists of a factor which is periodic
with maximum period TR, and a factor which decays exponentially with
decay period τR/2. In Figure 7a is a depiction of the MF for a resonator
reservoir which conﬁrms this. C.2. Approximation of the memory quality
Here we will derive the approximation for the memory quality µq(TR) for
resonator reservoirs. We will split the calculations up in two main
parts. First, we investigate interference from the signal beyond the
reservoir period, next we will account for the ﬁnite number of Fourier
coef- ﬁcients. We will redeﬁne the windowed signal sW (t, t′) as being
equal to s(t −t′) exp(−t′/τR) for 0 < t′ < TR and zero elsewhere. It is
useful to state this as its full Fourier series: sW (t, t′) = ∞ X i=−∞
eωit′ai(t), where ai(t) = 1 TR Z TR 0 eωit′e−t′ τR s(t −t′)dt′. Next
we deﬁne ˜ sW (t −t′) as the truncated Fourier series: ˜ sW (t, t′) =
(N−1)/2 X i=−(N−1)/2 eωit′ai(t). Thirdly, we deﬁne the actually
reconstructed windowed signal b sW (t, t′), which is also a truncated
Fourier series, but has coeﬃcients which are deﬁned by equation (19),
i.e: b sW (t, t′) = (N−1)/2 X i=−(N−1)/2 eωit′a′ i(t), with (adding the
scaling factor T −1 R ) a′ i(t) = 1 TR Z ∞ 0 eωit′e−t′ τR s(t −t′)dt′.
(C.1) Similar to sW (t, t′) we deﬁne ˜ sW (t, t′) and b sW (t, t′) to be
zero outside the interval 0 < t′ < TR. C.2.1. Signal interference
Looking at equation (C.1), we can divide the integra- tion in equal
intervals, i.e., we deﬁne Z ∞ 0 f(t)dt = ∞ X j=0 Z TR 0 f(t + jTR)dt.
Since 2π ω = TR, this yields a′ i(t) = Z ∞ 0 eωit′e−t′ τR s(t −t′)dt′ =
∞ X j=0 e−j TR τR Z TR 0 eωit′e−t′ τR s(t −t′ −jTR)dt′ = ∞ X j=0 ai(t
−jTR)e−j TR τR . We can then redeﬁne b sW (t, t′) as 15 b sW (t, t′) =
N−1 2 X i=−N−1 2 eωit′ ∞ X j=0 ai(t −jTR)e−j TR τR = ∞ X j=0 e−j TR τR
N−1 2 X i=−N−1 2 eωit′ai(t −jTR) = ∞ X j=0 e−j TR τR ˜ sW (t −jTR, t′).
Finally, we can use this expression in the ﬁrst step to ﬁnd- ing the
memory quality. Since the MF does not depend on the scaling of the
reconstructed signal, we can write (replacing t′ with τ)
m(τ)[τ∈{0···TR}] = ⟨sW (t, τ)b sW (t, τ)⟩2 t σ2(sW (t, τ))σ2(b sW (t,
τ)). The numerator is given by ⟨sW (t, τ)b sW (t, τ)⟩2 t = ∞ X j=0 e−j
TR τR ⟨sW (t, τ)˜ sW (t −jTR, τ)⟩2 t ≈ ⟨sW (t, τ)˜ sW (t, τ)⟩2 t , since
we can safely assume that the present signal will be virtually
uncorrelated with the signal from a multiple of TR in the past. The
denominator can be worked out in a similar manner. Calculating the
variance for b sW (t, t′), we ﬁnd σ2(b sw(t, τ)) = lim P →∞ 1 2P Z P −P
b s2 W (t, τ)dt = lim P →∞ 1 2P Z P −P   ∞ X j=0 e−j TR τR ˜ sW (t
−jTR, τ)   2 dt Again, we neglect correlation between the present
signal and the signals which extend multiple times TR in the past. As
such we can rewrite this as σ2(b sw(t, τ)) ≈ ∞ X j=0 e−2j TR τR lim P →∞
1 2P Z P −P ˜ s2 W (t −jTR, τ)dt = σ2(˜ sW (t, τ)) ∞ X j=0 e−2j TR τR =
σ2(˜ sW (t, τ)) 1 −e−2 TR τR , which ﬁnally leads us to the MF up to TR
in the past: m(τ)[τ∈{0···TR}] =  1 −e−2 TR τR  ⟨sW (t, τ)˜ sW (t, τ)⟩2
σ2(sW (t, τ))σ2(˜ sW (t, τ)). C.2.2. Consequences of truncated Fourier
series To calculate the memory quality, we will have to make further
approximations: 1. We assume that m(τ) is nearly constant in the range τ
∈{0 · · ·TR}. This constant is equal to the memory quality µq(TR). The
assumption can be validated by looking at Figure 5. 2. We assume that
⟨sW (t, τ)˜ sW (t, τ)⟩, σ2(sW (t, τ)), and σ2(˜ sW (t, τ)) all evolve
with τ as exp(−2τ/τR), multiplied by some constant value. This
assumption is exactly true for σ2(sW (t, τ)), and approximately for the
others as long as τR is not too small relative to TR. Applying this
approximation, we only need to ﬁnd the relative proportions of ⟨sW (t,
τ)˜ sW (t, τ)⟩, σ2(sW (t, τ)), and σ2(˜ sW (t, τ)) to ﬁnd the actual
memory quality. In order to do this, we integrate these expressions over
τ in the reservoir period. We ﬁnd R TR 0 ⟨sW (t, τ)˜ sW (t, τ)⟩tdτ = Z
TR 0 dτ * ∞ X i=−∞ N−1 2 X j=−N−1 2 eω(i−j)τai(t)a∗ j(t) + t = ∞ X i=−∞
N−1 2 X j=−N−1 2

ai(t)a∗ j(t)  t Z TR 0 eω(i−j)τ dτ = N−1 2 X i=−N−1 2

|ai(t)|2 t . Similarly, we ﬁnd Z TR 0 σ2(sW (t, τ))dτ = ∞ X i=−∞

|ai(t)|2 Z TR 0 σ2(˜ sW (t, τ))dτ = N−1 2 X i=−N−1 2

|ai(t)|2 . This ﬁnally yields for the memory quality µq(TR) ≈[1
−exp(−2TR/τR)] P N−1 2 i=−N−1 2

|ai(t)|2 P∞ i=−∞⟨|ai(t)|2⟩. (C.2) The terms

|ai(t)|2 form the power spectrum of the windowed signal. We can use the
Wiener-Khinchin theorem which states that the power spectrum of a signal
is equal to the spectrum of its autocorrelation function, which we shall
denote as RW (t). Since this is a discrete 16 spectrum, we have to
assume the windowed function is periodic and calculate the
autocorrelation function accordingly. Since we take the mean power
spectrum over t, we shall take the mean over t for the autocorrelation
function as well. This will allow us to incorporate the signal
statistics R(t) = exp(−α|t|). We can calculate ⟨RW (t′)⟩t = Z TR−t′ 0 sW
(t, τ)sW (t, τ + θ)dτ + t + Z t′ 0 sW (t, TR −t′ + τ)sW (t, τ)dτ + t = Z
TR−t′ 0 e−2τ+t′ τR ⟨s(t −τ)s(t −τ + t′)⟩t | {z } R(t′) dτ + Z t′ 0
e−TR−t′+2τ τR ⟨s(t −τ)s(t −τ + t′ −TR)⟩t | {z } R(t′−TR) dτ = τR 2 
e−t′(α+τ −1 R )(1 −e(t′−TR)τ −1 R )  + τR 2  e(t′−TR)(α+τ −1 R )(1
−e−t′τ −1 R )  ≈ τR 2  e−t′(α+τ −1 R ) + e(t′−TR)(α+τ −1 R ) . The
discrete Fourier spectrum of this function can be cal- culated as

|ai(t)|2 t = 1 TR Z TR 0 exp(ωit′) ⟨RW (t′)⟩t dt′, which yields

|ai(t)|2 t ∼ 1 T 2 R 4π2  α + τ −1 R 2 + i2 . The sums in equation
(C.2) can be approximated by inte- grals: N−1 2 X i=−N−1 2

|ai(t)|2 t ∼ Z N/2 −N/2 1 T 2 R 4π2  α + τ −1 R 2 + q2 dq, and similar
for the denominator. This ﬁnally yields for the memory quality µq(TR) =
2 π  1 −e−2 TR τR  arctan  πN TR(α + τ −1 R )  . (C.3) The validity
for this approximation is pictured in Figure 7b and 7c. It appears this
gives a good estimate for the mem- ory quality as long as τR is not too
small compared to TR. C.2.3. Asymptotic memory capacity The limit
situation τR →∞can now also be worked out. Notice that the assumptions
we made in Section 2.5 for the normalized κi implies that κi remains
ﬁnite, and so the imaginary parts to have to be ﬁnite. This means that,
since λi = κi/τR, the imaginary parts of the eigen- values will go to
zero as well, implying that TR has to go to inﬁnity together with τR for
the derivation to remain valid. When we assume that the MF is ﬂat in
intervals {iTR · · · (i + 1)TR}, and using the results from (C.1), we
can write µc = TR ∞ X j=0 µq(TR)e−2j TR τR = TR µq(TR) 1 −e−2j TR τR ,
and together with equation (C.3) this becomes µc = 2 π TR arctan  πN
TR(α + τ −1 R )  . When τR and TR go to inﬁnity, the limit is lim
TR,τR→∞µc = 2N α , conﬁrming equation (13). C.2.4. Inﬂuence of noise
When noise is added to the reservoirs states, the reconstructed windowed
signal will consist of the noiseless windowed signal, plus a signal
consisting of the Fourier transform of random entries (the noise). When
we assume that the noise is generated by a stationary process, we can
assume that the second signal has a constant variance in the range {0 ·
· ·TR}. If we divide the reconstructed windowed signal through
exp(−τ/τR) to reconstruct the original signal, the part of it caused by
noise will increase exponentially with τ. If we write the recon-
structed windowed signal without noise divided through exp(−τ/τR) as b
s(t −τ) and the signal caused by noise as c√ǫr(t) exp(τ/τR) where c is a
constant and r(t) is the random signal scaled to unit variance. We can
then write for the MF m(τ)[τ∈{0···TR}] = D s(t −τ)(b s(t −τ) + √ǫcr(t)e
τ τR ) E2 t σ2(s(t −τ))σ2(s(t −τ) + c√ǫr(t)e τ τR ) = µq(TR)ǫ=0 1 + ǫ c2
σ2(b s(t −τ)) | {z } c′2 e2 τ τR = µq(TR)ǫ=0 1 + c′2ǫe2 τ τR , where we
again assume that the covariance and variances in the above expressions
do not depend on τ. Unfortu- nately, calculating c′ is far from trivial
since it requires detailed knowledge of the optimal readout weights,
which 17 also depend on ǫ. Nevertheless, an interesting observation can
be made when writing the above expression as m(τ)[τ∈{0···TR}] =
µq(TR)ǫ=0 1 + e2 τ τR +ln(ǫc′2) = µq(TR)ǫ=0 1 + e 2 τR (τ+τS) , (C.4)
where τS = τR 2 ln(ǫc′2). This is basically a reversed sig- moid curve
(a Fermi function to be precise). If we consider τS to rise
monotonically with ǫ, this means that increas- ing the noise level
shifts the above function to the left without changing its shape. The
actual MF then approx- imately ﬁts a reversed Fermi function within the
interval τ ∈{0 · · ·TR}. This behavior can be seen in Figure 5.
References Antonelo, E. A., Schrauwen, B., Stroobandt, D., 2008. Event
de- tection and localization for small mobile robots using reservoir
computing. Neural Networks 21, 862–871. Bertschinger, N., Natschl¨ ager,
T., 2004. Real-time computation at the edge of chaos in recurrent neural
networks. Neural Computa- tion 16 (7), 1413–1436. Erdogmus, D., Hild,
K.E., I., Principe, J., 2003. Online entropy manipulation: stochastic
information gradient. Signal Processing Letters 10 (8), 242– 245.
Fernando, C., Sojakka, S., 2003. Pattern recognition in a bucket. In:
Proceedings of the 7th European Conference on Artiﬁcial Life. pp.
588–597. Ganguli, S., Huh, D., Sompolinsky, H., November 2008. Mem- ory
traces in dynamical systems. Proceedings of the National Academy of
Sciences of the United States of America 105 (48), 18970–18975.
Gerstner, W., Kistler, W., 2002. Spiking Neuron Models. Cambridge
University Press. Girko, V. L., 1984. Circular law. Theory of
Probability and Its Ap- plications 29, 694–706. Hammer, B., Steil, J.
J., 2002. Perspectives on learning with recur- rent neural networks. In:
Proceedings of the European Symposium on Artiﬁcial Neural Networks
(ESANN). pp. 357–369. Hermans, M., Schrauwen, B., Stroobandt, D., 2008.
Biologically in- spired features in spiking neural networks. In:
Proceedings of the 19th Annual Workshop on Circuits, Systems and Signal
Process- ing. pp. 328–334. Hopﬁeld, J. J., Apr 1982. Neural networks and
physical systems with emergent collective computational abilities.
Proceedings of the Na- tional Academy of Sciences of the United States
of America 79 (8), 2554–2558. Jaeger, H., 2001a. The “echo state”
approach to analysing and train- ing recurrent neural networks. Tech.
Rep. GMD Report 148, Ger- man National Research Center for Information
Technology. Jaeger, H., 2001b. Short term memory in echo state networks.
Tech. Rep. GMD Report 152, German National Research Center for
Information Technology. Jaeger, H., Haas, H., April 2 2004. Harnessing
nonlinearity: predict- ing chaotic systems and saving energy in wireless
telecommunica- tion. Science 308, 78–80. Jaeger, H., Lukosevicius, M.,
Popovici, D., 2007. Optimization and applications of echo state networks
with leaky integrator neurons. Neural Networks 20, 335–352. Jolliﬀe, I.
T., 2002. Principal Component Analysis. Springer. Jones, B., Stekel, D.,
Rowe, J., Fernando, C., 2007. Is there a liq- uid state machine in the
bacterium escherichia coli? In: IEEE Symposium on Artiﬁcial Life.
pp. 187–191. Jury, E. I., 1964. Theory and Application of the
Z-Transform Method. Wiley, New York. Legenstein, R. A., Maass, W., 2007.
Edge of chaos and prediction of computational performance for neural
microcircuit models. Neural Networks 20 (3), 323–333. Maass, W.,
Legenstein, R. A., Bertschinger, N., 2005. Methods for estimating the
computational power and generalization capabil- ity of neural
microcircuits. In: Saul, L. K., Weiss, Y., Bottou, L. (Eds.), Advances
in Neural Information Processing Systems. Vol. 17. MIT Press,
pp. 865–872. Maass, W., Markram, H., 2004. On the computational power of
re- current circuits of spiking neurons. Journal of Computer and Sys-
tem Sciences 69 (4), 593–616. Maass, W., Natschl¨ ager, T., Markram, H.,
2002. Real-time com- puting without stable states: A new framework for
neural com- putation based on perturbations. Neural Computation 14 (11),
2531–2560. Ozturk, M. C., Xu, D., Principe, J. C., 2006. Analysis and
design of echo state networks. Neural Computation 19, 111–138. Pearson,
K., 1901. On lines and planes of closest ﬁt to systems of points in
space. Philosophical Magazine 2 (6), 559–572. Prokhorov, D., 2005. Echo
state networks: Appeal and challenges. In: Proceedings of the
International Joint Conference on Neural Networks. pp. 1463–1466.
Rumelhart, D., Hinton, G., Williams, R., 1986. Learning inter- nal
representations by error propagation. MIT Press, Cambridge, MA., Ch. 8.
Schrauwen, B., Defour, J., Verstraeten, D., Van Campenhout, J., 2007.
The introduction of time-scales in reservoir computing, ap- plied to
isolated digits recognition. In: Proceedings of the Inter- national
Conference on Artiﬁcial Neural Networks (ICANN). pp. 471–479. Sontag,
E., 1998. Mathematical Control Theory: Deterministic Fi- nite
Dimensional Systems. Springer, New York. Tikhonov, A. N., Arsenin, V.
Y., 1977. Solutions of Ill-Posed Prob- lems. Winston and Sons.
Vandoorne, K., Dierckx, W., Schrauwen, B., Verstraeten, D., Baets, R.,
Bienstman, P., van Campenhout, J., 2008. Toward optical sig- nal
processing using photonic reservoir computing. Optics Express 16 (15),
11182–11192. Verstraeten, D., Schrauwen, B., Stroobandt, D., 2006.
Reservoir- based techniques for speech recognition. In: Proceedings of
the World Conference on Computational Intelligence. pp. 1050–1053.
White, O. L., Lee, D. D., Sompolinsky, H., 2004. Short-term memory in
orthogonal neural networks. Physical Review Letters 92 (14), 148102.
wyﬀels, F., Schrauwen, B., Stroobandt, D., 2008a. Regularization methods
for reservoir computing. In: Proceedings of the Inter- national
Conference on Artiﬁcial Neural Networks (ICANN). pp. 808–817. wyﬀels,
F., Schrauwen, B., Verstraeten, D., Stroobandt, D., 2008b. Band-pass
reservoir computing. In: Proceedings of the Interna- tional Joint
Conference on Neural Networks. pp. 3203–3208. 18
