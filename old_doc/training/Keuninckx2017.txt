Cogn Comput DOI 10.1007/s12559-017-9457-5 Real-time Audio Processing
with a Cascade of Discrete-Time Delay Line-Based Reservoir Computers
Lars Keuninckx1 · Jan Danckaert1 · Guy Van der Sande1 Received: 6
October 2016 / Accepted: 21 February 2017 © Springer Science+Business
Media New York 2017 Abstract Background: Real-time processing of audio
or audio-like signals is a promising research topic for the field of
machine learning, with many potential applications in music and
communications. We present a cascaded delay line reservoir computer
capable of real-time audio process- ing on standard computing equipment,
aimed at black-box system identification of nonlinear audio systems. The
cas- caded reservoir blocks use two-pole filtered virtual neurons to
match their timescales to that of the target signals. The reservoir
blocks receive both the global input signal and the target estimate from
the previous block (local input). The units in the cascade are trained
in a successive manner on a single input output training pair, such that
a successively better approximation of the target is reached. A cascade
of 5 dual-input reservoir blocks of 100 neurons each is trained to mimic
the distortion of a measured guitar amplifier. This cascade outperforms
both a single delay reservoir having the same total number of neurons as
well as a cascade with only single-input blocks. We show that the
presented struc- ture is a viable platform for real-time audio
applications on present-day computing hardware. A benefit of this
structure is that it works directly from the audio samples as input,
avoiding computationally intensive preprocessing.  Lars Keuninckx
lars.keuninckx@vub.ac.be Jan Danckaert jan.danckaert@vub.ac.be Guy Van
der Sande guy.van.der.sande@vub.ac.be 1 Applied Physics Research Group
(APHY), Vrije Universiteit Brussel, Brussels 1050, Belgium Keywords
Reservoir computing applications · Real-time · Audio · Cascade ·
Black-box Introduction Classical echo state networks (ESNs) [1] and
liquid state machines (LSMs) [2], now collectively known as reservoir
computers (RCs) [3], utilize a large recurrent network of many randomly
connected nonlinear nodes or neurons. In contrast, in a delay line-based
reservoir computer (DLRC) [4], only a single nonlinear node with delayed
feedback is used to perform much the same function. Consequently, DLRC
has attracted much attention due to its potential for relatively simple
hardware implementations in photonics [5, 6], electronics [7], and
optoelectronics [8, 9]. We explore real-time audio processing as a
possible field of application for delay line reservoir computing.
Reservoir computing, in the form of ESNs has been applied earlier to
audio process- ing [10], for tasks such as reconstructing short
drop-outs in recordings and emulating vacuum tube distortion. Classifi-
cation of sounds using ESNs is shown in Ref. [11]. In Ref. [12], ESNs
are used to improve audio quality by extending the bandwidth of a
bandwidth-limited audio source. To the best of our knowledge, no
attempts at real-time audio RC applications have been reported. Our goal
was to solve a challenging real-world audio-related problem with a
reser- voir computer operating in real-time at 44.1 kHz on standard
computing equipment. Specifically, we chose the task of training in a
black-box fashion and then emulating in real- time the distortion of a
guitar amplifier, using a standard laptop as the computing platform.
Contrary to Hi-Fi ampli- fiers, which are designed to faithfully amplify
their input signals, electric guitar amplifiers are designed to add
nonlin- ear distortion and filtering to color the sound. The black-box
Cogn Comput requirement implies we rely only on signals measured at the
input and output of the amplifier without further knowledge of its
internal circuitry. Often in audio applications, a com- putationally
intensive preprocessing step is required. For example, this is the case
in Ref. [4] for the task of spoken digit recognition, where a Lyon ear
model feature extrac- tion precedes the actual processing in the
reservoir. To not infringe on the real-time requirement, we further put
onto ourselves the constraint of working directly from the audio
samples. The time-discretized DLRC seemed a natural can- didate for a
practical and fast software reservoir computer, precisely because of the
serialization inherent in the delay description. Note that the first
(mixed analog-digital) elec- tronic DLRC implementation reported [4]
processed input samples at approximately 500 Hz, which is at least an
order below that what is needed for (telephone quality) audio
processing. As a paradigm matures from an academic idea to a specific
application, one can expect the form of the solution to become more
specialized and adapted to that application. This phenomenon is common
to all branches of machine learning, where insight into the task one
wants to solve is paramount in order to achieve decent results. The
cascaded reservoir computer structure we present in this article is no
different. Two key biologically inspired elements help to achieve our
goals. The first is the use of two-pole filtered virtual neurons, which
adjust the timescale at which the dynamical behavior of the virtual
neurons evolves to the timescale of the systems we wish to emulate. Any
neural system that processes real-world input signals must somehow
respond on a timescale which is adapted to the timescale of the incoming
data. The second adaptation is the use of a cascade in which each RC
(except the first one) receives as input both the global or original
input and the trained output of the previous RC. Each RC in the cas-
cade is trained to the same desired output. This allows for a
successively better approximation, while achieving our goal of a
black-box system identification. The dual-input archi- tecture offers a
significant improvement in performance, as compared to “standard” single
input cascades in which more than two RC in series offers almost no
benefit [13]. The biological inspiration of the cascade is that of the
neuronal microcircuit, which partly processes information and passes its
results on to the next unit for further processing. We note that in the
same vein, parallel DLRC pools have been shown beneficial for financial
timeseries forecasting [14]. This article is organized as follows. In
“Background: Guitar Amplifier Distortion as a System Identification
Task”, we provide a brief background about distorting guitar amplifiers
and methods that have been tradition- ally used for emulation. In
“Method 1: The Discrete-time Delay Line Reservoir Computer”, we
introduce the time- discretized DLRC as derived from the time-continuous
delay differential equation (DDE)-based RC found in Ref. [4]. In
“Results 1a: Comparison to an Existing Result—the LADSPA “Valve
Saturation” Task”, we apply this struc- ture to a task also found in
Ref. [10], namely the emu- lation of the “Valve saturation” plug-in
found in the open source Linux Audio Developers Simple Plugin API
(LADSPA) library [15], and compare the results. Not with- standing the
good results, we got on the LADSPA task, our first attempts at emulating
a measured real-world dis- torting amplifier, left much to be desired,
as shown in “Result 1b: Emulating a Marshall Valvestate 10 Practice
amp”. Insight from these experiments as to why a single RC fails leads
us to a novel cascaded reservoir comput- ing structure with dual inputs
and filtered virtual neu- rons, discussed in “Method 2: Successive
Approximation with a Cascaded Dual Input Reservoir Computer Structure”.
Results for the real-world amplifier emulation task for the proposed
structure are given in “Results 2: Cascaded Struc- ture”. As a
benchmark, these results are also compared against a Volterra series
approach called the Functional Synchrony Model [16]. Since technical
details matter, we briefly discuss our implementation in “Implementation
Details”. Finally, our conclusions and outlook for future work are
summarized in “Conclusions and Outlook”. Background: Guitar Amplifier
Distortion as a System Identification Task Electric guitar amplifiers
are designed to “color” the sound, using both nonlinear distortion and
strong linear filtering. Historically, the first guitar amplifiers (or
“amps” for short) were over-driven vacuum tube phono record amplifiers.
Even nowadays, many reputable guitar amp manufactur- ers still make use
of vacuum tubes instead of transistors. Over-driving a tube amplifier
introduces progressively more clipping or limiting of the signal, often
describes as “warm”, “soft” or “round”, and this can be beneficial to
the sub- jective auditory experience [17]. Furthermore, every brand,
Marshall, Vox, and Fender are a few of the big names, and even model
type of amp has its own characteristic sound, often instantly
recognizable. Thus, the amplifier is at least as much responsible for
the resulting sound as the guitar is. The severity of the distortion can
go from adding light bluesy accents over a more crunchy sound to full-on
clip- ping found in rock or metal styles of music. Vacuum tubes have
many technological downsides, being more costly, fragile, and shorter
lived than their semiconductor counter- parts. Therefore, manufacturers
have sought ways to replace tubes by semiconductor-based electronics,
while somehow mimicking the distortion so typical for a tube amplifier.
A diode clipper is a simple and effective way to induce distortion, by
limiting the signal excursion. Over time, a Cogn Comput myriad of diode
clipper-based circuits have been introduced on the market. Similar
circuits have been designed around JFETs, MOSFETs, and LEDs. These
clipper circuits are often preceded and followed by tone controls
(linear filters) to further shape the frequency content of the signal
and can be built into solid-state guitar amplifiers, or sold as separate
units, known as “effect pedals”. Even though the initial aim of these
clipping circuits was to emulate and thus replace tubes, they often
perform poorly at this job. Nowadays, these diode clippers and related
circuits are seen as additions to the sound processing armory, rather
than replacements for tubes. Around the 1980s, cheap mass produced digi-
tal signal processors became available [18], and less than a decade
later, amp manufacturers turned to digital techniques to obtain
tube-like distortion. This freed the implementation of the limitations
of fixed analog circuitry, thereby allowing many models within a single
amplifier, and thus giving the musician more freedom. Much of the
methodology of how these digital modelers work is only described in the
form of patents. An in-depth overview of digital techniques for mod-
eling tube amplifiers is given in Ref. [19]. Amp modelers roughly fall
into two categories: white-box and black-box approaches. A white-box
approach uses internal knowledge of the amplifier that is to be model
led. The consecutive stages of the amplifier are seen as consisting of
linear filters, followed by static nonlinearities. The linear parts are
model led by measuring their impulse response and matching this to a
digital filter impulse response. The nonlinear distor- tion in each
stage is then modeled by using static nonlinear functions. These can be
either low order for efficient com- putability, or stored as look-up
tables. In recent years, these modelers have gained increasing
popularity, with Digitech, Roland, and Boss being the top names in the
field. In another form of white-box approach, one starts from the
circuit dia- gram and builds a simplified partial circuit emulator [20].
Note that even though a large number of parameters may freely be
accessible, accurately modeling an existing amp by the end-user is not
straightforward and requires much skill and patience. Typically, a
black-box approach, where there is no infor- mation except for what can
be directly measured without opening the amplifier, is much harder, and
consequently, only a few manufacturers present a solution. A Volterra
series offers a general black-box system identification approach and is
applied in the Nebula Effects Processor by Acustica Audio [21].
Unfortunately, the number of covari- ates rises quickly with the order
of the nonlinearity. This limits the real-time applicability of Volterra
series for audio. Nevertheless, as a benchmark, we compare the
performance of our proposed structure to that of a modern Volterra
series approach called the Function Synchrony Model (FSM) [16]. This
method uses cubic b-splines and is capable of modeling deeply hidden
causal connections between signals, although it is not fit for real-time
implementation and thus the only comparison possible is that of how well
the distortion of the guitar signal by the amplifier is captured. We
have knowledge of only one device capable of true black-box learning on
the market today, namely the Kem- per Profiling Amplifier [22, 23]. In
this system, during the modeling phase, the responses of the device
under test to impulses of several different amplitudes are measured.
Then during the emulation, the input amplitude is evaluated from sample
to sample and the appropriate impulse response is selected. This is
called dynamic convolution. Note this solu- tion is not offered as a
separate software solution, and only runs on custom hardware. We set out
to develop an RC structure capable of per- forming a black-box
system-identification of a guitar ampli- fier and then emulate this
amplifier in real-time, using a “standard” computer or laptop. Method 1:
The Discrete-time Delay Line Reservoir Computer In a DLRC, the
difficulties of building a recurrent network of many nonlinear nodes,
needed for a classical reservoir computer or ESN, are avoided by using
only a single non- linear node and delayed feedback [4, 24]. The
underlying dynamical system often is a well-known DDE such as the
Mackey-Glass system [25]: dx dt = −c1x(t) + c2 x(t −τ) 1 + x(t −τ)p ,
(1) which originated as a model for physiological control of
concentrations of various constituents in blood. In Eq. 1, c1, c2 and
exponent p > 1 are positive constants, which govern the dynamic regime
the system operates in. For use as a DLRC, this system is biased at the
edge of stability. The delay time τ causes the phase space to be
(theoreti- cally) infinite dimensional and thus the system can offer
very complex and rich dynamics, in response to an addi- tional input
signal. Taps placed on the delay line then form what is known as
“virtual neurons”. The wanted response can be assembled by a weighted
linear combination of the virtual neuron signals. With software
implementations in mind, numerical integration of such DDEs with
sufficiently small time-steps in order to faithfully represent the
proper dynamical behavior can be problematic for long delays. For-
tunately, a coarse time-discretization of those DDEs can be equally
performant when used as the kernel in a delay RC structure, even if
these delay difference equations do not offer the same dynamical
behavior of their continuous-time DDEs counterparts. For example, in
Ref. [14], the authors use parallel pools of time-discretized delay RC
for financial time series prediction. Cogn Comput Such a discrete-time
delay reservoir computer can be described by the update equation: x(n) =
ax(n −1) + (1 −a)f [γ J(n) + ηx(n −N)] . (2) Here, n is the discrete
time, N is the length of the delay, 0 < a < 1 is a filter factor, and
J(n) is the masked input signal, explained below. Analogous to the
Mackey- Glass system, the nonlinear function often takes the form f (x)
= x/(1 + xp) with p > 1. γ and η are called the input and feedback
strength, respectively. Figure 1 shows a block schematic of Eq. 2.
Clearly, this structure is readily programmable in an FPGA, or on any
(embedded) comput- ing platform. Equation 2 can be derived in a
straightforward manner from the DDE that describes the continuous-time
RC of Ref. [4], by a coarse backward Euler discretization. As shown in
Fig. 1, the differential then takes the form of a first order infinite
impulse response (IIR) filter with a single pole at z = a. This filter
is stable for |a| < 1. Every delay interval N, a new sample xin ∈R is
applied to the input of the machine. Each sample is kept fixed dur- ing
this delay time N. The sample is multiplied with an N-periodic mask
signal m(n) = m(n+kN), k ∈N, to form the masked input signal: J(n) =
xinm(n). (3) The purpose of the mask is to keep the system in a
transient state. The mask can be randomly drawn from a limited set of
numbers {b1, . . . , bK ∈R}., called a K-valued mask. For vector-valued
inputs, the mask takes on the form of a matrix. Every N-interval, a new
output sample yout is constructed as a linear combination from the
signals found along the N taps of the delay line: yout = N−1  j=0 wjx(n
−j). (4) Each delay tap (called a “virtual” neuron or node) represents a
complicated nonlinear transformation of the input signal, from which the
output is assembled to solve a certain task. The slowness of the system,
represented by filter factor a, causes mixing of the states of adjacent
virtual nodes. The crux of the usefulness of the DLRC for simplified
hardware implementations lies in the fact that the taps on the delay
line do not have to be physically realized. Since the signal revolves in
the delay, the system can be observed at a single sampling point. To
calculate the weights w0, . . . , wN−1, a training set of L samples is
input to the machine. The responses of the vir- tual neurons are
assembled in an L×N state matrix S, such that every row constitutes the
delay system’s response to a single sample: Sk,j = x(kN −j). (5) Then,
every column j = 0 . . . N −1 represents a time- series which is the
response of a virtual neuron to the training set. Writing the desired
output as column vector Y = [yout(0), . . . , yout(L −1)], the weight
vector W = [w0, . . . , wN−1] is found by solving the linear regression
problem Y = SW. The ordinary least squares (OLS) estima- tor of the
weights then is W = (S′S)−1S′Y. More advanced methods such as ridge
regression to counter overfitting are also applied. After calculating
the readout layer weights, the performance is assessed on a previously
unseen testing set. A measure for the performance is the normalized mean
squared error (NMSE) between the wanted output yout and actual output ˆ
yout: NMSE(yout, ˆ yout) = mean[|yout −ˆ yout|2] stdev2[yout] . (6)
Alternatively, the performance is given as the root of the normalized
mean squared error (NRMSE). Many variations to the above scheme have
been pro- posed. For example, an offset may be added to the masked
samples in order to bias the delay system in a suitable dynamical
regime, as this is often needed in hardware implementations [7].
Usually, the training of the readout layer is quite fast. Unfortunately,
optimal values of the input strength γ , feed- back strength η, and
filter factor a depend on the dynamical system as well as the task at
hand. The simplest method for finding optimal values is scanning this
parameter space, reassessing the performance at every point. Fig. 1
Schematic of the discrete-time reservoir computer of Eq. 2 Cogn Comput
Implementation Details We implemented the structure of Fig. 1 in a
C-program and found it readily capable of operating in real-time at the
stan- dard audio sample rate of 44.1 kHz, with N = 1000 virtual neurons
on a common laptop.1 The code was compiled with GCC version 4.7, with
optimization flag -O3. All processing is done with 64-bit floating
points. The audio infrastructure of the computer is addressed using the
multi-platform open source audio library Portaudio [26]. This library
shields off much of the complexity associated with handling audio
interfaces. Training of the reservoir computer is done offline using the
least-squares routines found in the Python SciPy package [27]. The
time-critical C code is interfaced to the Python code via the Ctypes
method. To be able to excite the amplifiers-under-test and record their
responses, we used a Behringer UCG102 Guitar-to-USB interface. This is
an external sound card with a high-impedance input suitable for
electrical guitars, and a headphone output. Typically, the underlying
operating systems’ software layers (over which we have no direct
control), introduce a shift of several hundred samples between the
excitation and the measured response. This shift needs to be compensated
for before training, since it will otherwise place a heavy bur- den on
the reservoir computer memory. A noise robust shift estimate is found by
first filtering the signals and then apply- ing a cross-correlation.
This is called the generalized cross correlation (GCC) [28]. As in Ref.
[29], we obtained good results using the phase transform (PHAT) method.
It is interesting to note that, other implementation details being
equal, delay reservoir computing has an inherent speed advantage over
ESNs that we believe has not been pointed out before. The basis of an
ESN is a large recurrent network of N randomly connected nonlinear
nodes. At each time-step, the N node states are updated. This takes on a
form that is similar to the following: x(n + 1) = f  Wnetx(n) + input 
, (7) with x(n) the state vector of the system at time n, f a non-
linear function (atan, sigmoid, etc.), and Wnet the network connection
weight matrix. In software, this comes down to processing two nested
loops ∼N × N per input sample, because of the matrix multiplication. In
contrast, a delay- based reservoir computer, Eq. 2 has only local
(trailing) connectivity between neighboring neurons (taps of the delay
line). Per input sample, this is programmed as a single loop of size ∼N.
Note that the N-sample delay is implemented (in C) using the well-known
method of a pointer indexing a chunk of memory, such that no separate
loop is needed. 1An Intel dual core running at 2.4 GHz, Dell Latitude
E4300, built in 2009. Thus, it is clear that an inherent gain in
per-sample process- ing speed can be had, at the cost of reduced
flexibility in connectivity. Results 1a: Comparison to an Existing
Result— the LADSPA “Valve Saturation” Task As a first course of action,
we assessed the usefulness of the DTRC of Fig. 1 for audio applications,
by comparing its performance to earlier results obtained on ESNs. The
open source Linux Audio Developers Simple Plugin API (LADSPA) library
[15] contains a simple model to emulate light tube-like distortion. In
Ref. [10], this “Valve Satura- tion” model is used in a system
identification task. The input signal, sampled at 44.1 kHz is upsampled
by a factor of five, and then processed by the following: zn = xn −q 1
−exp−d(xn−q) + q 1 −edq , (8) with q = −0.198 and d = 20.1. Thereafter,
this is followed by the recursive filter: yn = 0.999yn−1 + zn −zn−1, (9)
and downsampled again. As an excitation training signal of L = 30000
samples of Gaussian white noise with a linearly swept amplitude, that is
shaped further with an additional low-pass filter is used in Ref. [10,
29]. Using this approach, with an ESN of 100 neurons and L = 100000
samples of a flute for testing, an NRMSE of 0.0997 is reported. It is
important to note that here, the readout layer also makes use of the
square of the outputs of the nodes, effectively doubling the number of
weights. To compare this to the performance of the discrete-time delay
RC, we use a N = 100 neurons in the delay line and a six-valued mask
drawn from the set ±{0.2, 0.6, 1.0}. As in Ref. [10], in the reservoir
output, we also make use of the square of the reservoir states.
Including the unpro- cessed input and a constant term, the OLS training
delivers a total of 202 weights. As the nonlinearity, we chose f (x) =
tanh(x). The training signal is constructed from of L = 30000 samples of
amplitude ramped Gaussian white noise in blocks of 500 samples with
standard deviation σ = 0.9 and zero mean. This is close to what is seen
in Ref. [29] p. 51. This signal was then bandlimited to 8 kHz, which is
close to the bandwidth of a guitar signal. As a test excitation, we use
L = 100000 samples of a real guitar signal, sampled at 44.1 kHz. This is
because the original flute sample, proposed in Ref. [10], was not avail-
able. The scale of the guitar signal was adjusted such that the RMS
values of the training and testing signal match. In Fig. 2a, we show
part of the test excitation in green and the Cogn Comput Fig. 2 a Part
of the guitar signal for testing (green), and the response (blue)
computed by Eqs. 8–9. b Response signal (blue) and estimate of the
reservoir computer (red) for the case where also the squared reservoir
states are used in the training response of the Valve Saturation model,
Eqs. 8–9 in blue. It is clear that this transformation performs an
asymmetri- cal compression. During training, the first 3000 reservoir
output samples are regarded as a “warm-up” period and thus discarded.
Gaussian noise with a standard deviation of σ = 10−3 is added to the
reservoir state matrix, to avoid overfitting as explained in Ref. [24].
We obtain an NRMSE of 0.0562 for parameter values γ = 0.991, η = 0.738,
and a = 0.173, which is comparable to the result from Ref. [10]. The
output estimate of the RC is shown in Fig. 2b in red, overlayed by the
Valve Saturation model response in blue. Clearly, these signals are very
close to each other. Repeating the experiment without making use of the
squared reservoir states, thus training for 102 weights, resulted in
NRMSE = 0.185. Changing the number of neu- rons to N = 200, also without
the squared reservoir states, did not improve this result. This
indicates that the squares of the reservoir states play an important
role in the result. To benchmark this result, we implemented a 3rd order
Volterra series FSM [16] approach using 20 b-splines and a memory depth
of 50 samples, resulting in 1771 covariates, from which a linear
combination is made to fit the response, in a least-squares fashion.
This resulted in NRMSE = 0.169. In what follows, we moved away from the
Gaussian noise ramp-based excitation. We found that a real guitar signal
of sufficient diversity and length (a few seconds of randomly plucked or
strummed notes or a short riff) works just as well. Also, we did not use
the squared reservoir states in the readout layer anymore, as it was
found that these do not in general lead to better results. Result 1b:
Emulating a Marshall Valvestate 10 Practice amp After the encouraging
previous results, we wanted to emu- late an actual measured guitar
amplifier distortion. Our device-under-test was a Marshall Valvestate
10. This is a small solid-state practice amp with a diode clipper
distor- tion stage. Our choice was motivated merely by availability. In
Fig. 3a, we show one second or L = 44100 samples of a guitar signal. The
response of the amplifier was recorded for a high gain setting,
resulting in a heavy distorted sound. Clearly, the distortion is much
more severe here than in the preceding LADSPA Valve Saturation task. We
optimized the parameters of a DTRC with N = 100 neurons on L = 44100
samples, tested on L = 100000 sam- ples, with a six-valued mask drawn
from ±{0.2, 0.6, 1.0} and with f (x) = x/(1 + x2). Figure 3b shows 1500
sam- ples of the amplifier output in blue, and the best estimate of the
RC in red. The guitar signal is shown as a green line. This resulted in
a disappointing testing NRMSE = 0.240, for γ = 0.189, η = 0.934, and a =
0.0730. Also, the auditory experience lacked in quality. Increasing the
num- ber of neurons to N = 200 and N = 400 did not improve this result.
In addition, we tried binary valued, normally Fig. 3 a One second of the
guitar input signal. b 1500 samples of the distorted response of the
Marshall Valvestate 10 amplifier (blue) and output from the
discrete-time RC (red) after training. The excitation guitar signal is
shown in green Cogn Comput Fig. 4 Damping transient of the guitar
amplifier (blue) after the guitar signal (green) has been set to zero at
the time indicated by the dashed line. A transient of ≈1000 samples is
seen distributed amplitude-ramped and stepped-value masks, and several
other nonlinear functions. We also added the squared reservoir states in
the output. None of these changes made much improvements. As in “Results
1a: Comparison to an Existing Result—the LADSPA “Valve Saturation”
Task”, we bench- mark this task using the Volterra series approach of
Ref. [16]. This results in a training NRMSE of 0.181 and a test- ing
NRMSE of 0.439. Here, the memory depth was set to 500 samples.
Increasing the memory depth to 1000 sam- ples did not improve this
result. This indicates again that the task of emulating the real-world
measured amplifier is more demanding than the LADSPA task. Method 2:
Successive Approximation with a Cascaded Dual Input Reservoir Computer
Structure It was clear that some more insight was needed. In Fig. 4, we
show the output of the guitar amplifier (blue) when the input (green)
drops to zero, at the time indicated by the dashed line. A (seemingly
second order) damping transient of roughly a thousand samples can be
seen. On the one hand, modeling long impulse responses forces the DLRC
to operate in a regime where it has a high memory capac- ity. This is in
the low γ and high η region [24]. On the other hand, emulating a strong
clipping nonlinearity asks for a high input scaling γ and a low feedback
η. Thus, the machine’s parameters are driven to a compromise, that is
optimal for neither requirements of the task. This explains why the
system failed to properly mimic the real distortion. A solution to this
dilemma would be a cascade of two or more reservoir computers. If a task
can be physically split into two or more parts, then the parameters of
each reservoir computer in a cascade can be optimized for a single part
of the task. In this way, a cascade can outperform a single reservoir
having the same number of virtual neurons of the entire cascade.
However, this approach is not compliant with the black-box modeling we
envisioned, because inter-stage excitation/response signals must be
available. In Ref. [13], series cascaded RCs are used for speech
recognition. The authors show that a cascaded reservoir computer where
each RC is trained to the same output and starts from the output of the
previous one offers little benefit beyond more than two units. In the
cascaded structure, we propose in Fig. 5, from one RC unit to the next,
not only the output but also the original or global input xin is passed
on. First, the param- eters and weights of RC1 are optimized, starting
from the input signal xin. This yields a first estimate y1 of the target
response yout. Then, the following units RCi, i = 2 . . . K, are trained
in a successive manner, using both the global input xin and the previous
estimate yi−1. The idea is that by combining both the original input and
the previous estimate, the units can make successively better
approximations of the target signal. There is a delay of one sample per
RC unit due to the processing. Therefore, in Fig. 5, the global input is
also delayed over one sample per unit. Only one global input/output
signal pair is needed, so that this is a black-box approach. To be able
to combine the global input and previous esti- mate, we have modified
the discrete time RC structure of Fig. 1 to what is shown in Fig. 6.
Here, the RC has two input scaling parameters, γ1 and γ2, two feedback
strength param- eters η1 and η2, and two node mixing parameters a1 and
a2. The (fixed and chosen in advance) switching sequence S(n) determines
how both inputs are mixed in the delay lines. After the switches have
changed position, the inertia of the nonlinear node ensures that for
some time, its out- put is influenced by both inputs. The contents of
the virtual neurons is then influenced by both inputs. The highest
inter- action between both input streams is reached by using a switching
sequence consisting of alternating ones and zeros: Fig. 5 A cascade of
discrete-time delay line reservoir computers. Each unit, except the
first one, receives the original or global input and the estimate of the
previous unit. The internal structure of the units is shown in Fig. 6
Cogn Comput Fig. 6 Modified structure of Fig. 1, allowing for two inputs
with programmable feedbacks and input scaling, and having two pole
filtered virtual neurons S(n) = n mod 2, n = 1 . . . N. A switching
sequence S(n) consisting of N/2 ones followed by N/2 zeros implies the
inputs have almost no interaction, except for those virtual neurons
following but close to halfway the delay line. Inter- mediate
interaction is possible by using sequences such as S(n) = 1, 1, 1, 1, 0,
0, 0, 0, 1, 1, 1, 1, . . .. Maximum length sequence (MLS) switching
would offer the most diverse interaction in the delay line, in much the
same way that MLS masks have been used [4]. At the time of writing, we
have only implemented and experimented with the minimal inter- action
switching sequence, i.e., the first half of the delay line receives
samples stemming from the first input, and the second half from the
second input. Note however that the output of the readout layer is still
a function of both inputs. Filtered virtual neurons Furthermore, we add
digital two pole resonator filters in the nodes of the delay line, in
such a way that for each virtual neuron, a different filter is addressed
and fed back to the nonlinear node. The purpose of these filters is to
twofold. Firstly, they allow emphasis on certain frequencies of
interest. Secondly, the filters incorporate extra memory in the virtual
neurons, which is independent of the feedback strength. In the
literature, this is known as (bandpass) fil- tered (BPF) neurons [30].
In Ref. [29], biquad filters are applied. We have opted for the simpler
two pole resonator for computational efficiency. As in Fig. 6, the state
of the nonlinear node is u(n), and the input to the delay line is v(n),
thus v(n) = b0(n)u(n)−b1(n)v(n−N)−b2(n)v(n−2N), (10) where N is the
length of the delay line. The filter coeffi- cients are periodic with
period N, i.e., b0(k + lN) = b0(k) for k = 0, . . . , N −1 and l ∈N, and
likewise for b1(n) and b2(n). Then, the state of virtual node k = n mod
N, at audio input sample n′ = ⌊n/N⌋is v(k)(n′) =
b0(k)u(n)−b1(k)v(k)(n′−1)−b2(k)v(k)(n′−2). (11) Thus, each virtual
neuron has its own filter. Dropping the k- indices to simplify the
notation, one such filter has a transfer function in the z-domain of the
following form [31]: H(z) = V (z) U(z) = b0 1 + b1z−1 + b2z−2 . (12) The
poles are chosen to be complex conjugate, to form a resonator at
normalized angular frequency θc = 2πfc/fs: p1 = Reiθc, p2 = Re−iθc, (13)
with pole radius R < 1 such that the poles lie in the unit circle and
the resonator is stable. Writing (12) as H(z) = b0z2  z −Reiθc  z
−Re−iθc, (14) leads to: H(z) = b0 1 −2R cos(θc)z−1 + R2z−2. (15)
Comparing (15) to Eq. 12 gives expressions for the filter coefficients:
b1 = −2R cos(θc), (16) b2 = R2. Cogn Comput Fig. 7 a Response of the two
pole resonators, with logarithmically spaced center frequencies, between
flow = 400 Hz and fhigh = 19 kHz. b Corresponding pole locations in the
z-plane The gain at resonant frequency is found by setting f = fc in z =
ei2πf/fs: |H(eiθc)| = |b0| (1 −R)  1 −2R cos(θc) + R2 . (17) We have
normalized the responses of the filters by setting b0 = 1/|H(eiθc)|. The
−3 dB bandwidth of the resonator B is approximately [31]: B ≈−fs ln(R) π
. (18) In our experiments, the N filter frequencies fc are log-
arithmically spaced between flow = 400 Hz and fhigh = 19 kHz. The pole
radii R are drawn from a uniform dis- tribution between 0.5 and 0.8. The
filters are randomly distributed over the virtual neurons. In Fig. 7a,
we show fre- quency response of these two pole resonators. The location
of the poles in the z-plane is shown in Fig. 7b. Note that due to the
pole locations, some of the filters have a low-pass or high-pass
characteristic. Also, we specif- ically chose high bandwidths (or radii
not to close to the unit circle), taking in mind that several RCs are
cascaded, such that sharper bandwidth responses can arise naturally. We
illustrate the effect these filters have in Fig. 8a, where we plot the
absolute values of N = 50 virtual neurons over time, for the case where
there are no filters present and for parameters η = 0.9, γ = 0.3, and a
= 0.1. At a certain instant, indicated by the dashed line, the guitar
signal input to the reservoir computer drops to zero. The RC will not be
able to properly construct a nonzero output signal beyond approximately
a hundred samples. When comparing this to the response under the same
conditions for the case with filters present in Fig. 8b, it is clear
that the machine is able to respond for a much longer time. Thus, by
using the filtered virtual neurons, we can match the timescale of the
reservoir computer to the timescale of the task. More so, the filters
cause the phases of the virtual neurons to be much more disjointed as
compared to the unfiltered case. This enriches the dynamics of the
reservoir further. DC Blocker A final tweak is the use of a digital DC
blocker, which is applied to the measured amplifier response signal
before the training procedure. We found this helps to limit long impulse
responses, while it is not detrimental to the auditory result. A
computationally effortless DC blocker is given by the transfer function
[32]: H(z) = 1 −z−1 1 −pz−1 , (19) with 0 < p < 1. This has a fixed zero
at z = 1, which is a pure differentiator. Since this zero also
attenuates frequen- cies close to DC, a compensating pole is placed
nearby. We Fig. 8 Absolute values of virtual neuron entries over time a
without and b with two pole filters active. In a and b, the machine
parameters are η = 0.9, γ = 0.3, and a = 0.1. At the time indicated by
the dashed line, the input of the RC drops to zero Cogn Comput chose p =
0.98, which puts the −3 dB frequency close to 139 Hz for a sampling
frequency of fs = 44100 Hz. Training and Parameter Optimization As
explained earlier, our training and testing excitations and responses
consists of real measured guitar and amplifier samples. Every RC in the
cascade of Fig. 5 has two input scaling factors, γ1 and γ2, two feedback
strengths η1 and η2, and two filter mixing constants a1 and a2. For a
cas- cade of K units, this makes for 6K parameters that need to be
fine-tuned. Then, there are the choices of mask, filter coefficients,
nonlinear function f , number of virtual neu- rons, and training and
testing sequence lengths. These do not have to be equal for every RC in
the cascade. It is clear that the structure proposed in Figs. 5 and 6
allows for much exploration. In the following exploratory tests, we
chose every DTRC in the cascade to be of the same size N. Also, each RC
is given the same filter coefficients, designed as outlined above, and
the same nonlinear func- tion f (x) = x/(1 + x2). Every RC is given a
distinct mask that is a random permutation drawn from the six values
±{0.2, 0.6, 1.0}. On the one hand, straightforward scanning of the six-
dimensional parameter space would be very time consum- ing. On the other
hand, it is known that the outcome of a gradient descent procedure can
depend on the starting position, since the optimization can get stuck in
a local min- imum of the error surface. Therefore, we use a form of
random optimization (RO) [33], proceeding as follows: 1. Choose an
initial set of P random (within certain limits) parameter tuples (γ1,2,
η1,2, a1,2)i, i = 1, . . . , P . 2. Train the weights for the P tuples,
in the usual least- squares fashion, and test their performances. 3.
Order the tuples by performance (NRMSEs). The parameters of the P/2
worst performing tuples are overwritten by the parameters of the best
P/2 tuples. 4. Perturb the parameters in all tuples by adding a random
value drawn from a normal distribution to each one of them, i.e., for p
in (γ1,2; η1,2; a1,2)i, set p →N(μ = p, σ = εp). We chose ε = 0.1. 5.
Permutate the mask. This might help find a mask ordering that is better
suited for the task at hand. 6. Repeat from step two onwards, until
adequate perfor- mance or a fixed number of iterations is reached. 7.
Continue from the first step and onwards for the next RC in the cascade.
By testing many initial parameter tuples in step 1, we hope to find good
initial parameter values. The third step results in the formation of
clusters of parameter tuples, with increasingly better performance.
Usually, one such cluster Fig. 9 Evolution of the NRMSE for a cascade of
K = 5 RCs, each having N = 100 filtered virtual neurons and a dual
input, consisting of the global input and the previous RC output. The
parameters are opti- mized using the RO algorithm with P = 50 parameter
tuples. After every 20 iterations, the algorithm keeps the best
parameters, and pro- ceeds to the next RC in the cascade. Each of the
black dots corresponds to a single NRMSE. Shown for comparison are the
NRMSEs obtained for a single RC having N = 500 filtered 1 ⃝, and
unfiltered 2 ⃝neurons, a cascade of 5 single input RCs with N = 100
unfiltered neurons 3 ⃝, a cascade of 5 single input RCs with N = 100
filtered neurons 4 ⃝, and a cascade of 5 dual input RCs with N = 100
unfiltered neurons 5 ⃝. The lines indicate the best result per RC stage
“wins” in the end. In Fig. 9, we show the RO procedure at work for P =
50. Results 2: Cascaded Structure The target here is the distorted
signal presented in “Result 1b: Emulating a Marshall Valvestate 10
Practice amp” and shown in Fig. 3b. The samples have been processed by
the DC blocker, Eq. 19, with p = 0.98 first. We train on L = 80000
samples, and test on L = 100000 samples. Here, the cascade consists of K
= 5 RCs, each having N = 100 filtered virtual neurons. Each dot in Fig.
9 presents an NRMSE obtained during the RO algorithm. After every 20
iterations, we save the best parameters and RC estimate, and continue to
the next RC in the cascade. With each of the cascade stages, the
performance improves. We thus reach NRMSE = 0.106. For compari- son, we
show the best NRMSE we obtained for a single RC with N = 500 filtered
virtual neurons,2 marked by 1 ⃝and a black dotted line (NRMSE = 0.220).
Somewhat unex- pected, a single RC of N = 500 unfiltered3 virtual
neurons, marked by 2 ⃝and a magenta dotted line, has a compara- ble
performance at NRMSE = 0.218. To not overload the 2Also after 20
iterations of the RO algorithm, on P = 50 parameter tuples. 3This is
equivalent to setting the filter coefficients to b0(n) = 1 and b1(n) =
b2(n) = 0. Cogn Comput Fig. 10 Response of the guitar amplifier (blue)
and output of a cas- cade of K = 5 DTRCs (red), each having a dual input
and N = 100 filtered virtual neurons. The guitar signal is shown in
green figure, for these and the following results, we show only the best
obtained NRMSE per RC stage. The blue dash-dot line in Fig. 9, marked
with 3 ⃝, shows the NRMSE for a cascade of K = 5 single input RCs, each
having N = 100 unfiltered neurons. Here, all but the first RC receive
only the output of the previous RC, and not the global input. This
results in NRMSE = 0.180. The green dash-dot-dot line marked with 4
⃝shows the results for a cascade of single input RCs with filtered
neurons, resulting in NRMSE = 0.144. Finally, the red dashed line marked
with 5 ⃝shows the case of the cascade with dual input RCs, but with
unfiltered virtual neu- rons, yielding NRMSE = 0.126. It is clear from
Fig. 9 that from the second RC stage and onwards, the combination of the
dual input RCs with filtered virtual neurons outperforms all others. In
Fig. 10, we compare the response of the guitar ampli- fier (blue) to
that of the optimized RC cascade (red). The guitar signal is shown in
green. The (admittedly subjective) auditory experience now comes much
closer to the desired response than in our first attempt in “Result 1b:
Emulating a Marshall Valvestate 10 Practice amp”. Conclusions and
Outlook The discrete-time delay line reservoir computer with a sin- gle
nonlinear node is attractive for real-time software-based audio
applications, due to the efficiency and ease with which it can be
programmed. We applied the discrete-time delay line RC to the the task
of emulating in real-time the distortion of a guitar amplifier, which is
a rather challenging black-box system identifica- tion task. For
computational efficiency, we required that our solution operates from
the raw input samples, refrain- ing from any preprocessing. This led us
to a cascaded RC structure with filtered virtual neurons and dual
inputs. Here, each RC in the cascade receives not only the output pre-
pared by the previous RC, but also the global input, such that the
target response is successively approximated. The structure is highly
adaptable, scalable, and readily imple- mentable on a variety of
computing platforms. In spite of being more complex than a single DTRC,
we found that when programmed with sufficient care in C, this cascaded
structure with 5 DTRCs of 100 virtual neurons each was capable of
real-time audio processing at 44.1 kHz, on stan- dard computing
equipment. Note this cascaded structure is also applicable to white-box
amplifier modeling, where each stage of the target system is separately
measured and emulated by a different DTRC in the cascade. Since for a
cascade of K reservoir computers also K times as many parameters must be
optimized, a sim- ple parameter space scan would take prohibitively
long. We opted for a random optimization technique to find good
parameters within reasonable time. Recent theoretical results outlining
methods to estimate good γ and η val- ues beforehand may help to speed
up the optimization stage [34]. Another interesting path would be the
combination of a cascaded RC structure with the delay readout described
in Ref. [29]. Here, every neuron has its own separately optimized delay.
Programmatically, this is compatible with real-time operation. It is
easy to find two signals which have a high NRMSE between them, yet are
still auditorily indistinguishable [35]. This is because human hearing
is quite insensitive to rel- ative phases of the harmonic components of
a periodic or near periodic signal. This suggests that for audio-related
tasks, the training of the RC should be altered such that it makes use
of a measure that is grounded in psychoacous- tics, while the actual
emulation is still done starting from the raw audio samples. In the
literature, several psychoacoustic distance measures can be found [36,
37]. In this way, pos- sibly a better auditory solution can be found, at
the expense of a more involved training. We see this as a viable course
for future explorations. Another course of exploration is into possible
applications. We recently started to investigate babble-noise
suppression (also known as “the cocktail party effect”), as applicable
to hearing aids, with encouraging first results. In engine diagnostics,
the vibrational patterns in the audio frequency range may be analyzed to
give of early warning signs of failure. Another application is communi-
cation drop-out suppression, where the reservoir cascade is trained to
fill in short interrupts of a (voice) communication channel. It is clear
that the proposed cascaded RC structure with filtered virtual neurons
and dual inputs is considerably more complex to program than a single
RC. There are many design choices that influence the performance, and we
feel we have barely scratched the surface. Nevertheless, real- time
audio processing with delay line reservoir computing has been
demonstrated. Cogn Comput Compliance with Ethical Standards Funding LK
and GVDS were partly funded by the Interuniversi- tary Attraction Poles
Program “Photonics@be” of the Belgian Science Policy Office and by the
Science Foundation - Flanders (FWO). Conflict of Interest The authors
declare that they have no conflict of interest. Ethical Approval This
article does not contain any studies with human participants or animals
performed by any of the authors. References 1. J¨ ager H. The echo state
approach to analysing and training recur- rent neural networks.
Technical report, German National Research Center for Information
Technology. 2001. 2. Maass W, Natschl¨ ager T, Markram H. Real-time
computing without stable states: a new framework for neural computation
based on perturbations. Neural Comput. 2002;14(11):2531–2560. 3.
Verstraeten D, Schrauwen B, d’Haene M, Stroobandt D. An experimental
unification of reservoir computing methods. Neural Netw.
2007;20:391–403. 4. Appeltant L, Soriano M, Van der Sande G, Danckaert
J, Massar S, Dambre J, Schrauwen B, Mirasso CR, Fischer I. Information
processing using a single dynamical node as complex system. Nat Commun.
2011;2:468. 5. Br¨ unner D., Soriano M, Mirasso CR, Fischer I. Parallel
photonic information processing at gigabyte per second data rates using
transient states. Nat Commun. 2013;4:1364. 6. Duport F, Schneider B,
Smerieri A, Haelterman M, Massar S. All-optical reservoir computing. Opt
Exp. 2012;20:22,783– 22,795. 7. Soriano M, Ort´ ın S, Keuninckx L,
Appeltant L, Danckaert J, Pesquera L, der Sande GV. Delay based
reservoir computing: Noise effects in a combined analog and digital
implementation. IEEE TNNLS. 2015;26(2):388–393. 8. Larger L, Soriano M,
Br¨ unner D, Appeltant L, Gutierrez JM, Pesquera L, Mirasso CR, Fischer
I. Photonic information processing beyond Turing: an opto-electronic
implementation of reservoir computing. Opt Exp. 2012;20:3241–3249. 9.
Paquot Y, Duport F, Smerieri A, Dambre J, Schrauwen B, Hael- terman M,
Massar S. Opto-electronic reservoir computing. Sci Rep. 2012;2:287. 10.
Holzmann G. Reservoir computing: a powerful black-box frame- work for
nonlinear audio processing. In: Proceedings of the 12th International
Conference on Digital Audio Effects (DAFx-09), pp 90–97. 2009. 11.
Scardapane S, Uncini A. Semi-supervised echo state networks for audio
classification. Cognitive Computation, pp 1–11. 2016. 12. Liu X, Bao CC.
Audio bandwidth extension using ensemble of recurrent neural networks.
EURASIP J Audio Speech, Music Process. 2016;2016(1):12. 13. Triefenbach
F, Jalalvand A, Demuynck K, Martens JP. Acous- tic modeling with
hierarchical reservoirs. IEEE Transactions on Audio. Speech Lang
Process. 2013;21(11):2439–2450. 14. Grigoryeva L, Henriques J, Larger L,
Ortega JP. Stochastic time series forecasting using time-delay
reservoirs. Neural Netw. 2014;55:59–71. 15. Furse R, et al. LADSPA:
Linux audio developers simple plugin API (2000). prefix
http://www.ladspa.org/http://www.ladspa.org/. [Online; accessed
13-January-2016]. 16. Schumacher J, Haslinger R, Pipa G. Statistical
modelling approach for detecting generalized synchronization. Phys Rev
E. 85 5 Pt 2 (2012). doi:10.1103/PhysRevE.85.056215. 17. Keen RG. A
musical distortion primer. http://www.geofex.com/ effxfaq/distn101.htm.
2000. [Online; accessed 28-August-2016]. 18. Tretter SA. Communication
system design using DSP algorithms, 1st ed.: Springer; 2008. 19.
Pakarinen J, Yeh DT. A review of digital techniques for modelling
vacuum-tube guitar amplifiers. Comput Music J. 2009;33(2):85– 100. 20.
Yeh DT. Digital implementation of musical distorcion circuits by
analysis and simulation. Ph.D. thesis, Helsinki University of
Technology. 2009. 21. Acustica Nebula Series Effects Processors.
http://www.acustica- audio.com. [Online; accessed 1-December-2015]. 22.
Kemper profiling amplifiers. http://www.kemper-amp.com (2015). [Online;
accessed 5-October-2015]. 23. Kemp MJ. Audio effects synthesizer with or
without analyzer, u.s. patent no. 7,039,194 b1.
http://www.google.com/patents/ US7039194. [Filed Aug. 8, 1997 issued May
2, 2006]. 24. Appeltant L. Reservoir computing based on delay dynamical
sys- tems. Ph.D. thesis, Vrije Universiteit Brussel (VUB) Universitat de
les Illes Balears. 2012. 25. Mackey MC, Glass L. Oscillation and chaos
in physiological control systems. Science. 1977;197(4300):287–289. 26.
Bencina R, et al. Portaudio—an open-source cross-platform audio API.
http://www.portaudio.com. 2011. 27. Jones E, Oliphant T, Peterson P, et
al. SciPy: Open source scientific tools for Python.
http://www.scipy.org. 2001. [Online; accessed 2015-11-30]. 28. Knapp C,
Carter G. Generalized correlation methods for estima- tion of time
delay. IEEE TASSP. 1976;24(4):320–327. 29. Holzmann G. Echo state
networks with filter neurons and a delay and sum readout with
applications in audio signal processing. Master’s thesis, Institute for
Theoretical Computer Science, TU Graz. 2008.
http://grh.mur.at/misc/MasterThesis.pdf. 30. Hauser H. Echo state
networks with filter neurons and a delay and sum readout. Neural Netw.
2009;23(2):244–256. 31. III JOS. Introduction to digital filter theory
with audio applica- tions: W3K Publishing; 2007. 32. Yates R, Lyons R.
DSP tips and tricks: DC blocker algorithms. IEEE Sig Proc Mag.
2008;25(2):132–134. 33. Matyas J. Random optimization. Autom Remote
Control. 1965;26(2):246–253. 34. Ortega JP. Time-delay reservoir
computers: nonlinear stability of functional differential systems and
optimal nonlinear infor- mation processing capacity.
http://trimestres-lmb.univ-fcomte.fr/ Speakers-and-presentations.html.
Besanc ¸on, France: Workshop on dynamical systems and brain-inspired
information processing; 2015. 35. Smith SW. The Scientist and Engineer’s
Guide to Digital Sig- nal Processing, 1st ed. San Diego, CA: California
Technical Publishing; 1997. 36. Lennes M, Lehtokoski A, Alku P, N¨ a¨
at¨ anen R. Acoustic, psy- choacoustic and psychophysiological measures
of distance in the Finnishvowelspace.In: ProceedingsICPhS99,
pp. 2465–2468. 1999. 37. Logan B, Salomon A. A music similarity function
based on signal analysis. In: Proceedings International Symposium.
Tokyo, Japan: Music Information Retrieval; 2001.
