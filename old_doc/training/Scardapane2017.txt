Cogn Comput (2017) 9:295–296 DOI 10.1007/s12559-017-9469-1 Advances in
Biologically Inspired Reservoir Computing Simone Scardapane1 · John B.
Butcher2 · Filippo M. Bianchi3 · Zeeshan K. Malik4 Received: 4 April
2017 / Accepted: 19 April 2017 / Published online: 28 April 2017 ©
Springer Science+Business Media New York 2017 The interplay between
randomness and optimization has always been a major theme in the design
of neural networks [3]. In the last 15 years, the success of reservoir
computing (RC) showed that, in many scenarios, the algebraic struc- ture
of the recurrent component is far more important than the precise
fine-tuning of its weights. As long as the recur- rent part of the
network possesses a form of fading memory of the input, the dynamics of
the neurons are enough to efficiently process many spatio-temporal
signals, provided that their activations are sufficiently heterogeneous.
Even if today it is feasible to fully optimize deep recurrent net-
works, their implementation still requires a vast degree of experience
and practice, not to mention vast computational resources, limiting
their applicability in simpler architec- tures (e.g., embedded systems)
or in areas where time is of key importance (e.g., online systems). Not
surprisingly, then, RC remains a powerful tool for quickly solving 
Simone Scardapane simone.scardapane@uniroma1.it John B. Butcher
j.b.butcher@keele.ac.uk Filippo M. Bianchi filippo.m.bianchi@uit.no
Zeeshan K. Malik zkm@cs.stir.ac.uk 1 Sapienza University of Rome, Rome,
Italy 2 Keele University, Keele, UK 3 University of Tromsø, Tromsø,
Norway 4 University of Stirling, Stirling, UK dynamical problems, and it
has become an invaluable tool for modeling and analysis in neuroscience.
Ten years after the last special issue entirely dedicated to the topic
[2], this issue aims at providing an up-to-date overview on (some of)
the latest developments in the field. Recently, Goudarzi and Teuscher
listed a series of 11 ques- tions that will drive research in RC from
here forward [1]. Although we cannot cover all of them in a single
issue, many of these questions are addressed in the articles that com-
pose the issue, which we believe provides a good overview on the
diversity and the vitality of the field. Overall, we hope the issue to
be of interest to the readers of Cognitive Computation. In particular,
we selected ten papers to appear in this special issue. All of them have
gone through at least two rounds of revision by two to four expert
reviewers. One paper, coauthored by one of the guest editors, underwent
an independent review process to guarantee fairness. The articles are
logically organized in three separate parts. The first third of the
issue is dedicated to the study of delay-line architectures, which have
recently been inspired by the pos- sibility of implementation on
non-conventional computing architectures, most notably photonic
computers. The sec- ond part of the issue investigates some theoretical
aspects of RC models, and the third part is devoted to innovative
formulations for designing architectures for learning and recognition
tasks. The first four papers of the special issue are dedicated to
photonic RC and time-delay architectures: – In ‘Online training for
high-performance analogue readout layers in photonic reservoir
computers’, Antonik et al. propose the use of online training algorithms
when exploiting analogue readouts in photonic RC. Their simulated
experiments show that online algorithms can 296 Cogn Comput (2017)
9:295–296 be beneficial to the task, particularly thanks to the
possibility of including nonlinearities in the readout. – In ‘A
Multiple-Input Strategy to Efficient Integrated Photonic Reservoir
Computing,’ Katumba et al. explore the design of optical reservoirs (in
terms of performance and power efficiency), whenever the input is fed to
more than a single node. Their results further extend the applicability
of such architectures. – In ‘Real-time audio processing with a cascade
of discrete-time delay line based reservoir computers,’ Keuninckx et
al. present a compelling use case for delay-line RC models for tasks of
real-time audio pro- cessing. Specifically, their experiments on guitar
ampli- fier distortion show the technique to be viable even in today’s
computing hardware. – Finally, ‘Reservoir computing with an ensemble of
time- delay reservoirs,’ by Ort´ ın and Pesquera, investigates ways of
building ensemble models with time-delay reservoirs, based on both
decoupled neurons and neu- rons coupled by feedback. These architectures
are able to boost the processing speed of the models with respect to the
standard RC approach. The second part of the issue is dedicated to
theoretical models of RC: – In ‘Echo State Property of Deep Reservoir
Computing Networks,’ Gallicchio and Micheli provide necessary and
sufficient conditions for the echo state property of multi-layered RC
modules. They show that stack- ing multiple layers of fixed recurrent
neurons drives the network to less stable regimes, but the resulting
mod- els are able to analyze a signal at different temporal resolutions.
– Nikiforou et al., in ‘An investigation of the dynami- cal transitions
in harmonically driven random networks of firing-rate neurons,’ study
the behavior of continu- ous recurrent networks when subjected to
harmonically oscillating stimuli. Their findings shed light on the
dynamics and structure of this class of neural networks. Finally, the
third part of the issue is devoted to the design of echo state networks
(ESNs): – In ‘Training Echo State Networks with Regulariza- tion Through
Dimensionality Reduction,’ Løkse et al. train ESNs by first projecting
the internal state to a space of lower dimensionality. Superior
predictive per- formance is then validated via the use of sophisticated
visualization tools from chaos theory. – Mayer and Yu, in ‘Orthogonal
Echo State Networks and stochastic evaluations of likelihoods,’ use ESNs
with orthogonal weight matrices to build probabilistic likelihood
estimates of time series. They analyze and evaluate several parameters
influencing the behavior of such networks. – Wootton et al., in
‘Optimizing Echo State Networks for Static Pattern Recognition,’ put
forth the idea of apply- ing ESNs for static tasks, by letting them run
until they reach stable outputs. These ESNs outperformed other machine
learning techniques, even when they were still in an unstable state. –
Finally, in ‘Reservoir computing with both neuronal intrinsic plasticity
and multi-clustered structure,’ Xue et al. combine two previously
proposed ways to enhance ESNs, namely multi-clustered reservoirs and
intrinsic plasticity. The results show that the two techniques are both
beneficial to the performance of the models. We would like to thank the
editor in chief of the journal, Amir Hussain, for the strong support
given when orga- nizing and publishing this special issue; all the
authors who participated in the issue; and the anonymous review- ers who
helped us evaluated and improve the quality of the submissions (listed
in purely alphabetical order): Piotr Antonik, Dorian Aur, Omri Barak,
Daniel Brunner, Yanne Chembo, Danilo Comminiello, Claudio Gallicchio,
Lorenzo Livi, Sigurd Løkse, Ali Rodan, Manuel Roveri, Miguel Soriano,
David Sussillo, Guy Van Der Sande, Thomas Van Vaerenbergh, and Adam
Wootton. References 1. Goudarzi A, Teuscher C. Reservoir computing: Quo
vadis?. Pro- ceedings of the 3rd ACM International Conference on
Nanoscale Computing and Communication. ACM; 2016. p. 13. 2. Jaeger H,
Maass W, Principe J. Editorial: Special issue on echo state networks and
liquid state machines. Neural Netw. 2007;20(3):287–289. 3. Scardapane S,
Wang D. Randomness in neural networks: an overview Wiley
Interdisciplinary Reviews. Data Min Knowl Disc. 2017;7(2):
