arXiv:1908.05202v2 [nlin.CD] 18 May 2020 EMBEDDING AND APPROXIMATION
THEOREMS FOR ECHO STATE NETWORKS NEURAL NETWORKS Allen Hart Department
of Mathematical Sciences University of Bath Bath BA2 7AY, UK
a.hart@bath.ac.uk James Hook Department of Mathematical Sciences
University of Bath Bath BA2 7AY, UK j.l.hook@bath.ac.uk Jonathan Dawes
Department of Mathematical Sciences University of Bath Bath BA2 7AY, UK
j.h.p.dawes@bath.ac.uk May 19, 2020 ABSTRACT Echo State Networks (ESNs)
are a class of single-layer recurrent neural networks that have enjoyed
recent attention. In this paper we prove that a suitable ESN, trained on
a series of measurements of an invertible dynamical system, induces a C1
map from the dynamical system’s phase space to the ESN’s reservoir
space. We call this the Echo State Map. We then prove that the Echo
State Map is generically an embedding with positive probability. Under
additional mild assumptions, we further conjecture that the Echo State
Map is almost surely an embedding. For sufﬁciently large, and specially
structured, but still randomly generated ESNs, we prove that there
exists a linear readout layer that allows the ESN to predict the next
observation of a dynamical system arbitrarily well. Consequently, if the
dynamical system under observation is structurally stable then the
trained ESN will exhibit dynamics that are topologically conjugate to
the future behaviour of the observed dynamical system. Our theoretical
results connect the theory of ESNs to the delay-embedding literature for
dynami- cal systems, and are supported by numerical evidence from
simulations of the traditional Lorenz equations. The simulations conﬁrm
that, from a one dimensional observation function, an ESN can accurately
infer a range of geometric and topological features of the dynamics such
as the eigenval- ues of equilibrium points, Lyapunov exponents and
homology groups. Keywords Reservoir computing; liquid state machine;
time series analysis; Lorenz equations; dynamical system; delay embed-
ding; Persistent Homology; recurrent neural networks. 1 Introduction An
Echo State Network (ESN) is a single-layer recurrent neural network
composed of a trainable readout layer con- nected to a reservoir of
randomly initialized, and randomly coupled, untrainable ‘neurons’. This
architecture has been investigated and used by many authors since the
seminal papers by Jaeger (2001) and Maass et al. (2002). Tanaka et al.
(2019) present a review of ESNs, among other recurrent neural network
models, under the umbrella term reservoir computing. The wide range of
problems to which the ESN framework has been applied include speech
recognition (Skowronski & Harris 2007), learning grammatical structure
(Tong et al. 2007), and ﬁnancial time series prediction (Ilies et
al. 2007), (Lin et al. 2009). Several authors including Gürel & Egert
(2010) have also discussed how the ESN is a plausible model for the
information processing performed by biological neurons. Most
ambitiously, Plöger et al. (2004) discuss ESNs in the context of
building by 2050, a team of fully autonomous humanoid robots to beat the
human winning team of the FIFA Soccer World Cup. The ESN has associated
to it a reservoir state denoted rk ∈Rn at time k. The structure of the
recurrent layer is described by an n × n matrix A that is the weighted
adjacency matrix of the system of n ‘neurons’. If neuron i is not
connected to neuron j then Aij = 0, and if they are connected with some
weight a ∈R then Aij = a. Connections need not be symmetric, so in
general Aij ̸= Aji. Typically, A is sparse and has approximately 1% of
its entries non-zero. Connection weights are usually i.i.d. random
variables, and typically are chosen to be either uniformly distributed
on a ﬁxed interval, or Gaussian. The ESN also contains an (n × m) input
matrix W in, where m is the dimension of the training data. Like the
reservoir A, W in is also populated with i.i.d random variables.
Finally, the ESN has an activation function ϕ : Rn →Rn, for which there
are several standard choices, for example tanh (performed
component-wise). The operation of the ESN is divided into two phases: an
initial training phase, followed by an autonomous phase. During the
training phase, the ESN is trained on a given input time series denoted
by vectors u0, u1, u2 … uK each in Rm. We will assume in this paper that
the input sequence is bounded, though we note the recent work of
Grigoryeva & Ortega (2019) establishes a framework that encompasses
unbounded input sequences as well. We will also assume in this paper
that m = 1, so that we consider a scalar input time series. The
reservoir state at time k is deﬁned by choosing an initial state e.g. r1
= (0, 0, …, 0)⊤and deﬁning subsequent states recursively by rk+1 = ϕ(Ark
+ W inuk). Having computed the the new reservoir states r1, r2…rK, the
output matrix W out is ﬁtted to solve the optimisation problem min W out
K X k=1 ∥W outrk −ak∥2 + λ∥W out∥2 2, where ak is some known target
sequence we want the ESN to mimic, often taken to be equal to the input
sequence uk, and λ > 0 is a regularisation parameter. Minimisation
problems of this kind are often referred to as ridge regression, or
Tikhonov, or L2 regularisation. Having trained the output matrix W out
the reservoir states sk can then be liberated from their reliance on the
driving input uk and evolve under the autonomous dynamical system deﬁned
by vk+1 = W outsk, sk+1 = ϕ(Ask + W invk+1), where s0 = rK. If the
training has been successful, then the trained ESN should provide good
predictions of the future time series v1 ≈uK+1, v2 ≈uK+2, etc, and
future evolution of the reservoir state s1 ≈rK+1, s2 ≈rK+2 etc. The
viewpoint we take here clearly distinguishes between the training phase
of the ESN where it is an externally-driven dynamical system, and the
‘test’ phase where we consider it as an autonomous dynamical system in
Rn. In complete generality the process deﬁning uk could be anything,
including a realisation of a random process. However, importantly,
throughout this paper, we will restrict our attention and assume that
u0, u1, u2, . . . are one dimensional observations of an invertible
discrete-time dynamical system with evolution operator φ ∈Diff1(M)
observed via a function ω ∈C1(M, R) on a compact manifold M. In
particular u0 = ω(x), u1 = ω ◦φ(x), u2 = ω ◦φ2(x), u3 = ω ◦φ3(x), etc.
The model we have in mind is that φ is the evolution operator for a time
∆t of a set of Lipschitz ordinary differential equations on M.
Illustrations of the training and autonomous phases are shown in Figure
1. The idea to draw training data from a dynamical system was by Jaeger
& Haas (2004) who drew observations from a trajectory through the
Mackey-Glass attractor. We were attracted to the idea by a recent paper
by Pathak et al. (2017) who trained ESNs on the Lorenz equations and the
Kuramoto–Sivashinsky equation (in one spatial dimension). In 2 (a)
Training phase W in ω A (b) Autonomous phase W in A W out Figure 1: (a)
During the training phase the ESN observes a dynamical system via the
function ω ∈C1(M, R); this sequence of observations is distributed into
the nodes in the reservoir r by the linear map W in. (b) After training,
in the autonomous phase, the driving is replaced by the output created
by the best-ﬁt linear map W out. These images were produced using the
TikZ-network package developed by Hackl (2018). 3 particular, we
conjecture that under the right technical conditions an ESN with random
reservoir matrix and input matrix trained on a one dimensional
observation of a dynamical system will embed the system into the
reservoir space almost surely. We call this the ESN Embedding Conjecture
(Conjecture 2.3.4). We believe this conjecture is true as a consequence
of Takens (1981) theorem stating that a generic delay observation map is
an embedding. This connection between Takens’ delay embedding theorem
and the ESN was remarked on by Jaeger (2001) and has been discussed in
several later works including by Xi et al. (2005), Schrauwen et
al. (2007), Shi & Han (2007), Yong Song et al. (2010), Løkse et
al. (2017), Yeo (2019), and Vlachas et al. (2019). We go on to prove
that our statement of the ESN Embedding Conjecture holds with
probability α > 0. We ﬁnally prove that when the ESN does successfully
embed a structurally stable dynamical system into its reservoir, there
exists a trainable readout layer such that the autonomous phase of the
ESN will adopt the topology of the driving dynamical system. We call
this the ESN Approximation Theorem (Theorem 2.4.13). This theorem
complements the results of Grigoryeva & Ortega (2018) and Gonon et
al. (2020) stating that the ESN (with tunable and randomly initialised A
and b respectively) is a universal approximator of discrete-time fading
memory ﬁlters. To demonstrate the theory we present numerical evidence
that an ESN trained on a numerically integrated trajectory of the Lorenz
system can replicate several of the Lorenz system’s geometric and
topological properties. In particular, we computed the Lyapunov
exponents of the ESN autonomous phase and compared them to the known
exponents of the Lorenz system. We also compared the eigenvalues of the
system linearisation on the Lorenz system’s ﬁxed points to the
eigenvalues of the linearisation on the ﬁxed points belonging to the ESN
autonomous phase. Finally we compared the homology of the driven and
autonomous reservoir attractors to the Lorenz attractor using persistent
homology. For the reader unfamiliar with persistent homology Ghrist
(2008) offers an excellent primer. The remainder of the paper is set out
as follows. In section 2 we present basic deﬁnitions and deﬁne a family
of maps that captures the effect on the reservoir state of training with
increasing amounts of data. In section 2.2 we prove that the family
converges to a C1 map that we call the Echo State Map. We conjecture in
section 2.3 that generically the Echo State Map is an embedding. In
section 2.4 we prove an ESN Approximation Theorem that guarantees that
the autonomous dynamics of the ESN is (in a suitable sense) conjugate
via a diffeomorphism to the original dynamical system on which the ESN
was trained. in section 3 we present numerical results supporting the
theory. 2 Theory of ESNs Our analysis makes use of several different
norms. In particular, if x ∈Rm is a vector then ∥x∥is the Euclidean
norm, and for A a matrix then ∥A∥2 is the matrix 2 norm. If f is a real
valued function, then ∥f∥∞will denote the supremum norm and if f is
continuously differentiable then we will use the C1 norm ∥f∥C1 deﬁned by
∥f∥C1 := ∥f∥∞+ ∥Df∥∞ where D is the derivative operator. 2.1 The Echo
State Network We begin our summary of the background to Echo State
Networks (ESNs) with a deﬁnition. Deﬁnition 2.1.1. (Echo State Network)
Let the activation function σ be a function σ ∈C1(R, (−1, 1)) that has
its derivative take values in the range (0, 1). Let n ∈N, A be a real n
× n matrix, and W in a real n × 1 matrix. Let bi ∈R ∀i ∈{1, …, n}. Let
In := [−1, 1]n and deﬁne the function ϕ : Rn →In component-wise by ϕi(r)
= σ(ri + bi) ∀i ∈{1, …, n}. (1) We then deﬁne an Echo State Network
(ESN) of size n to be the triple (ϕ, A, W in). σ is often chosen to be
the hyperbolic function tanh, though other choices of activation
function abound in the machine learning literature. The conditions on
these functions are sometimes less restrictive than those imposed above
on σ; other common choices of activation function include the linear
unit (also known as the identity map) and the rectiﬁed linear unit
(often abbreviated relu) deﬁned by relu(ri) = ri if ri > 0 0 otherwise.
Glorot et al. (2011) discuss how Recurrent Neural Networks supported by
a relu activation function are less prone to the vanishing gradient
problem than sigmoidal activation functions. More exotic activation
functions include radial basis functions, which take the shape of bell
curves. Throughout this paper however, we will restrict ourselves to
activation functions as deﬁned above, i.e. functions σ ∈C1(R, (−1, 1))
who’s derivatives take values in (0, 1). 4 2.2 The Echo State Map We
will begin by introducing a family of functions that describe the
mapping between this time series of observations and the reservoir
state; this will be of fundamental importance throughout the remainder
of the paper. Deﬁnition 2.2.1. (Echo State Family) Let M be a compact
m-manifold and n ∈N. Let A be an n × n, and let W in an n × 1 matrix:
let the triple (ϕ, A, W in) be an ESN. Let the discrete dynamical system
be φ ∈Diff1(M) and let the observation function ω ∈C1(M, R). Let the
family of functions F = {f r0 k : M →In : r0 ∈In, k ∈N0} be deﬁned as
follows: f r0 0 (x) = r0 f r0 k+1(x) = ϕ(Af r0 k ◦φ−1(x) + W inω(x)). We
call the set of functions F the Echo State Family. To provide some
intuition as to where this family came from, we observe that f r0 k is
the function that takes a point x ∈M and ﬁrst applies the inverse
evolution operator k times, yielding the past state φ−k(x) of the
dynamical system. A list of k + 1 observations ω ◦φ−k(x), ω ◦φ1−k(x), ω
◦φ2−k(x), … are then obtained, in sequence, forward from this point. An
ESN with initial reservoir state r0 is trained on this list of inputs,
and its reservoir state is then exactly given by the value of f r0 k
(x). The function f r0 k is therefore the map induced by k + 1 steps of
the training phase of the ESN, i.e. it sends a point x ∈M to reservoir
space In according to its one dimensional history. Our plan for the
upcoming section is to show that for any r0 ∈In f r0 := lim k→∞f r0 k
exists, and that f r0 = f s0 =: f for any r0, s0 ∈In. We will call f the
Echo State Map, and show it is continuously differentiable, i.e. f
∈C1(M, Rn). These results will appear together and called the Echo State
Mapping Theorem. Equivalently, we could say the Echo State Map f is the
unique C1 generalized synchronisation (in the sense described by Kocarev
& Parlitz (1996) ) between a pair of unidirectionally coupled systems,
the dynamics given by φ and the driven ESN phase. We will further
conjecture that f is a C1 embedding almost surely, and therefore (almost
surely) it is a topology- preserving map from the manifold M to the
reservoir space In. We will call this the ESN Embedding Conjecture, and
go on to prove a partial result that f is a C1 embedding with positive
probability. Theorem 2.2.2. (Echo State Mapping Theorem) With the
notation and hypotheses of Deﬁnition 2.2.1, and the further assumption
that ∥A∥2 < min(1, 1/∥Dφ−1∥∞), there exists a unique solution f ∈C1(M,
Rn) of the equation f = ϕ(Af ◦φ−1 + W inω) such that for all r0 ∈In the
sequence f r0 k converges in the C1 topology to f as k →∞. We call f the
Echo State Map. Proof. Let ˜ Ψ : C1(M, Rn) →C1(M, Rn) be deﬁned by ˜
Ψ(f) = ϕ(Af ◦φ−1 + W inω) then we can see that f r0 k = ˜ Ψ(f r0 k−1) =
˜ Ψk(f r0 0 ). Now, we will show that ˜ Ψ is a contraction mapping and
therefore has a unique ﬁxed point f ∈C1(M, Rn) by the contraction
mapping theorem (Banach 1922). This will complete the proof. ∥˜ Ψ(f) −˜
Ψ(g)∥C1 = ∥ϕ(Af ◦φ−1 + W inω) −ϕ(Ag ◦φ−1 + W inω)∥C1 ≤ ∥Af ◦φ−1 + W inω
−Af ◦φ−1 −W inω∥C1 because ϕ is contracting in C1 = ∥A(f ◦φ−1 −g
◦φ−1)∥C1 ≤ ∥A∥2∥f ◦φ−1 −g ◦φ−1∥C1 = ∥A∥2(∥f ◦φ−1 −g ◦φ−1∥∞+ ∥Df ◦φ−1Dφ−1
−Dg ◦φ−1Dφ−1∥∞) ≤ ∥A∥2(∥f ◦φ−1 −g ◦φ−1∥∞+ ∥Dφ−1∥∞∥Df ◦φ−1 −Dg ◦φ−1∥∞) ≤
∥A∥2 max(1, ∥Dφ−1∥∞)∥f −g∥C1 and ∥A∥2 max(1, ∥Dφ−1∥∞) < 1, so we have
that ˜ Ψ is contracting. 5 We remark here that if φ is obtained by the
discretisation of a continuous time ﬂow with a small time step, the
evolution operator φ is close to the identity map, so ∥Dφ−1∥∞is close to
1. Consequently, the condition ∥A∥2 < min(1, 1/∥Dφ−1∥∞) is not much more
restrictive than enforcing ∥A∥2 < 1. 2.3 The ESN Embedding Theorem In
this section we will discuss the conditions under which the Echo State
Map f ∈C1(M, Rn) is a C1 embedding (i.e. an injective immersion whose
domain and image are diffeomorphic). We will also conjecture that for a
generic observation function ω and random matrices A and W in, the Echo
State Map f is a C1 embedding almost surely. To set the scene for these
results, we ﬁrst recall Whitney’s Weak Embedding Theorem and Takens’
Theorem for delay observation maps. Theorem 2.3.1. (Whitney’s Weak
Embedding Theorem) Let M be a compact m-manifold and choose n ∈N such
that n > 2m. Then the set of Cr embeddings is generic in Cr(M, Rn) with
respect to the Whitney C1 topology (This is the topology on C1(M, Rn)
induced by the C1-norm). Proof. Whitney (1944). Corollary 2.3.2. Let M
be a compact m-manifold and n ∈N such that n > 2m. Let A be an n × n
matrix for which ∥A∥2 < min(1/∥Dφ−1∥∞, 1). Let W in be an n × 1 matrix,
and let the triple (ϕ, A, W in) be an ESN. As usual, let φ ∈Diff1(M),
and ω ∈C1(M, R). If n > 2m, then the ESM f ∈C1(M, Rn) is a limit point
in the Whitney C1 topology of the set of C1 embeddings. Proof. f ∈C1(M,
Rn) by Theorem 2.2.2 so, by the Weak Whitney Embedding Theorem (Theorem
2.3.1), f is a limit point of the C1 embeddings with respect to the
Whitney C1 topology. From Corollary 2.3.2 it is clear that the Echo
State Map f is always close to an embedding, but this says nothing about
necessary or sufﬁcient conditions for f to actually be an embedding. In
fact f may never actually be an embedding. That said, since embeddings
are generic in the space C1(M, Rn) we expect heuristically that a
function in C1(M, Rn) that is assembled without explicitly desiring that
it is not an embedding, is overwhelmingly likely actually to be an
embedding. This suggests (heuristically) that a generic Echo State Map f
is indeed an embedding. The ﬁrst step we take toward proving this is to
introduce Takens’ Theorem. Theorem 2.3.3. (Huke’s Formulation of Takens’
Theorem) Let M be a compact manifold of dimension m. Suppose φ ∈Diff2(M)
has the following two properties: (1) φ has only ﬁnitely many periodic
points with periods less than or equal to 2m. (2) If x ∈M is any
periodic point with period k < 2m then the eigenvalues of the derivative
Dφk at x are distinct. Then for a generic C2 observation function ω
∈C2(M, R) the (2m+1) delay observation map Φ(φ,ω) : M →R2m+1 deﬁned by
Φ(φ,ω)(x) := (ω(x), ω ◦φ(x), ω ◦φ2(x), …, ω ◦φ2m(x)) is a C1 embedding.
Proof. Huke (2006). Huke’s proof that Φ(φ,ω) is a C1 embedding for
generic ω comprises two steps. First, he shows that Φ(φ,ω) is a C1
embedding for an open subset of C2 observation functions, and second, he
shows that Φ(φ,ω) is an embedding for a dense subset of all C2
observation functions. The ﬁrst step (to prove openness) is fairly
simple while the second (the proof of density) is long and delicate. A
brief summary of the density part of the proof is as follows. An
arbitrary C2 observation function ω is carefully perturbed on each open
set in a cover of the manifold M such that ω becomes immersive on each
set. The observation function ω is then perturbed again on each open set
in the cover in order to make ω injective, with care taken to ensure ω
remains immersive on each open set. This procedure is applied separately
to open sets which contain periodic points and open sets that do not. We
believe it is possible to build on this result and modify the proof of
Huke’s Theorem in order to prove an ESN Embedding Conjecture in the form
that we now state. 6 Conjecture 2.3.4. (ESN Embedding Conjecture) Let M
be a compact m-manifold and n ∈N such that n > 2m. Let A be an n × n
matrix with ∥A∥2 < min(1/∥Dφ−1∥∞, 1), and W in a n × 1 matrix, and let
the triple (ϕ, A, W in) be an ESN. Let ω ∈C2(M, R) and let φ ∈Diff2(M)
(and possibly requiring additional properties), and let A, W in be
generic matrices in the topology induced by the matrix 2-norm. Then the
Echo State Map f ∈C1(M, Rn) is a C1 embedding. We now summarise our
partial success towards proving this conjecture. In particular we can
establish the properties analogous to the ﬁrst part of Huke’s proof of
Takens’ Theorem: we will show that the set of triples (A, W in, ω) of
reservoir matrix, input matrix, and observation function for which f is
a C1 embedding, is open and non-empty. Consequently, for a generic
observation function ω, and matrices A and W in drawn from a
distribution with full support (if the pdf is well deﬁned, it is greater
than 0 over its domain), f is a C1 embedding with probability α > 0. To
prove the full ESN Embedding Conjecture, all that remains is to show
that the triples (A, W in, ω) for which f is an embedding are dense in
the space of admissible triples, but this is no easy task, so we will be
satisﬁed here with the proof of only openness and non-emptiness. Lemma
2.3.5. Let M be a compact m-manifold and n ∈N. Let A be an n × n matrix,
and suppose that ∥A∥2 < min(1/∥Dφ−1∥∞, 1). As usual let W in a n × 1
matrix, let the triple (ϕ, A, W in) be an ESN, φ ∈Diff1(M) and ω ∈C1(M,
R). Deﬁne the set Ω:= {(A, W in, ω) | fA,W in,ω is a C1 embedding.}.
Then the set Ωis open in the C1 topology. Proof. First we deﬁne the map
Ψ that associates the ESM f to the triple (A, W in, ω); let Ψ : (A, W
in, ω) → C1(M, Rn) be deﬁned by Ψ(A, W in, ω) = fA,W in,ω. We now argue
as follows. Since C1 embeddings form an open subset of C1(M, R), and the
inverse image of a continuous map is open, it sufﬁces to show that Ψ is
continuous in order to then conclude that Ωis open. To show continuity
of Ψ we must prove that if (An, W in n , ωn)n∈N →(A, W in, ω) then
∥Ψ(An, W in n , ωn) −Ψ(A, W in, ω)∥C1 →0. To lighten the notation we
will write f for fA,W in,ω and fn for fAn,W in n,ωn. As a preliminary
result we estimate as follows: ∥Anfn ◦φ−1 −Af ◦φ−1∥C1 = ∥Anfn ◦φ−1 −Af
◦φ−1∥∞ + ∥AnDfn ◦φ−1Dφ−1 −ADf ◦φ−1Dφ−1∥∞ (2) by deﬁnition of the C1 norm
≤∥Anfn ◦φ−1 −Af ◦φ−1∥∞+ ∥Dφ−1∥∞∥AnDfn ◦φ−1 −ADf ◦φ−1∥∞ ≤∥Anfn −Af∥∞+
∥Dφ−1∥∞∥AnDfn −ADf∥∞ ≤max(1, ∥Dφ−1∥∞)(∥Anfn −Af∥∞+ ∥AnDfn −ADf∥∞)
≤max(1, ∥Dφ−1∥∞)∥Anfn −Af∥C1 = τ∥Anfn −Af∥C1 (3) where we have deﬁned τ
:= max(1, ∥Dφ−1∥∞). We will prove one more preliminary result: that
∥fn∥C1 is bounded. We can see that ∥fn∥∞is bounded by boundedness of ϕ
so all that remains is to bound ∥Dfn∥∞. Since fn = ϕ(Anfn ◦φ−1 + W in n
ωn) we compute directly that Dfn = Dϕ(Anfn ◦φ−1W in n ωn)(AnDfn ◦φ−1Dφ−1
+ W in n Dωn) from which we can estimate that ∥Dfn∥∞ = ∥Dϕ(Anfn ◦φ−1W in
n ωn)(AnDfn ◦φ−1Dφ−1 + W in n Dωn)∥∞ ≤ ∥AnDfn ◦φ−1Dφ−1 + W in n Dωn∥∞ ≤
∥A∥2∥Dfn ◦φ−1∥∞∥Dφ−1∥∞+ ∥W in n ωn∥∞ < ¯ ρ∥Dfn ◦φ−1∥∞∥Dφ−1∥∞+ ∥W in n
Dωn∥∞where ¯ ρ = sup n∈N ∥An∥2 < 1 = ¯ ρ∥Dfn∥∞∥Dφ−1∥∞+ ∥W in n Dωn∥∞ < ¯
ρ∥Dfn∥∞∥Dφ−1∥∞+ ν 7 where ν is a bound for the sequence ∥W in n Dωn∥∞,
which we know exists because ∥W in n Dωn∥∞converges. Now upon
rearrangement ∥Dfn∥∞< ν 1 −¯ ρ∥Dφ−1∥∞ , hence we have bounded ∥Dfn∥∞and
∥fn∥∞thus we have a bound for ∥fn∥C1, which we will call µ. Now, for all
ǫ > 0 there exists n ∈N such that both ∥An −A∥2 < ǫ(1 −τ∥A∥2) 2τµ (4)
and ∥W in n ωn −W inω∥C1 < ǫ(1 −τ∥A∥2) 2 . (5) Armed with these
estimates we can now compute that ∥fn −f∥C1 = ∥ϕ(Anfn ◦φ−1 + W in n ωn)
−ϕ(Af ◦φ−1 + W inω)∥C1 by Theorem 2.2.2 ≤ ∥Anfn ◦φ−1 + W in n ωn −Af
◦φ−1 −W inω∥C1 because ϕ is contracting ≤ ∥Anfn ◦φ−1 −Af ◦φ−1 + W in n
ωn −W inω∥C1 ≤ ∥Anfn ◦φ−1 −Af ◦φ−1∥C1 + ∥W in n ωn −W inω∥C1 ≤ τ∥Anfn
−Af∥C1 + ∥W in n ωn −W inω∥C1 by equations (2)-(3) ≤ τ∥Anfn −Afn + Afn
−Af∥C1 + ∥W in n ωn −W inω∥C1 ≤ τ(∥Afn −Af∥C1 + ∥Anfn −Afn∥C1) + ∥W in n
ωn −W inω∥C1 ≤ τ∥A∥2∥fn −f∥C1 + τ∥fn∥C1∥An −A∥2 + ∥W in n ωn −W inω∥C1 <
τ∥A∥2∥fn −f∥C1 + τµ∥An −A∥2 + ∥W in n ωn −W inω∥C1 < τ∥A∥2∥fn −f∥C1 +
ǫ(1 −τ∥A∥2) 2 + ǫ(1 −τ∥A∥2) 2 by equations (4) and (5) < τ∥A∥2∥fn −f∥C1
+ ǫ(1 −τ∥A∥2). Hence, rearranging we see that ∥fn−f∥C1(1−τ∥A∥2) <
ǫ(1−τ∥A∥2) which implies ∥fn−f∥C1 < ǫ as required. To prove
non-emptiness we construct an explicit reservoir matrix A and input
matrix W in for which the Echo State Map f is an embedding, using a
trick borrowed from Shi & Han (2007). First, for a given observation
function ω we deﬁne Λω to be the subset of matrices A and W in for which
the associated map f is a C1 embedding: Λω := {(A, W in) | fA,W in,ω is
a C1 embedding.} (6) Lemma 2.3.6. Let M be a compact m-manifold and n
∈N. Let A be an n × n matrix and suppose ∥A∥2 < min(1/∥Dφ−1∥∞, 1). Let W
in be an n×1 matrix and let the triple (ϕ, A, W in) be an ESN. Suppose
that φ ∈Diff2(M) has the following two properties: (1) φ has only
ﬁnitely many periodic points with periods less than or equal to 2m. (2)
If x ∈M is any periodic point with period k < 2m then the eigenvalues of
the derivative Dφk at x are distinct. Then for a generic ω ∈C2(M, R), Λω
is non-empty. Proof. Let A = 1 2       0 0 0 . . . 0 1 0 0 . . . 0
0 1 0 . . . 0 0 0 1 . . . 0 0 0 0 … 0       8 and W in 1 = 1, W in
j = 0 for 2 ≤j ≤n. Then the ESM f :=       ϕ1 ◦ω ϕ2 ◦2−1ϕ1 ◦ω ◦φ−1
ϕ3 ◦2−1ϕ2 ◦2−1ϕ1 ◦ω ◦φ−2 . . . ϕn ◦21−nϕn−1 . . . 2−1ϕ1 ◦ω ◦φ−n+1   
   , where ϕi(ri) = σ(ri + bi) is the ith component function of ϕ, as
deﬁned in (1), solves the equation f = ϕ(Af ◦φ−1 + W inω). We can see
moreover that f ≡g ◦Φ(φ,ω) where g :=       ϕ1 ϕ2 ◦2−1ϕ1 ϕ3 ◦2−2ϕ2
◦2−1ϕ1 . . . ϕn ◦21−nϕn−1 . . . 2−1ϕ1       and Φ(φ,ω) is the
delay observation map Φ(φ,ω)(x) = (ω(x), ω ◦φ−1(x), ω ◦φ−2(x), …, ω
◦φ−n+1(x)). By design, each ϕi is a C1 embedding hence g is a C1
embedding. For generic ω ∈C2(M, R) the delay observation map Φ(φ,ω) is
also a C1 embedding, thanks to Takens’ Theorem. Noting that the
composition of C1 embeddings is a C1 embedding completes the proof.
Theorem 2.3.7. (Weak ESN Embedding Theorem) Let M be a compact
m-manifold and n ≥2m + 1. Let A be a random variable with a distribution
that has full support on the space of n × n matrices for which ∥A∥2 <
min(1/∥Dφ−1∥∞, 1), and let W in be a random variable with a distribution
that has full support on the space of n × 1 matrices, and let the triple
(ϕ, A, W in) be an ESN. Suppose φ ∈Diff2(M) has the following two
properties: (1) φ has only ﬁnitely many periodic points with periods
less than or equal to 2m. (2) If x ∈M is any periodic point with period
k < 2m then the eigenvalues of the derivative Dφk at x are distinct.
Then for a generic observation function ω ∈C2(M, R) the Echo State Map f
is a C1 embedding with probability α > 0. Proof. The space of all
observation functions ω ∈C2(M, R) such that the delay observation map
Φ(φ,ω) is an embedding is generic in C2(M, R), thanks to Takens’
Theorem. For any one of these observation functions, the set Λω deﬁned
in (6) is non-empty by Lemma 2.3.6 and open by Lemma 2.3.5. Since A, and
W in are random variables with full support, they take values in Λ with
probability α > 0. Remark 2.3.8. The Embedding Conjecture and Weak ESN
Embedding Theorem state that under the right conditions f is an
embedding. In practical examples we cannot compute f exactly because it
is obtained in the limit of inﬁnitely many past observations. In
practice, if we have k observations the best we can do is to use all
available observations and compute f r0 k . Fortunately, the set of C1
embeddings is open in the C1 topology, and f r0 k converges to f in this
topology, so there exists a sufﬁciently large number ℓof previous
observations such that for all k > ℓ, f r0 k is an embedding. The ESN
Embedding Conjecture also admits a biological interpretation. Consider
an organism with a (primitive) ner- vous system (‘brain’) comprised of
neurons. Neurons are connected to each other with random connection
weights (including zero) representing the strength of the connection (or
no connection). The adjacency matrix forms the reser- voir matrix A. The
reservoir state r is a vector representing the ﬁring rate of every
neuron. Suppose that the organism has a sensory organ connected to the
brain which at any point in time senses a scalar measure of the
environment, for example an average environmental light intensity. The
connection weight from the sensory organ to the ith neuron is then the
ith entry of W in. Suppose that the light intensity depends on the state
of the environment which evolves 9 as a high dimensional dynamical
system. Then the nervous system and sensory organ together operate as an
ESN. Since the entries of A and W in are random variables, the ESN
Embedding Conjecture states that the dynamics of the environment are
indeed embedded into the nervous system without the nervous system
needing to possess any special structure provided by learning or natural
selection. The embedding of the natural world into the brain is obtained
‘for free’. This leaves cognition, deﬁned as ‘the art of performing
computation on our representation of the environment’, as the faculty
that requires optimisation by natural selection or learning. 2.4 The ESN
Approximation Theorem In this section we will state and prove the ESN
Approximation Theorem - that an ESN which successfully embeds a
dynamical system into the reservoir space can approximate the system’s
dynamics during the autonomous phase, hence replicate the topology of a
structurally stable dynamical system. We will use several preliminary
results introduced over the proceeding subsections. 2.4.1 The Universal
Approximation Theorem The ﬁrst major result we will use to prove the ESN
Approximation Theorem is the Universal Approximation Theorem. This
theorem is highly celebrated in the literature on mathematical analysis
of neural networks, and states that smooth functions and any number of
their derivatives can be approximated by single layer neural network
with sufﬁciently many neurons. In this section we recall this theorem
and then present an extension suitable for ESNs, to take account of the
fact that for an ESN the neural network weights vi and biases bi are
randomly chosen but then ﬁxed; only the output weights wi can be chosen
to give a good approximation to an input function f. We will use the
Universal Approximation Theorem presented by Hornik et al. (1990),
because it concerns smooth functions and any number of their derivatives
while the earlier seminal paper by Cybenko (1989) does not. Deﬁnition
2.4.1. (ℓ-ﬁnite) Let ℓ∈N0. Then we say an ℓ-times differentiable scalar
function σ ∈Cℓ(R) is ℓ-ﬁnite if 0 < Z R     dℓσ dxℓ    dx < ∞.
Remark 2.4.2. The activation function σ ∈C1(R, (−1, 1)) with derivative
in the range (0, 1) is 1-ﬁnite; meaning ℓ-ﬁnite with ℓ= 1. Theorem
2.4.3. (Universal Approximation Theorem) If the activation function σ is
ℓ-ﬁnite, then for all 0 ≤m ≤ℓ functions g : In →R of the form g(x) = N X
j=1 wjσ(v⊤ j x + bj) are dense in Cm(In, R). Proof. Hornik et
al. (1990). The Universal Approximation Theorem essentially states that
if we are interested in approximating a function f to some tolerance ǫ
we can create a neural network of size N and modify the weights until
the network approximates f to the tolerance ǫ. We want to slightly
extend the theorem for our purposes. Recall that an ESN has random
reservoir weights comprising the matrix A and random input weights
comprising the matrix W in, and it is only the output connection weights
W out that are trained. We therefore want to show that for any
continuously differentiable function f and a sufﬁciently large neural
network with random weights vi and biases bi, we can choose linear
readout weights wi such that the resulting neural network approximates f
arbitrarily well with probability arbitrarily close to 1. We will call
this the Random Universal Approximation Theorem (RUAT), and remark that
the RUAT is highly related to Theorem 2.1 appearing in the seminal paper
on Extreme Learning Machines by Huang et al. (2006). We can also view
the RUAT as a special case of Theorem 1 presented by Gonon et
al. (2020), who prove a stronger result in the more general context of
ﬁlters. The idea behind the proof of the RUAT is as follows. First we
note that there is a neural network ˆ g that approximates f by the
Universal Approximation Theorem. Then, we create sample sequences of
weights and biases vi, bi by repeated draws from appropriate random
variables. There will eventually be some randomly generated samples vj,
bj that are close to each of the weights and biases of the network ˆ g.
From this list of weights and biases in the sample sequences we select
those that match closely, and so create a neural network g, choosing
linear readout weights wi either to match the respective weight in ˆ g
or choosing to set wi = 0 in order effectively to discard those samples
vi, bi that not close 10 to values in ˆ g. Now by construction g is a
good approximation to ˆ g which is itself a good approximation to f. The
details are presented in the following lemma and theorem. Lemma 2.4.4.
Let (Xj)j∈N be a sequence of i.i.d. random variables and S1, . . . ,
Sℓbe a list of ℓevents, and suppose that for each i (and for any j since
they are i.i.d.) there exists θi such that P(Xj ∈Si) = θi > 0. Then for
all α ∈(0, 1) there exists N ∈N such that P  ∃injective φ : {1, . . .,
ℓ} →{1, . . . , N} : Xφ(i) ∈Si, ∀i ∈{1, . . . , ℓ}  > α. Proof. First,
ﬁx α ∈(0, 1). Then deﬁne the set {n0, . . . , nℓ} as follows. Set n0 = 0
and for any i ∈{1, …, ℓ} let ni −ni−1 := ceil log(1 −α1/ℓ) log(1 −θi) 
+ 1. Finally, set N = nℓ. Then we can calculate that P  ∃injective φ :
Xφ(i) ∈Si ∀i ∈{1, . . ., ℓ}  > P  ∀i ∈{1, . . . , ℓ} ∃j ∈{1 + ni−1, . .
. , ni} : Xj ∈Si  = ℓ Y i=1 P  ∃j ∈{1 + ni−1, . . . , ni} : Xj ∈Si  =
ℓ Y i=1 1 −P  Xj / ∈Si ∀j ∈{1 + ni−1, . . . , ni}  ≥ ℓ Y i=1 1 −(1
−θi)ni−ni−1 = ℓ Y i=1 1 −(1 −θi)ceil  log(1−α1/ℓ)/ log(1−θi)  +1 > ℓ Y
i=1 1 −(1 −θi)  log(1−α1/ℓ)/ log(1−θi)  = ℓ Y i=1 1 −exp log(1 −α1/ℓ)
log(1 −θi) log(1 −θi)  = ℓ Y i=1 1 −(1 −α1/ℓ) = ℓ Y i=1 α1/ℓ= α.
Theorem 2.4.5. (Random Universal Approximation Theorem) Let In denote
the unit hypercube of dimension n and let f ∈C1(In, R). Let σ ∈C1(R) be
1-ﬁnite, and let (bj)j∈N, (vj)j∈N be sequences of i.i.d. random
variables with full support. Then for any α ∈(0, 1) and ǫ > 0 there
exists some natural number N ∈N such with, probability greater than α,
there exist real numbers w1, . . . , wN ∈R such that the random neural
network g : In →R deﬁned by g(x) = N X j=1 wjσ(v⊤ j x + bj) satisﬁes ∥f
−g∥C1 < ǫ. Proof. First, by the Universal Approximation Theorem we know
that for any ǫ > 0 there exists a neural network ˆ g : In →R of size
ℓdeﬁned by ˆ g(x) = ℓ X i=1 ˆ wiσ(ˆ v⊤ i x + ˆ bi) 11 such that ∥f −ˆ
g∥C1 < ǫ 2. (7) Now, consider two sequences of i.i.d. random variables
(bj)j∈N and (vj)j∈N with full support, and let Xj := (bj, vj). Fix ǫ > 0
and deﬁne a collection of ℓevents S1, …, Sℓby Si :=  (b, v) ∈R × Rn :
∥σ(ˆ v⊤ i · +ˆ bi) −σ(v⊤· +b)∥C1 < ǫ 2ℓmaxk( ˆ wk)  , where the weights
ˆ wk are given by the form of the network ˆ g. Observe that each of the
Si have strictly positive measure, so there exists θi > 0 such that P(Xj
∈Si) > θi > 0 ∀j ∈N. Hence it follows by Lemma 2.4.4 that for all α ∈(0,
1) there exists N ∈N such that P  ∃injective φ : {1, . . . , ℓ} →{1, . .
. , N} : Xφ(i) ∈Si ∀i ∈{1, . . . , ℓ}  > α. Now, on the event
∃injective φ : {1, . . ., ℓ} →{1, . . . , N} : Xφ(i) ∈Si ∀i ∈{1, . . .,
ℓ} we deﬁne wj :=  ˆ wi if φ(i) = j 0 otherwise for all j ∈{1, . . .,
N}, and deﬁne the random neural network g : In →R by g(x) = N X j=1
wjσ(v⊤ j x + bj). Now observe ∥ˆ g −g∥C1 =

ℓ X i=1 ˆ wiσ(ˆ v⊤ i · +ˆ bi) − N X j=1 wjσ(v⊤ j · +bj)

C1

ℓ X i=1 ˆ wi  σ(ˆ v⊤ i · +ˆ bi) −σ  v⊤ φ(i) · +bφ(i) 

C1 ≤ ℓ X i=1 ˆ wi

 σ(ˆ v⊤ i · +ˆ bi) −σ  v⊤ φ(i) · +bφ(i) 

C1 < ℓ X i=1 ˆ wiǫ 2ℓmaxk( ˆ wk) < ǫ 2. Combining this with (7) and
using the triangle inequality we obtain ∥f −g∥C1 ≤∥f −ˆ g∥C1 + ∥ˆ g
−g∥C1 < ǫ 2 + ǫ 2 = ǫ, which completes the proof. 2.4.2 The ESN
Approximation Theorem In this subsection we will state and prove the ESN
Approximation Theorem which states that there exists a linear readout
layer W out giving rise to an autonomous ESN phase with a normally
hyperbolic attracting m-submanifold on which the autonomous dynamics are
topologically conjugate to a structurally stable φ. The idea behind the
theorem is observe that the ESN looks enough like a single layer neural
network that the Random Universal Approximation Theorem holds.
Consequently we can choose linear readout weights stored in the matrix W
out to approximate any C1 function. We will assume that f is an
embedding, and therefore invertible on its image, and choose readout
weights W out such that the autonomous ESN approximates a C1 dynamical
system possessing an m dimensional normally hyperbolic attracting
submanifold on which the dynamics approximate f ◦φ ◦f −1. We want the
manifold to be normally hyperbolic and attracting to ensure that an
autonomous trajectory that leaves the manifold by some small distance is
attracted back toward the manifold, preventing an accumulation of errors
from sending the trajectory too far away. Autonomous trajectories
originating near the manifold therefore remain near, all the while
approximating f ◦φ ◦f −1. To formalise these ideas, we will ﬁrst deﬁne a
normally hyperbolic attracting submanifold. 12 Deﬁnition 2.4.6.
(Normally Hyperbolic Attracting Submanifold) Let φ ∈Diff1(M), then, a
φ-invariant submanifold Λ ⊂M is a normally hyperbolic attracting
submanifold if the restriction to Λ of the tangent bundle of M admits a
splitting into a direct sum of two Dφ-invariant subbundles, the tangent
bundle of Λ, and the stable bundle Es. Furthermore, with respect to some
Riemannian metric on M, the restriction of Dφ to Es must be a
contraction, and must be relatively neutral on T Λ. Thus, there exist
constants 0 < λ < µ−1 < 1 and c > 0 such that TΛM = T Λ ⊕Es (Dφ)xEs x =
Es φ(x) ∀x ∈Λ ∥Dφkv∥≤cλk∥v∥∀v ∈Es, ∀k ∈N ∥Dφkv∥≤cµ|k|∥v∥. Before we
present the ESN Approximation Theorem itself we will prove that there
exists a C1 evolution operator η deﬁned on Rd that has a normally
hyperbolic attracting submanifold on which the dynamics of η are
conjugate to φ. The existence of this map η is guaranteed by standard
topological machinery which we recall brieﬂy here, and which is
presented in detail by Warner (1971). Deﬁnition 2.4.7. (Cubic centred
chart) A chart (V, ϕ) belonging to a d-manifold is called a cubic chart
if ϕ(V ) is an open cube centred about the origin in Rd. If x ∈V and
ϕ(x) = 0, then the chart (V, ϕ) is centred at x. Deﬁnition 2.4.8. (Slice
coordinates) Suppose that (V, ϕ) is a chart on a d-manifold D with
coordinate functions x1, …, xd and that m is an integer 0 ≤m ≤d. Let a
∈ϕ(V ) and let S = {q ∈V | xi(q) = ai, i = m + 1, …, d}. The subspace S
of D together with coordinate maps x|S for j = 1, …, m forms a
submanifold of D, called a slice of the chart (V, ϕ). Lemma 2.4.9.
(Slice Lemma) Let M be a compact m-manifold, let f : M →Rd be an
immersion, and let x ∈M. Then there exists a cubic centred chart (V, ϕ)
about f(x) and a neighbourhood U of x such that f|U is injective and
f(U) is a slice of (V, ϕ). Proof. Warner (1971) page 28 prop 1.35. Lemma
2.4.10. Let d > m and M be a compact m-manifold. Let φ ∈Diff1(M).
Suppose f ∈C1(M, Rd) is a C1 embedding. Then there is an open subset
Ω∈Rd and η ∈Diff1(Ω) with f(M) a normally hyperbolic attracting
submanifold such that η|f(M)= f ◦φ ◦f −1 (where we have deﬁned f −1 on
the image of f). Proof. We will make a similar argument to Warner (1971)
in the proof of his Proposition 1.36, on page 29. First let x ∈M. Then
by the Slice Lemma there exists a cubic centred chart (Vx, ϕx) about
f(x) and a neighbourhood Ux of x such that f(Ux) is a slice (Vx, ϕx).
Let x1, . . . , xm be the slice coordinates in the chart (Vx, ϕx) of
points in f(Ux). Then we can deﬁne a map ηx ∈Diff1(Vx, Rd) applying the
map f ◦φ ◦f −1 on the slice co-ordinates and dividing the remaining
co-ordinates by 2. We can make this argument for every x ∈M hence deﬁne
a collection of maps {ηx} over a collection of open sets {Vx} which
cover f(M). Now we let {αj | j ∈N} form a partition of unity subordinate
to the cover {Vx}. We take a subsequence {αk} such that supp(αk) ∩f(M)
̸= ∅and denote the collection of sets to which {αk} is subordinate by
{Vk}. We then deﬁne a map η on a neighbourhood Ω:= ∪kVk of f(M) by η = X
k αkηx. By construction, η|f(M) = f ◦φ ◦f −1 and η has a normally
hyperbolic attracting submanifold f(M). Not only does the dynamical
system η exist, but importantly, its normally hyperbolic attracting
submanifold is pre- served by any sufﬁciently good approximation. This
is made formal in the Invariant Manifold Theorem, which we will use in
the proof of the ESN Approximation Theorem. Theorem 2.4.11. (Invariant
Manifold Theorem) Let K be a compact manifold and η ∈Diff1(K) with
normally hyperbolic attracting submanifold Λ. Then, ∃ǫ > 0 such that for
any u ∈Diff1(K) with ∥η −u∥C1 < ǫ, the diffeomorphism u has a normally
hyperbolic attracting submanifold U such that ∥U −Λ∥C1 < ǫ. Proof.
Hirsch et al. (1977). 13 W in Y X A Strongly recurrent Weakly recurrent
W out Figure 2: The ESN with sparsity structure imposed on A so that we
can prove the ESN Approximation Theorem. The matrix X and vector Y are
deﬁned in the statement of the ESN Approximation Theorem. With these
preliminaries established we are ready to prove our ESN Approximation
Theorem. Our strategy involves imposing a special structure on the
reservoir matrix A in order to obtain sufﬁciently many neurons for the
Random Universal Approximation Theorem to hold while controlling the
dimension of the codomain of the Echo State Map. The structure of A is
made clear in the statement of the ESN Approximation Theorem and
illustrated in Figure 2, where we call the connections represented by
the matrix A ‘strongly recurrent’ and those represented by X ‘weakly
recurrent’. The weakly recurrent neurons and the vector Y of inputs are
introduced in the proof of the ESN Approximation Theorem in order to
satisfy the conditions of the Random Universal Approximation Theorem.
Deﬁnition 2.4.12. (ESN autonomous phase) The ESN autonomous phase with
parameters (A, W in, W out, ϕ) is a discrete time autonomous dynamical
system ψ ∈C1(Rn) deﬁned by ψ(s) = ϕ  (A + W inW out)s  . Theorem
2.4.13. (ESN Approximation Theorem) Let M be a compact m-manifold and n
∈N such that n > 2m. Let A be an n×n matrix where ∥A∥2 < min(1/∥Dφ−1∥∞,
1), and W in an n×1 matrix, and let the triple (ϕ, A, W in) be an ESN.
Let φ ∈Diff1(M) be structurally stable, and let ω ∈C1(M, R). Suppose the
Echo State Map f ∈C1(M, Rn) is a C1 embedding. Let (xj)j∈N, (yj)j∈N, and
(bj)j∈N be sequences of i.i.d. Rn, R, and R-valued random variables,
respectively, with full support. Let α ∈(0, 1). Then, with probability
α, there exists d ∈N with d > n, a d × 1 matrix 14 W out, a d × d matrix
˜ A, and a d × 1 matrix ˜ W in assembled from the n × n matrix A, the (d
−n) × n matrix X with jth row xj, and the (d −n) × 1 matrix Y with jth
row yj, like so: ˜ A =  A 0 X 0  and ˜ W in =  W in Y  , and an
activation function ˜ ϕi(r) = σ(ri + bi) ∀i ∈{1, …, d} such that the
autonomous ESN ψ ∈C1(Rd) with parameters ( ˜ A, ˜ W in, W out, ˜ ϕ) has
a normally hyperbolic attracting submanifold on which ψ is topologically
conjugate to φ. Proof. By assumption, the Echo State Map f deﬁned for
the ESN (ϕ, A, W in) with respect to (φ, ω) is an embedding, so the Echo
State Map ˜ f deﬁned for ( ˜ ϕ, ˜ A, ˜ W in) with respect to (φ, ω) is
also an embedding. For the remainder of the proof we will restrict the
codomain of ˜ f to its image in order to yield a C1 diffeomorphism.
Before we proceed, we will establish some preliminary results. First we
deﬁne y : M →y(M) ⊂Rn+1 by y1(x) = ω(x) and     y2(x) y3(x) . . .
yn+1(x)    = f ◦φ−1(x). Furthermore we will deﬁne maps F : C1(M, Rd)
→C1( ˜ f(M), Rd) by F(g) = g ◦˜ f −1 and Y : C1(y(M), R) →C1(M, R) by
Y(g) = g ◦y. Next we will show that F and Y are Lipschitz continuous. To
see that F is Lipschitz continuous observe ∥F(g) −F(h)∥C1 = ∥g ◦˜ f −1
−h ◦˜ f −1∥C1 = ∥g ◦˜ f −1 −h ◦˜ f −1∥∞+ ∥Dg ◦˜ f −1D ˜ f −1 −Dh ◦˜ f
−1D ˜ f −1∥∞ ≤ ∥g ◦˜ f −1 −h ◦˜ f −1∥∞+ ∥D ˜ f −1∥∞∥Dg ◦˜ f −1 −Dh ◦˜ f
−1∥∞ = ∥g −h∥∞+ ∥D ˜ f −1∥∞∥Dg −Dh∥∞ ≤ max(1, ∥D ˜ f −1∥∞)(∥g −h∥∞+ ∥Dg
−Dh∥∞) = max(1, ∥D ˜ f −1∥∞)∥g −h∥C1. We can make an almost identical
argument to show that Y is Lipschitz continuous. We will denote the
Lipschitz constants for F and Y by L and M respectively. We are now
ready to proceed with the proof. By Lemma 2.4.10, there exists an open
subset Ω∈Rd containing ˜ f(M) and η ∈Diff1(Ω) with ˜ f(M) a normally
hyperbolic attracting submanifold such that η| ˜ f(M) = ˜ f ◦φ ◦˜ f −1.
Now let K ⊂Ωbe a compact manifold containing ˜ f(M). Normally hyperbolic
invariant submanifolds persist under small perturbations, by the
Invariant Manifold Theorem, so ∃ǫ > 0 such that any u ∈Diff1(K) which
satisﬁes ∥u − η|K∥C1 < ǫ is topologically conjugate to η. For any given
value α ∈(0, 1), by the Random Universal Approximation Theorem, there
exists a d ∈N and a d × 1 matrix W out such that g ∈C1(Rn+1, R) deﬁned
by g(z) = d X i=1 W out i σ   ˜ W in ˜ A  i z + bi  (8) satisﬁes ∥g
−ω ◦φ ◦y−1∥C1< ǫ LM∥W in∥ (9) 15 M ˜ f(M) h ◦˜ f(M) M ˜ f(M) h ◦˜ f(M) φ
˜ f η h ψ ˜ f h Figure 3: A commuting diagram representing the ESN
Approximation Theorem where the terms are deﬁned through- out the
theorem’s proof. where [ ˜ W in, ˜ A]i is a 1 × (n + 1) matrix with 1st
entry ˜ W in i and (j + 1)th entry ˜ Aij. Now ∥ψ| ˜ f(M) −η| ˜ f(M)∥C1 ≤
L∥ψ ◦˜ f −η ◦˜ f∥C1 = L∥ψ ◦˜ f −˜ f ◦φ∥C1 because η| ˜ f(M) = ˜ f ◦φ ◦˜
f −1 = L∥˜ ϕ( ˜ A ˜ f + ˜ W inW out ˜ f) −˜ ϕ( ˜ A ˜ f + ˜ W inω ◦φ)∥C1
by deﬁnition of ψ ≤ L∥( ˜ A ˜ f + ˜ W inW out ˜ f) −( ˜ A ˜ f + ˜ W inω
◦φ)∥C1 because ˜ ϕ is contracting = L∥( ˜ W inW out ˜ f) −( ˜ W inω
◦φ)∥C1 because ˜ A ˜ f −˜ A ˜ f = 0 ≤ L∥˜ W in∥2∥W out ˜ f −ω ◦φ)∥C1 by
factoring out W in = L∥˜ W in∥2∥W out ˜ ϕ( ˜ A ˜ f ◦φ−1 + ˜ W inω) −ω
◦φ∥C1 by Theorem 2.2.2 = L∥˜ W in∥2

d X i=1 W out i σ   ˜ W in ˜ A  i y + bi  −ω ◦φ

C1 by deﬁnition of ˜ ϕ and y = L∥˜ W in∥2∥g ◦y −ω ◦φ∥C1 by (8) ≤ LM∥˜ W
in∥2∥g|y(M)−ω ◦φ ◦y−1∥C1 < LM∥˜ W in∥2 ǫ LM∥˜ W in∥2 by (9) = ǫ hence
there is some open set ˜ Ω⊂K containing ˜ f(M) such that ∥ψ|˜ Ω−η|˜ Ω∥C1
< ǫ so ψ|˜ Ωis conjugate to η|˜ Ω. Consequently, there exists an h
∈Diff1(˜ Ω) such that ψ|˜ Ω= h ◦η|˜ Ω◦h−1. Now ˜ f(M) is a normally
hyperbolic attracting submanifold of η where η| ˜ f(M)= ˜ f ◦φ ◦˜ f −1
so h ◦˜ f(M) is a normally hyperbolic attracting submanifold of ψ on
which ψ = h ◦η ◦h−1 = h ◦˜ f ◦φ ◦˜ f −1 ◦h−1 ∼ = φ. Remark 2.4.14. A
consequence of the ESN Approximation Theorem is that the diagram shown
in Figure 3 commutes. 3 Numerical Experiments with ESNs In the previous
section we showed that for a given structurally stable dynamical system
and a sufﬁciently large ESN there exists a linear output matrix W out
that gives rise to an autonomous ESN with dynamics that are
topologically conjugate to those of the given dynamical system. To test
whether these results hold in practice we took a 1D observation of a
numerically integrated trajectory of the Lorenz system, fed this into an
ESN implemented on a commercial laptop, and sought to discover whether
the autonomous phase of the ESN would adopt dynamics topologically
conjugate to the Lorenz system. In particular we computed several
topological invariants of the ESN autonomous phase including the
Lyapunov exponents, ﬁxed point eigenvalues, and homology, then compared
these to the known invariants of the Lorenz system. This work was
inspired by a paper by Pathak et al. (2017) who trained an ESN on a full
3D trajectory of the Lorenz system, rather than a 1D observation, and
compared the Lyapunov exponents of the autonomous phase to the known
exponents of the Lorenz 16 -20 0 -10 10 0 20 x z 10 30 20 40 50 Figure
4: A picture of the famous Lorenz attractor. Here the trajectory was
initialised at (1, 1, 1) and quickly converges to the attractor. system.
In a more recent works, Vlachas et al. (2019) train an ESN on 1D
observations of the Lorenz-96 system, and also compare the Lyapunov
exponents of the autonomous phase to the known exponents of the Lorenz
system. Chattopadhyay et al. (2019) also train an ESN on observations of
the Lorenz-96 system and evaluate the accuracy of future prediction for
reservoirs of different size. We used MATLAB’s ODE45 to integrate a
trajectory of the Lorenz (1963) system ˙ x = σ(y −x) (10) ˙ y = x(ρ −z)
−y ˙ z = xy −βz with parameters σ = 10, β = 8/3, ρ = 28 chosen so the
system produces the celebrated Lorenz attractor shown in Figure 4. We
then observed the x component of the trajectory by choosing the
observation function ω(x, y, z) = x to create a 1 dimensional time
series. We fed this time series into an ESN with the following
parameters: spectral radius ρ = 1, reservoir size n = 300, and
activation function ϕ = tanh. The reservoir matrix A is an Erd˝ os-Rényi
matrix with mean 6 and connection weights (where they are non-zero)
i.i.d Gaussian, re-scaled such that ρ = 1. The keen reader will notice
that the structure of A does not conform to the reservoir matrix ˜ A
described in the statement of the ESN Approximation Theorem. The fact
that our numerical experiments produce good results despite this
suggests this weakly connected ˜ A is unnecessary, but rather a decision
we made to make the ESN Approximation Theorem easier to prove.
Furthermore, insisting that ρ < 1 is not sufﬁcient in to ensure that
∥A∥2 < 1, but this is a common choice in practical applications. The
matrix W out is populated with i.i.d Gaussian weights ∼N(0, 1) which are
then scaled by a ‘strength parameter’ p = 0.1. We choose a
regularisation parameter λ = 10−6 to solve the regularised least squares
problem min W out K X k=1 ∥W outrk −uk∥2 + λ∥W out∥2 2 using the SVD
method presented by Hansen et al. (2006). We will note here that the
linear output layer W out ob- tained by this procedure is not
necessarily the same as that guaranteed by the ESN Approximation
Theorem. These parameters were carefully hand tuned so that the
autonomous phase appeared (by eye) to match the driven phase. The
question of how to systematically choose good parameters is discussed by
Yperman & Becker (2016) who searched through parameter space using
Bayesian optimisation, and used cross validation to test the goodness of
ﬁt. Now, with W out obtained, we ran the autonomous ESN and plotted the
future observations vi in Figure 5. We can see from this Figure that the
ESN seems to predict the qualitative features of the future trajectory
very well. 17 75 80 85 90 95 100 105 110 115 120 125 time -20 0 20 x
Figure 5: Here the 1D observations are shown in blue (up to time 100 for
those of you reading in black and white) and future predictions shown in
red (onwards from time 100). -10 -5 0 5 10 -0.6 -0.4 -0.2 0 0.2 0.4 0.6
Training Data Future Prediction Newton Iterates Figure 6: The driven
reservoir dynamics are plotted in blue and autonomous dynamics are
plotted in red. Both were projected onto the ﬁrst three principal
components of the driven dynamics, then the axes are rotated such that
the projection appears on the ﬁrst 2 components. The black line
indicates the iterates of Newton’s method, used to locate a ﬁxed point -
the method eventually converges to a ﬁxed point in the middle of the
right wing of the ﬁgure. We can see by eye that the reservoir dynamics
appear by eye to be topologically conjugate to the Lorenz system. Since
the Lorenz system is deﬁned on a 3-manifold, we can usefully plot
trajectories of the entire system. To check by eye whether the reservoir
dynamics of both the driven phase and autonomous phase are topologically
conjugate to the Lorenz dynamics, we projected the driven and autonomous
dynamics onto the ﬁrst 3 principal components of the driven trajectory
and present them in Figure 6. 3.1 Locating Fixed Points and Determining
their Eigenvalues If the ESM f is an embedding, then f will embed the
ﬁxed points of the Lorenz system into the reservoir space. Moreover if
the autonomous ESN approximates the embedded Lorenz system on a
neighbourhood of the embedded ﬁxed points sufﬁcently well, the
autonomous dynamics will contain ﬁxed points very close to those of the
embedded Lorenz system. To verify this, we searched for the autonomous
ESN’s ﬁxed points using Newton’s method, and found them, as illustrated
in Figure 6. Further, if the ESM f is a C1 embedding of the original
dynamics, we expect f to preserve the stability of ﬁxed points, i.e. we
expect the eigenvalues of the linearisation of the autonomous phase to
be preserved at every ﬁxed point. Now, 18 -1 -0.5 0 0.5 1 Real -1 -0.5 0
0.5 1 Imaginary ESN autonomous sytem Lorenz system Figure 7: Here the 3
eigenvalues of the linearisation of the Lorenz system on the ﬁxed point
inside one of the Lorenz attractor’s wings are represented by blue
crosses. The 300 eigenvalues of the linearisation of the ESN autonomous
system at the ﬁxed point found with Newton’s method are represented by
red dots. comparing the eigenvalues of the linearisation of the Lorenz
system and autonomous phase at the respective ﬁxed points requires some
subtlety, because the Lorenz system is a continuous time ﬂow, while the
autonomous phase is a discrete time map. So, we began by considering one
of the known ﬁxed points found in the Lorenz attractor’s wings x∗= ( p
β(ρ −1), p β(ρ −1), ρ −1), and noted the Jacobian J of the continuous
time Lorenz system evaluated at the ﬁxed point x∗is therefore J   
x∗=   −σ σ 0 1 −1 − p β(ρ −1) p β(ρ −1) p β(ρ −1) −β  . Now we can
discretise the Lorenz system ˙ x = s(x) with the following map xk+1 = xk
+ Z tk+1 tk s ◦x(t)dt, hence the discrete time linearisation about the
ﬁxed point x∗is xk+1 = exp  J    x∗(tk+1 −tk)  xk, which has 3
eigenvalues, which we have compared with the ESN autonomous eigenvalues
in Figure 7. If the ESM f is indeed a C1 embedding, the dynamics of the
autonomous phase are topologically conjugate to the discrete time Lorenz
system on some 3-submanifold. This manifold is spanned by 3
eigenvectors, each with an associated eigenvalue, which will coincide
with the eigenvalues of the linearisation of the Lorenz system on the
ﬁxed point. Figure 7 appears to show 3 overlapping eigenvalues,
suggesting that the autonomous phase is diffeomorphic to the Lorenz
system (at least in a neighbourhood of x∗) in this simulation. This is
particularly remarkable because x∗is distant from the training data. The
ESN has successfully inferred the existence, position and eigenvalues of
a ﬁxed point from training data, which contains no ﬁxed points. In the
machine learning parlance, the ESN has generalised patterns in the
training data to an unseen region of the phase space. 19 Figure 8: The
Lyapunov spectrum of the autonomous phase as the iterates increases is
shown. The true Lyapunov exponents of the autonomous phase is given by
the limit of these exponents as the iterations tend to inﬁnity. These
autonomous exponents are compared to the black dotted lines representing
the 3 exponents of the Lorenz system. 3.2 Comparison of Lyapunov Spectra
Another topological invariant of the Lorenz system is the Lyapunov
spectrum, which captures how quickly very close trajectories diverge
from eachother, and is used as a measure of chaos. To deﬁne the
spectrum, let J be the Jacobian of the evolution operator of a
continuous time dynamical system. Let Y be the solution of the ODE ˙ Y =
JY with initial condition Y (0) = x0. Then the Lyapunov Spectrum of the
invariant set containing x0 is the spectrum of the matrix Λ deﬁned Λ =
lim t→∞ 1 2tY Y ⊤. Each eigenvalue in the spectrum is called a Lyapunov
exponent to signify that two initially close trajectories diverge or
converge exponentially fast with exponentiation constant in the
direction of each eigenvector of J given by a Lyapunov exponent. Details
are discussed by Darbyshire & Broomhead (1996). The Lyapunov spectrum
for the Lorenz system was estimated by Sprott (2003) as 0.9056, 0,
-14.5723. In order to compare the Lorenz spectrum to the spectrum of the
autonomous ESN, we computed the autonomous system’s spectrum using the
discrete time QR method discussed in Darbyshire & Broomhead (1996) and
plotted each Lyapunov exponent against the known exponents of the Lorenz
system in Figure 8. We found the largest 2 in good agreement while there
was signiﬁcant error in the smallest, which is a common problem also
encountered by Pathak et al. (2017). 3.3 Persistent Homology We compared
the homology groups of the Lorenz attractor to the persistent homology
groups of the autonomous and driven attractors. We followed the lead of
Garland et al. (2016) who computed the persistent homology of the Lorenz
system reconstructed from a sequence of 1D observations of a Lorenz
trajectory using the delay observation map described in Takens’ Theorem.
The authors used the open source software Javaplex created by Tausz et
al. (2014) to ﬁnd the Witness Complex for the delay embedded Lorenz
attractor and computed the homology of the complex. They discuss a few
subtleties that arise, in particular that the Lorenz attractor is a
fractal, whose structure cannot be reconstructed exactly from any ﬁnite
number of sample points. The authors therefore satisﬁed themselves by
approximating the Lorenz attractor with a branched manifold model
presented by Williams (1979) which has the homology of the ﬁgure 8. We
made the same approximation, and expected to ﬁnd that the application of
persistent homology to the Lorenz system, driven ESN dynamics, and
autonomous ESN dynamics would reveal that all three have the ﬁgure 8
homology groups. In particular the persistence diagrams of these three
systems would exhibit a pair 20 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
Birth 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Death Figure 9: We have
plotted the H1 persistence diagrams of the driven ESN dynamics,
autonomous ESN dynamics, and Lorenz dynamics as blue circles, red
downward triangles, and purple upward triangles. We can see that each of
these 3 objects has a pair of points ﬂoating well above the diagonal,
suggesting each has 2 holes. This is consistent with our expectation
that all three adopt the topology of the ﬁgure 8. of H1 persistent
homology groups ﬂoating well above the diagonal. To verify this, we
produced persistence diagrams using the open source software Ripser
produced by Tralie et al. (2018) and plotted the results in Figure 9.
The reader may wonder why we would use persistent homology to show that
the Lorenz system, driven ESN dynamics, and autonomous ESN dynamics all
have the homology of the ﬁgure 8 when this can clearly be seen in
Figures 4 and 6. The homology of a 3D system is usually apparent from a
plot, but persistent homology can reveal the holes, voids and higher
dimensional hypervoids of high dimensional systems that cannot be easily
visualised. For example Muldoon et al. (1993) computed the homology of a
delay embedded time series from a ﬂuid dynamics experiment, which could
in general be of much higher dimension. 4 Conclusions and Outlook In
this paper, we showed that an Echo State Network driven by a sequence of
one dimensional observations of a dynamical system, evolving on a
manifold M, induces a map f ∈C1(M, Rn), which we called the Echo State
Map. We proved that for a randomly initialed ESN and generic observation
function ω, that f is an embedding with positive probability, and called
this the weak ESN Embedding Theorem. We conjectured that the theorem
holds with probability 1, by analogy to Takens’ Theorem. We went on to
show that a randomly initialised ESN has a universal approximation
property and called this the Random Universal Approximation Theorem
(RUAT). Finally, we used both the RUAT and Embedding Theorem to prove
that for an ESN trained a sequence of scalar observations of a
structurally stable dynamical system, there is a choice of linear
readout weights W out for which the autonomous ESN has dynamics that are
topologically conjugate to the input dynamical system, and we called
this the ESN Approximation Theorem. The theory presented here leaves
some questions unanswered. In practice we use regularized least squares
regression to learn an output matrix from the one-dimensional and ﬁnite
training trajectory, but currently, we have no guarantee that this will
result in an autonomous phase ESN that is topologically conjugate to the
underlying dynamical system. This is analogous to the case of the
Universal Approximation Theorem for feed forwards neural networks, where
the theoretical result proves the existence of suitable set of weights
but does not guarantee that a particular learning algorithm will be able
to ﬁnd them or how much training data may be required. It may be that
imposing extra conditions on the target dynamical system, like
ergodicity, allows us to prove that W out obtained by least squares
regression results in an arbitrarily good approximation. This seems to
be supported by the experiments in Section 3. 21 Furthermore, it seems
worthwhile to prove the ESN Embedding Conjecture, or some modiﬁcation of
it that is actually correct, by carefully modifying the proof of Takens’
Theorem provided by Huke (2006). A sceptical reader may wonder why we
would bother using an ESN to embed the trajectory in the ﬁrst place,
when a delay embedding would do. The reason being that it seems the
ESN’s learning and predictive powers are much more resilient to noise
than the simple delay embedding presented by Takens. Heuristically it
seems as an observed trajectory passes through the ESN, the noise
cancels itself out by taking a nonlinear combination of positive and
negative noise. We could therefore view the ESN as a nonlinear ﬁlter,
generalising the linear ﬁlters discussed by Sauer et al. (1991) in the
context of embedology - the art building delay observation maps with
special features, which include being more resiliant to noise than
Takens’ original map. Understanding the noise cancelling beneﬁts of the
ESN could be a fruitful direction of future work. Many of the
assumptions we made throughout this paper are likely stronger than they
need to be. For example Sauer et al. (1991) prove versions of Takens’
Theorem for dynamics on a compact invariant set with real box counting
dimension - generalising dynamics on a manifold with integer dimension.
This is particularly worthwhile because chaotic attractors of interest
often lie on invariant sets with non-integer dimension, with the Lorenz
attractor serving as a perfect example. We also create a strangely
shaped reservoir ˜ A in our proof of the ESN Approximation Theorem,
which numerical experiments suggests is unnecessary. Acknowledgements We
offer gracious thanks to the anonymous reviewers for their detailed
suggestions. In particular, we thank Juan-Pablo Ortega and Lyudmila
Grigoryeva for thorough and fruitful discussions at Universität Sankt
Gallen which lead to signiﬁcant improvements to the manuscript. We also
thank Allen Hart’s PhD conﬁrmation examiners Alastair Spence and Chris
Guiver who helpfully criti- cised and improved the content of this
paper. We acknowledge that Allen Hart is supported by a scholarship from
the ESPRC Centre for Doctoral Training in Statistical Applied
Mathematics at Bath (SAMBa), under the project EP/L015684/1. References
Banach, S. (1922), ‘Sur les opérations dans les ensembles abstraits et
leur application aux équations intégrales’, Fun- damenta Mathematicae
22, 133–181. Chattopadhyay, A., Hassanzadeh, P., Palem, K. &
Subramanian, D. (2019), ‘Data-driven prediction of a multi-scale lorenz
96 chaotic system using a hierarchy of deep learning methods: Reservoir
computing, ann, and rnn-lstm’, arXiv:1906.08829 . Cybenko, G. (1989),
‘Approximation by superpositions of a sigmoidal function’, Mathematics
of Control, Signals and Systems 2(4), 303–314. Darbyshire, A. &
Broomhead, D. (1996), ‘Robust estimation of tangent maps and liapunov
spectra’, Physica D: Nonlinear Phenomena 89(3), 287 – 305. Garland, J.,
Bradley, E. & Meiss, J. D. (2016), ‘Exploring the topology of dynamical
reconstructions’, Physica D: Nonlinear Phenomena 334, 49 – 59. Topology
in Dynamics, Differential Equations, and Data. Ghrist, R. (2008),
‘Barcodes: The persistent topology of data’, Bulletin of the American
Mathematical Society 45, 61– 75. Glorot, X., Bordes, A. & Bengio, Y.
(2011), Deep sparse rectiﬁer neural networks, in G. Gordon, D. Dunson &
M. Dudík, eds, ‘Proceedings of the Fourteenth International Conference
on Artiﬁcial Intelligence and Statistics’, Vol. 15 of Proceedings of
Machine Learning Research, PMLR, Fort Lauderdale, FL, USA, pp. 315–323.
Gonon, L., Grigoryeva, L. & Ortega, J.-P. (2020), ‘Approximation bounds
for random neural networks and reservoir systems’, arXiv:2002.05933 .
Grigoryeva, L. & Ortega, J. (2018), ‘Echo state networks are universal’,
Neural Networks 108, 495 – 508. Grigoryeva, L. & Ortega, J.-P. (2019),
‘Differentiable reservoir computing’, Journal of Machine Learning
Research 20(179), 1–62. URL: http://jmlr.org/papers/v20/19-150.html
Gürel, T. & Egert, S. R. U. (2010), ‘Functional identiﬁcation of
biological neural networks using reservoir adaptation for point
processes’, Journal of Computational Neuroscience p. 279–299. 22 Hackl,
J. (2018), ‘Tikz-network manual’, arXiv:1709.06005 . Hansen, P. C.,
Nagy, J. G. & O’leary, D. P. (2006), Deblurring Images: Matrices,
Spectra, and Filtering, SIAM. Hirsch, M. W., Pugh, C. C. & Shub, M.
(1977), Invariant Manifolds, Springer-Verlag. Hornik, K., Stinchcombe,
M. & White, H. (1990), ‘Universal approximation of an unknown mapping
and its deriva- tives using multilayer feedforward networks’, Neural
Networks 3(5), 551 – 560. Huang, G.-B., Zhu, Q.-Y. & Siew, C.-K. (2006),
‘Extreme learning machine: Theory and applications’, Neurocomput- ing
70(1), 489 – 501. Neural Networks. Huke, J. P. (2006), ‘Embedding
nonlinear dynamical systems: A guide to Takens’ theorem’. Ilies, I.,
Jaeger, H., Kosuchinas, O., Rincon, M., Sakenas, V. & Vaskevicius, N.
(2007), ‘Stepping forward through echoes of the past: forecasting with
echo state networks’. Jaeger, H. (2001), ‘The “echo state” approach to
analysing and training recurrent neural networks’. Jaeger, H. & Haas, H.
(2004), ‘Harnessing nonlinearity: Predicting chaotic systems and saving
energy in wireless communication’, Science 304(5667), 78–80. Kocarev, L.
& Parlitz, U. (1996), ‘Generalized synchronization, predictability, and
equivalence of unidirectionally coupled dynamical systems’, Phys.
Rev. Lett. 76, 1816–1819. Lin, X., Yang, Z. & Song, Y. (2009),
‘Short-term stock price prediction based on echo state networks’, Expert
Systems with Applications 36(3, Part 2), 7313 – 7317. Løkse, S.,
Bianchi, F. M. & Jenssen, R. (2017), ‘Training echo state networks with
regularization through dimension- ality reduction’, Cognitive
Computation 9(3), 364–378. Lorenz, E. N. (1963), ‘Deterministic
nonperiodic ﬂow’, Journal of the Atmospheric Sciences 20(2), 130–141.
Maass, W., Natschläger, T. & Markram, H. (2002), ‘Real-time computing
without stable states: A new framework for neural computation based on
perturbations’, Neural Computation 14(11), 2531–2560. Muldoon, M.,
MacKay, R., Huke, J. & Broomhead, D. (1993), ‘Topology from time
series’, Physica D: Nonlinear Phenomena 65(1), 1 – 16. Pathak, J., Lu,
Z., Hunt, B. R., Girvan, M. & Ott, E. (2017), ‘Using machine learning to
replicate chaotic attractors and calculate lyapunov exponents from
data’, Chaos 27. Plöger, P. G., Arghir, A., Günther, T. & Hosseiny, R.
(2004), Echo state networks for mobile robot modeling and control, in D.
Polani, B. Browning, A. Bonarini & K. Yoshida, eds, ‘RoboCup 2003: Robot
Soccer World Cup VII’, Springer Berlin Heidelberg, Berlin, Heidelberg,
pp. 157–168. Sauer, T., Yorke, J. A. & Casdagli, M. (1991),
‘Embedology’, Journal of Statistical Physics 65(3), 579–616. Schrauwen,
B., Verstraeten, D. & Van Campenhout, J. (2007), An overview of
reservoir computing: theory, appli- cations and implementations, in
‘Proceedings of the 15th European Symposium on Artiﬁcial Neural
Networks. p. 471-482 2007’, pp. 471–482. Shi, Z. & Han, M. (2007),
‘Support vector echo-state machine for chaotic time-series prediction’,
IEEE Transactions on Neural Networks 18(2), 359–372. Skowronski, M. D. &
Harris, J. G. (2007), ‘Automatic speech recognition using a predictive
echo state network classi- ﬁer’, Neural Networks 20(3), 414 – 423. Echo
State Networks and Liquid State Machines. Sprott, J. C. (2003), Chaos
and time-series analysis, Oxford University Press. Takens, F. (1981),
‘Detecting strange attractors in turbulence’, Lecture Notes in
Mathematics, Berlin Springer Verlag 898, 366. Tanaka, G., Yamane, T.,
Héroux, J. B., Nakane, R., Kanazawa, N., Takeda, S., Numata, H., Nakano,
D. & Hirose, A. (2019), ‘Recent advances in physical reservoir
computing: A review’, Neural Networks 115, 100 – 123. Tausz, A.,
Vejdemo-Johansson, M. & Adams, H. (2014), JavaPlex: A research software
package for persistent (co)homology, in H. Hong & C. Yap, eds,
‘Proceedings of ICMS 2014’, Lecture Notes in Computer Science 8592,
pp. 129–136. Tong, M. H., Bickett, A. D., Christiansen, E. M. &
Cottrell, G. W. (2007), ‘Learning grammatical structure with echo state
networks’, Neural Networks 20(3), 424 – 432. Echo State Networks and
Liquid State Machines. Tralie, C., Saul, N. & Bar-On, R. (2018),
‘Ripser.py: A lean persistent homology library for python’, The Journal
of Open Source Software 3(29), 925. 23 Vlachas, P. R., Pathak, J., Hunt,
B. R., Sapsis, T. P., Girvan, M., Ott, E. & Koumoutsakos, P. (2019),
‘Forecasting of spatio-temporal chaotic dynamics with recurrent neural
networks: a comparative study of reservoir computing and backpropagation
algorithms’, arXiv:1910.05266 . Warner, F. W. (1971), Foundations of
Differentiable Manifolds and Lie Groups, Scott, Foresman and Co.
Whitney, H. (1944), ‘The self-intersections of a smooth n-manifold in
2n-space’, The Annals of Mathematics 45(2), 220–246. Williams, R. F.
(1979), ‘The structure of lorenz attractors’, Publications Mathématiques
de l’IHÉS 50, 73–99. Xi, J., Shi, Z. & Han, M. (2005), Analyzing the
state space property of echo state networks for chaotic system
prediction, in ‘Proceedings. 2005 IEEE International Joint Conference on
Neural Networks, 2005.’, Vol. 3, pp. 1412– 1417 vol. 3. Yeo, K. (2019),
‘Data-driven reconstruction of nonlinear dynamics from sparse
observation’, Journal of Computational Physics 395, 671 – 689. Yong
Song, Yibin Li, Qun Wang & Caihong Li (2010), Multi-steps prediction of
chaotic time series based on echo state network, in ‘2010 IEEE Fifth
International Conference on Bio-Inspired Computing: Theories and
Applications (BIC-TA)’, pp. 669–672. Yperman, J. & Becker, T. (2016),
‘Bayesian optimization of hyper-parameters in reservoir computing’,
arXiv:1611.05193 . 24
