arXiv:1908.05202v2 [nlin.CD] 18 May 2020 EMBEDDING AND APPROXIMATION
THEOREMS FOR ECHO STATE NETWORKS NEURAL NETWORKS Allen Hart Department
of Mathematical Sciences University of Bath Bath BA2 7AY, UK
a.hart@bath.ac.uk James Hook Department of Mathematical Sciences
University of Bath Bath BA2 7AY, UK j.l.hook@bath.ac.uk Jonathan Dawes
Department of Mathematical Sciences University of Bath Bath BA2 7AY, UK
j.h.p.dawes@bath.ac.uk May 19, 2020 ABSTRACT Echo State Networks (ESNs)
are a class of single-layer recurrent neural networks that have enjoyed
recent attention. In this paper we prove that a suitable ESN, trained on
a series of measurements of an invertible dynamical system, induces a C1
map from the dynamical systemâ€™s phase space to the ESNâ€™s reservoir
space. We call this the Echo State Map. We then prove that the Echo
State Map is generically an embedding with positive probability. Under
additional mild assumptions, we further conjecture that the Echo State
Map is almost surely an embedding. For sufï¬ciently large, and specially
structured, but still randomly generated ESNs, we prove that there
exists a linear readout layer that allows the ESN to predict the next
observation of a dynamical system arbitrarily well. Consequently, if the
dynamical system under observation is structurally stable then the
trained ESN will exhibit dynamics that are topologically conjugate to
the future behaviour of the observed dynamical system. Our theoretical
results connect the theory of ESNs to the delay-embedding literature for
dynami- cal systems, and are supported by numerical evidence from
simulations of the traditional Lorenz equations. The simulations conï¬rm
that, from a one dimensional observation function, an ESN can accurately
infer a range of geometric and topological features of the dynamics such
as the eigenval- ues of equilibrium points, Lyapunov exponents and
homology groups. Keywords Reservoir computing; liquid state machine;
time series analysis; Lorenz equations; dynamical system; delay embed-
ding; Persistent Homology; recurrent neural networks. 1 Introduction An
Echo State Network (ESN) is a single-layer recurrent neural network
composed of a trainable readout layer con- nected to a reservoir of
randomly initialized, and randomly coupled, untrainable â€˜neuronsâ€™. This
architecture has been investigated and used by many authors since the
seminal papers by Jaeger (2001) and Maass et al.Â (2002). Tanaka et al.
(2019) present a review of ESNs, among other recurrent neural network
models, under the umbrella term reservoir computing. The wide range of
problems to which the ESN framework has been applied include speech
recognition (Skowronski & Harris 2007), learning grammatical structure
(Tong et al.Â 2007), and ï¬nancial time series prediction (Ilies et
al.Â 2007), (Lin et al.Â 2009). Several authors including GÃ¼rel & Egert
(2010) have also discussed how the ESN is a plausible model for the
information processing performed by biological neurons. Most
ambitiously, PlÃ¶ger et al. (2004) discuss ESNs in the context of
building by 2050, a team of fully autonomous humanoid robots to beat the
human winning team of the FIFA Soccer World Cup. The ESN has associated
to it a reservoir state denoted rk âˆˆRn at time k. The structure of the
recurrent layer is described by an n Ã— n matrix A that is the weighted
adjacency matrix of the system of n â€˜neuronsâ€™. If neuron i is not
connected to neuron j then Aij = 0, and if they are connected with some
weight a âˆˆR then Aij = a. Connections need not be symmetric, so in
general Aij Ì¸= Aji. Typically, A is sparse and has approximately 1% of
its entries non-zero. Connection weights are usually i.i.d. random
variables, and typically are chosen to be either uniformly distributed
on a ï¬xed interval, or Gaussian. The ESN also contains an (n Ã— m) input
matrix W in, where m is the dimension of the training data. Like the
reservoir A, W in is also populated with i.i.d random variables.
Finally, the ESN has an activation function Ï• : Rn â†’Rn, for which there
are several standard choices, for example tanh (performed
component-wise). The operation of the ESN is divided into two phases: an
initial training phase, followed by an autonomous phase. During the
training phase, the ESN is trained on a given input time series denoted
by vectors u0, u1, u2 â€¦ uK each in Rm. We will assume in this paper that
the input sequence is bounded, though we note the recent work of
Grigoryeva & Ortega (2019) establishes a framework that encompasses
unbounded input sequences as well. We will also assume in this paper
that m = 1, so that we consider a scalar input time series. The
reservoir state at time k is deï¬ned by choosing an initial state e.g.Â r1
= (0, 0, â€¦, 0)âŠ¤and deï¬ning subsequent states recursively by rk+1 = Ï•(Ark
+ W inuk). Having computed the the new reservoir states r1, r2â€¦rK, the
output matrix W out is ï¬tted to solve the optimisation problem min W out
K X k=1 âˆ¥W outrk âˆ’akâˆ¥2 + Î»âˆ¥W outâˆ¥2 2, where ak is some known target
sequence we want the ESN to mimic, often taken to be equal to the input
sequence uk, and Î» > 0 is a regularisation parameter. Minimisation
problems of this kind are often referred to as ridge regression, or
Tikhonov, or L2 regularisation. Having trained the output matrix W out
the reservoir states sk can then be liberated from their reliance on the
driving input uk and evolve under the autonomous dynamical system deï¬ned
by vk+1 = W outsk, sk+1 = Ï•(Ask + W invk+1), where s0 = rK. If the
training has been successful, then the trained ESN should provide good
predictions of the future time series v1 â‰ˆuK+1, v2 â‰ˆuK+2, etc, and
future evolution of the reservoir state s1 â‰ˆrK+1, s2 â‰ˆrK+2 etc. The
viewpoint we take here clearly distinguishes between the training phase
of the ESN where it is an externally-driven dynamical system, and the
â€˜testâ€™ phase where we consider it as an autonomous dynamical system in
Rn. In complete generality the process deï¬ning uk could be anything,
including a realisation of a random process. However, importantly,
throughout this paper, we will restrict our attention and assume that
u0, u1, u2, . . . are one dimensional observations of an invertible
discrete-time dynamical system with evolution operator Ï† âˆˆDiff1(M)
observed via a function Ï‰ âˆˆC1(M, R) on a compact manifold M. In
particular u0 = Ï‰(x), u1 = Ï‰ â—¦Ï†(x), u2 = Ï‰ â—¦Ï†2(x), u3 = Ï‰ â—¦Ï†3(x), etc.
The model we have in mind is that Ï† is the evolution operator for a time
âˆ†t of a set of Lipschitz ordinary differential equations on M.
Illustrations of the training and autonomous phases are shown in Figure
1. The idea to draw training data from a dynamical system was by Jaeger
& Haas (2004) who drew observations from a trajectory through the
Mackey-Glass attractor. We were attracted to the idea by a recent paper
by Pathak et al.Â (2017) who trained ESNs on the Lorenz equations and the
Kuramotoâ€“Sivashinsky equation (in one spatial dimension). In 2 (a)
Training phase W in Ï‰ A (b) Autonomous phase W in A W out Figure 1: (a)
During the training phase the ESN observes a dynamical system via the
function Ï‰ âˆˆC1(M, R); this sequence of observations is distributed into
the nodes in the reservoir r by the linear map W in. (b) After training,
in the autonomous phase, the driving is replaced by the output created
by the best-ï¬t linear map W out. These images were produced using the
TikZ-network package developed by Hackl (2018). 3 particular, we
conjecture that under the right technical conditions an ESN with random
reservoir matrix and input matrix trained on a one dimensional
observation of a dynamical system will embed the system into the
reservoir space almost surely. We call this the ESN Embedding Conjecture
(Conjecture 2.3.4). We believe this conjecture is true as a consequence
of Takens (1981) theorem stating that a generic delay observation map is
an embedding. This connection between Takensâ€™ delay embedding theorem
and the ESN was remarked on by Jaeger (2001) and has been discussed in
several later works including by Xi et al.Â (2005), Schrauwen et
al.Â (2007), Shi & Han (2007), Yong Song et al.Â (2010), LÃ¸kse et
al.Â (2017), Yeo (2019), and Vlachas et al.Â (2019). We go on to prove
that our statement of the ESN Embedding Conjecture holds with
probability Î± > 0. We ï¬nally prove that when the ESN does successfully
embed a structurally stable dynamical system into its reservoir, there
exists a trainable readout layer such that the autonomous phase of the
ESN will adopt the topology of the driving dynamical system. We call
this the ESN Approximation Theorem (Theorem 2.4.13). This theorem
complements the results of Grigoryeva & Ortega (2018) and Gonon et
al.Â (2020) stating that the ESN (with tunable and randomly initialised A
and b respectively) is a universal approximator of discrete-time fading
memory ï¬lters. To demonstrate the theory we present numerical evidence
that an ESN trained on a numerically integrated trajectory of the Lorenz
system can replicate several of the Lorenz systemâ€™s geometric and
topological properties. In particular, we computed the Lyapunov
exponents of the ESN autonomous phase and compared them to the known
exponents of the Lorenz system. We also compared the eigenvalues of the
system linearisation on the Lorenz systemâ€™s ï¬xed points to the
eigenvalues of the linearisation on the ï¬xed points belonging to the ESN
autonomous phase. Finally we compared the homology of the driven and
autonomous reservoir attractors to the Lorenz attractor using persistent
homology. For the reader unfamiliar with persistent homology Ghrist
(2008) offers an excellent primer. The remainder of the paper is set out
as follows. In section 2 we present basic deï¬nitions and deï¬ne a family
of maps that captures the effect on the reservoir state of training with
increasing amounts of data. In section 2.2 we prove that the family
converges to a C1 map that we call the Echo State Map. We conjecture in
section 2.3 that generically the Echo State Map is an embedding. In
section 2.4 we prove an ESN Approximation Theorem that guarantees that
the autonomous dynamics of the ESN is (in a suitable sense) conjugate
via a diffeomorphism to the original dynamical system on which the ESN
was trained. in section 3 we present numerical results supporting the
theory. 2 Theory of ESNs Our analysis makes use of several different
norms. In particular, if x âˆˆRm is a vector then âˆ¥xâˆ¥is the Euclidean
norm, and for A a matrix then âˆ¥Aâˆ¥2 is the matrix 2 norm. If f is a real
valued function, then âˆ¥fâˆ¥âˆwill denote the supremum norm and if f is
continuously differentiable then we will use the C1 norm âˆ¥fâˆ¥C1 deï¬ned by
âˆ¥fâˆ¥C1 := âˆ¥fâˆ¥âˆ+ âˆ¥Dfâˆ¥âˆ where D is the derivative operator. 2.1 The Echo
State Network We begin our summary of the background to Echo State
Networks (ESNs) with a deï¬nition. Deï¬nition 2.1.1. (Echo State Network)
Let the activation function Ïƒ be a function Ïƒ âˆˆC1(R, (âˆ’1, 1)) that has
its derivative take values in the range (0, 1). Let n âˆˆN, A be a real n
Ã— n matrix, and W in a real n Ã— 1 matrix. Let bi âˆˆR âˆ€i âˆˆ{1, â€¦, n}. Let
In := [âˆ’1, 1]n and deï¬ne the function Ï• : Rn â†’In component-wise by Ï•i(r)
= Ïƒ(ri + bi) âˆ€i âˆˆ{1, â€¦, n}. (1) We then deï¬ne an Echo State Network
(ESN) of size n to be the triple (Ï•, A, W in). Ïƒ is often chosen to be
the hyperbolic function tanh, though other choices of activation
function abound in the machine learning literature. The conditions on
these functions are sometimes less restrictive than those imposed above
on Ïƒ; other common choices of activation function include the linear
unit (also known as the identity map) and the rectiï¬ed linear unit
(often abbreviated relu) deï¬ned by relu(ri) = ri if ri > 0 0 otherwise.
Glorot et al.Â (2011) discuss how Recurrent Neural Networks supported by
a relu activation function are less prone to the vanishing gradient
problem than sigmoidal activation functions. More exotic activation
functions include radial basis functions, which take the shape of bell
curves. Throughout this paper however, we will restrict ourselves to
activation functions as deï¬ned above, i.e.Â functions Ïƒ âˆˆC1(R, (âˆ’1, 1))
whoâ€™s derivatives take values in (0, 1). 4 2.2 The Echo State Map We
will begin by introducing a family of functions that describe the
mapping between this time series of observations and the reservoir
state; this will be of fundamental importance throughout the remainder
of the paper. Deï¬nition 2.2.1. (Echo State Family) Let M be a compact
m-manifold and n âˆˆN. Let A be an n Ã— n, and let W in an n Ã— 1 matrix:
let the triple (Ï•, A, W in) be an ESN. Let the discrete dynamical system
be Ï† âˆˆDiff1(M) and let the observation function Ï‰ âˆˆC1(M, R). Let the
family of functions F = {f r0 k : M â†’In : r0 âˆˆIn, k âˆˆN0} be deï¬ned as
follows: f r0 0 (x) = r0 f r0 k+1(x) = Ï•(Af r0 k â—¦Ï†âˆ’1(x) + W inÏ‰(x)). We
call the set of functions F the Echo State Family. To provide some
intuition as to where this family came from, we observe that f r0 k is
the function that takes a point x âˆˆM and ï¬rst applies the inverse
evolution operator k times, yielding the past state Ï†âˆ’k(x) of the
dynamical system. A list of k + 1 observations Ï‰ â—¦Ï†âˆ’k(x), Ï‰ â—¦Ï†1âˆ’k(x), Ï‰
â—¦Ï†2âˆ’k(x), â€¦ are then obtained, in sequence, forward from this point. An
ESN with initial reservoir state r0 is trained on this list of inputs,
and its reservoir state is then exactly given by the value of f r0 k
(x). The function f r0 k is therefore the map induced by k + 1 steps of
the training phase of the ESN, i.e.Â it sends a point x âˆˆM to reservoir
space In according to its one dimensional history. Our plan for the
upcoming section is to show that for any r0 âˆˆIn f r0 := lim kâ†’âˆf r0 k
exists, and that f r0 = f s0 =: f for any r0, s0 âˆˆIn. We will call f the
Echo State Map, and show it is continuously differentiable, i.e.Â f
âˆˆC1(M, Rn). These results will appear together and called the Echo State
Mapping Theorem. Equivalently, we could say the Echo State Map f is the
unique C1 generalized synchronisation (in the sense described by Kocarev
& Parlitz (1996) ) between a pair of unidirectionally coupled systems,
the dynamics given by Ï† and the driven ESN phase. We will further
conjecture that f is a C1 embedding almost surely, and therefore (almost
surely) it is a topology- preserving map from the manifold M to the
reservoir space In. We will call this the ESN Embedding Conjecture, and
go on to prove a partial result that f is a C1 embedding with positive
probability. Theorem 2.2.2. (Echo State Mapping Theorem) With the
notation and hypotheses of Deï¬nition 2.2.1, and the further assumption
that âˆ¥Aâˆ¥2 < min(1, 1/âˆ¥DÏ†âˆ’1âˆ¥âˆ), there exists a unique solution f âˆˆC1(M,
Rn) of the equation f = Ï•(Af â—¦Ï†âˆ’1 + W inÏ‰) such that for all r0 âˆˆIn the
sequence f r0 k converges in the C1 topology to f as k â†’âˆ. We call f the
Echo State Map. Proof. Let Ëœ Î¨ : C1(M, Rn) â†’C1(M, Rn) be deï¬ned by Ëœ
Î¨(f) = Ï•(Af â—¦Ï†âˆ’1 + W inÏ‰) then we can see that f r0 k = Ëœ Î¨(f r0 kâˆ’1) =
Ëœ Î¨k(f r0 0 ). Now, we will show that Ëœ Î¨ is a contraction mapping and
therefore has a unique ï¬xed point f âˆˆC1(M, Rn) by the contraction
mapping theorem (Banach 1922). This will complete the proof. âˆ¥Ëœ Î¨(f) âˆ’Ëœ
Î¨(g)âˆ¥C1 = âˆ¥Ï•(Af â—¦Ï†âˆ’1 + W inÏ‰) âˆ’Ï•(Ag â—¦Ï†âˆ’1 + W inÏ‰)âˆ¥C1 â‰¤ âˆ¥Af â—¦Ï†âˆ’1 + W inÏ‰
âˆ’Af â—¦Ï†âˆ’1 âˆ’W inÏ‰âˆ¥C1 because Ï• is contracting in C1 = âˆ¥A(f â—¦Ï†âˆ’1 âˆ’g
â—¦Ï†âˆ’1)âˆ¥C1 â‰¤ âˆ¥Aâˆ¥2âˆ¥f â—¦Ï†âˆ’1 âˆ’g â—¦Ï†âˆ’1âˆ¥C1 = âˆ¥Aâˆ¥2(âˆ¥f â—¦Ï†âˆ’1 âˆ’g â—¦Ï†âˆ’1âˆ¥âˆ+ âˆ¥Df â—¦Ï†âˆ’1DÏ†âˆ’1
âˆ’Dg â—¦Ï†âˆ’1DÏ†âˆ’1âˆ¥âˆ) â‰¤ âˆ¥Aâˆ¥2(âˆ¥f â—¦Ï†âˆ’1 âˆ’g â—¦Ï†âˆ’1âˆ¥âˆ+ âˆ¥DÏ†âˆ’1âˆ¥âˆâˆ¥Df â—¦Ï†âˆ’1 âˆ’Dg â—¦Ï†âˆ’1âˆ¥âˆ) â‰¤
âˆ¥Aâˆ¥2 max(1, âˆ¥DÏ†âˆ’1âˆ¥âˆ)âˆ¥f âˆ’gâˆ¥C1 and âˆ¥Aâˆ¥2 max(1, âˆ¥DÏ†âˆ’1âˆ¥âˆ) < 1, so we have
that Ëœ Î¨ is contracting. 5 We remark here that if Ï† is obtained by the
discretisation of a continuous time ï¬‚ow with a small time step, the
evolution operator Ï† is close to the identity map, so âˆ¥DÏ†âˆ’1âˆ¥âˆis close to
1. Consequently, the condition âˆ¥Aâˆ¥2 < min(1, 1/âˆ¥DÏ†âˆ’1âˆ¥âˆ) is not much more
restrictive than enforcing âˆ¥Aâˆ¥2 < 1. 2.3 The ESN Embedding Theorem In
this section we will discuss the conditions under which the Echo State
Map f âˆˆC1(M, Rn) is a C1 embedding (i.e.Â an injective immersion whose
domain and image are diffeomorphic). We will also conjecture that for a
generic observation function Ï‰ and random matrices A and W in, the Echo
State Map f is a C1 embedding almost surely. To set the scene for these
results, we ï¬rst recall Whitneyâ€™s Weak Embedding Theorem and Takensâ€™
Theorem for delay observation maps. Theorem 2.3.1. (Whitneyâ€™s Weak
Embedding Theorem) Let M be a compact m-manifold and choose n âˆˆN such
that n > 2m. Then the set of Cr embeddings is generic in Cr(M, Rn) with
respect to the Whitney C1 topology (This is the topology on C1(M, Rn)
induced by the C1-norm). Proof. Whitney (1944). Corollary 2.3.2. Let M
be a compact m-manifold and n âˆˆN such that n > 2m. Let A be an n Ã— n
matrix for which âˆ¥Aâˆ¥2 < min(1/âˆ¥DÏ†âˆ’1âˆ¥âˆ, 1). Let W in be an n Ã— 1 matrix,
and let the triple (Ï•, A, W in) be an ESN. As usual, let Ï† âˆˆDiff1(M),
and Ï‰ âˆˆC1(M, R). If n > 2m, then the ESM f âˆˆC1(M, Rn) is a limit point
in the Whitney C1 topology of the set of C1 embeddings. Proof. f âˆˆC1(M,
Rn) by Theorem 2.2.2 so, by the Weak Whitney Embedding Theorem (Theorem
2.3.1), f is a limit point of the C1 embeddings with respect to the
Whitney C1 topology. From Corollary 2.3.2 it is clear that the Echo
State Map f is always close to an embedding, but this says nothing about
necessary or sufï¬cient conditions for f to actually be an embedding. In
fact f may never actually be an embedding. That said, since embeddings
are generic in the space C1(M, Rn) we expect heuristically that a
function in C1(M, Rn) that is assembled without explicitly desiring that
it is not an embedding, is overwhelmingly likely actually to be an
embedding. This suggests (heuristically) that a generic Echo State Map f
is indeed an embedding. The ï¬rst step we take toward proving this is to
introduce Takensâ€™ Theorem. Theorem 2.3.3. (Hukeâ€™s Formulation of Takensâ€™
Theorem) Let M be a compact manifold of dimension m. Suppose Ï† âˆˆDiff2(M)
has the following two properties: (1) Ï† has only ï¬nitely many periodic
points with periods less than or equal to 2m. (2) If x âˆˆM is any
periodic point with period k < 2m then the eigenvalues of the derivative
DÏ†k at x are distinct. Then for a generic C2 observation function Ï‰
âˆˆC2(M, R) the (2m+1) delay observation map Î¦(Ï†,Ï‰) : M â†’R2m+1 deï¬ned by
Î¦(Ï†,Ï‰)(x) := (Ï‰(x), Ï‰ â—¦Ï†(x), Ï‰ â—¦Ï†2(x), â€¦, Ï‰ â—¦Ï†2m(x)) is a C1 embedding.
Proof. Huke (2006). Hukeâ€™s proof that Î¦(Ï†,Ï‰) is a C1 embedding for
generic Ï‰ comprises two steps. First, he shows that Î¦(Ï†,Ï‰) is a C1
embedding for an open subset of C2 observation functions, and second, he
shows that Î¦(Ï†,Ï‰) is an embedding for a dense subset of all C2
observation functions. The ï¬rst step (to prove openness) is fairly
simple while the second (the proof of density) is long and delicate. A
brief summary of the density part of the proof is as follows. An
arbitrary C2 observation function Ï‰ is carefully perturbed on each open
set in a cover of the manifold M such that Ï‰ becomes immersive on each
set. The observation function Ï‰ is then perturbed again on each open set
in the cover in order to make Ï‰ injective, with care taken to ensure Ï‰
remains immersive on each open set. This procedure is applied separately
to open sets which contain periodic points and open sets that do not. We
believe it is possible to build on this result and modify the proof of
Hukeâ€™s Theorem in order to prove an ESN Embedding Conjecture in the form
that we now state. 6 Conjecture 2.3.4. (ESN Embedding Conjecture) Let M
be a compact m-manifold and n âˆˆN such that n > 2m. Let A be an n Ã— n
matrix with âˆ¥Aâˆ¥2 < min(1/âˆ¥DÏ†âˆ’1âˆ¥âˆ, 1), and W in a n Ã— 1 matrix, and let
the triple (Ï•, A, W in) be an ESN. Let Ï‰ âˆˆC2(M, R) and let Ï† âˆˆDiff2(M)
(and possibly requiring additional properties), and let A, W in be
generic matrices in the topology induced by the matrix 2-norm. Then the
Echo State Map f âˆˆC1(M, Rn) is a C1 embedding. We now summarise our
partial success towards proving this conjecture. In particular we can
establish the properties analogous to the ï¬rst part of Hukeâ€™s proof of
Takensâ€™ Theorem: we will show that the set of triples (A, W in, Ï‰) of
reservoir matrix, input matrix, and observation function for which f is
a C1 embedding, is open and non-empty. Consequently, for a generic
observation function Ï‰, and matrices A and W in drawn from a
distribution with full support (if the pdf is well deï¬ned, it is greater
than 0 over its domain), f is a C1 embedding with probability Î± > 0. To
prove the full ESN Embedding Conjecture, all that remains is to show
that the triples (A, W in, Ï‰) for which f is an embedding are dense in
the space of admissible triples, but this is no easy task, so we will be
satisï¬ed here with the proof of only openness and non-emptiness. Lemma
2.3.5. Let M be a compact m-manifold and n âˆˆN. Let A be an n Ã— n matrix,
and suppose that âˆ¥Aâˆ¥2 < min(1/âˆ¥DÏ†âˆ’1âˆ¥âˆ, 1). As usual let W in a n Ã— 1
matrix, let the triple (Ï•, A, W in) be an ESN, Ï† âˆˆDiff1(M) and Ï‰ âˆˆC1(M,
R). Deï¬ne the set â„¦:= {(A, W in, Ï‰) | fA,W in,Ï‰ is a C1 embedding.}.
Then the set â„¦is open in the C1 topology. Proof. First we deï¬ne the map
Î¨ that associates the ESM f to the triple (A, W in, Ï‰); let Î¨ : (A, W
in, Ï‰) â†’ C1(M, Rn) be deï¬ned by Î¨(A, W in, Ï‰) = fA,W in,Ï‰. We now argue
as follows. Since C1 embeddings form an open subset of C1(M, R), and the
inverse image of a continuous map is open, it sufï¬ces to show that Î¨ is
continuous in order to then conclude that â„¦is open. To show continuity
of Î¨ we must prove that if (An, W in n , Ï‰n)nâˆˆN â†’(A, W in, Ï‰) then
âˆ¥Î¨(An, W in n , Ï‰n) âˆ’Î¨(A, W in, Ï‰)âˆ¥C1 â†’0. To lighten the notation we
will write f for fA,W in,Ï‰ and fn for fAn,W in n,Ï‰n. As a preliminary
result we estimate as follows: âˆ¥Anfn â—¦Ï†âˆ’1 âˆ’Af â—¦Ï†âˆ’1âˆ¥C1 = âˆ¥Anfn â—¦Ï†âˆ’1 âˆ’Af
â—¦Ï†âˆ’1âˆ¥âˆ + âˆ¥AnDfn â—¦Ï†âˆ’1DÏ†âˆ’1 âˆ’ADf â—¦Ï†âˆ’1DÏ†âˆ’1âˆ¥âˆ (2) by deï¬nition of the C1 norm
â‰¤âˆ¥Anfn â—¦Ï†âˆ’1 âˆ’Af â—¦Ï†âˆ’1âˆ¥âˆ+ âˆ¥DÏ†âˆ’1âˆ¥âˆâˆ¥AnDfn â—¦Ï†âˆ’1 âˆ’ADf â—¦Ï†âˆ’1âˆ¥âˆ â‰¤âˆ¥Anfn âˆ’Afâˆ¥âˆ+
âˆ¥DÏ†âˆ’1âˆ¥âˆâˆ¥AnDfn âˆ’ADfâˆ¥âˆ â‰¤max(1, âˆ¥DÏ†âˆ’1âˆ¥âˆ)(âˆ¥Anfn âˆ’Afâˆ¥âˆ+ âˆ¥AnDfn âˆ’ADfâˆ¥âˆ)
â‰¤max(1, âˆ¥DÏ†âˆ’1âˆ¥âˆ)âˆ¥Anfn âˆ’Afâˆ¥C1 = Ï„âˆ¥Anfn âˆ’Afâˆ¥C1 (3) where we have deï¬ned Ï„
:= max(1, âˆ¥DÏ†âˆ’1âˆ¥âˆ). We will prove one more preliminary result: that
âˆ¥fnâˆ¥C1 is bounded. We can see that âˆ¥fnâˆ¥âˆis bounded by boundedness of Ï•
so all that remains is to bound âˆ¥Dfnâˆ¥âˆ. Since fn = Ï•(Anfn â—¦Ï†âˆ’1 + W in n
Ï‰n) we compute directly that Dfn = DÏ•(Anfn â—¦Ï†âˆ’1W in n Ï‰n)(AnDfn â—¦Ï†âˆ’1DÏ†âˆ’1
+ W in n DÏ‰n) from which we can estimate that âˆ¥Dfnâˆ¥âˆ = âˆ¥DÏ•(Anfn â—¦Ï†âˆ’1W in
n Ï‰n)(AnDfn â—¦Ï†âˆ’1DÏ†âˆ’1 + W in n DÏ‰n)âˆ¥âˆ â‰¤ âˆ¥AnDfn â—¦Ï†âˆ’1DÏ†âˆ’1 + W in n DÏ‰nâˆ¥âˆ â‰¤
âˆ¥Aâˆ¥2âˆ¥Dfn â—¦Ï†âˆ’1âˆ¥âˆâˆ¥DÏ†âˆ’1âˆ¥âˆ+ âˆ¥W in n Ï‰nâˆ¥âˆ < Â¯ Ïâˆ¥Dfn â—¦Ï†âˆ’1âˆ¥âˆâˆ¥DÏ†âˆ’1âˆ¥âˆ+ âˆ¥W in n
DÏ‰nâˆ¥âˆwhere Â¯ Ï = sup nâˆˆN âˆ¥Anâˆ¥2 < 1 = Â¯ Ïâˆ¥Dfnâˆ¥âˆâˆ¥DÏ†âˆ’1âˆ¥âˆ+ âˆ¥W in n DÏ‰nâˆ¥âˆ < Â¯
Ïâˆ¥Dfnâˆ¥âˆâˆ¥DÏ†âˆ’1âˆ¥âˆ+ Î½ 7 where Î½ is a bound for the sequence âˆ¥W in n DÏ‰nâˆ¥âˆ,
which we know exists because âˆ¥W in n DÏ‰nâˆ¥âˆconverges. Now upon
rearrangement âˆ¥Dfnâˆ¥âˆ< Î½ 1 âˆ’Â¯ Ïâˆ¥DÏ†âˆ’1âˆ¥âˆ , hence we have bounded âˆ¥Dfnâˆ¥âˆand
âˆ¥fnâˆ¥âˆthus we have a bound for âˆ¥fnâˆ¥C1, which we will call Âµ. Now, for all
Ç« > 0 there exists n âˆˆN such that both âˆ¥An âˆ’Aâˆ¥2 < Ç«(1 âˆ’Ï„âˆ¥Aâˆ¥2) 2Ï„Âµ (4)
and âˆ¥W in n Ï‰n âˆ’W inÏ‰âˆ¥C1 < Ç«(1 âˆ’Ï„âˆ¥Aâˆ¥2) 2 . (5) Armed with these
estimates we can now compute that âˆ¥fn âˆ’fâˆ¥C1 = âˆ¥Ï•(Anfn â—¦Ï†âˆ’1 + W in n Ï‰n)
âˆ’Ï•(Af â—¦Ï†âˆ’1 + W inÏ‰)âˆ¥C1 by Theorem 2.2.2 â‰¤ âˆ¥Anfn â—¦Ï†âˆ’1 + W in n Ï‰n âˆ’Af
â—¦Ï†âˆ’1 âˆ’W inÏ‰âˆ¥C1 because Ï• is contracting â‰¤ âˆ¥Anfn â—¦Ï†âˆ’1 âˆ’Af â—¦Ï†âˆ’1 + W in n
Ï‰n âˆ’W inÏ‰âˆ¥C1 â‰¤ âˆ¥Anfn â—¦Ï†âˆ’1 âˆ’Af â—¦Ï†âˆ’1âˆ¥C1 + âˆ¥W in n Ï‰n âˆ’W inÏ‰âˆ¥C1 â‰¤ Ï„âˆ¥Anfn
âˆ’Afâˆ¥C1 + âˆ¥W in n Ï‰n âˆ’W inÏ‰âˆ¥C1 by equations (2)-(3) â‰¤ Ï„âˆ¥Anfn âˆ’Afn + Afn
âˆ’Afâˆ¥C1 + âˆ¥W in n Ï‰n âˆ’W inÏ‰âˆ¥C1 â‰¤ Ï„(âˆ¥Afn âˆ’Afâˆ¥C1 + âˆ¥Anfn âˆ’Afnâˆ¥C1) + âˆ¥W in n
Ï‰n âˆ’W inÏ‰âˆ¥C1 â‰¤ Ï„âˆ¥Aâˆ¥2âˆ¥fn âˆ’fâˆ¥C1 + Ï„âˆ¥fnâˆ¥C1âˆ¥An âˆ’Aâˆ¥2 + âˆ¥W in n Ï‰n âˆ’W inÏ‰âˆ¥C1 <
Ï„âˆ¥Aâˆ¥2âˆ¥fn âˆ’fâˆ¥C1 + Ï„Âµâˆ¥An âˆ’Aâˆ¥2 + âˆ¥W in n Ï‰n âˆ’W inÏ‰âˆ¥C1 < Ï„âˆ¥Aâˆ¥2âˆ¥fn âˆ’fâˆ¥C1 +
Ç«(1 âˆ’Ï„âˆ¥Aâˆ¥2) 2 + Ç«(1 âˆ’Ï„âˆ¥Aâˆ¥2) 2 by equations (4) and (5) < Ï„âˆ¥Aâˆ¥2âˆ¥fn âˆ’fâˆ¥C1
+ Ç«(1 âˆ’Ï„âˆ¥Aâˆ¥2). Hence, rearranging we see that âˆ¥fnâˆ’fâˆ¥C1(1âˆ’Ï„âˆ¥Aâˆ¥2) <
Ç«(1âˆ’Ï„âˆ¥Aâˆ¥2) which implies âˆ¥fnâˆ’fâˆ¥C1 < Ç« as required. To prove
non-emptiness we construct an explicit reservoir matrix A and input
matrix W in for which the Echo State Map f is an embedding, using a
trick borrowed from Shi & Han (2007). First, for a given observation
function Ï‰ we deï¬ne Î›Ï‰ to be the subset of matrices A and W in for which
the associated map f is a C1 embedding: Î›Ï‰ := {(A, W in) | fA,W in,Ï‰ is
a C1 embedding.} (6) Lemma 2.3.6. Let M be a compact m-manifold and n
âˆˆN. Let A be an n Ã— n matrix and suppose âˆ¥Aâˆ¥2 < min(1/âˆ¥DÏ†âˆ’1âˆ¥âˆ, 1). Let W
in be an nÃ—1 matrix and let the triple (Ï•, A, W in) be an ESN. Suppose
that Ï† âˆˆDiff2(M) has the following two properties: (1) Ï† has only
ï¬nitely many periodic points with periods less than or equal to 2m. (2)
If x âˆˆM is any periodic point with period k < 2m then the eigenvalues of
the derivative DÏ†k at x are distinct. Then for a generic Ï‰ âˆˆC2(M, R), Î›Ï‰
is non-empty. Proof. Let A = 1 2 ï£® ï£¯ ï£¯ ï£¯ ï£¯ ï£° 0 0 0 . . . 0 1 0 0 . . . 0
0 1 0 . . . 0 0 0 1 . . . 0 0 0 0 â€¦ 0 ï£¹ ï£º ï£º ï£º ï£º ï£» 8 and W in 1 = 1, W in
j = 0 for 2 â‰¤j â‰¤n.Â Then the ESM f := ï£® ï£¯ ï£¯ ï£¯ ï£¯ ï£° Ï•1 â—¦Ï‰ Ï•2 â—¦2âˆ’1Ï•1 â—¦Ï‰ â—¦Ï†âˆ’1
Ï•3 â—¦2âˆ’1Ï•2 â—¦2âˆ’1Ï•1 â—¦Ï‰ â—¦Ï†âˆ’2 . . . Ï•n â—¦21âˆ’nÏ•nâˆ’1 . . . 2âˆ’1Ï•1 â—¦Ï‰ â—¦Ï†âˆ’n+1 ï£¹ ï£º ï£º
ï£º ï£º ï£» , where Ï•i(ri) = Ïƒ(ri + bi) is the ith component function of Ï•, as
deï¬ned in (1), solves the equation f = Ï•(Af â—¦Ï†âˆ’1 + W inÏ‰). We can see
moreover that f â‰¡g â—¦Î¦(Ï†,Ï‰) where g := ï£® ï£¯ ï£¯ ï£¯ ï£¯ ï£° Ï•1 Ï•2 â—¦2âˆ’1Ï•1 Ï•3 â—¦2âˆ’2Ï•2
â—¦2âˆ’1Ï•1 . . . Ï•n â—¦21âˆ’nÏ•nâˆ’1 . . . 2âˆ’1Ï•1 ï£¹ ï£º ï£º ï£º ï£º ï£» and Î¦(Ï†,Ï‰) is the
delay observation map Î¦(Ï†,Ï‰)(x) = (Ï‰(x), Ï‰ â—¦Ï†âˆ’1(x), Ï‰ â—¦Ï†âˆ’2(x), â€¦, Ï‰
â—¦Ï†âˆ’n+1(x)). By design, each Ï•i is a C1 embedding hence g is a C1
embedding. For generic Ï‰ âˆˆC2(M, R) the delay observation map Î¦(Ï†,Ï‰) is
also a C1 embedding, thanks to Takensâ€™ Theorem. Noting that the
composition of C1 embeddings is a C1 embedding completes the proof.
Theorem 2.3.7. (Weak ESN Embedding Theorem) Let M be a compact
m-manifold and n â‰¥2m + 1. Let A be a random variable with a distribution
that has full support on the space of n Ã— n matrices for which âˆ¥Aâˆ¥2 <
min(1/âˆ¥DÏ†âˆ’1âˆ¥âˆ, 1), and let W in be a random variable with a distribution
that has full support on the space of n Ã— 1 matrices, and let the triple
(Ï•, A, W in) be an ESN. Suppose Ï† âˆˆDiff2(M) has the following two
properties: (1) Ï† has only ï¬nitely many periodic points with periods
less than or equal to 2m. (2) If x âˆˆM is any periodic point with period
k < 2m then the eigenvalues of the derivative DÏ†k at x are distinct.
Then for a generic observation function Ï‰ âˆˆC2(M, R) the Echo State Map f
is a C1 embedding with probability Î± > 0. Proof. The space of all
observation functions Ï‰ âˆˆC2(M, R) such that the delay observation map
Î¦(Ï†,Ï‰) is an embedding is generic in C2(M, R), thanks to Takensâ€™
Theorem. For any one of these observation functions, the set Î›Ï‰ deï¬ned
in (6) is non-empty by Lemma 2.3.6 and open by Lemma 2.3.5. Since A, and
W in are random variables with full support, they take values in Î› with
probability Î± > 0. Remark 2.3.8. The Embedding Conjecture and Weak ESN
Embedding Theorem state that under the right conditions f is an
embedding. In practical examples we cannot compute f exactly because it
is obtained in the limit of inï¬nitely many past observations. In
practice, if we have k observations the best we can do is to use all
available observations and compute f r0 k . Fortunately, the set of C1
embeddings is open in the C1 topology, and f r0 k converges to f in this
topology, so there exists a sufï¬ciently large number â„“of previous
observations such that for all k > â„“, f r0 k is an embedding. The ESN
Embedding Conjecture also admits a biological interpretation. Consider
an organism with a (primitive) ner- vous system (â€˜brainâ€™) comprised of
neurons. Neurons are connected to each other with random connection
weights (including zero) representing the strength of the connection (or
no connection). The adjacency matrix forms the reser- voir matrix A. The
reservoir state r is a vector representing the ï¬ring rate of every
neuron. Suppose that the organism has a sensory organ connected to the
brain which at any point in time senses a scalar measure of the
environment, for example an average environmental light intensity. The
connection weight from the sensory organ to the ith neuron is then the
ith entry of W in. Suppose that the light intensity depends on the state
of the environment which evolves 9 as a high dimensional dynamical
system. Then the nervous system and sensory organ together operate as an
ESN. Since the entries of A and W in are random variables, the ESN
Embedding Conjecture states that the dynamics of the environment are
indeed embedded into the nervous system without the nervous system
needing to possess any special structure provided by learning or natural
selection. The embedding of the natural world into the brain is obtained
â€˜for freeâ€™. This leaves cognition, deï¬ned as â€˜the art of performing
computation on our representation of the environmentâ€™, as the faculty
that requires optimisation by natural selection or learning. 2.4 The ESN
Approximation Theorem In this section we will state and prove the ESN
Approximation Theorem - that an ESN which successfully embeds a
dynamical system into the reservoir space can approximate the systemâ€™s
dynamics during the autonomous phase, hence replicate the topology of a
structurally stable dynamical system. We will use several preliminary
results introduced over the proceeding subsections. 2.4.1 The Universal
Approximation Theorem The ï¬rst major result we will use to prove the ESN
Approximation Theorem is the Universal Approximation Theorem. This
theorem is highly celebrated in the literature on mathematical analysis
of neural networks, and states that smooth functions and any number of
their derivatives can be approximated by single layer neural network
with sufï¬ciently many neurons. In this section we recall this theorem
and then present an extension suitable for ESNs, to take account of the
fact that for an ESN the neural network weights vi and biases bi are
randomly chosen but then ï¬xed; only the output weights wi can be chosen
to give a good approximation to an input function f.Â We will use the
Universal Approximation Theorem presented by Hornik et al.Â (1990),
because it concerns smooth functions and any number of their derivatives
while the earlier seminal paper by Cybenko (1989) does not. Deï¬nition
2.4.1. (â„“-ï¬nite) Let â„“âˆˆN0. Then we say an â„“-times differentiable scalar
function Ïƒ âˆˆCâ„“(R) is â„“-ï¬nite if 0 < Z R     dâ„“Ïƒ dxâ„“    dx < âˆ.
Remark 2.4.2. The activation function Ïƒ âˆˆC1(R, (âˆ’1, 1)) with derivative
in the range (0, 1) is 1-ï¬nite; meaning â„“-ï¬nite with â„“= 1. Theorem
2.4.3. (Universal Approximation Theorem) If the activation function Ïƒ is
â„“-ï¬nite, then for all 0 â‰¤m â‰¤â„“ functions g : In â†’R of the form g(x) = N X
j=1 wjÏƒ(vâŠ¤ j x + bj) are dense in Cm(In, R). Proof. Hornik et
al.Â (1990). The Universal Approximation Theorem essentially states that
if we are interested in approximating a function f to some tolerance Ç«
we can create a neural network of size N and modify the weights until
the network approximates f to the tolerance Ç«. We want to slightly
extend the theorem for our purposes. Recall that an ESN has random
reservoir weights comprising the matrix A and random input weights
comprising the matrix W in, and it is only the output connection weights
W out that are trained. We therefore want to show that for any
continuously differentiable function f and a sufï¬ciently large neural
network with random weights vi and biases bi, we can choose linear
readout weights wi such that the resulting neural network approximates f
arbitrarily well with probability arbitrarily close to 1. We will call
this the Random Universal Approximation Theorem (RUAT), and remark that
the RUAT is highly related to Theorem 2.1 appearing in the seminal paper
on Extreme Learning Machines by Huang et al.Â (2006). We can also view
the RUAT as a special case of Theorem 1 presented by Gonon et
al.Â (2020), who prove a stronger result in the more general context of
ï¬lters. The idea behind the proof of the RUAT is as follows. First we
note that there is a neural network Ë† g that approximates f by the
Universal Approximation Theorem. Then, we create sample sequences of
weights and biases vi, bi by repeated draws from appropriate random
variables. There will eventually be some randomly generated samples vj,
bj that are close to each of the weights and biases of the network Ë† g.
From this list of weights and biases in the sample sequences we select
those that match closely, and so create a neural network g, choosing
linear readout weights wi either to match the respective weight in Ë† g
or choosing to set wi = 0 in order effectively to discard those samples
vi, bi that not close 10 to values in Ë† g. Now by construction g is a
good approximation to Ë† g which is itself a good approximation to f.Â The
details are presented in the following lemma and theorem. Lemma 2.4.4.
Let (Xj)jâˆˆN be a sequence of i.i.d. random variables and S1, . . . ,
Sâ„“be a list of â„“events, and suppose that for each i (and for any j since
they are i.i.d.) there exists Î¸i such that P(Xj âˆˆSi) = Î¸i > 0. Then for
all Î± âˆˆ(0, 1) there exists N âˆˆN such that P  âˆƒinjective Ï† : {1, . . .,
â„“} â†’{1, . . . , N} : XÏ†(i) âˆˆSi, âˆ€i âˆˆ{1, . . . , â„“}  > Î±. Proof. First,
ï¬x Î± âˆˆ(0, 1). Then deï¬ne the set {n0, . . . , nâ„“} as follows. Set n0 = 0
and for any i âˆˆ{1, â€¦, â„“} let ni âˆ’niâˆ’1 := ceil log(1 âˆ’Î±1/â„“) log(1 âˆ’Î¸i) 
+ 1. Finally, set N = nâ„“. Then we can calculate that P  âˆƒinjective Ï† :
XÏ†(i) âˆˆSi âˆ€i âˆˆ{1, . . ., â„“}  > P  âˆ€i âˆˆ{1, . . . , â„“} âˆƒj âˆˆ{1 + niâˆ’1, . .
. , ni} : Xj âˆˆSi  = â„“ Y i=1 P  âˆƒj âˆˆ{1 + niâˆ’1, . . . , ni} : Xj âˆˆSi  =
â„“ Y i=1 1 âˆ’P  Xj / âˆˆSi âˆ€j âˆˆ{1 + niâˆ’1, . . . , ni}  â‰¥ â„“ Y i=1 1 âˆ’(1
âˆ’Î¸i)niâˆ’niâˆ’1 = â„“ Y i=1 1 âˆ’(1 âˆ’Î¸i)ceil  log(1âˆ’Î±1/â„“)/ log(1âˆ’Î¸i)  +1 > â„“ Y
i=1 1 âˆ’(1 âˆ’Î¸i)  log(1âˆ’Î±1/â„“)/ log(1âˆ’Î¸i)  = â„“ Y i=1 1 âˆ’exp log(1 âˆ’Î±1/â„“)
log(1 âˆ’Î¸i) log(1 âˆ’Î¸i)  = â„“ Y i=1 1 âˆ’(1 âˆ’Î±1/â„“) = â„“ Y i=1 Î±1/â„“= Î±.
Theorem 2.4.5. (Random Universal Approximation Theorem) Let In denote
the unit hypercube of dimension n and let f âˆˆC1(In, R). Let Ïƒ âˆˆC1(R) be
1-ï¬nite, and let (bj)jâˆˆN, (vj)jâˆˆN be sequences of i.i.d. random
variables with full support. Then for any Î± âˆˆ(0, 1) and Ç« > 0 there
exists some natural number N âˆˆN such with, probability greater than Î±,
there exist real numbers w1, . . . , wN âˆˆR such that the random neural
network g : In â†’R deï¬ned by g(x) = N X j=1 wjÏƒ(vâŠ¤ j x + bj) satisï¬es âˆ¥f
âˆ’gâˆ¥C1 < Ç«. Proof. First, by the Universal Approximation Theorem we know
that for any Ç« > 0 there exists a neural network Ë† g : In â†’R of size
â„“deï¬ned by Ë† g(x) = â„“ X i=1 Ë† wiÏƒ(Ë† vâŠ¤ i x + Ë† bi) 11 such that âˆ¥f âˆ’Ë†
gâˆ¥C1 < Ç« 2. (7) Now, consider two sequences of i.i.d. random variables
(bj)jâˆˆN and (vj)jâˆˆN with full support, and let Xj := (bj, vj). Fix Ç« > 0
and deï¬ne a collection of â„“events S1, â€¦, Sâ„“by Si :=  (b, v) âˆˆR Ã— Rn :
âˆ¥Ïƒ(Ë† vâŠ¤ i Â· +Ë† bi) âˆ’Ïƒ(vâŠ¤Â· +b)âˆ¥C1 < Ç« 2â„“maxk( Ë† wk)  , where the weights
Ë† wk are given by the form of the network Ë† g. Observe that each of the
Si have strictly positive measure, so there exists Î¸i > 0 such that P(Xj
âˆˆSi) > Î¸i > 0 âˆ€j âˆˆN. Hence it follows by Lemma 2.4.4 that for all Î± âˆˆ(0,
1) there exists N âˆˆN such that P  âˆƒinjective Ï† : {1, . . . , â„“} â†’{1, . .
. , N} : XÏ†(i) âˆˆSi âˆ€i âˆˆ{1, . . . , â„“}  > Î±. Now, on the event
âˆƒinjective Ï† : {1, . . ., â„“} â†’{1, . . . , N} : XÏ†(i) âˆˆSi âˆ€i âˆˆ{1, . . .,
â„“} we deï¬ne wj :=  Ë† wi if Ï†(i) = j 0 otherwise for all j âˆˆ{1, . . .,
N}, and deï¬ne the random neural network g : In â†’R by g(x) = N X j=1
wjÏƒ(vâŠ¤ j x + bj). Now observe âˆ¥Ë† g âˆ’gâˆ¥C1 =

â„“ X i=1 Ë† wiÏƒ(Ë† vâŠ¤ i Â· +Ë† bi) âˆ’ N X j=1 wjÏƒ(vâŠ¤ j Â· +bj)

C1

â„“ X i=1 Ë† wi  Ïƒ(Ë† vâŠ¤ i Â· +Ë† bi) âˆ’Ïƒ  vâŠ¤ Ï†(i) Â· +bÏ†(i) 

C1 â‰¤ â„“ X i=1 Ë† wi

 Ïƒ(Ë† vâŠ¤ i Â· +Ë† bi) âˆ’Ïƒ  vâŠ¤ Ï†(i) Â· +bÏ†(i) 

C1 < â„“ X i=1 Ë† wiÇ« 2â„“maxk( Ë† wk) < Ç« 2. Combining this with (7) and
using the triangle inequality we obtain âˆ¥f âˆ’gâˆ¥C1 â‰¤âˆ¥f âˆ’Ë† gâˆ¥C1 + âˆ¥Ë† g
âˆ’gâˆ¥C1 < Ç« 2 + Ç« 2 = Ç«, which completes the proof. 2.4.2 The ESN
Approximation Theorem In this subsection we will state and prove the ESN
Approximation Theorem which states that there exists a linear readout
layer W out giving rise to an autonomous ESN phase with a normally
hyperbolic attracting m-submanifold on which the autonomous dynamics are
topologically conjugate to a structurally stable Ï†. The idea behind the
theorem is observe that the ESN looks enough like a single layer neural
network that the Random Universal Approximation Theorem holds.
Consequently we can choose linear readout weights stored in the matrix W
out to approximate any C1 function. We will assume that f is an
embedding, and therefore invertible on its image, and choose readout
weights W out such that the autonomous ESN approximates a C1 dynamical
system possessing an m dimensional normally hyperbolic attracting
submanifold on which the dynamics approximate f â—¦Ï† â—¦f âˆ’1. We want the
manifold to be normally hyperbolic and attracting to ensure that an
autonomous trajectory that leaves the manifold by some small distance is
attracted back toward the manifold, preventing an accumulation of errors
from sending the trajectory too far away. Autonomous trajectories
originating near the manifold therefore remain near, all the while
approximating f â—¦Ï† â—¦f âˆ’1. To formalise these ideas, we will ï¬rst deï¬ne a
normally hyperbolic attracting submanifold. 12 Deï¬nition 2.4.6.
(Normally Hyperbolic Attracting Submanifold) Let Ï† âˆˆDiff1(M), then, a
Ï†-invariant submanifold Î› âŠ‚M is a normally hyperbolic attracting
submanifold if the restriction to Î› of the tangent bundle of M admits a
splitting into a direct sum of two DÏ†-invariant subbundles, the tangent
bundle of Î›, and the stable bundle Es. Furthermore, with respect to some
Riemannian metric on M, the restriction of DÏ† to Es must be a
contraction, and must be relatively neutral on T Î›. Thus, there exist
constants 0 < Î» < Âµâˆ’1 < 1 and c > 0 such that TÎ›M = T Î› âŠ•Es (DÏ†)xEs x =
Es Ï†(x) âˆ€x âˆˆÎ› âˆ¥DÏ†kvâˆ¥â‰¤cÎ»kâˆ¥vâˆ¥âˆ€v âˆˆEs, âˆ€k âˆˆN âˆ¥DÏ†kvâˆ¥â‰¤cÂµ|k|âˆ¥vâˆ¥. Before we
present the ESN Approximation Theorem itself we will prove that there
exists a C1 evolution operator Î· deï¬ned on Rd that has a normally
hyperbolic attracting submanifold on which the dynamics of Î· are
conjugate to Ï†. The existence of this map Î· is guaranteed by standard
topological machinery which we recall brieï¬‚y here, and which is
presented in detail by Warner (1971). Deï¬nition 2.4.7. (Cubic centred
chart) A chart (V, Ï•) belonging to a d-manifold is called a cubic chart
if Ï•(V ) is an open cube centred about the origin in Rd. If x âˆˆV and
Ï•(x) = 0, then the chart (V, Ï•) is centred at x. Deï¬nition 2.4.8. (Slice
coordinates) Suppose that (V, Ï•) is a chart on a d-manifold D with
coordinate functions x1, â€¦, xd and that m is an integer 0 â‰¤m â‰¤d.Â Let a
âˆˆÏ•(V ) and let S = {q âˆˆV | xi(q) = ai, i = m + 1, â€¦, d}. The subspace S
of D together with coordinate maps x|S for j = 1, â€¦, m forms a
submanifold of D, called a slice of the chart (V, Ï•). Lemma 2.4.9.
(Slice Lemma) Let M be a compact m-manifold, let f : M â†’Rd be an
immersion, and let x âˆˆM. Then there exists a cubic centred chart (V, Ï•)
about f(x) and a neighbourhood U of x such that f|U is injective and
f(U) is a slice of (V, Ï•). Proof. Warner (1971) page 28 prop 1.35. Lemma
2.4.10. Let d > m and M be a compact m-manifold. Let Ï† âˆˆDiff1(M).
Suppose f âˆˆC1(M, Rd) is a C1 embedding. Then there is an open subset
â„¦âˆˆRd and Î· âˆˆDiff1(â„¦) with f(M) a normally hyperbolic attracting
submanifold such that Î·|f(M)= f â—¦Ï† â—¦f âˆ’1 (where we have deï¬ned f âˆ’1 on
the image of f). Proof. We will make a similar argument to Warner (1971)
in the proof of his Proposition 1.36, on page 29. First let x âˆˆM. Then
by the Slice Lemma there exists a cubic centred chart (Vx, Ï•x) about
f(x) and a neighbourhood Ux of x such that f(Ux) is a slice (Vx, Ï•x).
Let x1, . . . , xm be the slice coordinates in the chart (Vx, Ï•x) of
points in f(Ux). Then we can deï¬ne a map Î·x âˆˆDiff1(Vx, Rd) applying the
map f â—¦Ï† â—¦f âˆ’1 on the slice co-ordinates and dividing the remaining
co-ordinates by 2. We can make this argument for every x âˆˆM hence deï¬ne
a collection of maps {Î·x} over a collection of open sets {Vx} which
cover f(M). Now we let {Î±j | j âˆˆN} form a partition of unity subordinate
to the cover {Vx}. We take a subsequence {Î±k} such that supp(Î±k) âˆ©f(M)
Ì¸= âˆ…and denote the collection of sets to which {Î±k} is subordinate by
{Vk}. We then deï¬ne a map Î· on a neighbourhood â„¦:= âˆªkVk of f(M) by Î· = X
k Î±kÎ·x. By construction, Î·|f(M) = f â—¦Ï† â—¦f âˆ’1 and Î· has a normally
hyperbolic attracting submanifold f(M). Not only does the dynamical
system Î· exist, but importantly, its normally hyperbolic attracting
submanifold is pre- served by any sufï¬ciently good approximation. This
is made formal in the Invariant Manifold Theorem, which we will use in
the proof of the ESN Approximation Theorem. Theorem 2.4.11. (Invariant
Manifold Theorem) Let K be a compact manifold and Î· âˆˆDiff1(K) with
normally hyperbolic attracting submanifold Î›. Then, âˆƒÇ« > 0 such that for
any u âˆˆDiff1(K) with âˆ¥Î· âˆ’uâˆ¥C1 < Ç«, the diffeomorphism u has a normally
hyperbolic attracting submanifold U such that âˆ¥U âˆ’Î›âˆ¥C1 < Ç«. Proof.
Hirsch et al.Â (1977). 13 W in Y X A Strongly recurrent Weakly recurrent
W out Figure 2: The ESN with sparsity structure imposed on A so that we
can prove the ESN Approximation Theorem. The matrix X and vector Y are
deï¬ned in the statement of the ESN Approximation Theorem. With these
preliminaries established we are ready to prove our ESN Approximation
Theorem. Our strategy involves imposing a special structure on the
reservoir matrix A in order to obtain sufï¬ciently many neurons for the
Random Universal Approximation Theorem to hold while controlling the
dimension of the codomain of the Echo State Map. The structure of A is
made clear in the statement of the ESN Approximation Theorem and
illustrated in Figure 2, where we call the connections represented by
the matrix A â€˜strongly recurrentâ€™ and those represented by X â€˜weakly
recurrentâ€™. The weakly recurrent neurons and the vector Y of inputs are
introduced in the proof of the ESN Approximation Theorem in order to
satisfy the conditions of the Random Universal Approximation Theorem.
Deï¬nition 2.4.12. (ESN autonomous phase) The ESN autonomous phase with
parameters (A, W in, W out, Ï•) is a discrete time autonomous dynamical
system Ïˆ âˆˆC1(Rn) deï¬ned by Ïˆ(s) = Ï•  (A + W inW out)s  . Theorem
2.4.13. (ESN Approximation Theorem) Let M be a compact m-manifold and n
âˆˆN such that n > 2m. Let A be an nÃ—n matrix where âˆ¥Aâˆ¥2 < min(1/âˆ¥DÏ†âˆ’1âˆ¥âˆ,
1), and W in an nÃ—1 matrix, and let the triple (Ï•, A, W in) be an ESN.
Let Ï† âˆˆDiff1(M) be structurally stable, and let Ï‰ âˆˆC1(M, R). Suppose the
Echo State Map f âˆˆC1(M, Rn) is a C1 embedding. Let (xj)jâˆˆN, (yj)jâˆˆN, and
(bj)jâˆˆN be sequences of i.i.d. Rn, R, and R-valued random variables,
respectively, with full support. Let Î± âˆˆ(0, 1). Then, with probability
Î±, there exists d âˆˆN with d > n, a d Ã— 1 matrix 14 W out, a d Ã— d matrix
Ëœ A, and a d Ã— 1 matrix Ëœ W in assembled from the n Ã— n matrix A, the (d
âˆ’n) Ã— n matrix X with jth row xj, and the (d âˆ’n) Ã— 1 matrix Y with jth
row yj, like so: Ëœ A =  A 0 X 0  and Ëœ W in =  W in Y  , and an
activation function Ëœ Ï•i(r) = Ïƒ(ri + bi) âˆ€i âˆˆ{1, â€¦, d} such that the
autonomous ESN Ïˆ âˆˆC1(Rd) with parameters ( Ëœ A, Ëœ W in, W out, Ëœ Ï•) has
a normally hyperbolic attracting submanifold on which Ïˆ is topologically
conjugate to Ï†. Proof. By assumption, the Echo State Map f deï¬ned for
the ESN (Ï•, A, W in) with respect to (Ï†, Ï‰) is an embedding, so the Echo
State Map Ëœ f deï¬ned for ( Ëœ Ï•, Ëœ A, Ëœ W in) with respect to (Ï†, Ï‰) is
also an embedding. For the remainder of the proof we will restrict the
codomain of Ëœ f to its image in order to yield a C1 diffeomorphism.
Before we proceed, we will establish some preliminary results. First we
deï¬ne y : M â†’y(M) âŠ‚Rn+1 by y1(x) = Ï‰(x) and ï£® ï£¯ ï£¯ ï£° y2(x) y3(x) . . .
yn+1(x) ï£¹ ï£º ï£º ï£»= f â—¦Ï†âˆ’1(x). Furthermore we will deï¬ne maps F : C1(M, Rd)
â†’C1( Ëœ f(M), Rd) by F(g) = g â—¦Ëœ f âˆ’1 and Y : C1(y(M), R) â†’C1(M, R) by
Y(g) = g â—¦y. Next we will show that F and Y are Lipschitz continuous. To
see that F is Lipschitz continuous observe âˆ¥F(g) âˆ’F(h)âˆ¥C1 = âˆ¥g â—¦Ëœ f âˆ’1
âˆ’h â—¦Ëœ f âˆ’1âˆ¥C1 = âˆ¥g â—¦Ëœ f âˆ’1 âˆ’h â—¦Ëœ f âˆ’1âˆ¥âˆ+ âˆ¥Dg â—¦Ëœ f âˆ’1D Ëœ f âˆ’1 âˆ’Dh â—¦Ëœ f
âˆ’1D Ëœ f âˆ’1âˆ¥âˆ â‰¤ âˆ¥g â—¦Ëœ f âˆ’1 âˆ’h â—¦Ëœ f âˆ’1âˆ¥âˆ+ âˆ¥D Ëœ f âˆ’1âˆ¥âˆâˆ¥Dg â—¦Ëœ f âˆ’1 âˆ’Dh â—¦Ëœ f
âˆ’1âˆ¥âˆ = âˆ¥g âˆ’hâˆ¥âˆ+ âˆ¥D Ëœ f âˆ’1âˆ¥âˆâˆ¥Dg âˆ’Dhâˆ¥âˆ â‰¤ max(1, âˆ¥D Ëœ f âˆ’1âˆ¥âˆ)(âˆ¥g âˆ’hâˆ¥âˆ+ âˆ¥Dg
âˆ’Dhâˆ¥âˆ) = max(1, âˆ¥D Ëœ f âˆ’1âˆ¥âˆ)âˆ¥g âˆ’hâˆ¥C1. We can make an almost identical
argument to show that Y is Lipschitz continuous. We will denote the
Lipschitz constants for F and Y by L and M respectively. We are now
ready to proceed with the proof. By Lemma 2.4.10, there exists an open
subset â„¦âˆˆRd containing Ëœ f(M) and Î· âˆˆDiff1(â„¦) with Ëœ f(M) a normally
hyperbolic attracting submanifold such that Î·| Ëœ f(M) = Ëœ f â—¦Ï† â—¦Ëœ f âˆ’1.
Now let K âŠ‚â„¦be a compact manifold containing Ëœ f(M). Normally hyperbolic
invariant submanifolds persist under small perturbations, by the
Invariant Manifold Theorem, so âˆƒÇ« > 0 such that any u âˆˆDiff1(K) which
satisï¬es âˆ¥u âˆ’ Î·|Kâˆ¥C1 < Ç« is topologically conjugate to Î·. For any given
value Î± âˆˆ(0, 1), by the Random Universal Approximation Theorem, there
exists a d âˆˆN and a d Ã— 1 matrix W out such that g âˆˆC1(Rn+1, R) deï¬ned
by g(z) = d X i=1 W out i Ïƒ   Ëœ W in Ëœ A  i z + bi  (8) satisï¬es âˆ¥g
âˆ’Ï‰ â—¦Ï† â—¦yâˆ’1âˆ¥C1< Ç« LMâˆ¥W inâˆ¥ (9) 15 M Ëœ f(M) h â—¦Ëœ f(M) M Ëœ f(M) h â—¦Ëœ f(M) Ï†
Ëœ f Î· h Ïˆ Ëœ f h Figure 3: A commuting diagram representing the ESN
Approximation Theorem where the terms are deï¬ned through- out the
theoremâ€™s proof. where [ Ëœ W in, Ëœ A]i is a 1 Ã— (n + 1) matrix with 1st
entry Ëœ W in i and (j + 1)th entry Ëœ Aij. Now âˆ¥Ïˆ| Ëœ f(M) âˆ’Î·| Ëœ f(M)âˆ¥C1 â‰¤
Lâˆ¥Ïˆ â—¦Ëœ f âˆ’Î· â—¦Ëœ fâˆ¥C1 = Lâˆ¥Ïˆ â—¦Ëœ f âˆ’Ëœ f â—¦Ï†âˆ¥C1 because Î·| Ëœ f(M) = Ëœ f â—¦Ï† â—¦Ëœ
f âˆ’1 = Lâˆ¥Ëœ Ï•( Ëœ A Ëœ f + Ëœ W inW out Ëœ f) âˆ’Ëœ Ï•( Ëœ A Ëœ f + Ëœ W inÏ‰ â—¦Ï†)âˆ¥C1
by deï¬nition of Ïˆ â‰¤ Lâˆ¥( Ëœ A Ëœ f + Ëœ W inW out Ëœ f) âˆ’( Ëœ A Ëœ f + Ëœ W inÏ‰
â—¦Ï†)âˆ¥C1 because Ëœ Ï• is contracting = Lâˆ¥( Ëœ W inW out Ëœ f) âˆ’( Ëœ W inÏ‰
â—¦Ï†)âˆ¥C1 because Ëœ A Ëœ f âˆ’Ëœ A Ëœ f = 0 â‰¤ Lâˆ¥Ëœ W inâˆ¥2âˆ¥W out Ëœ f âˆ’Ï‰ â—¦Ï†)âˆ¥C1 by
factoring out W in = Lâˆ¥Ëœ W inâˆ¥2âˆ¥W out Ëœ Ï•( Ëœ A Ëœ f â—¦Ï†âˆ’1 + Ëœ W inÏ‰) âˆ’Ï‰
â—¦Ï†âˆ¥C1 by Theorem 2.2.2 = Lâˆ¥Ëœ W inâˆ¥2

d X i=1 W out i Ïƒ   Ëœ W in Ëœ A  i y + bi  âˆ’Ï‰ â—¦Ï†

C1 by deï¬nition of Ëœ Ï• and y = Lâˆ¥Ëœ W inâˆ¥2âˆ¥g â—¦y âˆ’Ï‰ â—¦Ï†âˆ¥C1 by (8) â‰¤ LMâˆ¥Ëœ W
inâˆ¥2âˆ¥g|y(M)âˆ’Ï‰ â—¦Ï† â—¦yâˆ’1âˆ¥C1 < LMâˆ¥Ëœ W inâˆ¥2 Ç« LMâˆ¥Ëœ W inâˆ¥2 by (9) = Ç« hence
there is some open set Ëœ â„¦âŠ‚K containing Ëœ f(M) such that âˆ¥Ïˆ|Ëœ â„¦âˆ’Î·|Ëœ â„¦âˆ¥C1
< Ç« so Ïˆ|Ëœ â„¦is conjugate to Î·|Ëœ â„¦. Consequently, there exists an h
âˆˆDiff1(Ëœ â„¦) such that Ïˆ|Ëœ â„¦= h â—¦Î·|Ëœ â„¦â—¦hâˆ’1. Now Ëœ f(M) is a normally
hyperbolic attracting submanifold of Î· where Î·| Ëœ f(M)= Ëœ f â—¦Ï† â—¦Ëœ f âˆ’1
so h â—¦Ëœ f(M) is a normally hyperbolic attracting submanifold of Ïˆ on
which Ïˆ = h â—¦Î· â—¦hâˆ’1 = h â—¦Ëœ f â—¦Ï† â—¦Ëœ f âˆ’1 â—¦hâˆ’1 âˆ¼ = Ï†. Remark 2.4.14. A
consequence of the ESN Approximation Theorem is that the diagram shown
in Figure 3 commutes. 3 Numerical Experiments with ESNs In the previous
section we showed that for a given structurally stable dynamical system
and a sufï¬ciently large ESN there exists a linear output matrix W out
that gives rise to an autonomous ESN with dynamics that are
topologically conjugate to those of the given dynamical system. To test
whether these results hold in practice we took a 1D observation of a
numerically integrated trajectory of the Lorenz system, fed this into an
ESN implemented on a commercial laptop, and sought to discover whether
the autonomous phase of the ESN would adopt dynamics topologically
conjugate to the Lorenz system. In particular we computed several
topological invariants of the ESN autonomous phase including the
Lyapunov exponents, ï¬xed point eigenvalues, and homology, then compared
these to the known invariants of the Lorenz system. This work was
inspired by a paper by Pathak et al.Â (2017) who trained an ESN on a full
3D trajectory of the Lorenz system, rather than a 1D observation, and
compared the Lyapunov exponents of the autonomous phase to the known
exponents of the Lorenz 16 -20 0 -10 10 0 20 x z 10 30 20 40 50 Figure
4: A picture of the famous Lorenz attractor. Here the trajectory was
initialised at (1, 1, 1) and quickly converges to the attractor. system.
In a more recent works, Vlachas et al.Â (2019) train an ESN on 1D
observations of the Lorenz-96 system, and also compare the Lyapunov
exponents of the autonomous phase to the known exponents of the Lorenz
system. Chattopadhyay et al.Â (2019) also train an ESN on observations of
the Lorenz-96 system and evaluate the accuracy of future prediction for
reservoirs of different size. We used MATLABâ€™s ODE45 to integrate a
trajectory of the Lorenz (1963) system Ë™ x = Ïƒ(y âˆ’x) (10) Ë™ y = x(Ï âˆ’z)
âˆ’y Ë™ z = xy âˆ’Î²z with parameters Ïƒ = 10, Î² = 8/3, Ï = 28 chosen so the
system produces the celebrated Lorenz attractor shown in Figure 4. We
then observed the x component of the trajectory by choosing the
observation function Ï‰(x, y, z) = x to create a 1 dimensional time
series. We fed this time series into an ESN with the following
parameters: spectral radius Ï = 1, reservoir size n = 300, and
activation function Ï• = tanh. The reservoir matrix A is an ErdË os-RÃ©nyi
matrix with mean 6 and connection weights (where they are non-zero)
i.i.d Gaussian, re-scaled such that Ï = 1. The keen reader will notice
that the structure of A does not conform to the reservoir matrix Ëœ A
described in the statement of the ESN Approximation Theorem. The fact
that our numerical experiments produce good results despite this
suggests this weakly connected Ëœ A is unnecessary, but rather a decision
we made to make the ESN Approximation Theorem easier to prove.
Furthermore, insisting that Ï < 1 is not sufï¬cient in to ensure that
âˆ¥Aâˆ¥2 < 1, but this is a common choice in practical applications. The
matrix W out is populated with i.i.d Gaussian weights âˆ¼N(0, 1) which are
then scaled by a â€˜strength parameterâ€™ p = 0.1. We choose a
regularisation parameter Î» = 10âˆ’6 to solve the regularised least squares
problem min W out K X k=1 âˆ¥W outrk âˆ’ukâˆ¥2 + Î»âˆ¥W outâˆ¥2 2 using the SVD
method presented by Hansen et al.Â (2006). We will note here that the
linear output layer W out ob- tained by this procedure is not
necessarily the same as that guaranteed by the ESN Approximation
Theorem. These parameters were carefully hand tuned so that the
autonomous phase appeared (by eye) to match the driven phase. The
question of how to systematically choose good parameters is discussed by
Yperman & Becker (2016) who searched through parameter space using
Bayesian optimisation, and used cross validation to test the goodness of
ï¬t. Now, with W out obtained, we ran the autonomous ESN and plotted the
future observations vi in Figure 5. We can see from this Figure that the
ESN seems to predict the qualitative features of the future trajectory
very well. 17 75 80 85 90 95 100 105 110 115 120 125 time -20 0 20 x
Figure 5: Here the 1D observations are shown in blue (up to time 100 for
those of you reading in black and white) and future predictions shown in
red (onwards from time 100). -10 -5 0 5 10 -0.6 -0.4 -0.2 0 0.2 0.4 0.6
Training Data Future Prediction Newton Iterates Figure 6: The driven
reservoir dynamics are plotted in blue and autonomous dynamics are
plotted in red. Both were projected onto the ï¬rst three principal
components of the driven dynamics, then the axes are rotated such that
the projection appears on the ï¬rst 2 components. The black line
indicates the iterates of Newtonâ€™s method, used to locate a ï¬xed point -
the method eventually converges to a ï¬xed point in the middle of the
right wing of the ï¬gure. We can see by eye that the reservoir dynamics
appear by eye to be topologically conjugate to the Lorenz system. Since
the Lorenz system is deï¬ned on a 3-manifold, we can usefully plot
trajectories of the entire system. To check by eye whether the reservoir
dynamics of both the driven phase and autonomous phase are topologically
conjugate to the Lorenz dynamics, we projected the driven and autonomous
dynamics onto the ï¬rst 3 principal components of the driven trajectory
and present them in Figure 6. 3.1 Locating Fixed Points and Determining
their Eigenvalues If the ESM f is an embedding, then f will embed the
ï¬xed points of the Lorenz system into the reservoir space. Moreover if
the autonomous ESN approximates the embedded Lorenz system on a
neighbourhood of the embedded ï¬xed points sufï¬cently well, the
autonomous dynamics will contain ï¬xed points very close to those of the
embedded Lorenz system. To verify this, we searched for the autonomous
ESNâ€™s ï¬xed points using Newtonâ€™s method, and found them, as illustrated
in Figure 6. Further, if the ESM f is a C1 embedding of the original
dynamics, we expect f to preserve the stability of ï¬xed points, i.e.Â we
expect the eigenvalues of the linearisation of the autonomous phase to
be preserved at every ï¬xed point. Now, 18 -1 -0.5 0 0.5 1 Real -1 -0.5 0
0.5 1 Imaginary ESN autonomous sytem Lorenz system Figure 7: Here the 3
eigenvalues of the linearisation of the Lorenz system on the ï¬xed point
inside one of the Lorenz attractorâ€™s wings are represented by blue
crosses. The 300 eigenvalues of the linearisation of the ESN autonomous
system at the ï¬xed point found with Newtonâ€™s method are represented by
red dots. comparing the eigenvalues of the linearisation of the Lorenz
system and autonomous phase at the respective ï¬xed points requires some
subtlety, because the Lorenz system is a continuous time ï¬‚ow, while the
autonomous phase is a discrete time map. So, we began by considering one
of the known ï¬xed points found in the Lorenz attractorâ€™s wings xâˆ—= ( p
Î²(Ï âˆ’1), p Î²(Ï âˆ’1), Ï âˆ’1), and noted the Jacobian J of the continuous
time Lorenz system evaluated at the ï¬xed point xâˆ—is therefore J   
xâˆ—= ï£® ï£° âˆ’Ïƒ Ïƒ 0 1 âˆ’1 âˆ’ p Î²(Ï âˆ’1) p Î²(Ï âˆ’1) p Î²(Ï âˆ’1) âˆ’Î² ï£¹ ï£». Now we can
discretise the Lorenz system Ë™ x = s(x) with the following map xk+1 = xk
+ Z tk+1 tk s â—¦x(t)dt, hence the discrete time linearisation about the
ï¬xed point xâˆ—is xk+1 = exp  J    xâˆ—(tk+1 âˆ’tk)  xk, which has 3
eigenvalues, which we have compared with the ESN autonomous eigenvalues
in Figure 7. If the ESM f is indeed a C1 embedding, the dynamics of the
autonomous phase are topologically conjugate to the discrete time Lorenz
system on some 3-submanifold. This manifold is spanned by 3
eigenvectors, each with an associated eigenvalue, which will coincide
with the eigenvalues of the linearisation of the Lorenz system on the
ï¬xed point. Figure 7 appears to show 3 overlapping eigenvalues,
suggesting that the autonomous phase is diffeomorphic to the Lorenz
system (at least in a neighbourhood of xâˆ—) in this simulation. This is
particularly remarkable because xâˆ—is distant from the training data. The
ESN has successfully inferred the existence, position and eigenvalues of
a ï¬xed point from training data, which contains no ï¬xed points. In the
machine learning parlance, the ESN has generalised patterns in the
training data to an unseen region of the phase space. 19 Figure 8: The
Lyapunov spectrum of the autonomous phase as the iterates increases is
shown. The true Lyapunov exponents of the autonomous phase is given by
the limit of these exponents as the iterations tend to inï¬nity. These
autonomous exponents are compared to the black dotted lines representing
the 3 exponents of the Lorenz system. 3.2 Comparison of Lyapunov Spectra
Another topological invariant of the Lorenz system is the Lyapunov
spectrum, which captures how quickly very close trajectories diverge
from eachother, and is used as a measure of chaos. To deï¬ne the
spectrum, let J be the Jacobian of the evolution operator of a
continuous time dynamical system. Let Y be the solution of the ODE Ë™ Y =
JY with initial condition Y (0) = x0. Then the Lyapunov Spectrum of the
invariant set containing x0 is the spectrum of the matrix Î› deï¬ned Î› =
lim tâ†’âˆ 1 2tY Y âŠ¤. Each eigenvalue in the spectrum is called a Lyapunov
exponent to signify that two initially close trajectories diverge or
converge exponentially fast with exponentiation constant in the
direction of each eigenvector of J given by a Lyapunov exponent. Details
are discussed by Darbyshire & Broomhead (1996). The Lyapunov spectrum
for the Lorenz system was estimated by Sprott (2003) as 0.9056, 0,
-14.5723. In order to compare the Lorenz spectrum to the spectrum of the
autonomous ESN, we computed the autonomous systemâ€™s spectrum using the
discrete time QR method discussed in Darbyshire & Broomhead (1996) and
plotted each Lyapunov exponent against the known exponents of the Lorenz
system in Figure 8. We found the largest 2 in good agreement while there
was signiï¬cant error in the smallest, which is a common problem also
encountered by Pathak et al.Â (2017). 3.3 Persistent Homology We compared
the homology groups of the Lorenz attractor to the persistent homology
groups of the autonomous and driven attractors. We followed the lead of
Garland et al.Â (2016) who computed the persistent homology of the Lorenz
system reconstructed from a sequence of 1D observations of a Lorenz
trajectory using the delay observation map described in Takensâ€™ Theorem.
The authors used the open source software Javaplex created by Tausz et
al.Â (2014) to ï¬nd the Witness Complex for the delay embedded Lorenz
attractor and computed the homology of the complex. They discuss a few
subtleties that arise, in particular that the Lorenz attractor is a
fractal, whose structure cannot be reconstructed exactly from any ï¬nite
number of sample points. The authors therefore satisï¬ed themselves by
approximating the Lorenz attractor with a branched manifold model
presented by Williams (1979) which has the homology of the ï¬gure 8. We
made the same approximation, and expected to ï¬nd that the application of
persistent homology to the Lorenz system, driven ESN dynamics, and
autonomous ESN dynamics would reveal that all three have the ï¬gure 8
homology groups. In particular the persistence diagrams of these three
systems would exhibit a pair 20 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
Birth 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Death Figure 9: We have
plotted the H1 persistence diagrams of the driven ESN dynamics,
autonomous ESN dynamics, and Lorenz dynamics as blue circles, red
downward triangles, and purple upward triangles. We can see that each of
these 3 objects has a pair of points ï¬‚oating well above the diagonal,
suggesting each has 2 holes. This is consistent with our expectation
that all three adopt the topology of the ï¬gure 8. of H1 persistent
homology groups ï¬‚oating well above the diagonal. To verify this, we
produced persistence diagrams using the open source software Ripser
produced by Tralie et al.Â (2018) and plotted the results in Figure 9.
The reader may wonder why we would use persistent homology to show that
the Lorenz system, driven ESN dynamics, and autonomous ESN dynamics all
have the homology of the ï¬gure 8 when this can clearly be seen in
Figures 4 and 6. The homology of a 3D system is usually apparent from a
plot, but persistent homology can reveal the holes, voids and higher
dimensional hypervoids of high dimensional systems that cannot be easily
visualised. For example Muldoon et al.Â (1993) computed the homology of a
delay embedded time series from a ï¬‚uid dynamics experiment, which could
in general be of much higher dimension. 4 Conclusions and Outlook In
this paper, we showed that an Echo State Network driven by a sequence of
one dimensional observations of a dynamical system, evolving on a
manifold M, induces a map f âˆˆC1(M, Rn), which we called the Echo State
Map. We proved that for a randomly initialed ESN and generic observation
function Ï‰, that f is an embedding with positive probability, and called
this the weak ESN Embedding Theorem. We conjectured that the theorem
holds with probability 1, by analogy to Takensâ€™ Theorem. We went on to
show that a randomly initialised ESN has a universal approximation
property and called this the Random Universal Approximation Theorem
(RUAT). Finally, we used both the RUAT and Embedding Theorem to prove
that for an ESN trained a sequence of scalar observations of a
structurally stable dynamical system, there is a choice of linear
readout weights W out for which the autonomous ESN has dynamics that are
topologically conjugate to the input dynamical system, and we called
this the ESN Approximation Theorem. The theory presented here leaves
some questions unanswered. In practice we use regularized least squares
regression to learn an output matrix from the one-dimensional and ï¬nite
training trajectory, but currently, we have no guarantee that this will
result in an autonomous phase ESN that is topologically conjugate to the
underlying dynamical system. This is analogous to the case of the
Universal Approximation Theorem for feed forwards neural networks, where
the theoretical result proves the existence of suitable set of weights
but does not guarantee that a particular learning algorithm will be able
to ï¬nd them or how much training data may be required. It may be that
imposing extra conditions on the target dynamical system, like
ergodicity, allows us to prove that W out obtained by least squares
regression results in an arbitrarily good approximation. This seems to
be supported by the experiments in Section 3. 21 Furthermore, it seems
worthwhile to prove the ESN Embedding Conjecture, or some modiï¬cation of
it that is actually correct, by carefully modifying the proof of Takensâ€™
Theorem provided by Huke (2006). A sceptical reader may wonder why we
would bother using an ESN to embed the trajectory in the ï¬rst place,
when a delay embedding would do. The reason being that it seems the
ESNâ€™s learning and predictive powers are much more resilient to noise
than the simple delay embedding presented by Takens. Heuristically it
seems as an observed trajectory passes through the ESN, the noise
cancels itself out by taking a nonlinear combination of positive and
negative noise. We could therefore view the ESN as a nonlinear ï¬lter,
generalising the linear ï¬lters discussed by Sauer et al.Â (1991) in the
context of embedology - the art building delay observation maps with
special features, which include being more resiliant to noise than
Takensâ€™ original map. Understanding the noise cancelling beneï¬ts of the
ESN could be a fruitful direction of future work. Many of the
assumptions we made throughout this paper are likely stronger than they
need to be. For example Sauer et al.Â (1991) prove versions of Takensâ€™
Theorem for dynamics on a compact invariant set with real box counting
dimension - generalising dynamics on a manifold with integer dimension.
This is particularly worthwhile because chaotic attractors of interest
often lie on invariant sets with non-integer dimension, with the Lorenz
attractor serving as a perfect example. We also create a strangely
shaped reservoir Ëœ A in our proof of the ESN Approximation Theorem,
which numerical experiments suggests is unnecessary. Acknowledgements We
offer gracious thanks to the anonymous reviewers for their detailed
suggestions. In particular, we thank Juan-Pablo Ortega and Lyudmila
Grigoryeva for thorough and fruitful discussions at UniversitÃ¤t Sankt
Gallen which lead to signiï¬cant improvements to the manuscript. We also
thank Allen Hartâ€™s PhD conï¬rmation examiners Alastair Spence and Chris
Guiver who helpfully criti- cised and improved the content of this
paper. We acknowledge that Allen Hart is supported by a scholarship from
the ESPRC Centre for Doctoral Training in Statistical Applied
Mathematics at Bath (SAMBa), under the project EP/L015684/1. References
Banach, S. (1922), â€˜Sur les opÃ©rations dans les ensembles abstraits et
leur application aux Ã©quations intÃ©gralesâ€™, Fun- damenta Mathematicae
22, 133â€“181. Chattopadhyay, A., Hassanzadeh, P., Palem, K. &
Subramanian, D. (2019), â€˜Data-driven prediction of a multi-scale lorenz
96 chaotic system using a hierarchy of deep learning methods: Reservoir
computing, ann, and rnn-lstmâ€™, arXiv:1906.08829 . Cybenko, G. (1989),
â€˜Approximation by superpositions of a sigmoidal functionâ€™, Mathematics
of Control, Signals and Systems 2(4), 303â€“314. Darbyshire, A. &
Broomhead, D. (1996), â€˜Robust estimation of tangent maps and liapunov
spectraâ€™, Physica D: Nonlinear Phenomena 89(3), 287 â€“ 305. Garland, J.,
Bradley, E. & Meiss, J. D. (2016), â€˜Exploring the topology of dynamical
reconstructionsâ€™, Physica D: Nonlinear Phenomena 334, 49 â€“ 59. Topology
in Dynamics, Differential Equations, and Data. Ghrist, R. (2008),
â€˜Barcodes: The persistent topology of dataâ€™, Bulletin of the American
Mathematical Society 45, 61â€“ 75. Glorot, X., Bordes, A. & Bengio, Y.
(2011), Deep sparse rectiï¬er neural networks, in G. Gordon, D. Dunson &
M. DudÃ­k, eds, â€˜Proceedings of the Fourteenth International Conference
on Artiï¬cial Intelligence and Statisticsâ€™, Vol. 15 of Proceedings of
Machine Learning Research, PMLR, Fort Lauderdale, FL, USA, pp.Â 315â€“323.
Gonon, L., Grigoryeva, L. & Ortega, J.-P. (2020), â€˜Approximation bounds
for random neural networks and reservoir systemsâ€™, arXiv:2002.05933 .
Grigoryeva, L. & Ortega, J. (2018), â€˜Echo state networks are universalâ€™,
Neural Networks 108, 495 â€“ 508. Grigoryeva, L. & Ortega, J.-P. (2019),
â€˜Differentiable reservoir computingâ€™, Journal of Machine Learning
Research 20(179), 1â€“62. URL: http://jmlr.org/papers/v20/19-150.html
GÃ¼rel, T. & Egert, S. R. U. (2010), â€˜Functional identiï¬cation of
biological neural networks using reservoir adaptation for point
processesâ€™, Journal of Computational Neuroscience p.Â 279â€“299. 22 Hackl,
J. (2018), â€˜Tikz-network manualâ€™, arXiv:1709.06005 . Hansen, P. C.,
Nagy, J. G. & Oâ€™leary, D. P. (2006), Deblurring Images: Matrices,
Spectra, and Filtering, SIAM. Hirsch, M. W., Pugh, C. C. & Shub, M.
(1977), Invariant Manifolds, Springer-Verlag. Hornik, K., Stinchcombe,
M. & White, H. (1990), â€˜Universal approximation of an unknown mapping
and its deriva- tives using multilayer feedforward networksâ€™, Neural
Networks 3(5), 551 â€“ 560. Huang, G.-B., Zhu, Q.-Y. & Siew, C.-K. (2006),
â€˜Extreme learning machine: Theory and applicationsâ€™, Neurocomput- ing
70(1), 489 â€“ 501. Neural Networks. Huke, J. P. (2006), â€˜Embedding
nonlinear dynamical systems: A guide to Takensâ€™ theoremâ€™. Ilies, I.,
Jaeger, H., Kosuchinas, O., Rincon, M., Sakenas, V. & Vaskevicius, N.
(2007), â€˜Stepping forward through echoes of the past: forecasting with
echo state networksâ€™. Jaeger, H. (2001), â€˜The â€œecho stateâ€ approach to
analysing and training recurrent neural networksâ€™. Jaeger, H. & Haas, H.
(2004), â€˜Harnessing nonlinearity: Predicting chaotic systems and saving
energy in wireless communicationâ€™, Science 304(5667), 78â€“80. Kocarev, L.
& Parlitz, U. (1996), â€˜Generalized synchronization, predictability, and
equivalence of unidirectionally coupled dynamical systemsâ€™, Phys.
Rev.Â Lett. 76, 1816â€“1819. Lin, X., Yang, Z. & Song, Y. (2009),
â€˜Short-term stock price prediction based on echo state networksâ€™, Expert
Systems with Applications 36(3, Part 2), 7313 â€“ 7317. LÃ¸kse, S.,
Bianchi, F. M. & Jenssen, R. (2017), â€˜Training echo state networks with
regularization through dimension- ality reductionâ€™, Cognitive
Computation 9(3), 364â€“378. Lorenz, E. N. (1963), â€˜Deterministic
nonperiodic ï¬‚owâ€™, Journal of the Atmospheric Sciences 20(2), 130â€“141.
Maass, W., NatschlÃ¤ger, T. & Markram, H. (2002), â€˜Real-time computing
without stable states: A new framework for neural computation based on
perturbationsâ€™, Neural Computation 14(11), 2531â€“2560. Muldoon, M.,
MacKay, R., Huke, J. & Broomhead, D. (1993), â€˜Topology from time
seriesâ€™, Physica D: Nonlinear Phenomena 65(1), 1 â€“ 16. Pathak, J., Lu,
Z., Hunt, B. R., Girvan, M. & Ott, E. (2017), â€˜Using machine learning to
replicate chaotic attractors and calculate lyapunov exponents from
dataâ€™, Chaos 27. PlÃ¶ger, P. G., Arghir, A., GÃ¼nther, T. & Hosseiny, R.
(2004), Echo state networks for mobile robot modeling and control, in D.
Polani, B. Browning, A. Bonarini & K. Yoshida, eds, â€˜RoboCup 2003: Robot
Soccer World Cup VIIâ€™, Springer Berlin Heidelberg, Berlin, Heidelberg,
pp.Â 157â€“168. Sauer, T., Yorke, J. A. & Casdagli, M. (1991),
â€˜Embedologyâ€™, Journal of Statistical Physics 65(3), 579â€“616. Schrauwen,
B., Verstraeten, D. & Van Campenhout, J. (2007), An overview of
reservoir computing: theory, appli- cations and implementations, in
â€˜Proceedings of the 15th European Symposium on Artiï¬cial Neural
Networks. p. 471-482 2007â€™, pp.Â 471â€“482. Shi, Z. & Han, M. (2007),
â€˜Support vector echo-state machine for chaotic time-series predictionâ€™,
IEEE Transactions on Neural Networks 18(2), 359â€“372. Skowronski, M. D. &
Harris, J. G. (2007), â€˜Automatic speech recognition using a predictive
echo state network classi- ï¬erâ€™, Neural Networks 20(3), 414 â€“ 423. Echo
State Networks and Liquid State Machines. Sprott, J. C. (2003), Chaos
and time-series analysis, Oxford University Press. Takens, F. (1981),
â€˜Detecting strange attractors in turbulenceâ€™, Lecture Notes in
Mathematics, Berlin Springer Verlag 898, 366. Tanaka, G., Yamane, T.,
HÃ©roux, J. B., Nakane, R., Kanazawa, N., Takeda, S., Numata, H., Nakano,
D. & Hirose, A. (2019), â€˜Recent advances in physical reservoir
computing: A reviewâ€™, Neural Networks 115, 100 â€“ 123. Tausz, A.,
Vejdemo-Johansson, M. & Adams, H. (2014), JavaPlex: A research software
package for persistent (co)homology, in H. Hong & C. Yap, eds,
â€˜Proceedings of ICMS 2014â€™, Lecture Notes in Computer Science 8592,
pp.Â 129â€“136. Tong, M. H., Bickett, A. D., Christiansen, E. M. &
Cottrell, G. W. (2007), â€˜Learning grammatical structure with echo state
networksâ€™, Neural Networks 20(3), 424 â€“ 432. Echo State Networks and
Liquid State Machines. Tralie, C., Saul, N. & Bar-On, R. (2018),
â€˜Ripser.py: A lean persistent homology library for pythonâ€™, The Journal
of Open Source Software 3(29), 925. 23 Vlachas, P. R., Pathak, J., Hunt,
B. R., Sapsis, T. P., Girvan, M., Ott, E. & Koumoutsakos, P. (2019),
â€˜Forecasting of spatio-temporal chaotic dynamics with recurrent neural
networks: a comparative study of reservoir computing and backpropagation
algorithmsâ€™, arXiv:1910.05266 . Warner, F. W. (1971), Foundations of
Differentiable Manifolds and Lie Groups, Scott, Foresman and Co.
Whitney, H. (1944), â€˜The self-intersections of a smooth n-manifold in
2n-spaceâ€™, The Annals of Mathematics 45(2), 220â€“246. Williams, R. F.
(1979), â€˜The structure of lorenz attractorsâ€™, Publications MathÃ©matiques
de lâ€™IHÃ‰S 50, 73â€“99. Xi, J., Shi, Z. & Han, M. (2005), Analyzing the
state space property of echo state networks for chaotic system
prediction, in â€˜Proceedings. 2005 IEEE International Joint Conference on
Neural Networks, 2005.â€™, Vol. 3, pp.Â 1412â€“ 1417 vol.Â 3. Yeo, K. (2019),
â€˜Data-driven reconstruction of nonlinear dynamics from sparse
observationâ€™, Journal of Computational Physics 395, 671 â€“ 689. Yong
Song, Yibin Li, Qun Wang & Caihong Li (2010), Multi-steps prediction of
chaotic time series based on echo state network, in â€˜2010 IEEE Fifth
International Conference on Bio-Inspired Computing: Theories and
Applications (BIC-TA)â€™, pp.Â 669â€“672. Yperman, J. & Becker, T. (2016),
â€˜Bayesian optimization of hyper-parameters in reservoir computingâ€™,
arXiv:1611.05193 . 24
