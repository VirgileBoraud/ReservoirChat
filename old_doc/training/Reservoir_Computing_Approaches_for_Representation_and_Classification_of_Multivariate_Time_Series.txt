IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO.
5, MAY 2021 2169 Reservoir Computing Approaches for Representation and
Classiﬁcation of Multivariate Time Series Filippo Maria Bianchi , Simone
Scardapane , Sigurd Løkse, and Robert Jenssen , Member, IEEE
Abstract—Classiﬁcation of multivariate time series (MTS) has been
tackled with a large variety of methodologies and applied to a wide
range of scenarios. Reservoir computing (RC) provides efﬁcient tools to
generate a vectorial, ﬁxed-size representation of the MTS that can be
further processed by standard classiﬁers. Despite their unrivaled
training speed, MTS classiﬁers based on a standard RC architecture fail
to achieve the same accuracy of fully trainable neural networks. In this
article, we introduce the reservoir model space, an unsupervised
approach based on RC to learn vectorial representations of MTS. Each MTS
is encoded within the parameters of a linear model trained to predict a
low-dimensional embedding of the reservoir dynamics. Compared with other
RC methods, our model space yields better representa- tions and attains
comparable computational performance due to an intermediate
dimensionality reduction procedure. As a second contribution, we propose
a modular RC framework for MTS classiﬁcation, with an associated
open-source Python library. The framework provides different modules to
seamlessly implement advanced RC architectures. The architectures are
compared with other MTS classiﬁers, including deep learning models and
time series kernels. Results obtained on the benchmark and real-world
MTS data sets show that RC classiﬁers are dramatically faster and, when
implemented using our proposed representation, also achieve superior
classiﬁcation accuracy. Index Terms—Model space, recurrent neural
networks (RNNs), reservoir computing (RC), time series classiﬁcation. I.
INTRODUCTION T HE problem of classifying multivariate time series (MTS)
consists in assigning each MTS to one of a ﬁxed number of classes. This
is a fundamental task in many applications, including (but not limited
to) health monitoring [1], civil engineering [2], action recognition
[3], and speech analysis [4]. The problem has been tackled by approaches
spanning from the deﬁnition of tailored distance measures over MTS to
the identiﬁcation of patterns in the form of dictionaries or Manuscript
received March 28, 2019; revised July 24, 2019 and January 10, 2020;
accepted June 5, 2020. Date of publication June 29, 2020; date of
current version May 3, 2021. (Corresponding author: Filippo Maria
Bianchi.) Filippo Maria Bianchi is with the Department of Physics and
Technology, UiT—The Arctic University of Norway, 9019 Tromsø, Norway,
and also with the NORCE Norwegian Research Centre, 5838 Bergen, Norway
(e-mail: ﬁlippombianchi@gmail.com). Simone Scardapane is with the
Department of Information Engineering, Electronics and
Telecommunications, Sapienza University of Rome, 00184 Rome, Italy.
Sigurd Løkse and Robert Jenssen are with the Department of Physics and
Technology, UiT—The Arctic University of Tromsø, 9019 Tromsø, Norway.
This article has supplementary downloadable material available at
https://ieeexplore.ieee.org, provided by the authors. Color versions of
one or more of the ﬁgures in this article are available online at
https://ieeexplore.ieee.org. Digital Object Identiﬁer
10.1109/TNNLS.2020.3001377 shapelets [5]–[8]. In this article, we focus
on classiﬁers based on recurrent neural networks (RNNs), which ﬁrst
process sequentially the MTS with a dynamic model and then exploit the
sequence of the model states generated over time to perform
classiﬁcation [9]. Reservoir computing (RC) is a family of RNN models
whose recurrent part is generated randomly and then kept ﬁxed [10],
[11]. Despite this strong simpliﬁcation, the recur- rent part of the
model (the reservoir) provides a rich pool of dynamic features that are
suitable for solving a large variety of tasks. Indeed, RC models
achieved excellent performance in time series forecasting [12], [13] and
process modeling [14]. In machine learning, RC techniques were
originally introduced under the name echo state networks (ESNs) [15]; in
this article, we use the two terms interchangeably. RC-based classiﬁers
represent the MTS either as the last or the mean of all the reservoir
states and then process it with a classiﬁcation algorithm for vectorial
data [16], [17]. Despite their unrivaled training speed, these
approaches fail to achieve the same accuracy of competing state-of-the-
art classiﬁers [18]. To learn more powerful representations, an
alternative approach originally proposed in [19] and later applied to
MTS classiﬁcation and fault detection [18], [20] advocates to map the
inputs in a “model-based” feature space where the MTS is represented by
the parameters of a model, trained to predict the next input from the
current reservoir state. As a drawback, this approach accounts only for
those reservoir dynamics useful to predict the next input and could
neglect important information that characterizes the MTS. To overcome
this limitation, we propose a new model-space criterion that
disentangles from the constraints imposed by this formulation. A.
Contributions of This Article We propose an unsupervised procedure to
generate MTS representations, called reservoir model space, that consist
in the parameters of the one-step-ahead predictor that estimates the
future reservoir state, as opposed to the future MTS input. As shown in
our previous work [21], the reservoir states carry all the information
necessary to reconstruct the phase space that, in turn, gives a complete
knowledge of the underlying dynamical system generating the observed
MTS. Therefore, a model capable of predicting the next reservoir state
accounts for all the system dynamics and provides a much more accurate
characterization of the MTS. Due to the large 2162-237X © 2020 IEEE.
Personal use is permitted, but republication/redistribution requires
IEEE permission. See https://www.ieee.org/publications/rights/index.html
for more information. Authorized licensed use limited to: INRIA.
Downloaded on January 16,2024 at 16:23:28 UTC from IEEE Xplore.
Restrictions apply. 2170 IEEE TRANSACTIONS ON NEURAL NETWORKS AND
LEARNING SYSTEMS, VOL. 32, NO. 5, MAY 2021 size of the reservoir, a
naïve formulation of the model space yields extremely large
representations that lead to overﬁt in the subsequent classiﬁer and
hamper the computational efﬁciency proper of the RC paradigm. We address
this issue by training the prediction model on a low-dimensional
embedding of the original dynamics. The embedding is obtained by
applying to the reservoir states sequence a modiﬁed version of principal
component analysis (PCA) for tensors, which keeps separated the modes of
variation among time steps and data samples. The proposed representation
is novel and, while our focus is on MTS, it naturally extends also to
univariate time series. As a second contribution, we introduce a uniﬁed
RC frame- work (with an associated open-source Python library) for MTS
classiﬁcation that generalizes both classic and advanced RC
architectures. Our framework consists of four independent modules that
specify the architecture of the reservoir, a dimen- sionality reduction
procedure applied to reservoir activations, the representation used to
describe the input MTS, and the readout that performs the ﬁnal
classiﬁcation. In the experiments, we compare several RC architectures
implemented with our framework, with state-of-the-art time series
classiﬁers, classiﬁers based on fully trainable RNNs, deep learning
models, dynamic time warping (DTW), and SVM conﬁgured with kernels for
MTS. The results obtained on several real-world data set show that the
RC classiﬁers are dramatically faster than the other methods and, when
implemented using our proposed representation, also achieve a
competitive classiﬁcation accuracy. B. Notation We denote variables as
lowercase letters (x); constants as uppercase letters (X); vectors as
boldface lowercase letters (x), matrices as boldface uppercase letters
(X), and tensors as calligraphic letters (X). All vectors are assumed to
be columns. The operator ∥·∥p is the standard ℓp norm in Euclidean
spaces. The notation x(t) indicates time step t and x[n] sample n in the
data set. II. PRELIMINARIES We consider classiﬁcation of generic
F-dimensional MTS with T time instants, whose observation at time t is
denoted as x(t) ∈RF. We represent an MTS in a compact form as a T × F
matrix X = [x(1), . . ., x(T )]T .1 Common in machine learning is to
express the classiﬁer as a combination of an encoding and a decoding
function. The encoder generates a representation of the input, whereas
the decoder is a discriminative (or predictive) model that computes the
posterior probability of the output given the encoder representation. An
encoder based on an RNN [22] is particularly suitable to model
sequential data and is governed by the state update equation h(t) = f
(x(t), h(t −1); θenc) (1) where h(t) is the RNN state at time t that
depends on its previous value h(t −1) and the current input x(t), f (·)
is a 1Since MTS may have different lengths, T is a function of the MTS.
nonlinear activation function (e.g., a sigmoid or hyperbolic tan- gent),
and θenc are adaptable parameters. The simplest (vanilla) formulation
reads h(t) = tanh(Winx(t) + Wrh(t −1)) (2) with θenc = {Win, Wr}. The
matrices Win and Wr are the weights of the input and recurrent
connections, respectively. From the sequence of the RNN states generated
over time, H = [h(1), . . ., h(T )]T , it is possible to extract a
representa- tion rX = r(H) of the input X. A common choice is to take rX
= h(T) since the RNN can embed into its last state all the information
required to reconstruct the original input [23]. The decoder maps the
MTS representation rX into the output space, which are the class labels
y for a classiﬁcation task y = g(rX; θdec) (3) where g(·) can be a
(feedforward) neural network or a linear model and θdec are the
trainable parameters. In the following, we describe two RNN-based
approaches for MTS classiﬁcation. The ﬁrst is based on fully trainable
architectures, and the second is based on RC where the RNN encoder is
left untrained. A. Fully Trainable RNNs and Gated Architectures In fully
trainable RNNs, given a set of MTS {X[n]}N n=1 and associated labels
{y[n]}N n=1, the encoder parameters θenc and the decoder parameters θdec
are jointly learned by minimizing an empirical cost θ∗ enc, θ∗ dec = arg
min θenc,θdec 1 N N  n=1 l(y[n], g(r( f (X[n])))) (4) where l(·, ·) is
a generic loss function (e.g., cross entropy over the labels). The
gradient of (4) with respect to θenc and θdec can be computed by
backpropagation through time [9]. The parameters in the encoding and
decoding functions are commonly regularized with an ℓ2 norm penalty,
controlled by a scalar λ. It is also possible to include a dropout
regular- ization that randomly drops connections during training with
probability pdrop [24]. In our experiments, we apply a dropout speciﬁc
for recurrent architectures [25]. Despite the theoretical capability of
basic RNNs to model any dynamical system, in practice, their
effectiveness is ham- pered by the difﬁculty of training their
parameters [26]. To ensure stability, the derivative of the recurrent
function in an RNN must not exceed unity. However, as an undesired
effect, the gradient of the loss shrinks when backpropagated in time
through the network. Using RC models (described in Section II-B) is one
way of avoiding this problem. Another solution is using the long
short-term memory (LSTM) net- work [27], which exploits gating
mechanisms to maintain its internal memory unaltered for long time
intervals. However, LSTM ﬂexibility comes at the cost of a higher
computational and architectural complexity. A popular variant is the
gated recurrent unit (GRU) [28] that provides a better memory
conservation by using fewer parameters than LSTM. Authorized licensed
use limited to: INRIA. Downloaded on January 16,2024 at 16:23:28 UTC
from IEEE Xplore. Restrictions apply. BIANCHI et al.: RC APPROACHES FOR
REPRESENTATION AND CLASSIFICATION OF MTS 2171 B. Reservoir Computing and
Output Model Space To avoid the costly operation of backpropagating
through time, the RC approach takes a radical different direction; it
still implements the encoding function in (2), but the encoder
parameters θenc = {Win, Wr} are randomly generated and left untrained.
To compensate for this lack of adaptability, a large recurrent layer,
the reservoir, generates a rich pool of heterogeneous dynamics useful to
solve many different tasks. The generalization capabilities of the
reservoir mainly depend on three ingredients: 1) a high number of
processing units in the recurrent layer; 2) sparsity of the recurrent
connections; and 3) a spectral radius of the connection weights matrix
Wr, set to bring the system to the edge of stability [29]. The behavior
of the reservoir is controlled by modifying the following
hyperparameters: the spectral radius ρ, the percentage of nonzero
connections β, and the number of hidden units R. Another important
hyperparameter is the scaling ω of the values in Win, which controls the
amount of nonlinearity in the processing units and, jointly with ρ, can
shift the internal dynamics from a chaotic to a contractive regime [30].
A Gaussian noise with standard deviation ξ can also be added in the
state update function (2) for regularization purposes [15]. In ESNs, the
decoder (commonly referred as readout) is usually a linear model y =
g(rX) = VorX + vo. (5) The decoder parameters θdec = {Vo, vo} can be
learned by minimizing a ridge regression loss function θ∗ dec = arg min
{Vo,vo} 1 2∥rXVo + vo −y∥2 + λ∥Vo∥2 (6) which admits a closed-form
solution [11]. The combination of an untrained reservoir and a linear
readout deﬁnes the basic ESN model [15]. A powerful representation rX is
the output model space [19], [31], [32], obtained by ﬁrst processing
each MTS with the same reservoir and then training a ridge regression
model to predict the input one step-ahead x(t + 1) = Uoh(t) + uo. (7)
The parameters θo = [vec(Uo); uo] ∈RF(R+1) becomes the representation rX
of the MTS, which is, in turn, processed by the classiﬁer in (5). In the
following, we propose a new model space that yields a more expressive
representation of the input. III. PROPOSED RESERVOIR MODEL-SPACE
REPRESENTATION In this section, we introduce the main contribution of
this article, the reservoir model space for representing a (multi-
variate) time series, and a dimensionality reduction method that extends
PCA to multidimensional temporal data. Related to our idea, but framed
in a setting different from RC, are the recent deep learning
architectures that learn unsupervised representations by predicting the
future in a small-dimensional latent space with autoregressive models
[33]. A. Formulation of the Reservoir Model Space The generalization
capability of the reservoir is grounded on the large amount of
heterogeneous dynamics it generates from the input. To predict the next
input values, different dynamics are selected depending on the forecast
horizon of interest. Therefore, when ﬁxing the prediction step (e.g.,
one step-ahead), all those dynamics that are not useful to solve the
task are discarded. This introduces a bias in the output model space
since the features that are not important for the prediction task can
still be useful to characterize the MTS. Therefore, we propose a new
model space, where each MTS is represented by the parameters of a linear
model, which predicts the next reservoir state by accounting for all the
reservoir dynamics. The linear model trained to predict the next
reservoir state reads h(t + 1) = Uhh(t) + uh (8) and rX = θh = [vec(Uh);
uh] ∈RR(R+1) is our proposed representation. The reservoir model-space
representation characterizes a generative model of the reservoir
sequence p(h(T ), h(T −1), . . . , h(1); rX). (9) The model (9) provides
a characterization of both the input and the generative process of its
high-level dynamical features and also induces a metric relationship
between the samples [34]. A classiﬁer that processes the reservoir model
representation combines the explanatory capability of generative models
with the classiﬁcation power of the discriminative methods. B.
Dimensionality Reduction for Reservoir States Tensor Due to the high
dimensionality of the reservoir, the number of parameters of the
prediction model in (8) would grow too large, making the proposed
representation intractable. Drawbacks in using large representations
include overﬁtting and the high amount of computational resources to
evaluate the ridge regression solution for each MTS. In the context of
RC, applying PCA to reduce the dimensionality of the last reservoir
state has shown to improve the performance achieved on the inference
task [35]. Compared with nonlinear methods for dimensionality reduction
such as kernel-PCA or autoencoders [36], PCA provides competitive
generalization capabilities when combined with RC models and can be
computed quickly, due to its linear formulation [21]. Our proposed MTS
representation does not coincide with the last reservoir state, but it
depends on the whole sequence of states generated over time. Therefore,
we conveniently describe our data set as a three-mode tensor H ∈RN×T×R
and require a transformation to map R →D s.t. D ≪R while maintaining the
other dimensions unaltered. Dimensionality reduction on high-order
tensors can be achieved through Tucker decomposition, which decomposes a
tensor into a core tensor (the lower dimensional representation)
multiplied by a matrix along each mode. When only one dimension of H is
modiﬁed, Tucker decomposition becomes equivalent to applying a 2-D PCA
on a speciﬁc matricization of H [37]. Speciﬁcally, to reduce the third
dimension (R), one computes Authorized licensed use limited to: INRIA.
Downloaded on January 16,2024 at 16:23:28 UTC from IEEE Xplore.
Restrictions apply. 2172 IEEE TRANSACTIONS ON NEURAL NETWORKS AND
LEARNING SYSTEMS, VOL. 32, NO. 5, MAY 2021 Fig. 1. Schematic of the
procedure to generate the reservoir model-space representation. For each
input MTS X[n], a sequence of states H[n] is generated by a ﬁxed
reservoir. Those are the frontal slices (dimension N) of H, but note
that in the ﬁgure, lateral slices (dimension T ) are shown. The proposed
dimensionality reduction reduces the reservoir features from R to D. An
independent model is trained to predict ˆ H[n], the nth frontal slice of
ˆ H, and its parameters θh[n] become the representation of X[n]. the
mode-3 matricization of H by arranging the mode-3 ﬁbers (high-order
analog of matrix rows/columns) to be the rows of a resulting matrix H(3)
∈RNT ×R. Then, the standard PCA projects the rows of H(3) on the
eigenvectors associated with the D largest eigenvalues of the covariance
matrix C ∈RR×R, which is deﬁned as C = 1 NT −1 NT  i=1  hi −¯ h  hi
−¯ h T . (10) In (10), hi is the ith row of H(3) and ¯ h = 1 N NT i
hi. As a result of the concatenation of the ﬁrst two dimensions in H, C
evaluates the variation of the components in the reservoir states across
all samples and time steps at the same time. Consequently, both the
original structure of the data set and the temporal orderings are lost,
as the reservoir states relative to different samples and generated in
different time steps are mixed together. This may lead to a potential
loss in the representation capability, as the existence of modes of
variation in time courses within individual samples is ignored. To
address this issue, we consider as individual samples the matrices Hn
∈RT×R, obtained by slicing H across its ﬁrst dimension (N). Our proposed
sample covariance matrix reads S = 1 N −1 N  n=1 Hn −¯ HT Hn −¯ H.
(11) The ﬁrst D leading eigenvectors of S are stacked in a matrix E
∈RR×D and the desired tensor of reduced dimensionality is obtained as ˆ
H = H ×3 E, where, ×3 denotes the three-mode product. Like C, S ∈RR×R
describes the variations of the variables in the reservoir. However,
since the whole sequence of reservoir states is treated as a single
observation, the temporal ordering in different MTS is preserved. After
dimensionality reduction, the model in (7) becomes ˆ h(t + 1) = Uh ˆ
h(t) + uh (12) where ˆ h(·) are the columns of a frontal slice ˆ H of ˆ
H, Uh ∈ RD×D, and uh ∈RD. The representation will now coincide with the
parameters vector rX = θh = [vec(Uh); uh] ∈ RD(D+1), as shown in Fig. 1.
The complexity for computing the reservoir model-space representations
for all the MTS in the data set is given by the sum of O(NTVH), the cost
for computing all the reservoir states, and O(H 2NT + H 3), the cost of
the dimensionality reduction procedure. IV. UNIFIED RESERVOIR COMPUTING
FRAMEWORK FOR TIME SERIES CLASSIFICATION In the last years, several
works independently extended the basic ESN architecture by designing
more sophisticated reservoirs, readouts, or representations of the
input. To eval- uate their synergy and efﬁcacy in the context of MTS
classi- ﬁcation, we introduce a uniﬁed framework that generalizes
several RC architectures by combining four modules: 1) a reservoir
module; 2) a dimensionality reduction module; 3) a representation
module; and 4) a readout module. Fig. 2 shows an overview of the models
that can be implemented in the framework (including the proposed
reservoir model space), by selecting one option in each module. The
input MTS X is processed by a reservoir, which is either unidirectional
or bidirectional, and it generates over time the states sequence H. An
optional dimensionality reduction step reduces the number of reservoir
features and yields a new sequence ¯ H. Three different approaches can
be chosen to generate the input representation rX from the sequence of
reservoir states: the last element in the sequence h(T), the output
state model θo (see Section II-B), or the proposed reservoir state model
θh. The representation rX is ﬁnally processed by a decoder (readout)
that predicts the class y. In the following, we describe the reservoir,
dimensionality reduction, and readout modules and discuss the
functionality of the variants implemented in our framework. A Python
soft- ware library implementing the uniﬁed framework is publicly
available online.2 A. Reservoir Module Several approaches have been
proposed to extend the ESN reservoir with additional features, such as
the capability of handling multiple time scales [38], or to simplify its
large and randomized structure [39]. Of particular interest for the
classiﬁcation of MTS is the bidirectional reservoir, which can replace
the standard reservoir in our framework. RNNs with bidirectional
architectures can extract from the input sequence features that account
for dependences very far in time [40]. In RC, a bidirectional reservoir
has been used in the context of time series prediction to incorporate
future information, only provided during training, to improve the
accuracy of the model [14]. In a classiﬁcation setting, the whole time
series is given at once and, thus, a bidirectional reservoir can be
exploited in both training and test to generate better MTS
representations [35].
2https://github.com/FilippoMB/Reservoir-Computing-framework-for-
multivariate-time-series-classiﬁcation Authorized licensed use limited
to: INRIA. Downloaded on January 16,2024 at 16:23:28 UTC from IEEE
Xplore. Restrictions apply. BIANCHI et al.: RC APPROACHES FOR
REPRESENTATION AND CLASSIFICATION OF MTS 2173 Fig. 2. Framework
overview. The encoder generates a representation rX of the MTS X,
whereas the decoder predicts the label y. Several models are obtained by
selecting one variant for each module. Arrows indicate mutually
exclusive choices. Bidirectionality is implemented by feeding into the
same reservoir an input sequence both in straight and reverse orders ⃗
h(t) = f  Winx(t) + Wr⃗ h(t −1)  ⃗ h(t) = f  Win ⃗ x(t) + Wr ⃗ h(t
−1)  (13) where ⃗ x(t) = x(T −t). The full state is obtained by
concate- nating the two state vectors in (13) and can capture longer
time dependences by summarizing at every step both recent information
and past information. When using a bidirectional reservoir, the linear
model in (8) deﬁning the reservoir model space changes into  h(t + 1);
⃗ h(t + 1)

= Ub h  ⃗ h(t); ⃗ h(t)

-   ub h

(14) where Ub h ∈R2R×2R and ub h ∈R2R are the new set of parameters. In
     this case, the linear model is trained to optimize two distinct
     objectives: predicting the next state h(t + 1) and reproducing the
     previous one ⃗ h(t + 1) (or equivalently their low-dimensional
     embeddings). We argue that such a model provides a more accurate
     representation of the input, by modeling temporal dependences in
     both time directions to jointly solve a prediction and a
     memorization task. B. Dimensionality Reduction Module The
     dimensionality reduction module projects the sequence of reservoir
     activations on a lower dimensional subspace using the unsupervised
     criteria. In the context of RC, commonly used algorithms for
     reducing the dimensionality of the reservoir are PCA and kernel
     PCA, which project data on the ﬁrst D eigen- vectors of a
     covariance matrix. When dealing with a prediction task,
     dimensionality reduction is applied to a single sequence of
     reservoir states generated by the input MTS [21]. On the other
     hand, in a classiﬁcation task, each MTS is associated with a
     different sequence of states [35]. If the MTSs are represented by
     the last reservoir states, those are stacked into a matrix to which
     standard dimensionality reduction procedures were applied. When
     instead the whole set of representations is represented by a
     tensor, as discussed in Section III, the dimensionality reduction
     technique should account for factors of variation across more than
     one dimension. Contrarily to the other modules, it is possible to
     implement an RC classiﬁer without the dimensionality reduction
     module (as depicted by the skip connection in Fig. 2). However, as
     discussed in Section III, dimensionality reduction is par-
     ticularly important when implementing the proposed reservoir
     model-space representation or when using a bidirectional reservoir,
     in which cases the size of the representation rX grows with respect
     to a standard implementation. C. Readout Module The readout module
     (decoder) classiﬁes the representations and is either implemented
     as a linear readout or a support vector machine (SVM) classiﬁer or
     a multilayer perceptron (MLP). In a standard ESN, the readout is
     linear and is quickly trained by solving a convex optimization
     problem. However, a linear readout might not possess sufﬁcient
     representational power for modeling the embeddings derived from the
     reservoir states. For this reason, several authors proposed to
     replace the linear decoding function g(·) in (5) with a nonlinear
     model, such as SVMs [12] or MLPs [41]–[43]. Readouts implemented as
     MLPs accomplished only modest results in the earliest works on RC
     [10]. However, nowadays, MLPs can be trained much more efﬁciently
     by means of sophisticated initialization procedures [44] and
     regularization techniques [24]. The combination of ESNs with MLPs
     trained with modern techniques can substantially improve the
     perfor- mance compared with a linear formulation [35]. Following
     Authorized licensed use limited to: INRIA. Downloaded on January
     16,2024 at 16:23:28 UTC from IEEE Xplore. Restrictions apply. 2174
     IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32,
     NO. 5, MAY 2021 TABLE I TIME SERIES BENCHMARK DATA SETS DETAILS.
     COLUMNS 2–5 REPORT THE NUMBER OF VARIABLES (#V ), SAMPLES IN
     TRAINING AND TEST SET, AND NUMBER OF CLASSES (#C), RESPECTIVELY.
     TMIN IS THE LENGTH OF THE SHORTEST MTS IN THE DATA SET AND TMAX IS
     THE LONGEST MTS. ALL DATA SETS ARE AVAILABLE AT OUR GITHUB
     REPOSITORY recent trends in the deep learning literature, we also
     investigate endowing the MLP readout with more expressive ﬂexible
     nonlinear activation functions, namely Maxout [45] and kernel
     activation functions (KAFs) [46]. V. EXPERIMENTS We test a variety
     of RC-based architectures for MTS classiﬁcation implemented with
     the proposed framework. We also compare against RNNs’ classiﬁers
     trained with gradient descent (LSTM and GRU), a one-NN classiﬁer
     based on the DTW similarity, SVM classiﬁers conﬁgured with
     precomputed kernels for MTS, different deep learning architectures,
     and other state-of-the-art methods for time series classiﬁcation.
     Depending whether the input MTS in the RC-based model is
     represented by the last reservoir state (rX = h(T )), by the output
     space model (see Section II-B), or by the reservoir space model
     (see Section III), we refer to the models as lESN, omESN, and
     rmESN, respectively. Whenever we use a bidirectional reservoir, a
     deep MLP readout or an SVM readout we add the preﬁx “bi-,” “dr-,”
     and “svm-,” respectively (e.g., bi-lESN or dr-bi-rmESN).

a)  MTS Data Sets: To evaluate the performance of each classiﬁer, we
    consider several MTS classiﬁcation data sets taken from the UCR,3
    UEA,4 and UCI repositories.5 For completeness, we also included
    three univariate time series data sets. Details of the data sets are
    reported in Table I.
b)  Blood samples data set: As a case study on medical data, we analyze
    MTS of blood measurements obtained from electronic health records of
    patients undergoing a gastroin- testinal surgery at the University
    Hospital of North Norway 3www.cs.ucr.edu/~eamonn/time_series_data
    4https://www.groundai.com/project/the-uea-multivariate-time-series-
    classiﬁcation-archive-2018/ 5archive.ics.uci.edu/ml/datasets.html in
    2004–2012.6 Each patient is represented by an MTS of ten blood
    sample measurements collected for 20 days after surgery. We consider
    the problem of classifying patients with and without surgical site
    infections from their blood sam- ples, collected 20 days after
    surgery. The data set consists of 883 MTS, of which 232 pertain to
    infected patients. The original MTSs contain missing data,
    corresponding to measurements not collected for a given patient at
    certain time intervals, which are replaced by zero imputation in a
    preprocessing step.
c)  Experimental setup: For each data set, we train the models ten times
    using independent random parameters initial- izations. Each model is
    conﬁgured with the same hyperpara- meters in all the experiments.
    Since reservoirs are sensitive to the hyperparameter setting [47], a
    ﬁne-tuning with independent cross validation for each task is
    usually more important in classic RC models than in RNNs trained
    with gradient descent, such as LSTM and GRU. Nevertheless, we show
    that the proposed rmESN achieves competitive results even with ﬁxed
    the hyperparameters. This indicates higher robustness and gives a
    practical advantage compared with traditional RC approaches. To
    provide a signiﬁcant comparison, lESN, omESN, and rmESN always share
    the same randomly generated reservoir conﬁgured with the following
    hyperparameters: number of internal units R = 800, spectral radius ρ
    = 0.99, nonzero connections percentage β = 0.25, input scaling ω =
    0.15, and noise level ξ = 0.001. When the classiﬁcation is performed
    with a ridge regression readout, we set the regularization value λ =
    1.0. The ridge regression prediction models, used to gen- erate the
    model-space representation in omESN and rmESN, are conﬁgured with λ
    = 5.0. We always apply dimensionality reduction, as it provides
    important computational advantages (both in terms of memory and CPU
    time), as well as a regularization that improves the generalization
    capability and robustness of all RC models. For all experiments, we
    select the number of subspace dimensions as D = 75, following a grid
    search with k-fold cross validation on the data sets of Table I (see
    the Supplementary Material for details). LSTM and GRU are conﬁgured
    with H = 30 hidden units, the decoding function is implemented as a
    neural network with 2 dense layers of 20 hidden units followed by a
    softmax layer, the dropout probability is pdrop = 0.1, the ℓ2
    regularization parameter is λ = 0.0001, gradient descent is
    performed with the Adam algorithm [48], and we train the models for
    5000 epochs Finally, the one-NN classiﬁer uses FastDTW [5], which is
    a computationally efﬁcient approximation of the DTW.7 We acknowledge
    additional approaches based on DTW [49], [50], which, however, are
    not discussed in this article. A. Performance Comparison on
    Benchmark Data Sets In this experiment, we compare the classiﬁcation
    accuracy obtained on the representations yielded by the RC models,
    lESN, omESN, and rmESN, by the fully trainable RNNs 6The data set
    has been published in the AMIA Data Competition 2016. 7We used the
    ofﬁcial Python library: https://pypi.org/project/fastdtw/ Authorized
    licensed use limited to: INRIA. Downloaded on January 16,2024 at
    16:23:28 UTC from IEEE Xplore. Restrictions apply. BIANCHI et al.:
    RC APPROACHES FOR REPRESENTATION AND CLASSIFICATION OF MTS 2175 Fig.
    3. Comparison of the average results obtained on all benchmark data
    sets. implementing either GRU or LSTM cells, and by the one- NN
    classiﬁer based on DTW. Evaluation is performed on the benchmark
    data sets in Table I. The decoder is implemented by linear
    regression in the RC models and by a dense nonlinear layer in LSTM
    and GRU. Since all the other parameters in LSTM and GRU are learned
    with gradient descent, the non- linearities in the decoding function
    do not result in additional computational costs. Results are
    reported in Fig. 3. The ﬁrst panel reports the mean classiﬁcation
    accuracy and standard deviation of ten independent runs on all
    benchmark data sets, whereas the second panel shows the average
    execution time (in minutes on a logarithmic scale) required for
    training and testing the models. The RC classiﬁers when conﬁgured
    with model-space rep- resentations achieve a much higher accuracy
    than the basic lESN. In particular, rmESN, which adopts our proposed
    rep- resentation, reaches the best overall mean accuracy and the low
    standard deviation indicates that it is also stable, i.e., it yields
    consistently good results regardless of the random initialization of
    the reservoir. The second-best accuracy is obtained by one- NN with
    DTW, while the classiﬁers based on LSTM and GRU perform only better
    than lESN. The results are particularly interesting since LSTM and
    GRU exploit supervised informa- tion to learn the representations
    rX, and they adopt a powerful nonlinear discriminative classiﬁer. On
    the other hand, the RC classiﬁer conﬁgured with the model-space
    representation outperforms the other RNN architectures, despite that
    it relies on a linear classiﬁer and the representations are learned
    in a completely unsupervised fashion. In terms of execution time,
    all the RC classiﬁers are much faster than the competitors, as the
    average time for training and test is only a few seconds.
    Remarkably, due to the proposed dimensionality reduction procedure,
    the rmESN classiﬁer can be executed in a time comparable to lESN.
    The classiﬁers based on fully trainable RNNs, LSTM and GRU, require
    on average more than 20 min. Finally, one-NN with DTW is much slower
    than the other methods despite the adopted “fast” implementation
    [5]. This is evident by looking at the huge gap in the execution
    time, which is more than 11 h on average and goes beyond 30 h in
    some data sets (see the Supplementary Material for the details). B.
    Experiments With Bidirectional Reservoir and Deep Readout In this
    experiment, we investigate how a bidirectional reser- voir and a
    deep readout, implemented by an MLP, inﬂuence Fig. 4. Classiﬁcation
    accuracy and execution time when using RC classiﬁers with a
    bidirectional reservoir and deep readouts, conﬁgured with ReLUs,
    KAFs, and Maxout activations. classiﬁcation accuracy and execution
    time in the RC-based classiﬁers. To further increase the ﬂexibility
    of the deep read- out, beside the standard rectiﬁed linear unit
    (ReLU), we also employ in the MLP more sophisticated transfer
    functions, namely Maxout [45] and KAFs [46]. Due to their adaptable
    parameters, trained jointly with the other MLP weights, these
    functions can improve the expressive capability of the MLP
    classiﬁer. We refer the reader to the original publications for
    details on their formulation. The deep readout is implemented with 3
    layers of 20 neurons each and is trained for 5000 epochs, using a
    dropout probability pdrop = 0.1 and L2 regularization parameter λ =
    0.001. We repeat the models’ evaluation on all the benchmark data
    sets, and in Fig. 4, we report results in terms of classiﬁcation
    accuracy and training time. We can see that both the bidirectional
    reservoir and deep readout improve, to different extents, the
    classiﬁcation accuracy of each RC classiﬁer. The largest improvement
    occurs for lESN when implemented with a bidirectional reservoir.
    This is expected since the last state representation in lESN depends
    mostly on the last observed values of the input MTS. Whenever the
    most relevant information is contained at the beginning of the input
    sequence or when the MTSs are too long and the reservoir memory
    limitation forestalls capturing long-term dependences, the
    bidirectional architecture greatly improves the lESN representation.
    The bidirectional reservoir improves the performance also in omESN
    and rmESN. We recall that in these cases, rather than learning only
    a model for predicting the next output/state, when using a
    bidirectional reservoir, the model also learns to solve a
    memorization task. The performance improvement for these models is
    lower than that for lESN, probably because the representations
    obtained with a unidirectional reservoir are already good enough.
    Neverthe- less, bi-rmESN reaches the highest overall accuracy. A
    deep readout enhances the capabilities of the classiﬁer;
    improvements are larger in lESN and more limited in omESN and rmESN.
    Once again, this underlines that the weaker lESN representation
    beneﬁts by adding more complexity in the pipeline. Even more than
    the bidirectional reservoir, a deep readout trades greater modeling
    capabilities with more computational resources, especially when
    implemented with Authorized licensed use limited to: INRIA.
    Downloaded on January 16,2024 at 16:23:28 UTC from IEEE Xplore.
    Restrictions apply. 2176 IEEE TRANSACTIONS ON NEURAL NETWORKS AND
    LEARNING SYSTEMS, VOL. 32, NO. 5, MAY 2021 Fig. 5. Ranking in terms
    of mean accuracy obtained by the MTS classiﬁers on all the 14 data
    sets. A lower value in ranking indicates better average accuracy.
    adaptive activation functions. Remarkably, when using Maxout
    functions rather than a standard ReLU, the training time is slightly
    higher, but there are signiﬁcant improvements in the average
    classiﬁcation accuracy. In particular, dr-omESN (Maxout) obtains
    almost the same performance of the basic version of rmESN. Another
    interesting result obtained by both Maxout and KAF is a reduction in
    the standard deviation of the accuracy, hence, a more robust
    classiﬁcation. In Fig. 5, we report the overall ranking, in terms of
    mean accuracy, of the 18 MTS classiﬁers presented so far on the 17
    classiﬁcation data sets. On each data set, the algorithms are ranked
    from 1 (best accuracy) to 18 (worst accuracy) and the table depicts
    the average of the ranks. It emerges that the proposed reservoir
    model-space representation is the key factor to achieve the highest
    classiﬁcation accuracy and that by introducing further complexity,
    by means of deep readouts and bidirectional reservoir, performance
    is further improved. C. Classiﬁcation of Blood Samples MTS Here, we
    analyze the blood sample MTS and evaluate the RC classiﬁers
    conﬁgured with an SVM readout. We consider only omESN and rmESN
    since, as demonstrated in the previous experiments, they provide an
    optimal compromise between training efﬁciency and classiﬁcation
    accuracy. Since we adopt a kernel method to implement the decoding
    function

(3) (readout), we compare against two state-of-the-art kernels for MTS.
    The ﬁrst is the learned pattern similarity (LPS) [51], which
    identiﬁes segments occurrence within the MTS by means of regression
    trees. Those are used to generate a bag-of- word-type compressed
    representation, on which the similarity scores are computed. The
    second method is the time series cluster kernel (TCK) [52], which is
    based on an ensemble learning procedure in which the clustering
    results of several Gaussian mixture models, which are ﬁt many times
    on random subsets of the original data set, are joined to form the
    ﬁnal kernel. For LPS and TCK, an SVM is conﬁgured with the precom-
    puted kernels returned by the two procedures, whereas for omESN and
    rmESN, we build an RBF kernel with bandwidth γ . We optimize on a
    validation set the SVM hyperparameters that are the smoothness of
    the decision hyperplane c and bandwidth γ (only omESN and rmESN).
    The hyperparameter Fig. 6. Classiﬁcation accuracy obtained with SVM
    using different precom- puted kernels. We also report the results
    obtained by rmESN and omESN on the same problem. space is explored
    with a grid search, by varying c in [0.1, 5.0] with resolution 0.1
    and γ in [0.01, 1.0] with a resolution of 0.01. LPS is conﬁgured
    using 200 regression trees and maxi- mum segments length 10. TCK is
    conﬁgured with 40 different random initializations and 30 maximum
    mixtures for each partition. RC classiﬁers use the same
    hyperparameters as in the previous experiments. To compute the
    performance of the models, those are evaluated 15 times with
    independent random initializations and randomly shufﬂing and
    splitting the original data set into the training, validation, and
    test sets, containing 70%, 10%, and 20% of the original samples,
    respectively. Each time, we normalize the data by subtracting the
    mean and dividing by the standard deviation of each variable in the
    training set, excluding the imputed values. The results in terms of
    classiﬁcation accuracy and training time are shown in Fig. 6. For
    completeness, we report also the classiﬁcation results obtained on
    this task by omESN and rmESN, with g(·) implemented as a linear
    readout. Also, in this case, rmESN outperforms omESN either when it
    is conﬁgured with a linear or an SVM readout. As for the deep
    readout, we notice that the more powerful decoding function improves
    the classiﬁcation accuracy in rmESN only slightly, while the
    increment in omESN is much larger. The svm-rmESN manages to slightly
    outperform the SVM classiﬁers conﬁgured with LPS and TCK kernels. We
    notice that standard deviations in all methods are quite high since
    the train/validation/test splits are generated randomly at every
    iteration and, therefore, the classiﬁcation task changes each time.
    svm-TCK yields the results with the lowest standard deviation and is
    followed by svm-rmESN and rmESN. The SVM readout slightly increases
    the training time of the RC models, but they are still much faster
    than the TCK and LPS kernels.

VI. COMPARISON WITH DEEP LEARNING BASELINES ON THE CLASSIFICATION OF
    UNIVARIATE TIME SERIES Although the proposed framework is
    speciﬁcally designed for the classiﬁcation of MTS, we conclude by
    considering additional experiments on univariate time series
    classiﬁcation data sets.8 Compared with the multivariate case,
    algorithms designed for this task can exploit stronger biases to
    attain 8We used data sets from
    http://www.timeseriesclassiﬁcation.com Authorized licensed use
    limited to: INRIA. Downloaded on January 16,2024 at 16:23:28 UTC
    from IEEE Xplore. Restrictions apply. BIANCHI et al.: RC APPROACHES
    FOR REPRESENTATION AND CLASSIFICATION OF MTS 2177 TABLE II RESULTS
    ON UNIVARIATE TS CLASSIFICATION. THE BEST RESULTS ARE IN BOLD, AND
    THE SECOND BEST ARE UNDERLINED high classiﬁcation performance. We
    also notice that in the case of univariate time series, we do not
    adopt the proposed extension of PCA for multivariate temporal data,
    but a reg- ular PCA is used instead. Nevertheless, we show that our
    method can achieve competitive results compared with state-
    of-the-art methods for time series classiﬁcation. We choose rmESN as
    the representative model of the RC classiﬁers, which provides a good
    tradeoff between classiﬁcation accuracy and training time. Table II
    reports the results obtained by rmESN and several different methods.
    We implement base- lines based on popular deep learning
    architectures (MLP, FCN, and ResNets) [53], [54] and report results
    where avail- able on the original articles for BOSS [55], PROP [56],
    COTE [57], an advanced deep learning architecture that combines an
    LSTM with attention to a CNN architecture (LSTM-FCN) [58], a
    model-metric colearning methodology for sequence classiﬁcation that
    learns in the model space

(MMCL) [59], and a feature-based model (TSML) [6]. It is possible to see
       that the complex deep learning architec- ture LSTM-FCN achieves,
       on average, the best classiﬁcation accuracy. On the other hand,
       the rmESN model equipped with a simple linear readout achieves
       the results that are competitive to those obtained by much more
       complex models while requiring only a few seconds to be trained.

VII. CONCLUSION AND FUTURE WORK We proposed an RC classiﬁer based on the
     reservoir model-space representation, which can be categorized as a
     hybrid generative–discriminative approach. Speciﬁcally, the
     parameters of a model that predict the next reservoir states
     characterize the generative process of the dynamical input
     features. Such parameters are, in turn, processed by a dis-
     criminative decoder that classiﬁes the original time series.
     Usually, in a hybrid generative–discriminative approach where data
     are assumed to be generated by a parametric distribution, the
     subsequent discriminative model cannot be speciﬁed inde- pendently
     of the generative model type, without introducing biases in the
     classiﬁcation [60]. However, in our case, the reservoir is ﬂexible
     and generic, as it can extract a large variety of features from the
     underlying dynamical system, without posing constraints on the
     particular model underlying the data distribution. This provides
     two advantages: 1) different discriminative models can be used in
     conjunction with the same reservoir model-space representation
     and 2) the same reservoir can model data from different
     distributions. To make the reservoir model space tractable, we
     designed an unsupervised dimensionality reduction procedure,
     suitable for data sets represented as high-order tensors. Our
     dimensionality reduction greatly reduces computational time and
     memory usage and provides a regularization that prevents overﬁt-
     ting, especially in complex discriminative classiﬁers. Finally, we
     deﬁned a uniﬁed framework and investigated several alternatives to
     build RC classiﬁers, focusing on unsupervised procedures to learn
     ﬁxed-size representations of the MTS. We considered several
     real-world data sets for classiﬁcation of MTS, showing that the RC
     classiﬁer equipped with the proposed representation achieves
     superior performance both in terms of classiﬁcation accuracy and
     execution time. We ana- lyzed how a bidirectional reservoir and a
     deep readout affect the performance (both in time and accuracy) of
     RC-based classiﬁers conﬁgured with different representations. We
     found that combining the reservoir model space with these more
     sophisticated architectures improves accuracy only slightly,
     pointing to the already strong informative content of this
     representation. We also considered a medical case study of blood
     samples time series and obtained superior performance compared with
     state-of-the-art kernels for MTS. We concluded by comparing with
     state-of-the-art methods on the classiﬁca- tion of univariate time
     series and showed that, even on those tasks, our approach achieves
     competitive results. REFERENCES [1] K. O. Mikalsen et al.,
     “Learning similarities between irregularly sampled short
     multivariate time series from EHRs,” in Proc. 3rd Int. Workshop
     Pattern Recognit. Healthcare Anal., 2016, pp. 1–6. [2] E. P. Carden
     and J. M. W. Brownjohn, “ARMA modelled time-series classiﬁcation
     for structural health monitoring of civil infrastructure,” Mech.
     Syst. Signal Process., vol. 22, no. 2, pp. 295–314, Feb. 2008.
     [3] D. Hunt and D. Parry, “Using echo state networks to classify
     unscripted, real-world punctual activity,” in Engineering
     Applications of Neural Networks. Cham, Switzerland: Springer, 2015,
     pp. 369–378. [4] E. Trentin, S. Scherer, and F. Schwenker, “Emotion
     recognition from speech signals via a probabilistic echo-state
     network,” Pattern Recognit. Lett., vol. 66, pp. 4–12, Nov. 2015.
     [5] S. Salvador and P. Chan, “Toward accurate dynamic time warping
     in linear time and space,” Intell. Data Anal., vol. 11, no. 5,
     pp. 561–580, Oct. 2007. [6] C. O’Reilly, K. Moessner, and M. Nati,
     “Univariate and multivariate time series manifold learning,”
     Knowl.-Based Syst., vol. 133, pp. 1–16, Oct. 2017. Authorized
     licensed use limited to: INRIA. Downloaded on January 16,2024 at
     16:23:28 UTC from IEEE Xplore. Restrictions apply. 2178 IEEE
     TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO.
     5, MAY 2021 [7] A. Bagnall, J. Lines, A. Bostrom, J. Large, and E.
     Keogh, “The great time series classiﬁcation bake off: A review and
     experimental evaluation of recent algorithmic advances,” Data
     Mining Knowl. Discovery, vol. 31, no. 3, pp. 606–660, May 2017.
     [8] M. G. Baydogan and G. Runger, “Learning a symbolic
     representation for multivariate time series classiﬁcation,” Data
     Mining Knowl. Discovery, vol. 29, no. 2, pp. 400–422, Mar. 2015.
     [9] A. Graves, A.-R. Mohamed, and G. Hinton, “Speech recognition
     with deep recurrent neural networks,” in Proc. IEEE Int. Conf.
     Acoust., Speech Signal Process., May 2013, pp. 6645–6649. [10] M.
     Lukoševiˇ cius and H. Jaeger, “Reservoir computing approaches to
     recurrent neural network training,” Comput. Sci. Rev., vol. 3, no.
     3, pp. 127–149, Aug. 2009. [11] S. Scardapane and D. Wang,
     “Randomness in neural networks: An overview,” Wiley
     Interdisciplinary Rev., Data Mining Knowl. Discovery, vol. 7, no.
     2, p. e1200, 2017. [12] F. M. Bianchi, S. Scardapane, A. Uncini, A.
     Rizzi, and A. Sadeghian, “Prediction of telephone calls load using
     echo state network with exogenous variables,” Neural Netw.,
     vol. 71, pp. 204–213, Nov. 2015. [13] F. M. Bianchi, E. De
     Santis, A. Rizzi, and A. Sadeghian, “Short- term electric load
     forecasting using echo state networks and PCA decomposition,” IEEE
     Access, vol. 3, pp. 1931–1943, Oct. 2015. [14] A. Rodan, A. F.
     Sheta, and H. Faris, “Bidirectional reservoir networks trained
     using SVM + privileged information for manufacturing process
     modeling,” Soft Comput., vol. 21, no. 22, pp. 6811–6824, Nov. 2017.
     [15] H. Jaeger, “The ‘echo state’ approach to analysing and
     training recurrent neural networks-with an erratum note,” German
     Nat. Res. Center Inf. Technol., Bonn, Germany, GMD Rep. 148,
     vol. 148, no. 34, 2001. [16] Q. Ma, L. Shen, W. Chen, J. Wang, J.
     Wei, and Z. Yu, “Functional echo state network for time series
     classiﬁcation,” Inf. Sci., vol. 373, pp. 1–20, Dec. 2016. [17] M.
     D. Skowronski and J. G. Harris, “Minimum mean squared error time
     series classiﬁcation using an echo state network prediction model,”
     in Proc. IEEE Int. Symp. Circuits Syst., May 2006, p. 4. [18] W.
     Aswolinskiy, R. Reinhart, and J. Steil, “Time series classiﬁcation
     in reservoir- and model-space: A comparison,” in Proc. IAPR
     Workshop Artif. Neural Netw. Pattern Recognit. Cham, Switzerland:
     Springer, 2016, pp. 197–208. [19] H. Chen, F. Tang, P. Tino, and X.
     Yao, “Model-based kernel for efﬁcient time series analysis,” in
     Proc. 19th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining
     (KDD), 2013, pp. 392–400. [20] H. Chen, P. Tino, A. Rodan, and X.
     Yao, “Learning in the model space for cognitive fault diagnosis,”
     IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 1, pp. 124–136,
     Jan. 2014. [21] S. Løkse, F. M. Bianchi, and R. Jenssen, “Training
     echo state networks with regularization through dimensionality
     reduction,” Cognit. Comput., vol. 9, no. 3, pp. 364–378, Jun. 2017.
     [22] F. M. Bianchi, E. Maiorino, M. C. Kampffmeyer, A. Rizzi,
     and R. Jenssen, Recurrent Neural Networks for Short-Term Load Fore-
     casting: An Overview and Comparative Analysis. Cham, Switzerland:
     Springer, 2017. [23] I. Sutskever, O. Vinyals, and Q. V. Le,
     “Sequence to sequence learning with neural networks,” in Proc. Adv.
     Neural Inf. Process. Syst., 2014, pp. 3104–3112. [24] N.
     Srivastava,

G.  Hinton,
H.  Krizhevsky,
I.  Sutskever, and R. Salakhutdinov, “Dropout: A simple way to prevent
    neural networks from overﬁtting,” J. Mach. Learn. Res., vol. 15, no.
    1, pp. 1929–1958,

2014. [25] W. Zaremba, I. Sutskever, and O. Vinyals, “Recurrent neural
      network regularization,” 2014, arXiv:1409.2329. [Online].
      Available: http://arxiv. org/abs/1409.2329 [26] R. Pascanu, T.
      Mikolov, and Y. Bengio, “On the difﬁculty of training recurrent
      neural networks,” in Proc. Int. Conf. Mach. Learn., 2013,
      pp. 1310–1318. [27] S. Hochreiter and J. Schmidhuber, “Long
      short-term memory,” Neural Comput., vol. 9, no. 8, pp. 1735–1780,
      1997. [28] K. Cho et al., “Learning phrase representations using
      RNN encoder- decoder for statistical machine translation,” 2014,
      arXiv:1406.1078. [Online]. Available:
      http://arxiv.org/abs/1406.1078 [29] F. M. Bianchi, L. Livi, and C.
      Alippi, “Investigating echo-state networks dynamics by means of
      recurrence analysis,” IEEE Trans. Neural Netw. Learn. Syst.,
      vol. 29, no. 2, pp. 427–439, Feb. 2018. [30] L. Livi, F. M.
      Bianchi, and C. Alippi, “Determination of the edge of criti-
      cality in echo state networks through Fisher information
      maximization,” IEEE Trans. Neural Netw. Learn. Syst., vol. 29, no.
      3, pp. 706–717, Mar. 2018. [31] Z. Gong, H. Chen, B. Yuan, and X.
      Yao, “Multiobjective learning in the model space for time series
      classiﬁcation,” IEEE Trans. Cybern., vol. 49, no. 3, pp. 918–932,
      Mar. 2019. [32] L. Wang, Z. Wang, and S. Liu, “An effective
      multivariate time series classiﬁcation approach using echo state
      network and adaptive differ- ential evolution algorithm,” Expert
      Syst. Appl., vol. 43, pp. 237–249, Jan. 2016. [33] A. van den
      Oord, Y. Li, and O. Vinyals, “Representation learning with
      contrastive predictive coding,” 2018, arXiv:1807.03748. [Online].
      Available: http://arxiv.org/abs/1807.03748 [34] A. Y. Ng and M. I.
      Jordan, “On discriminative vs. Generative classiﬁers: A comparison
      of logistic regression and naive Bayes,” in Proc. Adv. Neural Inf.
      Process. Syst., 2002, pp. 841–848. [35] F. M. Bianchi, S.
      Scardapane, S. Løkse, and R. Jenssen, “Bidirectional deep-readout
      echo state networks,” in Proc. Eur. Symp. Artif. Neural Netw.,
      2018, pp. 1–6. [36] F. M. Bianchi, L. Livi, K. Ø. Mikalsen, M.
      Kampffmeyer, and R. Jenssen, “Learning representations of
      multivariate time series with missing data,” Pattern Recognit.,
      vol. 96, Dec. 2019, Art. no. 106973. [37] T. G. Kolda and B. W.
      Bader, “Tensor decompositions and applications,” SIAM Rev.,
      vol. 51, no. 3, pp. 455–500, Aug. 2009. [38] C. Gallicchio and A.
      Micheli, “Echo state property of deep reservoir computing
      networks,” Cognit. Comput., vol. 9, no. 3, pp. 337–350, Jun. 2017.
      [39] A. Rodan and P. Tino, “Simple deterministically constructed
      cycle reservoirs with regular jumps,” Neural Comput., vol. 24, no.
      7, pp. 1822–1852, Jul. 2012. [40] A. Graves and J. Schmidhuber,
      “Framewise phoneme classiﬁcation with bidirectional LSTM and other
      neural network architectures,” Neural Netw., vol. 18, nos. 5–6,
      pp. 602–610, Jul. 2005. [41] W. Maass, T. Natschläger, and H.
      Markram, “Real-time computing without stable states: A new
      framework for neural computation based on perturbations,” Neural
      Comput., vol. 14, no. 11, pp. 2531–2560, Nov. 2002. [42] K. Bush
      and C. Anderson, “Modeling reward functions for incomplete state
      representations via echo state networks,” in Proc. IEEE Int. Joint
      Conf. Neural Netw., vol. 5, Jul./Aug. 2005, pp. 2995–3000. [43] V.
      S. Babinec and J. Pospíchal, “Merging echo state and feedforward
      neural networks for time series forecasting,” in Proc. Int. Conf.
      Artif. Neural Netw. Berlin, Germany: Springer, 2006, pp. 367–375.
      [44] X. Glorot and Y. Bengio, “Understanding the difﬁculty of
      training deep feedforward neural networks,” in Proc. 13th Int.
      Conf. Artif. Intell. Statist., 2010, pp. 249–256. [45] I. J.
      Goodfellow, D. Warde-Farley, M. Mirza, A. C. Courville, and Y.
      Bengio, “Maxout networks,” in Proc. 30th Int. Conf. Mach. Learn.
      (ICML), 2013, pp. 1319–1327. [46] S. Scardapane, S. Van
      Vaerenbergh, S. Totaro, and A. Uncini, “Kafnets: Kernel-based
      non-parametric activation functions for neural networks,” 2017,
      arXiv:1707.04035. [Online]. Available: http://
      arxiv.org/abs/1707.04035 [47] F. M. Bianchi, L. Livi, C. Alippi,
      and R. Jenssen, “Multiplex visibility graphs to investigate
      recurrent neural network dynamics,” Sci. Rep., vol. 7, no. 1,
      p. 44037, Apr. 2017. [48] D. P. Kingma and J. Ba, “Adam: A method
      for stochastic opti- mization,” 2014, arXiv:1412.6980. [Online].
      Available: http://arxiv. org/abs/1412.6980 [49] T. Górecki and M.
      Łuczak, “Multivariate time series classiﬁcation with parametric
      derivative dynamic time warping,” Expert Syst. Appl., vol. 42, no.
      5, pp. 2305–2312, 2015. [50] J. Mei, M. Liu, Y.-F. Wang, and H.
      Gao, “Learning a maha- lanobis distance-based dynamic time warping
      measure for multivari- ate time series classiﬁcation,” IEEE Trans.
      Cybern., vol. 46, no. 6, pp. 1363–1374, Jun. 2016. [51] M. G.
      Baydogan and G. Runger, “Time series representation and similarity
      based on local autopatterns,” Data Mining Knowl. Discovery,
      vol. 30, no. 2, pp. 476–509, Mar. 2016. [52] K. Ø. Mikalsen, F. M.
      Bianchi, C. Soguero-Ruiz, and R. Jenssen, “Time series cluster
      kernel for learning similarities between multivariate time series
      with missing data,” Pattern Recognit., vol. 76, pp. 569–581,
      Apr. 2018. [53] H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar,
      and P.-A. Müller, “Deep learning for time series classiﬁcation: A
      review,” Data Mining Knowl. Discovery, vol. 33, no. 4,
      pp. 917–963, Jul. 2019. [54] Z. Wang, W. Yan, and T. Oates, “Time
      series classiﬁcation from scratch with deep neural networks: A
      strong baseline,” in Proc. Int. Joint Conf. Neural Netw. (IJCNN),
      May 2017, pp. 1578–1585. Authorized licensed use limited to:
      INRIA. Downloaded on January 16,2024 at 16:23:28 UTC from IEEE
      Xplore. Restrictions apply. BIANCHI et al.: RC APPROACHES FOR
      REPRESENTATION AND CLASSIFICATION OF MTS 2179 [55] P. Schäfer,
      “Scalable time series classiﬁcation,” Data Mining Knowl.
      Discovery, vol. 30, no. 5, pp. 1273–1298, Sep. 2016. [56] J. Lines
      and A. Bagnall, “Time series classiﬁcation with ensembles of
      elastic distance measures,” Data Mining Knowl. Discovery, vol. 29,
      no. 3, pp. 565–592, May 2015. [57] A. Bagnall, J. Lines, J. Hills,
      and A. Bostrom, “Time-series classiﬁcation with COTE: The
      collective of transformation-based ensembles,” IEEE Trans. Knowl.
      Data Eng., vol. 27, no. 9, pp. 2522–2535, Sep. 2015. [58] F.
      Karim, S. Majumdar, H. Darabi, and S. Chen, “LSTM fully convo-
      lutional networks for time series classiﬁcation,” IEEE Access,
      vol. 6, pp. 1662–1669, 2017. [59] H. Chen, F. Tang, P. Tino, A. G.
      Cohn, and X. Yao, “Model metric co-learning for time series
      classiﬁcation,” in Proc. 24th Int. Joint Conf. Artif. Intell.,
      2015, pp. 1–8. [60] K. H. Brodersen et al., “Generative embedding
      for model-based classi- ﬁcation of fMRI data,” PLoS Comput. Biol.,
      vol. 7, no. 6, Jun. 2011, Art. no. e1002079. Filippo Maria Bianchi
      received the Ph.D. degree from the Department of Information
      Engineering, Electronics, and Telecommunications, Sapienza Uni-
      versity of Rome, Rome, Italy, in 2016. He has been with Ryerson
      University, Toronto, ON, Canada, the Universita’ della Svizzera
      Italiana, Lugano, Switzerland, and the UiT—The Arctic Uni- versity
      of Norway, Tromsø, Norway. He is currently a Research Scientist at
      the NORCE Norwegian Research Centre, Bergen, Norway. He has
      authored more than 40 scientiﬁc publications in international
      conferences and journals. His research lies at the intersection
      between machine learning, dynamical systems, and complex networks.
      He worked for several years on developing new methodologies and
      interpretability tools for recurrent neural networks, including
      reservoir computing architectures. In this context, he worked on
      applications in time series prediction and multivariate time
      series classiﬁcation, with a special focus on handling missing
      data. Dr. Bianchi is a member of the IEEE Task Force on Reservoir
      Computing and the Vice-Chair of the IEEE Task Force on Learning
      for Structured Data. Simone Scardapane received the B.Sc. degree
      in computer engineering from Roma Tre University, Rome, Italy, in
      2009, the M.Sc. degree in artiﬁcial intelligence and robotics from
      the Sapienza Univer- sity of Rome, Rome, in 2011, and the
      Ph.D. degree from the Sapienza University of Rome in 2016, with a
      focus on distributed machine learning and adaptive audio
      processing. He worked one year as a software/web developer. He was
      a Post-Doctoral Fellow with the Sapienza University of Rome, where
      he is currently an Assis- tant Professor. During his studies, he
      visited Tohoku University, Sendai, Japan, for a summer school in
      robotics, and La Trobe University, Melbourne, Vic, Australia, as a
      Visiting Ph.D. Student. He has a strong interest in promoting
      machine learning in Italy. Dr. Scardapane is the Co-Founder and
      the Chairman of the Italian Associa- tion for Machine Learning,
      the Co-Organizer of the Rome Machine Learning and Data Science
      Meetup, and a current Google Developer Expert for Machine
      Learning. Sigurd Løkse received the B.Sc. degree in automa- tion
      engineering and the M.Sc. degree in electrical engineering from
      the UiT—The Arctic University of Norway, Tromsø, Norway, in 2012
      and 2014, respectively, where he is currently pursuing the
      Ph.D. degree at the Machine Learning Group. Robert Jenssen
      (Member, IEEE) received the Dr. Scient (Ph.D.) degree from the
      University of Tromsø (UiT), Tromsø, Norway, in 2005. He has had
      long-term research stays at the Uni- versity of Florida,
      Gainesville, FL, USA, the Tech- nical University of Berlin,
      Berlin, Germany, and the Technical University of Denmark, Kongens
      Lyngby, Denmark. He was a Senior Researcher (20%) with the
      Norwegian Centre for E-health Research. He is currently a
      Professor and the Head of the Machine Learning Group, UiT—The
      Arctic University of Norway, Tromsø, Norway. He is also an Adjunct
      Professor with the Norwegian Computing Center, Oslo, Norway. His
      research interests are at the intersection of deep learning and
      kernel machines, with a special focus on data-driven health
      technology. Dr. Jenssen is a member of the IEEE Technical
      Committee on Machine Learning for Signal Processing and an
      Associate Editor of Pattern Recognition. Authorized licensed use
      limited to: INRIA. Downloaded on January 16,2024 at 16:23:28 UTC
      from IEEE Xplore. Restrictions apply.
