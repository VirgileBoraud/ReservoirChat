Neural Language Taskonomy: Which NLP Tasks are the
most Predictive of fMRI Brain Activity?
Subba Reddy Oota, Jashn Arora, Veeral Agarwal, Mounika Marreddy, Manish

Gupta, Bapi Surampudi

To cite this version:

Subba Reddy Oota, Jashn Arora, Veeral Agarwal, Mounika Marreddy, Manish Gupta, et al.. Neural
Language Taskonomy: Which NLP Tasks are the most Predictive of fMRI Brain Activity?. NAACL
2022 - Conference of the North American Chapter of the Association for Computational Linguistics -
Human Language Technologies (NAACL-HLT2022), Jul 2022, Seattle, United States. pp.3220-3237,
￿10.18653/v1/2022.naacl-main.235￿. ￿hal-03946624￿

HAL Id: hal-03946624

https://hal.science/hal-03946624

Submitted on 19 Jan 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

3220
Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 3220 - 3237
July 10-15, 2022 ©2022 Association for Computational Linguistics

NeuralLanguageTaskonomy:WhichNLPTasksarethemostPredictiveoffMRIBrainActivity?SubbaReddyOota1,2\*,JashnArora2\*,VeeralAgarwal2,MounikaMarreddy2ManishGupta2,3andBapiRajuSurampudi21INRIA,Bordeaux,France;2IIITHyderabad,India;3Microsoft,Indiasubba-reddy.oota@inria.fr,jashn.arora@research.iiit.ac.inveeral.agarwal@research.iiit.ac.in,mounika.marreddy@research.iiit.ac.ingmanish@microsoft.com,raju.bapi@iiit.ac.inAbstractSeveralpopularTransformerbasedlanguagemodelshavebeenfoundtobesuccessfulfortext-drivenbrainencoding.However,existingliteratureleveragesonlypretrainedtextTrans-formermodelsandhasnotexploredtheeffi-cacyoftask-specificlearnedTransformerrep-resentations.Inthiswork,weexploretrans-ferlearningfromrepresentationslearnedfortenpopularnaturallanguageprocessingtasks(twosyntacticandeightsemantic)forpredict-ingbrainresponsesfromtwodiversedatasets:Pereira(subjectsreadingsentencesfrompara-graphs)andNarratives(subjectslisteningtothespokenstories).Encodingmodelsbasedontaskfeaturesareusedtopredictactivityindif-ferentregionsacrossthewholebrain.Featuresfromcoreferenceresolution,NER,andshallowsyntaxparsingexplaingreatervarianceforthereadingactivity.Ontheotherhand,forthelisteningactivity,taskssuchasparaphrasegen-eration,summarization,andnaturallanguageinferenceshowbetterencodingperformance.Experimentsacrossall10taskrepresentationsprovidethefollowingcognitiveinsights:(i)languagelefthemispherehashigherpredic-tivebrainactivityversuslanguagerighthemi-sphere,(ii)posteriormedialcortex,temporo-parieto-occipitaljunction,dorsalfrontallobehavehighercorrelationversusearlyauditoryandauditoryassociationcortex,(iii)syntacticandsemantictasksdisplayagoodpredictiveperformanceacrossbrainregionsforreadingandlisteningstimuliresp.1IntroductionBrainencodingaimsatconstructingneuralbrainactivitygivenaninputstimulus.Sincethediscov-eryoftherelationshipbetweenlanguagestimuliandfunctionsofbrainnetworksusingfMRI[forex.,(Constableetal.,2004)],researchershavebeeninterestedinunderstandinghowtheneuralencod-ingmodelspredictthefMRIbrainactivity.Sev-eralbrainencodingmodelshavebeendevelopedto(i)understandtheventralstreaminbiologicalvision(Yaminsetal.,2014;Kietzmannetal.,2019;Baoetal.,2020),and(ii)tostudythehigher-levelcognitionlikelanguageprocessing(GauthierandLevy,2019;Schrimpfetal.,2021;Schwartzetal.,2019).Somerecentstudies(Nishidaetal.,2015;Huthetal.,2016)havebeenabletoidentifybrainROIs(RegionofInterest)thatrespondtowordsthathaveasimilarmeaningandhavethusbuilta“semanticatlas”ofhowthehumanbrainorganizeslanguage.Further,severalstudies(Ootaetal.,2018;JainandHuth,2018;Hollensteinetal.,2019)haveusedawidevarietyofwordembeddingswherewordsrepresentedasvectorsinanembeddingspacearemappedtobrainactivationforimprovedneuralcoding.Recently,Transformer(Vaswanietal.,2017)basedmodelslikeBERT(Devlinetal.,2019)havebeenfoundtobeveryeffectiveacrossalargenum-berofnaturallanguageprocessing(NLP)tasks.TheseTransformerbasedmodelshavebeenpre-trainedonmillionsoftextinstancesinanunsuper-visedmannerandfurtherfinetunedtospecializeforvariousNLPtasks.Naturallanguageunderstand-ingrequiresintegratingseveralcognitiveskillslikesyntacticparsingofthelanguagestructure,identify-ingthenamedentities,capturingthewordmeaninginthecontext,coreferenceresolution,etc.Learn-ingfrommassivecorporaenablesthesemodelstoexcelatcognitiveskillsrequiredforlanguageun-derstanding.Interestingly,suchTransformer-basedneuralrepresentationshavebeenfoundtobeveryeffectiveforbrainencodingaswell(Schrimpfetal.,2021).Despitetherecentadvancesinmappingbe-tweenlanguageTransformersandthebrainactivityrecordedwithreading(Schrimpfetal.,2021),theTransformerfeaturesthemselvesarenotoriouslydifficulttointerpret.Inrecentworks,Caucheteuxetal.(2021a);Antonelloetal.(2021)addressthis3221

issuebydisentanglingthehigh-dimensionalTrans-formerrepresentationsoflanguagemodelsintofourcombinatorialclasses:lexical,compositional,syntactic,andsemanticrepresentationstoexplorewhichclassishighlyassociatedwithlanguagecor-ticalROIs.Representationsdonotexistinavac-uumbutbecomemeaningfulonlywhentheyac-complishatask.Therefore,thenextlogicalstepistoseewhichoftheseTransformerrepresentationsmosteffectivelydrivethelinearmappingbetweenlanguagemodelsandthebraininthecontextofNLPtasks.GauthierandLevy(2019)fine-tuneapretrainedBERTmodelonmultipletaskstofindtasksbestcorrelatedwithhighdecodingperfor-mance.Inthisstudy,weinvestigatethecorrelationbetweenbrainactivationandfeaturerepresenta-tionslearnedbydifferenttask-specificnetworks,andaskwhichtasksleadtoimprovementsinbrain-encodingperformance.Recently,astudyusingmultiplecomputervi-siontaskshasshownthat3DvisiontaskmodelspredictbetterfMRIbrainactivitythan2Dvisiontaskmodels(Wangetal.,2019)forvisualstim-uli.Inspiredbythesuccessofcorrelationsinthevisionfield(Wangetal.,2019),andbrainencod-ingstudyofavarietyoflanguageTransformermodels(Schrimpfetal.,2021;Caucheteuxetal.,2021b,a),webuildneurallanguagetaskonomymodelsforbrainencodingandaimtofindNLPtasksthataremostexplanatoryofbrainactivationsforreadingandlisteningtasks.Inthispaper,weuncoverinsightsabouttheas-sociationbetweenfMRIvoxelactivationsandrep-resentationsofdiverseNLPtasksrepresentations.Thepredictivepoweroftask-specificrepresenta-tionswithbrainactivationisascertainedby(1)usingridgeregressiononsuchrepresentationsandpredictingactivationsand(2)computingpopularmetricslike2V2accuracyandPearsoncorrelationbetweenactualandpredictedactivations.Specifically,wemakethefollowingcontribu-tionsinthispaper.•GivenTransformermodelsfinetunedforvar-iousNLPtasks,weproposetheproblemoffindingwhichofthesearethemostpredic-tiveoffMRIbrainactivityforreadingandlisteningtasks.•OurlanguagetaskonomyresultsrevealthatCoreferenceResolution,NamedEntityRecog-nition,andShallowSyntaxParsingtaskshavehigherpredictiveperformancewhilereadingthetext.Ontheotherhand,paraphrasedetec-tion,summarization,andNaturalLanguageInferencetasksdisplaybettercorrelationdur-inglistening.•Wealsoperformsimilarityanalysisbetweentaskrepresentationsfromtransferlearningandneuraltaskonomyandderiveinterestingcog-nitiveinsightsfrombrainmaps.2RelatedWorkOldermethodsfortext-basedstimulusrep-resentationincludetextcorpusco-occurrencecounts(Mitchelletal.,2008;Pereiraetal.,2013;Huthetal.,2016),syntacticanddiscoursefea-tures(Wehbeetal.,2014).Inrecenttimes,bothsemanticandexperientialattributemodelshavebeenexploredfortext-basedstimuli.Semanticrep-resentationmodelsincludedistributedwordembed-dings(Pereiraetal.,2016;Andersonetal.,2017a;Pereiraetal.,2018;TonevaandWehbe,2019;Hol-lensteinetal.,2019;Wangetal.,2020),sentencerepresentationmodels(Sunetal.,2019;TonevaandWehbe,2019;Sunetal.,2020),recurrentneuralnet-works(JainandHuth,2018;Ootaetal.,2019),andTransformer-basedlanguagemodels(GauthierandLevy,2019;TonevaandWehbe,2019;Schwartzetal.,2019;Ootaetal.,2022a,b).Experientialat-tributemodelsrepresentwordsintermsofhumanratingsoftheirdegreeofassociationwithdifferentattributesofexperience,typicallyonascaleof0-6(Andersonetal.,2019,2020;Berezutskayaetal.,2020;Jatetal.,2020;Caucheteuxetal.,2021a;Antonelloetal.,2021)orbinary(Handjarasetal.,2016;Wangetal.,2017).Fine-graineddetailssuchaslexical,compositional,syntactic,andsemanticrepresentationsofnarrativesarefactorizedfromTransformer-basedmodelsandutilizedfortrain-ingencodingmodels.TheresultingmodelsarebetterabletodisentanglethecorrespondingbrainresponsesinfMRI(Caucheteuxetal.,2021a).Inthispaper,wefocusonTransformer-basedlin-guisticstimulirepresentationssincetheyhavebeenfoundtobemosteffective.Unlikepreviousstud-ieswhichdirectlyusedexistingtask-agnosticpre-trainedmodels,wetraintask-specificTransformermodelsandaimtofindwhichmodelleadstothebestencodingaccuracygivenreadingandlisteninglanguagestimuli.3222

3BrainImagingDatasetsWeworkwithtwodatasets:PereiraandNarratives-Pieman.ResultsonNarratives-LucyandNarratives-SlumLordshowsimilartrends.Hence,wealsoshowresultsonNarratives-LucyandNarratives-SlumLordintheappendix.PereiraDataset(ReadingSentencesfromPas-sages)ForthePereiradataset,similartoearlierwork(Sunetal.,2019,2020),wecombinethedatafromsentence-basedexperiments(experiments-2and3)fromPereiraetal.(2018).Fivesubjectswerepresentedatotalof627sentencesfrom48broadtopics,spanningover168passages,whereeachpassageconsistsof3-4sentences.Asin(Pereiraetal.,2018),wefocusedonninebrainROIs(re-gionsofinterest)correspondingtofourbrainnet-works:(i)DefaultModeNetwork(DMN)(linkedtothefunctionalityofsemanticprocessing),(ii)LanguageNetwork(relatedtolanguageprocess-ing,understanding,wordmeaning,andsentencecomprehension),(iii)TaskPositiveNetwork(TP)(relatedtoattention,salienceinformation),and(iv)VisualNetwork(relatedtotheprocessingofvisualobjects,objectrecognition).WebrieflysummarizethedetailsofthedatasetandthenumberofvoxelscorrespondingtoeachROIinTable1.WeusetheAALparcellationAtlas(116×116brainROIs)topresentthebrainmapresults,sincePereiradatasetcontainsannotationstiedtothisatlas.ROIs→LanguageVisionDMNTaskPositive↓SubjLHRHBodyFaceObjectSceneVisionRHLHP01526561723774496380854141128291719035120M02493058613873478275523173117291507030594M04590654013867480378123602122781801134024M07562950014190499386173721124541702030408M15531561414112494183233496123831599531610Table1:#VoxelsineachROIinthePereiraDataset.LH-LeftHemisphere.RH-RightHemisphere.ROIs→EACAACPMCTPOJDFLLHRHLHRHLHRHLHRHLHRH#Voxels808638142014931198120484711881061875Table2:#VoxelsineachROIintheNarrativesDataset.LH-LeftHemisphere.RH-RightHemisphere.Piemanhas82,Lucyhas16andSlumLordhas18subjects.#VoxelsacrossROIsaresameforallthethree.Narratives-Pieman(ListeningtoStories)The“Narratives”collectionaggregatesavarietyoffMRIdatasetscollectedwhilehumansubjectslistenedtonaturalisticspokenstories.TheNarrativesdatasetthatincludes345subjects,891functionalscans,and27diversestoriesofvaryingdurationtotaling∼4.6hoursofuniquestimuli(∼43,000words)wasproposedin(Nastaseetal.,2021).Similartoear-lierworks(Caucheteuxetal.,2021b),weanalyzedatafrom82subjectslisteningtothestorytitled‘PieMan’with259TRs(repetitiontime–fMRIrecordedevery1.5sec.).WelistnumberofvoxelsperROIinthisdatasetinTable2.Weusethemulti-modalparcellationofthehumancerebralcortex(GlassarAtlas:consistsof180ROIsineachhemi-sphere)todisplaythebrainmaps(Glasseretal.,2016),sinceNarrativesdatasetcontainsannota-tionstiedtothisatlas.ThedatacoverstenbrainROIsinthehumanbrain,i.e.,Lefthemisphere(L),andRighthemisphere(R)foreachofthefollowing:(i)earlyauditorycortex(EAC:A1,LBelt,MBelt,PBelt,andR1)whichplaysakeyroleforsoundper-ceptionsinceitrepresentsoneofthefirstcorticalprocessingstationsforsounds;(ii)auditoryassoci-ationcortex(AAC:A4,A5,STSdp,STSda,STSvp,STSva,STGa,andTA2)whichisconcernedwiththememoryandclassificationofsounds;(iii)pos-teriormedialcortex(PMC:POS1,POS2,v23ab,d23ab,31pv,31pd,7m);(iv)thetemporoparietooccipitaljunction(TPOJ:TPOJ1,TPOJ2,TPOJ3,STV,PSL)whichisacomplexbrainterritoryheav-ilyinvolvedinseveralhigh-levelneurologicalfunc-tions,suchaslanguage,visuo-spatialrecognition,writing,reading,symbolprocessing,calculation,self-processing,workingmemory,musicalmem-ory,andfaceandobjectrecognition;and(v)thedorsalfrontallobe(DFL:L\_55b,SFL,L\_44,L\_45,IFJA,IFSP)whichcoverstheaspectsofpragmaticprocessingsuchasdiscoursemanagement,integra-tionofprosody,interpretationofnonliteralmean-ings,inferencemaking,ambiguityresolution,anderrorrepair.4EncodingModelToexplorehowandwherecontextuallanguagefeaturesarerepresentedinthebrainwhenread-ingsentencesandlisteningtostories,weextractdifferentfeaturesspacesdescribingeachstimulussentenceandusetheminanencodingmodeltopredictbrainresponses.Ourreasoningisasfol-lows.Ifafeatureisagoodpredictorofaspe-cificbrainregion,informationaboutthatfeatureislikelyencodedinthatregion.Inthispaper,forbothdatasets,wetrainfMRIencodingmodelsus-ingRidgeregressiononstimulirepresentationsob-tainedusingavarietyofNLPtasks.ThemaingoalofeachfMRIencodermodelistopredictbrain3223

responsesassociatedwitheachbrainregiongivenastimuli.Inallcases,wetrainamodelpersubjectseparately.Followingliteratureonbrainencod-ing(Caucheteuxetal.,2021b;Tonevaetal.,2020),wechoosetousearidgeregressionmodelinsteadofmorecomplicatedmodels.Weplantoexploremoresuchmodelsaspartoffuturework.WefollowK-fold(K=10)cross-validation.Allthedatasam-plesfromK-1foldswereusedfortraining,andthemodelwastestedonsamplesoftheleft-outfold.Weusedsklearn’sridge-regressionwithdefaultparameters,10-foldcross-validation,Stochastic-Average-GradientDescentOptimizer,HuggingfaceforTransformermodels,MSElossfunction,andL2-decay(λ)as1.0.WeusedBERTWord-PiecetokenizerforthelinguisticTransformerinput.Allexperimentswereconductedonamachinewith1NVIDIAGEFORCE-GTXGPUwith16GBGPURAM.Wemakethecodepubliclyavailable1.4.1FeatureSpacesTosimultaneouslytestrepresentationsfrommul-tipleNLPtasks,weusedthelatentspacefeaturesfromeachofthefollowingtenpopularNLPtasks:coreferenceresolution(CR),namedentityrecog-nition(NER),naturallanguageinference(NLI),paraphrasedetection(PD),questionanswering(QA),sentimentanalysis(SA),semanticrolela-beling(SRL),shallowsyntaxparsing(SS),sum-marization(Sum)andwordsensedisambiguation(WSD).AllofthesearediscriminativeNLPtasks,andthusweusemodelsobtainedbytask-specificfinetuningofthesamepretrainedTransformeren-codermodel(BERT-base-casedwithdimension-ality=768).Givenaninputsentence,eachtaskTransformeroutputstokenrepresentationsatthefinallayer.Weusethe#tokens×768dimensionvectorobtainedfromthelasthiddenlayertoobtainlatentfeaturesforthestimuli.Wethenbuildindi-vidualridgeregressionmodelswiththeextractedlatentfeaturestopredictbrainresponsesandmea-surethecorrelationbetweenthepredictionandthetrueresponse.Pereira:Sinceindividualsentenceswerepresentedtothesubjectswhilemodeling,sentenceswerepassedonebyonetothetaskTransformermodel,andaverage-pooledrepresentationswereusedtoencodethesentencestimuli.Narratives-Pieman:DuetotheconstraintoninputsequencelengthforBERT(512),weconsidered1https://tinyurl.com/langTaskawindowsizeof10sentenceswiththelasttwosentencesofonewindowoverlappingwiththenexttobegivenasinputtotheBERTmodel.Weusetheaverage-pooledrepresentationfromBERTtoencodetextstimuli.TogettherepresentationforaTR,wepooledtherepresentationsofonlythosewordsofthesentencesinthatTR.4.2TaskDescriptionsHerewedescribethefunctionalityofeachNLPtaskthatweusedforfMRIencoding.CR:involvesfindingallexpressionsthatrefertothesameentityinatext.PD:involvestakingapassage–eitherspokenorwritten–andrewordingitinshorterorownwords.Summarization(Sum):involvesselectingafewimportantsentencesfromadocu-mentorparagraph.NER:involvesdetectionofthenamedentitiessuchaspersonnames,locationnames,companynamesfromagiventext.NLI:in-vestigatestheentailmentrelationshipbetweentwotexts:premiseandhypothesis.QA:aimstoselectananswergivenapassage,aquestion,andasetofcandidateanswers.SA:involvesdeterminingwhetherapieceoftextispositive,negative,orneu-tral.SRL:assignslabelstowordsorphrasesinasentencethatindicatestheirsemanticroleinthesentence,suchasthatofanagent,goal,orresult.SS:providesanapproximationofphrase-syntacticstructureofsentences.WSD:involvesdeterminingwhichsense(meaning)ofawordisactivatedbytheuseofthewordinaparticularcontext.Syntacticreasoningisrathershallowcomparedtodeepsemanticreasoning.Syntacticreasoningfollowssomewhatobjectivegrammarrules.Com-parativelysemanticreasoningisoftensubjectiveinnatureandcomplex.TheemergingevidencefromfMRIstudies(Fedorenkoetal.,2020,2012)alsopointsoutthatprocessingofbothsyntaxandsemanticsisdistributedinthebrainanditisonlywhenviolationsoftheseprocessesareprobed,weseelocalizationoffunction(Friedericietal.,2003).Thus,inthiswork,weexploresyntacticandseman-tictasksseparately.Oftheabovementionedtasks,NERandSSaresyntactic,whiletheothersinvolvesemanticreasoning.Ourselectionofthesetaskswasbasedonthefol-lowingdesignprinciples:(1)Wewantedtoselectasetoftaskscoveringdiversecognitive-linguisticskills.(2)WewantedtoselecttasksthatareapartofpopularNLPbenchmarkslikeGLUE(Wangetal.,2018).(3)Weselectedtasksforwhich3224

Figure1:Pereira–2V2Accuracy(topfigure)andPearsoncorrelationcoefficient(bottomfigure)betweenpredictedandtrueresponsesacrossdifferentbrainregionsusingavarietyofNLPtasks.Resultsareaveragedacrossallparticipants.CR,NER,andSSperformthebest.BERT-base-casedfinetunedmodelswereavailable.Notethatwedidnotfinetuneanyofthesemodelsourselvesbutleveragedthestate-of-the-artfine-tunedmodelsavailableonHuggingface.Detailsofthespecificfinetunedmodelcheckpointsaremen-tionedinTable3intheAppendix.4.3EvaluationMetricsWeevaluateourmodelsusingpopularbrainencod-ingevaluationmetricsdescribedinthefollowing.Givenasubjectandabrainregion,letNbethenumberofsamples.Let{Yi}Ni=1and{ˆYi}Ni=1de-notetheactualandpredictedvoxelvaluevectorsfortheithsample.Thus,Y∈RN×VandˆY∈RN×VwhereVisthenumberofvoxelsinthatregion.2V2Accuracyiscomputedas2V2Acc=1NC2PN−1i=1PNj=i+1I[cosD(Yi,ˆYi)+cosD(Yj,ˆYj)<cosD(Yi,ˆYj)+cosD(Yj,ˆYi)]wherecosDisthecosinedistancefunction.I[c]isanindicatorfunctionsuchthatI[c]=1ifcistrue,elseitis0.Thehigherthe2V2accuracy,thebetter.PearsonCorrelation(PC)iscomputedasPC=1NPni=1corr[Yi,ˆYi]wherecorristhecorre-lationfunction.MeanAbsoluteError(MAE)iscomputedasMAE=1NPni=1|[Yi−ˆYi]|.StatisticalSignificance:Inordertoestimatethestatisticalsignificanceoftheperformancediffer-ences(acrossallresults),weperformedone-wayANOVAonthemeanvaluesforthesubjects.Inallsuchcaseswereportp-valuescorrectedusingBonferronicorrection.4.4NeuralLanguageTasksSimilarityComputationToestimatethesimilaritybetween10languagetasks,wetookthepredictionperformancescoresacrossallthevoxelsinPereira(97,539)andNarratives-Piemandatasets(10,732).Toanalyzetherelationshipbetweentasksbasedonneuralrep-resentations,wecalculatedthePearsoncorrelationbetweenpredictedvoxelsofeachtaskwiththere-mainingtasks.ThesePearsoncorrelationvalueswereusedtoconstructheatmapsandthetasksimi-laritytrees(dendograms)usinghierarchicalcluster-ingforPereiraandNarratives-Piemandatasets.5ResultsInordertoassesstheperformanceofthefMRIencodermodelslearnedusingtherepresentations3225

Figure2:Narratives-Pieman–2V2Accuracy(topfigure)andPearsoncorrelationcoefficient(bottomfigure)betweenpredictedandtrueresponsesacrossdifferentbrainregionsusingavarietyofNLPtasks.Resultsareaveragedacrossallparticipants.NLI,PD,andSummarizationperformthebest.fromavarietyofNLPtasks,wecomputedthe2V2accuracyandPearsoncorrelationcoefficientbetweenthepredictedandtrueresponsesacrossvariousROIsforboththereading(Pereira)dataset(Fig.1)aswellasthelistening(Narratives-Pieman)dataset(Fig.2).5.1EncodingperformanceofLanguageTaskmodelsforreadingvslisteningtasksReadingSentences(Pereira):FromFig.1,weobservethattaskssuchasCR,NER,SRL,andSSappeartohaveabettercorrelationtothebrainresponsescomparedtotheothertasks.Inor-dertoestimatethestatisticalsignificanceoftheperformancedifferences,weperformedone-wayANOVAonthemeancorrelationvaluesforthesubjectsacrossthetenlanguagetasksfortheninebrainROIs.ThemaineffectoftheANOVAtestwassignificantforalltheROIswithp≤10−2withconfidence95%(seeAppendixfordetailedANOVAresults).Further,posthocpairwisecom-parisons(RuxtonandBeauchamp,2008)confirmedthevisualobservationsthatonboth2V2accuracyandPearsoncorrelationmeasures,taskssuchasCR,NER,SRL,andSSperformedsignificantlybettercomparedtoothertasks(seeAppendixforpairwisecomparisonresults).Theseresultsdemon-stratethatwhenreadingasentence,informationprocessingoperationsrelatedtorecognizingnamedentities,labelingsemanticrolestotheconstituentsofasentence,identifyingthereferencesfromasentencetothegiventopic(concept),andsyntacticprocessingmaybeengaged.Further,weobservethattheROIcorrespondingtolanguageprocessinginthelefthemisphere(Lan-guage\_LH)hashigherencodingperformancethanthatoftherighthemisphere(Language\_RH).Thisisinlinewiththelefthemispheredominanceforlanguageprocessing(Binderetal.,2009).Also,lateralvisualROIssuchasVision\_Object,Vi-sion\_Body,Vision\_Face,andVisionROIsdisplayhighercorrelationwiththelanguagetasksassoci-atedwithnamedentities(NER),relatingtheen-tities(CR),andsyntaxprocessing(SS).Highercorrelationswithallthevisualbrainregionspointtothepossiblealignmentofvisualandlanguageregionsforsemanticunderstanding(Pophametal.,2021)inareadingtask.Finally,acrossallregions,pretrainedBERTmodelhasworsecorrelationcom-paredtoatleast5othertaskmodels.ListeningStories(Narratives-Pieman):FromFig.2,weobservethattheprofilesofperformance3226

showlowscoresintheearlyauditorycortex(EAC),auditoryassociationcortex(AAC);averagescoresinTPOJandDFL;andsuperiorscoresinPMC.Thisalignswiththeknownlanguagehierarchyforspokenlanguageunderstanding(Nastaseetal.,2020).TaskssuchasPD,Summarization,andNLIseemtoyieldbetterperformanceinpredict-ingthebrainresponsesthantheotherNLPtasksacrossalltheROIs.ThesePearsoncorrelation(τ)resultsarecomparativelymuchhighercomparedtothoseobtainedusingpretrained(task-agnostic)GPT2modelin(Caucheteuxetal.,2021a)(τrang-ingfrom0.02−0.06).AsshowninFig.2,ourmethodobtainsmuchhighercorrelations(τrang-ingfrom0.02−0.229).SimilartothePereiradataset,weestimatethestatisticalsignificanceoftheperformancedifferencesusingtheone-wayANOVAtest.Themaineffectoftaskwassignifi-cantforalltheROIswithp≤10−3withconfidence95%(seeAppendixfordetailedANOVAresults).Also,Posthocpairwisecomparisons(RuxtonandBeauchamp,2008)revealedthatonboth2V2accu-racyandPearsoncorrelationmeasures,taskssuchasPD,Sum,andNLIperformedsignificantlybettercomparedtoothertasks(seeAppendixforpairwisecomparisonresults).Further,fromFig.2,weseethatthebilateralposteriormedialcortex(PMC)associatedwithhigherlanguagefunctionexhibitsahighercorre-lationamongallthebrainROIs.ROIs,includingbilateralTPOJandbilateralDFL,yieldhighercor-relationswiththefiveNLPtasks,whichisinlinewiththelanguageprocessinghierarchyinthehu-manbrain.Finally,acrossallregions,pretrainedBERTmodelhasworsecorrelationcomparedtoatleast5othertaskmodels.Insummary,differentanddistinctlanguageTaskonomyfeaturesseemtoberelatedtotheencod-ingperformanceinreadingversuslisteningtasks.CR,NER,SRL,andSSperformbetterforread-ing.PD,Sum,andNLIperformbetterforlistening.Whilelisteningthesubjectiscognitivelymorein-volvedintheactivitycomparedtoreading(Buch-weitzetal.,2009).Thus,itmakessensethatshal-lowtaskslikeNERandSSareusefulforreadingwhilemorecomplexNLPtaskslikePD,SumandNLIareeffectiveforencodinglisteningstimuli.5.2LanguageTaskSimilarityComputationPearsoncorrelationvaluesbetweenpredictedre-sponsesforeachpairoftaskswereusedtocon-Figure3:Pereira–PredictionSimilarityMatrixcon-structedfromthetask-wisebrainresponsepredictionsacross10tasksaveragedacrossallsubjects.Figure4:Narratives-Pieman–PredictionTaskSim-ilarityconstructedfromthetask-wisebrainresponsepredictionsacross10tasksaveragedacrossallsubjects.structthesimilaritymatrixwithheatmapforbothPereiraandNarratives-Piemandatasets,asshowninFigs.3and4.WeobservethatthefollowingtaskpairsarehighlycorrelatedforthePereiradataset:(NERandCR),(SSandCR)and(PDandSum).AlsothesetaskpairsarehighlycorrelatedfortheNarratives-Piemandataset:(CRandNLI),(NLIandSA)and(PDandSum).Similaritiesarerela-tivelyhigherforNarratives-PiemancomparedtothePereiradataset.Surprisingly,the(NLI,SA)pairhaslowestsimilarityforPereira(reading)andclosetohighestinNarratives-Pieman(listening).Wehypothesizethatthisisbecausesentimentisbestconveyedwhilethesubjectislistening.Readingsentences(Pereira):Thestimulussen-tencesfromthePereiradatasetwerefedasinputtoeachofthe10taskTransformers.Thesimilarityamongtheresultingrepresentationswasanalyzedusinghierarchicalclustering,andtheclustersarevisualizedasdendrogramsinFig.5(left).Weob-servethatthetasksareclusteredintothreegroupsdenotedusingred,green,andbluecolors.Next,wewishedtocheckifsimilartaskgroupingisob-servedonbrainactivationspredictedbyridgere-gressiontrainedontask-specificrepresentations.Hence,similarclusteringanalysiswasconductedontheneuralspacerepresentations,andtheclus-tersarevisualizedasdendrogramsinFig.5(right)3227

Figure5:Left:PereiraDendrogramconstructedusingsimilarityonrepresentationsfromtask-specificTrans-formerencodermodelswithstimulifromthedatasetpassedasinput.Right:PereiraDendrogramconstructedusingsimilaritymatrixshowninFig.3.Figure6:Left:Narratives-PiemanDendrogramcon-structedusingsimilarityonrepresentationsfromtask-specificTransformerencodermodelswithstimulifromthedatasetpassedasinput.Right:Narratives-PiemanDendrogramconstructedusingsimilaritymatrixshowninFig.4.acrossallsubjects.Interestingly,thetreederivedfrombrainrepresentationalsoshowsasimilardis-tributionoftasksacrossthethreegroups.SimilardendrogramsforindividualsubjectsareillustratedinAppendix-Fig.11.ListeningStories(Narratives-Pieman):Fig.6comparesthetasksimilaritytreebasedonthepat-ternsfromthepretrainedtaskTransformers,withthetasksimilaritytreegeneratedbasedonsimilar-ityinbrainresponsepredictionperformanceaver-agedacrossallsubjects.Weobservethatthetasksareclusteredintothreegroupsdenotedusingred,green,andbluecolors.Again,thetreederivedfrombrainrepresentationalsoshowsasimilardistribu-tionoftasksacrossthethreegroups.DendrogramsforindividualsubjectsareintheAppendix-Fig.12.5.3BrainmapsforwholebrainpredictionsThemeanabsoluteerror(MAE)betweenpredictiveandactualresponsesisobtainedusingindividualtaskfeaturesfromthetaskonomy.MAEvaluesareobtainedforallthevoxelsinthebrainforboththereading(Fig.7)andlisteningdatasets(Fig.8).Inthereadingtask,weobservefromFig.7thatCRhaslowerMAEcomparedtoPDwhichinturnhaslowerMAEcomparedtotheNLItask(brainmapsfortheothertasksarereportedinFig.17intheAppendix).Overall,forthereadingstim-uli,taskssuchasNLI,QA,andSAdisplayhigherMAEvalues.TofurtherinvestigatewhichsubROIs(LPTG,LMTG,LATG,LFus,Lpar,Lang,LIFGorb,LIFG,LaMFG,LpMFG,andLmMFG)oftheLanguagenetworkarerelatedtothepredic-tivetaskfeatures,wetrainencodingmodelsforallthesubROIsforthebestencodingtask,i.e.,fortheCRtask(seeFig.14inAppendix).Weno-ticethatbothLMTG(middletemporalgyrus)andLPTG(posteriortemporalgyrus)aremoreaccu-ratelypredictedthantheothersubROIs.Ontheotherhand,LIFG-orbdisplaysalowerPearsoncor-relationfortheCRtask.ThepresenceofsuperiorencodinginformationintheROIsinthetemporalgyrusascomparedtothoseintheinferiorfrontalgyrusseemstomirrorsimilarobservationsseenindecoderperformance(Andersonetal.,2017b).Ontheotherhand,inthelisteningtask,weob-servefromFig.8thatParaphraseandWSDdisplaylowerMAEvaluescomparedtoQAtask(brainmapsfortheothertasksarereportedinFig.18intheAppendix).Takentogether,forlisteningstimuli,taskssuchasNER,QA,SA,CR,andSSdisplayhigherMAEvalues.FromFig.8,weseethatROIssuchasEACandAAChavehigherMAEcomparedtoPMCandTPOJbrainROIs.Wefurtherdemonstratethepredictionperfor-manceoftheencodermodeltrainedonsubROIsfortheparaphrasetaskinFig.15intheAppendix.ItcanbeobservedthatsubROIssuchasPos1andPos2haveahigherPearsoncorrelationthanothersubROIsofthePMCregion.Bothsflandl55bdis-playahighercorrelationamongallthesubROIsfortheDFLROI.However,allthesubROIsintheTPOJyieldhighercorrelation,asshowninFig.15.ThecontrolandattentionROIsintheposteriorcingulatecortex(forex.,POS1inPMC),togetherwiththesuperiorfrontallanguageregion(sflinDFL)andTPOJ,arepartofthewell-knownlan-guagenetworkassociatedwithnarrativecompre-hension(Nastaseetal.,2020),anditishearteningtoseethattaskfeaturesfromPDtaskalsorelatetosemanticanalysisoftheongoingnarrative.5.4Discussion(1)Weusedaridgeregressionmodelinsteadofmorecomplicatedmodelsforencoding.Webe-lievethatmorecomplexmodelscanleadtofurtherexcitinginsights.(2)Weexperimentedwith10NLPtasks.Modelscanbepretrainedformoresuchtaskstocheckifothertasksarebetterpredic-tiveofvoxelactivations.(3)Weleveragedmodelsfinetunedusingdatasetsofdifferentsizesacross3228

Coreference ResolutionParaphraseNatural Language InferenceFigure7:PereiraBrainMaps:Meanabsoluteerror(MAE)betweenpredictivevoxelsandactualvoxelsusingtaskfeaturesfromTaskonomyinonesamplesubject(subject1).Predictiveregionsofdifferenttasksaredissimilaracrosstasks.TheMAEvaluesofeachbrainROIare:CR(Language:0.64,Visual:0.57,DMN:1.19,TP:0.67),PD(Language:0.81,Visual:0.74,DMN:1.34,TP:0.87)andNLI(Language:1.9,Visual:1.88,DMN:2.1,TP:2.03).Word Sense Disambiguation ParaphraseQuestion Answering LHRHLHRHLHRHFigure8:Narratives-PiemanBrainMaps:Meanabsoluteerror(MAE)betweenpredictivevoxelsandactualvoxelsusingtaskfeaturesfromTaskonomyinonesamplesubject(subject1)ofPieMandataset.Predictiveregionsofvarioustasksaredifferentacrosstasks.TheMAEvaluesofeachbrainROI:PDtask(EAC:0.74,AAC:0.66,PMC:0.60,TPOJ:0.61,andDFL:0.694),WSDtask(EAC:0.83,AAC:0.75,PMC:0.68,TPOJ:0.68,andDFL:0.76),QAtask(EAC:0.92,AAC:0.83,PMC:0.74,TPOJ:0.75,andDFL:0.76).tasks.Whileafaircomparisonofdatasetsizesacrosstasksisimpossible,weunderstandthatthiscouldhaveresultedinsomebiasinourresults.(4)Weusedadifferentdatasetforreadingvslis-tening.Whilewebelievethatthedifferencesintask-specificmodelperformancesacrossreadingandlisteningaremainlyduetothelearnedstimu-lusrepresentations,buttheycouldalsoarisefromotherfactorssuchasexperimentalconditions,thetextdomainofthestimuliornumberofvoxels,etc.(5)OnNaturalLanguageUnderstandingtaskssuchasNLI,SA,QAandPD,GauthierandLevy(2019)observedthatscrambledsentencerepresen-tationsgavebetterdecodingperformance.Buten-codingmodels(especiallyforthelisteningtask),scrambledorderwouldbedetrimentaltomakingsenseofwhatisbeingheard.Itisaninterestingfuturetasktoseeiftheoppositeresultisseeninthecaseofbrainencodingmodels.Itisplausiblethatbrainusesencodingmodelsinaflexiblewaywhenitcomestodecoding(KriegeskorteandDouglas,2019).KriegeskorteandDouglas(2019)mentionthat“Decodingmodelscanhelprevealwhetherpar-ticularinformationispresentinabrainregioninaformatthedecodercanexploit.Encodingmodelsmakecomprehensivepredictionsaboutrepresenta-tionalspaces.”Inthissense,resultsofcurrentworkarenotdirectlycomparabletothoseofGauthierandLevy(2019).6ConclusionInthispaper,westudiedtheeffectivenessoftaskspecificNLPmodelsforbrainencoding.Weob-servethatbuildingindividualencodingmodelsandexploitingexistingrelationshipsamongmodelscanprovideamorein-depthunderstandingoftheneu-ralrepresentationoflanguageinformation.OurexperimentsonPereiraandNarrativedatasetsleadtointerestingcognitiveinsights.3229

7EthicalStatementWereusedpubliclyavailabledatasetsforthiswork:PereiraandNarratives.Wedidnotcollectanynewdataset.Pereiradatasetcanbedownloadedfromhttps://osf.io/crwz7/.Pleasereadtheirtermsofuse2formoredetails.Narrativesdatasetcanbedowloadedfromhttps://datasets.datalad.org/?dir=/labs/hasson/narratives.Pleasereadtheirtermsofuse3formoredetails.Wedonotforeseeanyharmfulusesofthistech-nology.ReferencesAndrewJAnderson,DouweKiela,StephenClark,andMassimoPoesio.2017a.Visuallygroundedandtex-tualsemanticmodelsdifferentiallydecodebrainac-tivityassociatedwithconcreteandabstractnouns.TransactionsoftheAssociationforComputationalLinguistics,5:17–30.AndrewJamesAnderson,JeffreyRBinder,LeonardoFernandino,ColinJHumphries,LisaLConant,MarioAguilar,XixiWang,DoniasDoko,andRa-jeevDSRaizada.2017b.Predictingneuralactivitypatternsassociatedwithsentencesusinganeurobio-logicallymotivatedmodelofsemanticrepresentation.CerebralCortex,27(9):4379–4395.AndrewJamesAnderson,JeffreyRBinder,LeonardoFernandino,ColinJHumphries,LisaLConant,Ra-jeevDSRaizada,FengLin,andEdmundCLalor.2019.Anintegratedneuraldecoderoflinguisticandexperientialmeaning.JournalofNeuroscience,39(45):8969–8987.AndrewJamesAnderson,KelseyMcDermott,BrianRooks,KathiLHeffner,DavidDodell-Feder,andFengVLin.2020.Decodingindividualidentityfrombrainactivityelicitedinimaginingcommonexperi-ences.Naturecommunications,11(1):1–14.RichardAntonello,JavierTurek,VyVo,andAlexan-derHuth.2021.Low-dimensionalstructureinthespaceoflanguagerepresentationsisreflectedinbrainresponses.arXivpreprintarXiv:2106.05426.PingleiBao,LiangShe,MasonMcGill,andDorisYTsao.2020.Amapofobjectspaceinprimateinfer-otemporalcortex.Nature,583(7814):103–108.JuliaBerezutskaya,ZacharyVFreudenburg,LucaAm-brogioni,UmutGüçlü,MarcelAJvanGerven,and2https://github.com/CenterForOpenScience/cos.io/blob/master/TERMS\_OF\_USE.md3https://datasets.datalad.org/labs/hasson/narratives/stimuli/READMENickFRamsey.2020.Corticalnetworkresponsesmapontodata-drivenfeaturesthatcapturevisualsemanticsofmoviefragments.Scientificreports,10(1):1–21.JeffreyRBinder,RutvikHDesai,WilliamWGraves,andLisaLConant.2009.Whereisthesemanticsystem?acriticalreviewandmeta-analysisof120functionalneuroimagingstudies.Cerebralcortex,19(12):2767–2796.AugustoBuchweitz,RobertAMason,LêdaTomitch,andMarcelAdamJust.2009.Brainactivationforreadingandlisteningcomprehension:Anfmristudyofmodalityeffectsandindividualdifferencesinlan-guagecomprehension.Psychology&neuroscience,2(2):111–123.CharlotteCaucheteux,AlexandreGramfort,andJean-RemiKing.2021a.Disentanglingsyntaxandseman-ticsinthebrainwithdeepnetworks.InInternationalConferenceonMachineLearning,pages1336–1348.PMLR.CharlotteCaucheteux,AlexandreGramfort,andJean-RémiKing.2021b.Model-basedanalysisofbrainactivityrevealsthehierarchyoflanguagein305sub-jects.arXivpreprintarXiv:2110.06078.RToddConstable,KennethRPugh,EllaBerroya,WEinarMencl,MichaelWesterveld,WeijiaNi,andDonaldShankweiler.2004.Sentencecomplexityandinputmodalityeffectsinsentencecomprehension:anfmristudy.NeuroImage,22(1):11–21.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171–4186.EvelinaFedorenko,IdanAsherBlank,MatthewSiegel-man,andZacharyMineroff.2020.Lackofselectivityforsyntaxrelativetowordmeaningsthroughoutthelanguagenetwork.Cognition,203:104348.EvelinaFedorenko,AlfonsoNieto-Castanon,andNancyKanwisher.2012.Lexicalandsyntacticrepresenta-tionsinthebrain:anfmriinvestigationwithmulti-voxelpatternanalyses.Neuropsychologia,50(4):499–513.AngelaDFriederici,Shirley-AnnRüschemeyer,AnjaHahne,andChristianJFiebach.2003.Theroleofleftinferiorfrontalandsuperiortemporalcortexinsentencecomprehension:localizingsyntacticandsemanticprocesses.Cerebralcortex,13(2):170–177.JonGauthierandRogerLevy.2019.Linkingartificialandhumanneuralrepresentationsoflanguage.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages529–539.3230

MatthewFGlasser,TimothySCoalson,EmmaCRobinson,CarlDHacker,JohnHarwell,EssaYa-coub,KamilUgurbil,JesperAndersson,ChristianFBeckmann,MarkJenkinson,etal.2016.Amulti-modalparcellationofhumancerebralcortex.Nature,536(7615):171–178.GiacomoHandjaras,EmilianoRicciardi,AndreaLeo,AlessandroLenci,LucaCecchetti,MircoCosottini,GiovannaMarotta,andPietroPietrini.2016.Howconceptsareencodedinthehumanbrain:amodalityindependent,category-basedcorticalorganizationofsemanticknowledge.Neuroimage,135:232–242.NoraHollenstein,AntoniodelaTorre,NicolasLanger,andCeZhang.2019.Cognival:Aframeworkforcog-nitivewordembeddingevaluation.InProceedingsofthe23rdConferenceonComputationalNaturalLanguageLearning(CoNLL),pages538–549.AlexanderGHuth,WendyADeHeer,ThomasLGrif-fiths,FrédéricETheunissen,andJackLGallant.2016.Naturalspeechrevealsthesemanticmapsthattilehumancerebralcortex.Nature,532(7600):453–458.ShaileeJainandAlexanderGHuth.2018.Incorporatingcontextintolanguageencodingmodelsforfmri.InProceedingsofthe32ndInternationalConferenceonNeuralInformationProcessingSystems,pages6629–6638.SJat,HTang,PTalukdar,andTMitchel.2020.Re-latingsimplesentencerepresentationsindeepneu-ralnetworksandthebrain.InACL2019-57thAn-nualMeetingoftheAssociationforComputationalLinguistics,ProceedingsoftheConference,pages5137–5154.AssociationforComputationalLinguis-tics(ACL).TimCKietzmann,CourtneyJSpoerer,LynnKASörensen,RadoslawMCichy,OlafHauk,andNiko-lausKriegeskorte.2019.Recurrenceisrequiredtocapturetherepresentationaldynamicsofthehumanvisualsystem.ProceedingsoftheNationalAcademyofSciences,116(43):21854–21863.NikolausKriegeskorteandPamelaKDouglas.2019.Interpretingencodinganddecodingmodels.Currentopinioninneurobiology,55:167–179.TomMMitchell,SvetlanaVShinkareva,AndrewCarl-son,Kai-MinChang,VicenteLMalave,RobertAMason,andMarcelAdamJust.2008.Predictinghu-manbrainactivityassociatedwiththemeaningsofnouns.science,320(5880):1191–1195.SamuelANastase,Yun-FeiLiu,HannaHillman,Ken-nethANorman,andUriHasson.2020.Leverag-ingsharedconnectivitytoaggregateheterogeneousdatasetsintoacommonresponsespace.NeuroImage,217:116865.SamuelANastase,Yun-FeiLiu,HannaHillman,AsiehZadbood,LiatHasenfratz,NegginKeshavarzian,Jan-iceChen,ChristopherJHoney,YaaraYeshurun,MorRegev,etal.2021.Narratives:fmridataforevaluat-ingmodelsofnaturalisticlanguagecomprehension.bioRxiv,pages2020–12.SatoshiNishida,AlexanderGHuth,JackLGallant,andShinjiNishimoto.2015.Wordstatisticsinlarge-scaletextsexplainthehumancorticalsemanticrepresenta-tionofobjects,actions,andimpressions.InSocietyforNeuroscienceAnnualMeeting,volume333.SubbaReddyOota,JashnArora,ManishGupta,andRajuSBapi.2022a.Cross-viewbraindecoding.arXivpreprintarXiv:2204.09564.SubbaReddyOota,JashnArora,VijayRowtula,ManishGupta,andRajuSBapi.2022b.Visio-linguisticbrainencoding.arXivpreprintarXiv:2204.08261.SubbaReddyOota,NareshManwani,andRajuSBapi.2018.fmrisemanticcategorydecodingusinglinguis-ticencodingofwordembeddings.InInternationalConferenceonNeuralInformationProcessing,pages3–15.Springer.SubbaReddyOota,VijayRowtula,ManishGupta,andRajuSBapi.2019.Stepencog:Aconvolutionallstmautoencoderfornear-perfectfmriencoding.In2019InternationalJointConferenceonNeuralNetworks(IJCNN),pages1–8.IEEE.FranciscoPereira,MatthewBotvinick,andGregDetre.2013.Usingwikipediatolearnsemanticfeaturerepresentationsofconcreteconceptsinneuroimagingexperiments.Artificialintelligence,194:240–252.FranciscoPereira,BinLou,BriannaPritchett,NancyKanwisher,MatthewBotvinick,andEvelinaFe-dorenko.2016.Decodingofgenericmentalrepresen-tationsfromfunctionalmridatausingwordembed-dings.bioRxiv,page057216.FranciscoPereira,BinLou,BriannaPritchett,SamuelRitter,SamuelJGershman,NancyKanwisher,MatthewBotvinick,andEvelinaFedorenko.2018.Towardauniversaldecoderoflinguisticmeaningfrombrainactivation.Naturecommunications,9(1):1–13.SaraFPopham,AlexanderGHuth,NataliaYBilenko,FatmaDeniz,JamesSGao,AnwarONunez-Elizalde,andJackLGallant.2021.Visualandlinguis-ticsemanticrepresentationsarealignedatthebor-derofhumanvisualcortex.NatureNeuroscience,24(11):1628–1636.GraemeDRuxtonandGuyBeauchamp.2008.Timeforsomeapriorithinkingaboutposthoctesting.Behavioralecology,19(3):690–693.MartinSchrimpf,IdanBlank,GretaTuckute,CarinaKauf,EghbalAHosseini,NancyKanwisher,JoshuaTenenbaum,andEvelinaFedorenko.2021.Theneuralarchitectureoflanguage:Integrativereverse-engineeringconvergesonamodelforpredictivepro-cessing.PNAS,Vol:Toappear.3231

DanSchwartz,MariyaToneva,andLeilaWehbe.2019.Inducingbrain-relevantbiasinnaturallanguagepro-cessingmodels.AdvancesinNeuralInformationProcessingSystems,32:14123–14133.JingyuanSun,ShaonanWang,JiajunZhang,andChengqingZong.2019.Towardssentence-levelbraindecodingwithdistributedrepresentations.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume33,pages7047–7054.JingyuanSun,ShaonanWang,JiajunZhang,andChengqingZong.2020.Neuralencodinganddecod-ingwithdistributedsentencerepresentations.IEEETransactionsonNeuralNetworksandLearningSys-tems,32(2):589–603.MariyaToneva,OtiliaStretcu,BarnabásPóczos,LeilaWehbe,andTomMMitchell.2020.Modelingtaskeffectsonmeaningrepresentationinthebrainviazero-shotmegprediction.AdvancesinNeuralInfor-mationProcessingSystems,33.MariyaTonevaandLeilaWehbe.2019.Interpret-ingandimprovingnatural-languageprocessing(inmachines)withnaturallanguage-processing(inthebrain).arXivpreprintarXiv:1905.11833.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998–6008.AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelBowman.2018.Glue:Amulti-taskbenchmarkandanalysisplatformfornaturallanguageunderstanding.InProceedingsofthe2018EMNLPWorkshopBlackboxNLP:Analyz-ingandInterpretingNeuralNetworksforNLP,pages353–355.AriaWang,MichaelTarr,andLeilaWehbe.2019.Neuraltaskonomy:Inferringthesimilarityoftask-derivedrepresentationsfrombrainactivity.AdvancesinNeuralInformationProcessingSystems,32:15501–15511.JingWang,VladimirLCherkassky,andMarcelAdamJust.2017.Predictingthebrainactivationpatternas-sociatedwiththepropositionalcontentofasentence:Modelingneuralrepresentationsofeventsandstates.Humanbrainmapping,38(10):4865–4881.ShaonanWang,JiajunZhang,HaiyanWang,NanLin,andChengqingZong.2020.Fine-grainedneuraldecodingwithdistributedwordrepresentations.In-formationSciences,507:256–272.LeilaWehbe,BrianMurphy,ParthaTalukdar,AlonaFyshe,AadityaRamdas,andTomMitchell.2014.Si-multaneouslyuncoveringthepatternsofbrainregionsinvolvedindifferentstoryreadingsubprocesses.inpress.DanielLKYamins,HaHong,CharlesFCadieu,EthanASolomon,DarrenSeibert,andJamesJDi-Carlo.2014.Performance-optimizedhierarchicalmodelspredictneuralresponsesinhighervisualcor-tex.Proceedingsofthenationalacademyofsciences,111(23):8619–8624.ADetailsoftheFinetunedModelsWeselectedtasksforwhichBERT-base-casedfine-tunedmodelswereavailable.Notethatwedidnotfinetuneanyofthesemodelsourselvesbutlever-agedthestate-of-the-artfinetunedmodelsavailableonHuggingface.DetailsofthespecificfinetunedmodelcheckpointsarementionedinTable3.BANOVAtestresultsB.1PereiradatasetThemaineffectofmodelwassignificantfortheROIswith95%confidencewiththesestatistics:•Language\_LH:[F(9,40)=3.95,p=0.0052]•Language\_RH:[F(9,40)=4.53,p=0.0015]•Vision\_Body:[F(9,40)=4.397,p=0.00227]•Vision\_Face:[F(9,40)=3.46,p=0.0085]•Vision\_Object:[F(9,40)=3.40,p=0.0121]•Vision\_Scenes:[F(9,40)=4.917,p=0.0007]•Vision:[F(9,40)=3.945,p=0.00385]•DMN:[F(9,40)=6.28,p=0.00034]•TP:[F(9,40)=6.54,p=0.00042]B.2Narratives-PiemandatasetThemaineffectofmodelwassignificantfortheROIswith95%confidencewiththesestatistics:•EAC\_L[F(9,810)=3.88,p=.00009]•EAC\_R[F(9,810)=3.34,p=.00055]•AAC\_L[F(9,810)=5.37,p=.0000007]•AAC\_R[F(9,810)=6.955,p=.00000]•PMC\_L[F(9,810)=37.21,p=.00000]•PMC\_R[F(9,810)=31.62,p=.00000]•TPOJ\_L[F(9,810)=9.166,p=.00000]•TPOJ\_R[F(9,810)=7.797,p=.00000]•DFL\_L[F(9,810)=12.445,p=.00000]•DFL\_R[F(9,810)=12.27,p=.00000]3232

TaskHuggingFaceModelNameDatasetURLNLIbert-base-nli-mean-tokensStanfordNaturalLanguageInference(SNLI),MultiNLIhttps://huggingface.co/sentence-transformers/bert-base-nli-mean-tokensPDbert-base-cased-finetuned-mrpcMicrosoftResearchParaphraseCorpus(MRPC)https://huggingface.co/bert-base-cased-finetuned-mrpcSSbert-base-chunlCoNLL-2003https://huggingface.co/vblagoje/bert-english-uncased-finetuned-chunkSumbart-base-samsumSAMSumhttps://huggingface.co/lidiya/bart-base-samsumWSDbert-base-baselineEnglishall-wordshttps://github.com/BPYap/BERT-WSDCRbert\_coreference\_baseOntoNotesandGAPhttps://github.com/mandarjoshi90/corefNERbert-base-NERCoNLL-2003https://huggingface.co/dslim/bert-base-NERQAbert-base-qaSQUADhttps://huggingface.co/docs/transformers/model\_doc/bert#bertforquestionansweringSAbert-base-sstStanfordSentimentTreebank(SST)https://huggingface.co/barissayil/bert-sentiment-analysis-sstSRLbert-base-srlEnglishPropBankSRLhttps://s3-us-west-2.amazonaws.com/allennlp/models/bert-base-srl-2020.02.10.tar.gzTable3:DetailsofthefinetunedmodelsT1T2p-valueCRQA0.024CRSA0.015CRNLI0.010Table4:Pairwisecomparisonone-wayANOVAresultsforLanguage\_LHregionT1T2p-valueCRSS0.021CRSRL0.0003CRSum0.003CRQA0.039CRSA0.013CRWSD0.016Table5:Pairwisecomparisonone-wayANOVAresultsforLanguage\_RHregionT1T2p-valueCRSRL0.0011CRSum0.0092CRSA0.039CRNLI0.0061Table6:Pairwisecomparisonone-wayANOVAresultsforVision\_bodyregionT1T2p-valueCRSA0.0404CRnli0.036Table7:Pairwisecomparisonone-wayANOVAresultsforVision\_faceregionT1T2p-valueCRSRL0.0027Table8:Pairwisecomparisonone-wayANOVAresultsforVision\_objectregionT1T2p-valueCRSum0.027CRQA0.0036CRSA0.0022CRNLI0.0010Table9:Pairwisecomparisonone-wayANOVAresultsforVision\_sceneregionT1T2p-valueCRSRL0.0014CRSum0.0431CRNLI0.0177Table10:Pairwisecomparisonone-wayANOVAresultsforVisionregionT1T2p-valueCRNLI0.027CRSum0.008CRPD0.0147NLISA0.056NLISS0.000011SASum0.0188SAPD0.032SSSum0.000002SSWSD0.0059SSPD0.000004SRLSum0.0545SRLPD0.08876Table11:Pairwisecomparisonone-wayAnovaresultsforEAC-LregionT1T2p-valueNLISS0.00157SumSS0.0015PDSS0.002SASS0.0565SSWSD0.052Table12:Pairwisecomparisonone-wayAnovaresultsforEAC-RregionT1T2p-valueNLISS0.000007SASS0.029SSSRL0.0084SSPD0.000023SSQA0.00128Table13:Pairwisecomparisonone-wayAnovaresultsforAAC-Lregion3233

Figure9:Narratives-LucyDataset:2V2Accuracy(topfigure)andPearsoncorrelationcoefficient(bottomfigure)betweenpredictedandtrueresponsesacrossdifferentbrainregionsusingavarietyofNLPtasks.Resultsareaveragedacrossallparticipants.NLI,Paraphrase,andSummarisationperformthebest.T1T2p-valueCRNLI0.0203CRPD0.0072NERPD0.0291NLISS0.0000013SASS0.0299SSSRL0.0011SSPD2.97929e-7SSWSD0.0444SSQA0.00099PDSum0.039Table14:Pairwisecomparisonone-wayAnovaresultsforAAC-RregionT1T2p-valueCRNER1.07034e-10CRNLI0.000001CRSRL0.0014CRPD0.0000047CRQA0.0023NERNLI9.02023e-11NERSA9.02993e-11NERSS0.000157159NERSRL9.02116e-11NERPD9.02023e-11NERSum9.03064e-11NERWSD9.03172e-11NERQA9.02116e-11NLISA0.0207013NLISS9.03255e-11NLISum0.0043NLIWSD0.00036SASS0.0000072SSSRL4.47012e-10SSPD9.04392e-11SSSum0.00011SSWSD0.00084SSQA6.36666e-10PDSum0.012PDWSD0.0012Table15:Pairwisecomparisonone-wayAnovaresultsforPMC-LregionT1T2p-valueCRNER1.52787e-9CRNLI0.0000042CRSS0.0039CRPD0.00011CRQA0.0101NERNLI8.86012e-11NERSA8.87732e-11NERSRL8.88714e-11NERPD8.86092e-11NERSum1.05034e-10NERWSD1.01319e-10NERQA8.86657e-11NLISA0.0059NLISS8.87066e-11NLISum0.000371NLIWSD0.000191SASS0.0000021SSSRL0.00000142SSPD8.87554e-11SSSum0.000126402SSWSD0.000128239SSQA1.31249e-10PDSum0.00619PDWSD0.0036Table16:Pairwisecomparisonone-wayAnovaresultsforPMC-Rregion3234

Figure10:Narratives-SlumlordDataset:2V2Accuracy(topfigure)andPearsoncorrelationcoefficient(bottomfigure)betweenpredictedandtrueresponsesacrossdifferentbrainregionsusingavarietyofNLPtasks.Resultsareaveragedacrossallparticipants.NLI,Paraphrase,andSummarisationperformthebest.T1T2p-valueCRNLI0.00069CRPD0.00395NERNLI0.0051NERSS0.0286244NERPD0.0235NLISS4.43074e-10NLISum0.0068987NLIWSD0.02709SASS0.0001732SSSRL0.0000530SSPD4.37008e-9SSSum0.0219850SSWSD0.005447SSQA0.0000016PDSum0.0306Table17:Pairwisecomparisonone-wayAnovaresultsforTPOJ-LregionT1T2p-valueCRNLI0.0064CRPD0.0148564NERNLI0.0449NLISS3.74353e-8NLIWSD0.0321627SASS0.0036278SSSRL0.001054SSPD1.33146e-7SSSum0.025420SSQA0.000049Table18:Pairwisecomparisonone-wayAnovaresultsforTPOJ-RregionT1T2p-valueCRNLI0.000032CRPD0.000019NERNLI0.000619887NERSS0.040NERPD0.000399NLISS1.61916e-10NLISum0.00074NLIWSD0.000462932SASS0.000221241SSSRL0.0000123345SSPD1.30279e-10SSSum0.0356814SSWSD0.0496343SSQA0.00000162PDSum0.0004803PDWSD0.000296713Table19:Pairwisecomparisonone-wayAnovaresultsforDFL-LregionT1T2p-valueCRNLI0.000191CRPD0.00010NERNLI0.0168115NERSS0.0168115NERPD0.000674NLISS1.05897e-10NLISum0.001194NLIWSD0.003894SASS0.0000256SSSRL0.00000224SSPD9.81710e-11SSSum0.0165866SSWSD0.0057237SSQA2.98083e-7PDSum0.000685PDWSD0.00231873Table20:Pairwisecomparisonone-wayAnovaresultsforDFL-Rregion3235

T1T2p-valueCRNLI0.0188070CRPD0.0099703NERPD0.0321010NLISS6.37802e-7SASS0.00642888SSSRL0.00051148SSPD2.42829e-7SSQA0.000162PDWSD0.0476Table21:Pairwisecomparisonone-wayAnovaresultsforVC-LregionT1T2p-valueCRNLI0.00498313CRPD0.000298933NERNLI0.024695NERPD0.0020556NLISS4.16645e-8NLISum0.0449825NLIWSD0.0352242SASS0.00120394SSSRL0.00002939SSPD7.70669e-10SSSum0.0417081SSQA0.00000742881PDSum0.00434934PDWSD0.0031Table22:Pairwisecomparisonone-wayAnovaresultsforVC-RregionFigure11:Dendrogramconstructedusingsimilaritymatrixconstructedfromthetask-wisebrainresponsepredictionsacross10tasksforsubjects1,2and7inPereiraDatasetFigure12:Dendrogramconstructedusingsimilaritymatrixconstructedfromthetask-wisebrainresponsepredictionsacross10tasksforsubjects1,21and31inNarrativesDatasetFigure13:PereiraDataset–Pearsoncorrelationco-efficientbetweenpredictedandtrueresponsesacrossdifferentsubROIsoftheLanguageNetworkusingSRLtask.Resultsareaveragedacrossallparticipants.3236

Figure14:PereiraDataset–Pearsoncorrelationco-efficientbetweenpredictedandtrueresponsesacrossdifferentsubROIsoftheLanguageNetworkusingCRtask.Resultsareaveragedacrossallparticipants.Figure15:Narratives-Pieman–PearsoncorrelationcoefficientbetweenpredictedandtrueresponsesacrossdifferentsubROIsof5brainROIsusingparaphrasetask.Resultsareaveragedacrossallparticipants.Figure16:Narratives-Pieman–PearsoncorrelationcoefficientbetweenpredictedandtrueresponsesacrossdifferentsubROIsof5brainROIsusingsummarizationtask.Resultsareaveragedacrossallparticipants.3237

Coreference ResolutionNERSemantic Role Labeling Shallow Syntax Word Sense DisambiguationParaphraseQuestion Answering Summarization Sentiment AnalysisNatural Language InferenceFigure17:PereiraBrainMaps:Meanabsoluteerror(MAE)betweenpredictivevoxelsandactualvoxelsusingtaskfeaturesfromTaskonomyinonesamplesubject(subject1).Predictiveregionsofdifferenttasksaredissimilaracrosstasks.ParaphraseSummarizationSemantic Role Labeling Natural Language Ineference NERQuestion Answering Sentiment Analysis Word Sense Disambiguation Coreference Resolution Shallow Syntax Figure18:NarrativesBrainMaps:Meanabsoluteerror(MAE)betweenpredictivevoxelsandactualvoxelsusingtaskfeaturesfromTaskonomyinonesamplesubject(subject1)ofPieMandataset.Predictiveregionsofvarioustasksaredifferentacrosstasks.