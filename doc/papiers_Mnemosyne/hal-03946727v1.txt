Deep Learning for Brain Encoding and Decoding Subba Reddy Oota, Jashn
Arora, Manish Gupta, Raju S. Bapi, Mariya

Toneva

To cite this version:

Subba Reddy Oota, Jashn Arora, Manish Gupta, Raju S. Bapi, Mariya
Toneva. Deep Learning for Brain Encoding and Decoding. COGSCI 2022 - the
44th Annual Conference of the Cognitive Science Society, Jul 2022,
Toronto (CA), Canada. ￿hal-03946727￿

HAL Id: hal-03946727

https://hal.science/hal-03946727

Submitted on 19 Jan 2023

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

Deep Learning for Brain Encoding and Decoding

Subba Reddy Oota1, Jashn Arora2, Manish Gupta2,3, Raju S. Bapi2, Mariya
Toneva4 1INRIA, Bordeaux, France, 2IIIT Hyderabad, India, 3Microsoft,
Hyderabad, India, 4Princeton Neuroscience Institute, USA
subba-reddy.oota@inria.fr, jashn.arora@research.iiit.ac.in,
gmanish@microsoft.com, raju.bapi@iiit.ac.in, mt6949@princeton.edu

Keywords: neuroscience; deep learning.

brain encoding; brain decoding;

cognitive

such as

Introduction How does the brain represent different modes of
information? Can we design a system that can automatically understand
what the user is thinking? We can make progress towards answering such
questions by studying brain recordings from devices functional magnetic
resonance imaging (fMRI). The brain encoding problem aims to
automatically generate fMRI brain representations given a stimulus. The
brain decoding problem is the inverse problem of reconstructing the
stimuli given the fMRI brain representation. Both the brain encoding and
decoding problems have been studied in detail in the past two decades
and the foremost attraction of studying these solutions is that they
serve as additional tools for basic research in cognitive science and
cognitive neuroscience. Recently, inspired by the effectiveness of deep
learning models for natural language processing and computer vision,
such models have been In this tutorial, we plan applied for neuroscience
as well. to discuss different kinds of stimulus representations, and
popular encoding and decoding architectures in detail. The tutorial will
provide a working knowledge of the state of the art methods for encoding
and decoding, a thorough understanding of the literature, and a better
understanding of the benefits and limitations of encoding/decoding with
deep learning.

Encoding models that accurately predict brain activity have several
practical applications in evaluation and diagnosis of neurological
conditions and thus also help designing therapies for brain damage.
Invertible encoding models enable principled formulation of brain
decoding models which in turn are useful for designing brain-machine or
brain-computer interfaces. Recent advances in the use of pretrained deep
network models enable us to use them as priors for brain decoding tasks.
Deep learning models are useful for improving accuracy but also offer
the flexibility of decoding across a gamut of tasks and domains.

Tutorial outline We present the overall tutorial outline with time slots
(adding up to 3 hours) in this section.

Intro to Brain encoding and decoding [30 min]

• Intro to Brain Encoding/Decoding and applications

Potter

(Harry

• Intro to popular datasets:

Text Words, Sentences, ZUCO EEG, Paragraphs Story, Question-Answering
MEG); Visual visual (Binary patterns, Natural Images (Vim-1), BOLD5000,
Algonauts and SS-fMRI); Videos (BBC’s Doctor Who, Japanese Ads, Pippi
Langkous, Algonauts); Multimodal (Words + line drawing of concept named
by each word, The Moth Radio Hour (Jain & Huth, 2018), Pereira,
Narratives).

Stimulus Representations [30 min]

• Text Stimuli: Syntactic (corpus co-occurrence counts, topic models,
syntactic features and discourse features), Semantic sentence
representation models, recurrent neural networks and Transformer
methods), Experiential attributes (Rated on 0-6 scale or binary).

embedding methods,

(word

• Visual Stimuli: Visual field filter banks, Gabor wavelet pyramid;
Convolutional neural networks; Concept recognition models.

• Audio Stimuli: Phoneme rate and presence of phonemes.

Brain Encoding [60 min]

activation; Words sequence models;

• Linguistic Encoding in the Brain: Words represented as vectors in an
embedding space and mapped to augmented with context neural using
representations Sentence based on Transformer models (Schrimpf et al.,
2021); Modulation of word meaning representations based on task settings
(Toneva, Stretcu, P´oczos, Wehbe, & Mitchell, 2020); Disentangling
lexical, syntactic, and semantic representations (Caucheteux, Gramfort,
& King, 2021). • Visual Encoding in the Brain: Representations derived
from object classification models (Kubilius et al., 2019);
Representations extracted from models of computer vision tasks (Wang,
Tarr, & Wehbe, 2019); Representation of temporal dynamics of vision.

Brain Decoding [60 min]

• Decoder Architectures: Ridge regression models; Most informative
voxels selection; Fully connected layer, MLPs; Representational
similarity analysis; Classifiers like SVMs and Gaussian Na¨ıve Bayes.
settings: task

vector representation using single-mode stimuli or multi-modal stimuli
(Pereira et al., 2018); Relationship between various natural language
understanding tasks and the brain

• Decoding

Decoding

to

a

decoding task (Gauthier & Levy, 2019); Reconstruct the visual stimuli
(Beliy et al., 2019); Reconstructing personal imagined experiences;
Application-based decoding like predicting individuals’ engineering exam
results, reflecting whether current thoughts are detailed, correspond to
the past or future, are verbal or in images, object recognition. •
Metrics: 2 versus 2 (2V2) Accuracy; Pearson Correlation; RSM; R2 score,
MSE; Pairwise Accuracy; Rank based Accuracy; Classification rate.

Target audience and prerequisites The tutorial should be beneficial for
researchers from both academia and industry. Specifically, academics
with interests in cognitive modeling and brain mapping will benefit from
the broad perspective of how deep learning methods can be applied for
these purposes. Researchers from companies interfaces and hardware who
innovate and design novel subsystems for the next generation of virtual,
augmented, and mixed reality experiences will find the tutorial
exciting. Researchers working on innovative user interfaces like
wearable devices, neuro-development of emotion processing and
regulation, and human perception are also expected to find the tutorial
relevant. Expected pre-requisites include basic maths, machine learning
and basic deep learning concepts.

Presenter experience and interests The tutorial presenters represent
great diversity with respect to academic as well as industry
affiliations, multi-geography and different career stages. We hope that
the tutorial will attract both industry as well as academic
participation. Subba Reddy Oota is a PhD Student at Inria-Research at
Bordeaux, France. He received his Masters from IIIT Hyderabad in 2016.
His research interests are in the areas of language analysis in the
brain, brain encoding, and decoding. He has presented several research
papers in refereed conferences like WACV, IJCNN, ICDAR, and ICONIP.
Jashn Arora is an Undergraduate Student at IIIT Hyderabad. His research
interests are in the areas of Natural language Processing, analysis of
language representation in the brain, and brain encoding and decoding.
He has experience conducting tutorials and lab sessions as a teaching
assistant for various courses at IIIT Hyderabad. Manish Gupta is a
Principal Applied Researcher at Microsoft, India. He is also an Adjunct
Faculty at IIIT Hyderabad and a visiting faculty at the Indian School of
Business (ISB). He received his Masters from IIT Bombay in 2007 and his
Ph.D. from UIUC in 2013. His research interests are in the areas of deep
learning, web mining, and neuroscience. He has extensive experience
offering tutorials at top conferences like CIKM’13, WWW’14, SIGIR’15,
CIKM’20, IJCAI’20, ECML/PKDD’21, and WSDM’22. Raju S. Bapi is a
professor and head of the Cognitive Science Lab, IIIT Hyderabad.
Earlier, he was a professor at Univ.

of Hyderabad, India; EPSRC Research Fellow at Univ. of Plymouth, UK; and
a Researcher at ATR Research Labs, Kyoto, Japan. He has over 20 years of
teaching and research experience in AI, Machine Learning, Neural
Networks, and Cognitive Science. He has a PhD (Computer Science) from
Univ. of Texas, Arlington. He is a senior member of IEEE, ACM, Society
for Neuroscience, and Cognitive Science Society. Mariya Toneva is a
postdoctoral fellow at the Neuroscience Institute at Princeton
University and a tenure-track faculty at the Max Planck Institute for
Software Systems, starting in Fall 2022. She obtained her PhD from
Carnegie Mellon University in a joint program between Machine Learning
and Neural Computation in 2021. Her research focuses on building
computational models of language processing in the brain that can also
improve natural language processing systems. Her work has appeared at
CogSci, ICLR, NeurIPS, and SNL, and she has organized workshops and
symposia at ICLR and SNL.

References Beliy, R., Gaziv, G., Hoogi, A., Strappini, F., Golan, T., &
Irani, M. (2019). From voxels to pixels and back: Self-supervision in
natural-image reconstruction from fmri. NeurIPS, 33, 6517–6527.

Caucheteux, C., Gramfort, A., & King, J.-R. (2021). Disentangling In

syntax and semantics in the brain with deep networks. ICML
(pp. 1336–1348).

Eickenberg, M., Gramfort, A., Varoquaux, G., & Thirion, B. (2017).
Seeing it all: Convolutional network layers map the function of the
human visual system. NeuroImage, 152, 184–194.

Gauthier, J., & Levy, R. (2019). Linking artificial and human neural

representations of language. EMNLP-IJCNLP, 529-539.

Jain, S., & Huth, A. G. (2018). Incorporating context into language
encoding models for fmri. In NeurIPS-32 (pp. 6629–6638). Kubilius, J.,
Schrimpf, M., Kar, K., Rajalingham, R., Hong, H., Majaj, N., . . .
others (2019). Brain-like object recognition with high-performing
shallow recurrent anns. NeurIPS, 33, 12805–12816.

Pereira, F., Lou, B., Pritchett, B., Ritter, S., Gershman, S. J.,
Kanwisher, N., . . . Fedorenko, E. (2018). Toward a universal decoder of
linguistic meaning from brain activation. Nature Communications, 9(1),
1–13.

Schrimpf, M., Blank, I., Tuckute, G., Kauf, C., Hosseini, E. A., (2021).
The neural . . . Fedorenko, E. Kanwisher, N., architecture of language:
Integrative reverse-engineering converges on a model for predictive
processing. PNAS, Vol(118), 45.

Schwartz, D., Toneva, M., & Wehbe, L.

Inducing brain-relevant bias in natural language processing models.
NeurIPS, 33, 14123–14133.

(2019).

Toneva, M., Stretcu, O., P´oczos, B., Wehbe, L., & Mitchell, T. M.
(2020). Modeling task effects on meaning representation in the brain via
zero-shot meg prediction. NeurIPS, 34.

Toneva, M., & Wehbe, L.

Interpreting and improving natural-language processing (in machines)
with natural language-processing (in the brain). NeurIPS, 33,
14954–14964.

(2019).

Wang, A., Tarr, M., & Wehbe, L.

(2019). Neural taskonomy: Inferring the similarity of task-derived
representations from brain activity. NeurIPS, 33, 15501–15511.


