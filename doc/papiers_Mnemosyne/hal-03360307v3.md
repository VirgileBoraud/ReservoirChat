Ontology as neuronal-space manifold: Towards symbolic
and numerical artificial embedding
Chloé Mercier, Hugo Chateau-Laurent, Frédéric Alexandre, Thierry Viéville

To cite this version:

Chloé Mercier, Hugo Chateau-Laurent, Frédéric Alexandre, Thierry Viéville. Ontology as neuronal-
space manifold: Towards symbolic and numerical artificial embedding. KRHCAI 2021 Workshop on
Knowledge Representation for Hybrid & Compositional AI @ KR2021, Nov 2021, Hanoi, Vietnam.
￿hal-03360307v3￿

HAL Id: hal-03360307

https://inria.hal.science/hal-03360307v3

Submitted on 8 Nov 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Ontology as neuronal-space manifold: Towards symbolic and numerical artiﬁcial
embedding ∗

Chlo´e Mercier1 , Hugo Chateau-Laurent1 , Fr´ed´eric Alexandre1 , Thierry Vi´eville1
1Mnemosyne Team, Inria Bordeaux, LaBRI and IMN
ﬁrstname.lastname@inria.fr

Abstract

Some human cognitive tasks may involve tightly interleaved
logical and numerical computations. On the one hand, on-
tologies allow us to describe symbolic structured knowledge
and perform logical inference, providing a rather natural rep-
resentation of human reasoning as modeled in cognitive psy-
chology. On the other hand, spiking neural networks are a
biologically plausible implementation of processing in brain
circuits, yet they process numeric vectors rather than sym-
bolic data. Unifying these symbolic and sub-symbolic ap-
proaches is still a wide and open question, and the Semantic
Pointer Architecture (SPA) based on the Vector Symbolic Ar-
chitecture (VSA) provides a way to manipulate symbols em-
bedded as numeric vectors that carry semantic information.
In this paper, as a step towards ﬁlling the symbolic/numerical
gap, we propose to map an ontology onto a SPA-based archi-
tecture with a preliminary partial implementation into spik-
ing neural networks. More speciﬁcally, we focus on ontol-
ogy standards used in the semantic web such as Resource
Description Framework [Schema] (RDF[S]) and the Web On-
tology Language (OWL). We provide a detailed implementa-
tion example in the case of speciﬁc RDFS entailments based
on predicate chaining. To that end, we used the neural sim-
ulator Nengo with two associative memories in interaction,
the ﬁrst one storing assertions and the second one storing
entailment rules. Reporting interesting formal results, our
embedding enjoys intrinsic properties allowing semantic rea-
soning through distributed numerical computing. This orig-
inal preliminary work thus combines symbolic and numeri-
cal approaches for cognitive modeling, which might be use-
ful to model some complex human tasks such as ill-deﬁned
problem-solving, involving neuronal knowledge manipula-
tion.

Keywords: Ontology, Resource Description Framework,
Vector Symbolic Architecture, Semantic Pointer Architec-
ture, Neural Engineering Framework, Neurosymbolism.

1

Introduction

Artiﬁcial Intelligence (AI) has recently made signiﬁcant
strides, both in numerical approaches and symbolic ap-
proaches; the former is based on numerical computation and
includes, among others, neural networks or Bayesian infer-
ence, while the latter manipulates knowledge bases such as

∗Supported
by
mnemosyne/en/aide.

Inria, AEx AIDE https://team.inria.fr/

used in expert systems or the semantic web. Accordingly,
large AI systems with impressive capabilities have been pro-
posed. Nonetheless, the AI ﬁeld is still lacking uniﬁed sys-
tems integrating both approaches (Sun and Alexandre 2013),
although the last decade has seen the emergence of a new
“neurosymbolic” wave aiming to make neural networks per-
form logical reasoning (Garcez and Lamb 2020).

It is generally considered that human cognitive capabil-
ities are not within the reach of either purely symbolic or
purely numerical artiﬁcial intelligence because of the limi-
tations of each approach. In hybrid systems, both symbolic
and numerical components are involved (Lallement, Hilario,
and Alexandre 1995), but their contributions are usually
kept separate in distinct aspects of the system. This is the
case for example with ontology-based deep learning (e.g.,
(Phan et al. 2017; Petrucci, Ghidini, and Rospocher 2016;
Hohenecker and Lukasiewicz 2020)) or “black-box” co-
operation between deep-networks and ontology reasoners
(e.g., (Ayadi et al. 2019; Jim´enez, Elizalde, and Raj 2018)).
However, geometric mapping of ontologies onto Euclidean
spaces or manifolds (e.g., (Eidoon, Yazdani, and Oroum-
chian 2008; Tous and Delgado 2006; Xiao, Huang, and
Zhu 2015)) allows to perform reasoning at both a symbolic
and to a limited extent numerical level, while the link be-
tween neural network representation and manifolds is well
established and understood (e.g., (Chui and Mhaskar 2018;
Zhu et al. 2017)).

In some cases, there is a need for a full integration of
both paradigms, as allowed by a uniﬁed system, because the
underlying task corresponds to a tight interaction between
the properties of the paradigms, which cannot be achieved
with separate modules. This is the case with the topic we
consider in our group, that is human learning. In a recent
development (Mercier et al. 2021), we propose to formal-
ize a problem-solving task, where children have to manip-
ulate and assemble objects to answer instructions given by
the teacher (Romero, David, and Lille 2019). It has been re-
viewed in the previous reference that the underlying cogni-
tive functions are as diverse as sensorimotor skills, exploita-
tion of explicit and sometimes partial knowledge, hypothesis
generation and creativity.

In the case of such tasks with so tightly interleaved sym-
bolic and numerical components, some authors propose that
a purely numerical, neuronal approach could implement the

needed uniﬁed system, with an obvious reference to the
brain as an example of a neuronal system manipulating sym-
bolic information with units generally considered as numer-
ical systems, especially to implement high level cognitive
functions (see, e.g., (Pulverm¨uller 2013) for a general re-
view). Yet it is a wide and still open issue to establish the
numerical primitives required for modeling symbolic rep-
resentation and manipulation (see (Alexandre 2019) for a
general discussion). In the last few years, models using nu-
merical mechanisms to process symbolic information such
as logical computation (Shi et al. 2020) or reasoning (Riegel
et al. 2020) have ﬂourished.

There are numerous approaches to vectorize graph data
models, such as translational distance models or random
walk based methods (see (Wang, Qiu, and Wang 2021) for a
recent review in link with approximate statistical reasoning,
or (Sajjad, Docherty, and Tyshetskiy 2019) regarding repre-
sentation learning from data) including taking into account
dynamic knowledge such as (Sauerwald and Zanetti 2019),
including approaches combining vector representations and
inference rules such as (Guo et al. 2016). There are also
many real valued ﬁrst order logic (e.g., probabilistic) ap-
proaches integrated in or interfaced with machine learning
algorithms (see (Garcez and Lamb 2020) for a general re-
view and, e.g., (Cohen, Yang, and Mazaitis 2017) for an ex-
ample). Our approach is complementary in the sense that
we focus on both a biologically plausible neuronal frame-
work, and a symbolic formalism based on triples (as deﬁned
in 2.4) that seems appropriate to model human reasoning in
psychology (McClelland and Rogers 2003).

Speciﬁc questions may arise when trying to integrate for
example relations between ontology speciﬁcations and par-
tial knowledge (Tettamanzi, Zucker, and Gandon 2017), al-
lowing one to consider not only true or false knowledge, but
relative degree of truth, as taken into account here. Most of
machine learning algorithms represent such “partial truth”
with only probability. However, the human “level of truth”
seems to be different and related to other notions such as
possibility and necessity, related to a given modality, that is
a context, a given time, and so on, which is also considered
as representative to what is modeled in educational science
and philosophy (see (Smith 1994) while (Rusawuk 2018)
proposes a discussion).

As a possible entry to a uniﬁed approach, Vector Sym-
bolic Architectures (VSA) were introduced as a way to ma-
nipulate symbolic information represented as numeric vec-
tors (see e.g. (Levy and Gayler 2008) for an introduction).
VSAs have been proven helpful to model high-level cog-
nition and account for multiple biological features (Gayler
2003; Eliasmith 2013). More speciﬁcally, the Semantic
Pointer Architecture (Eliasmith 2013) instantiates so-called
semantic pointers (i.e. vectors that carry semantic informa-
tion) and their manipulation in networks of spiking neurons.
This approach makes a signiﬁcant step towards the uniﬁca-
tion of symbolic and sub-symbolic processing in that it pro-
vides a way to translate the former into the latter. Conse-
quently, complex knowledge representation in the form of
compositional structures that are traditionally restricted to
symbolic approaches can now be distilled in numerical and

even neural systems (Crawford, Gingerich, and Eliasmith
2016).

On the other hand, considering knowledge representation
and reasoning, the capabilities of Semantic Web modeling
languages, such as RDFS (Resource Description Framework
Schema) and OWL (Web Ontology Language) (see, e.g.
(Allemang, Hendler, and Gandon 2020) for a recent didactic
reference) is a rather accessible and very powerful way of
solving modeling problem and manipulate high-level data
representation. To what extent could such mechanism be bi-
ologically plausible ? In order to contribute to this issue,
we show here that suitable design choices allow us to make
explicit how to implement a RDFS1 speciﬁcation using the
Semantic Pointer Architecture. We illustrate this on a simple
example and also discuss to which extent OWL speciﬁcation
could beneﬁt from the same method.

2 Basic design choices

2.1 From symbols to numbers
In order to represent symbolic information, we use RDFS to
structure knowledge representation. It is based on the RDF
data model, which represent knowledge as triples, as made
explicit in section 2.4. More precisely, the universe of dis-
course is made of resources, referenced by some universal
resource identiﬁer (IRI), i.e. a ﬁxed lexical token. To struc-
ture this universe of discourse, we consider:

(i) individuals that refer to real-world concrete or abstract

objects, or

(ii) literals to characterize individuals using data attributes,
i.e., numerical values, character strings, or any structured
information such as dates

(iii) concepts and roles (namely classes and properties) that
allow to structure the knowledge about individuals.

Before going further, we can point out that the present
deﬁnition follows the RDF/RDFS framework, with the fol-
lowing variants:

• we conﬂate name with both IRI and blank node, since
on the one hand blank node can be eliminated,2 and on
the other hand because we only process the information
locally at this stage, thus avoiding considering all is-
sues regarding distributed information between different
sources;

• we do not consider (i) semantic web speciﬁc literal (e.g.,
rdf:XMLLiteral), or (ii) utility and annotation or
other human-targeted properties (e.g., rdfs:seeAlso)
at this stage;

• we will introduce both containers, i.e., ordered or un-
ordered sequences, and collections, i.e., chained lists,
later in these speciﬁcations, but in a somehow different
form, adapted to the numerical representation and obvi-
ous to map on RDF representations;

1According to the https://www.w3.org/TR/rdf-schema speciﬁ-

cation.

2Using a standard process related to skolemisation.

• we do not consider all XSD data-types, but will introduce
a precise notion of numerical values and will detail how
to represent structured data in our framework.

At the numerical level, each resource is implemented as
a randomly drawn ﬁxed unit d-dimensional vector, x ∈ Rd,
and the key idea is to study to what extent symbolic rea-
soning on resources may correspond to algebraic operations
implemented via numerical computations (that we make ex-
plicit in 3.1., based on a framework introduced in (Eliasmith
2013)). Typically d (cid:39) 100 · · · 1000 and we expect to manip-
ulate k (cid:39) 100 · · · 10000 resources.

A similarity measure is now introduced in order to seman-

tically compare two vectors.

2.2 Semantic similarity

Classically, the cosine similarity (i.e., normalized dot prod-
uct, denoted ·) is used to compute the semantic similarity
between two unit vectors:

x · y def= x(cid:62)y
where x(cid:62) denotes the transpose of x.

The key property is that, provided that the space dimen-
sion d is large enough, two randomly chosen different vec-
tors will be approximately orthogonal. More precisely,

x · y ∼ N (0, O(1/d)),
i.e., follows a centered normal distribution (Schlegel, Neu-
bert, and Protzel 2020), while by construction x · x = 1.

Based on this, most VSA approaches consider that 2 vec-
tors x and y are semantically equivalent when this similarity
τ equals to 1, but with different ways to interpret the result:
- Closed-world reasoning: Anything, that cannot be stated
as true is false, thus τ ∈ {0, 1}, often obtained by projection
and rectiﬁcation (to avoid negative values). This is the most
common interpretation in VSAs.
- Open-world reasoning: Anything might be true unless it
can be proven false, prompting a need to characterize un-
known statements; furthermore the notion of negation is ei-
ther not deﬁned (as in the RDFS model yielding monotonic
reasoning only), or deﬁned at a higher level (as in OWL and
more generally in description logics) that we also consider
as a perspective of this work. Here we enrich the notion
of being either false or true, by a numeric representation of
partial knowledge, as illustrated in Fig 1. The true value
corresponds to 1 (fully possible and fully necessary), the
false value to -1 (neither possible nor necessary, i.e., im-
possible) and the unknown value to 0, which corresponds
to a fully possible but absolutely not necessary value. This
representation has been designed to be compatible with the
ternary Kleene logic, beside being also coherent with respect
to the possibility theory3 (not developed here, please refer to

3To make the link explicit, given necessity ν and possibility
π, with ν ≤ π by construction, while ν > 0 ⇒ π = 1 and
π < 1 ⇒ ν = 0, we have the one to one correspondence with our
representation using τ ∈ [−1, 1]:

τ def= ν + π − 1 with

(cid:26) π = 1 + H(−τ ) τ
ν = H(τ ) τ,

(Denœux, Dubois, and Prade 2020) for a general introduc-
tion). This deterministic representation of partial knowledge
can be generalized in order to also include a probabilistic
representation (using a 2D representation), although we will
not develop this aspect any further.

Figure 1: Representation of partial truth τ ∈ [−1, 1], in link with
necessity and possibility.

2.3 Classes and approximate Boolean properties

The ﬁrst kind of concept to structure the knowledge is the hi-
erarchical notion of class, which is equivalent to the notion
of Boolean property, deﬁning the class of all individuals en-
joying (or not) this property, and deﬁning the property that
an individual belongs (or not) to a given class.

In RDFS this translates into, given a scoring τ as intro-

duced previously: x rdf:type c (cid:46) τ

We assume in the following that classes and individuals
are in disjoint semantic sets. Although this is not necessar-
ily the case in RDFS, this assumption is compatible with
description logics (i.e. the OWL-DL level of speciﬁcation)
we target to use in future developments, and ensures to avoid
self-reference paradoxes.

At the numeric level, they both correspond to unit vectors,

with the following interpretation:
- For a given vector x encoding an individual, the vector
c = x corresponds to the singleton class C = {x}.
- For a given class vector c the individual vector x = c can
be interpreted as a “prototype” for this class.

At a geometric level, this can be interpreted as covari-
ant/contravariant duality, and the similarity between a class
covariant vector c and an individual contravariant vector x
may be interpreted as the fact this individual approximately
belongs, or not, to the class.

2.4 Statements: facts and rules

In RDF, the knowledge about the universe of discourse
the form subject
is structured into statements of
predicate object, called triples, where subject
and object are two resources linked by a relationship
(property) explicitized by the predicate. We introduce
weighted triples of the form:

subject predicate object (cid:46) τ
with a value τ as discussed previously, which generalizes
usual RDF statements, introducing a scoring value (refer to
(Tettamanzi, Zucker, and Gandon 2017) for an introduction
and a recent literature review). This is easily stated in the
RDF language itself, using reiﬁcation, while in our case, it
is an intrinsic feature of our design. More precisely, when
considering RDFS where only true (but not false) assertions
can be stated, we have τ ∈ [0, 1], while when generalizing to
the OWL language where negation can be stated we will use

τ ∈ [−1, 1], this setting being compatible with both levels
of speciﬁcation.

It is worth noting that such statements are of three kinds,

although this is not explicit in the model

entity data-property literal (cid:46) τ
deﬁnes attributes allowing to specify some entity attributes;
entity object-property entity (cid:46) τ

deﬁnes relations between entities;

concept predicate resource (cid:46) τ
deﬁnes meta-properties about classes or properties.

3 Ontology numerical mapping

3.1 A Semantic Pointer Triplestore
Let us suppose we need to represent the following weighted
statements:

subject1 predicate1 object1 (cid:46) τ1
subject2 predicate2 object2 (cid:46) τ2
subject2 predicate3 object3 (cid:46) τ3
(note that the same subject subject2 is used in the two

last statements).

The ﬁrst step towards a Semantic Pointer representation
is to encode each resource by a randomly sampled vector on
the unit hypersphere of our d-dimensional vector space: let
us denote si, pi, oi the respective vector representations of
the resources subjecti, predicatei, objecti.

Next, we may store each of these statements into an asso-
ciative memory (Stewart, Tang, and Eliasmith 2011), similar
to a hash table, where each “key” would be a resource and
the corresponding value would express the statements for
which this resource is a subject:4

s1 → τ1 B(o1, p1)

s2 → τ2 B(o2, p2) + τ3 B(o3, p3)

for which we need to introduce the following operations:
1. A scalar multiplication a = τ b that scales a vector b by

a factor of τ and preserves its direction.

2. A vector superposition (e.g. element-wise addition) a =
b+c that results in a vector a with a·b = 1 and a·c = 1,
assuming b and c are orthonormal.

3. A binding operation a = B(b, c) that outputs a vector a

that is not collinear with either b or c.
Let us discuss in detail this last ingredient. The Seman-
tic Pointer Architecture (SPA) developed by Eliasmith et
al(Eliasmith 2013) implements such operations. This cog-
nitive architecture builds upon a particular case of Vector
Symbolic Architecture (VSA) and the Neural Engineering
Framework (NEF) (Eliasmith and Anderson 2002). The
NEF provides a set of principles for representing vectors
into biologically plausible networks of neurons, and imple-
menting the desired transformations through synaptic con-
nections (see (Eliasmith and Anderson 2002) for technical
details). This framework (including both NEF and SPA) is
already implemented into a simulator called Nengo (Beko-
lay et al. 2014).

4In RDF/RDFS, this kind of database is known as a triplestore,

as it stores statements also referred to as triples.

Several choices are available for the binding operation.
The most classical one is the circular convolution a =
b (cid:126) c, which is used in Holographic Reduced Represen-
tations (HRR) (Plate 1995). This operation is commutative,
associative and distributive. In order to retrieve an element
vector b from the resulting vector a, the circular convolution
can be used with the approximate convolutive inverse of the
other element (c−1): b ≈ a (cid:126) c−1.

However, commutativity and associativity could lead to
serious misunderstandings. Let us illustrate this point with
an example:

“Luigi eats this Pizza” and “this Pizza has a topping of

Mozzarella”, encoded as:

Luigi → eats (cid:126) Pizza (1)
Pizza → hasTopping (cid:126) Mozzarella (2)
By injecting the Pizza subject from the statement (2) into
the Pizza object in the statement (1), we can infer the new
statement:

Luigi → eats (cid:126) (Pizza (cid:126) (hasTopping (cid:126) Mozzarella) (3)
Under this form, (3) may be interpretated as “Luigi eats
this Pizza which has a topping of Mozzarella”, but the right
member can be rewritten as:

(3r) = eats (cid:126) Pizza (cid:126) hasTopping (cid:126) Mozzarella (associa-

tivity)

= hasTopping (cid:126) Mozzarella (cid:126) eats (cid:126) Pizza (commutativ-

ity)

interpreted as “Luigi has a topping of Mozzarella which
eats a Pizza”. Note that circular references are easily man-
aged with this setup, thanks to the binding mechanism gen-
erating always an almost orthogonal combined vector.

Instead, we will use another binding operation offered by
the Vector-derived Transformation Binding (VTB) algebra,
described by Gosmann and Eliasmith in (Gosmann and Elia-
smith 2019), which is also implemented in Nengo and is
neither commutative nor associative, but distributive and bi-
linear.

3.2 Using the VTB algebra

The binding operation

With d the dimensionality of the vector space, which the
VTB requires to be square, and d(cid:48)2 = d, the binding opera-
tion is deﬁned, following (Gosmann and Eliasmith 2019):
B(x, y) def= By x

where By is block-diagonal matrix deﬁned as




B(cid:48)
0
y
0 B(cid:48)
y
...
...
0
0

0
. . .
0
. . .
...
. . .
. . . B(cid:48)
y






By

def=






where

B(cid:48)

y = d

1
4







y1
yd(cid:48)+1
...
yd−d(cid:48)+1

y2
yd(cid:48)+2
...
yd−d(cid:48)+2

. . .
. . .
. . .
. . .







yd(cid:48)
y2d(cid:48)
...
yd

With these notations, the ﬁrst relation above becomes
s1 → τ1 Bp1 o1
This design choice enjoys the following interesting proper-
ties:
- The relation is not commutative in x and y, in the general
case, as required.
- The relation is bilinear in x and y (this is obvious for the
former, and easily veriﬁed on the matrix form for the latter).
- The relation is approximately left and right invertible as
discussed now.

The right unbinding operation

In order to retrieve an element vector from the bound vec-
tor, we need to bind it to the inverse of the other element
vector. Unlike HRR and the circular convolution, the VTB
algebra does not provide two-side inverses: there is no left
inverse for VTB.

The right approximate inverse y∼ must enjoy the follow-

ing property:

∀x, B(B(x, y), y∼)) = By∼ By x = x.
The key point here is that since the coefﬁcients of y are cho-
sen randomly and independently identically distributed, the
matrix By is almost orthogonal, thus:
y (cid:39) By∼,

B(cid:62)

which corresponds to simply permuting the elements of y.

identity vector iB such that

Consequently,

the right
BiB = I, writes explicitly:
(cid:26)

[iB]i =

4

d− 1
0

if i = (k − 1) d(cid:48) + k, 0 < k ≤ d(cid:48)
otherwise.

where [iB]i stands for the i-th coordinate of the vector. We
get iB by “unfolding” the identity matrix Id(cid:48) line by line and
concatenating it d(cid:48) times (and adjusting the resulting vector
by a d− 1

4 factor).

This allows us to check if a property is a predicate for a
given resource and, if so, retrieve the corresponding object
(thus performing right unbinding):

x → z = B(y, p) = Bp y
(cid:62) z = Bp

(cid:62) Bp y = y and y belongs to
if and only if Bp
the vocabulary. Speciﬁcally, “y belongs to the vocabulary”
means that there exists a vector v in the vocabulary such that
v · y (cid:39) 1. Otherwise the result is simply undeﬁned, leading
to the conclusion that the predicate is undeﬁned for the given
resource.

The left unbinding operation

A step further, we may also want to check if two resources
are linked to each other by a predicate and, if so, retrieve the
corresponding property (referred to as left unbinding).
In
order to do that, we ﬁrst need to “ﬂip” the order of the terms
involved in the binding. This is achievable by performing
the following operation:

B(a, b) = B↔ B(b, a)

considering the matrix B↔ deﬁned as:

[B↔]ij

def=




1

if j = 1 +

(cid:23)

(cid:22) i − 1
d(cid:48)

+ d(cid:48)[(i − 1) mod d(cid:48)]



0
Thanks to this, we obtain:

otherwise.

x → z = B(y, p) = Bp y
(cid:62) B↔ z = By

(cid:62) By p = p and p belongs

if and only if By
to the vocabulary.

4 Relationship and membership composition

4.1 Hierarchical representations
RDF/RDFS provides two preset properties to account for hi-
erarchical representations between individuals and classes:
• x rdf:type c, expresses that the individual x belongs

to the class c.

• c rdfs:subClassOf c’ expresses that the class c
is a subclass of the class c’, or in other words, that the
concept c is included in the class c’. This means that all
individuals that belong to c automatically belong to c’.
Consequently, some entailment rules 5 can be used to dis-

close implicit memberships:

- inheritance (referred to as rdfs9 by the W3C):

x rdf:type c ∧ c rdfs:subClassOf c’
=⇒ x rdf:type c’

- transitivity of rdfs:subClassOf (rdfs11):

c rdfs:subClassOf c’ ∧ c’ rdfs:subClassOf c’’
=⇒ c rdfs:subClassOf c’’

The Semantic Pointer representation that we proposed
earlier, using the VTB algebra, allows us to implement such
rules.

For example, the inheritance rule can be expressed as fol-

lows: x → Btype c and c → BsubClassOf c(cid:48) implies

x → Btype c + Btype BsubClassOf c(cid:48) + Btype c(cid:48)
A sufﬁcient condition for this implication to be true
would be Btype BsubClassOf = Btype, which would mean
subClassOf = iB; that may not be desirable as it would
lead to many wrongly inherited memberships.

Instead, we prefer using a second associative memory to
encode entailment rules such as Btype BsubClassOf → Btype,
as schematized in Fig.3. However, we cannot store matrices
in the associative memory, only vectors. But we can observe
that both of these matrices are block-diagonal, and of the
form

Bz =

(cid:34) Bz

(cid:48)

0
0 Bz
0

0
0
0 Bz

(cid:48)

(cid:35)

,

(cid:48)

thus correspond to a given vector z. We thus may simply
store the vectors z into the associative memory. In this par-
ticular case, we would therefore store the association:

type (cid:11) subClassOf → type
where we introduce a vector composition operator (cid:11) deﬁned

5A recapitulative table of RDFS entailment rules as deﬁned by
the W3C can be found at https://www.w3.org/TR/2004/REC-rdf-
mt-20040210

as: z = a (cid:11) b such that:
Bz

(cid:48) = d 1

4 Ba

(cid:48) Bb

(cid:48),

using tensorial notations:

(x p y) i.e., x = Bp y

so that Bz = Ba Bb.
Similarly, we

can express

rdfs:subClassOf c’:

the

transitivity of c

c → BsubClassOf c(cid:48)

c(cid:48) → BsubClassOf c(cid:48)(cid:48)

and

implies

c → BsubClassOf c(cid:48) + BsubClassOf BsubClassOf c(cid:48)(cid:48) + BsubClassOf c(cid:48)(cid:48)

As a consequence, the inference rule will be stored as:

subClassOf (cid:11) subClassOf → subClassOf

Then, by iterating on such inference rule, we easily obtain
the transitive closure of properties inferred from rdf:type
and rdfs:subClassOf statements. At the distributed
implementation level, this means using a loop between the
second associative memory encoding inference rules and the
ﬁrst associative memory encoding the input statements, as
schematized in Fig.3, on the last page. This general princi-
ple allows the system to compute the closure of the reason-
ing rules.
i.e., ﬁxed point iteration, adding any new state-
ment derived from the application of the rules, until a ﬁxed
point is reached.

Let us now discuss, to which extents we can generalize

the present formalism, to any property.

4.2 Relational representations
The previous mechanism allows us to express any entailment
of the form:

(x p y) ∧ (y p’ z) =⇒ (x p’’ z)

by storing the rule

p (cid:11) p’ → p”
thus implemented by an operator product, for some predi-
cate p, p’, p”. In other words, we may express any entail-
ment based on predicate chaining.

A step further, we need to implement property restric-
tions, i.e. at the RDFS level, property range rdfs:range
and domain rdfs:domain, e.g., inference of the form:

(x p y) ∧ (p rdfs:domain c) =⇒ (x rdf:type c)

and

(x p y) ∧ (p rdfs:range c) =⇒ (y rdf:type c)

and subproperty hierarchy of the form:

(x p y) ∧ (p rdfs:subPropertyOf q) =⇒ (x q y).

for the rdfs2, rdfs3 and rdfs7 entailment rules, respectively.
These rules belong to Description Logic Programming
(DLP), roughly speaking at the intersection of OWL and
Logic Programming with Horn clauses (Grosof et al. 2003),
allowing to implement the complete RDFS language as tar-
geted here and beyond an interesting part of OWL axioms
(Levesque 1986).

In order to derive such generalization, we need non trivial

algebra. Let us rewrite, for a given triple:

x = po
x = op

s B p y, p (cid:39) so
s B yp, p (cid:39) os

p B x y, y (cid:39) sp
p B y x, y (cid:39) ps

o B x p,
o B p x,

these notations being well deﬁned because, as stated in the
previous section, since (i) the operator is bi-linear, (ii) the
B↔ operator allows to swap property and subject, (iii) the
operator is approximately invertible, while we also can de-
ﬁne:

x = p
x = o

pB x, y (cid:39) s
pB y, y (cid:39) p

sB p, p (cid:39) s
sB y, p (cid:39) o

oB x,
oB p,
introducing projection (e.g., x = p
sB p simply states that the
subject x has a property p for some unknown object value,
and so on). Explicitly deriving these expressions is a per-
spective of the present preliminary work.

If we are able to explicitly compute the previous forms of
the B which exist, thanks to the stated algebraic properties,
it would allow us to translate, at least the three previous in-
ference rules at the numerical level. However, in this case, at
the implement level, we do not set a single value to accumu-
late all possible derived results. Given this precision, since:

x = p
we may implement

sB p, p = po

s B domain c

while x = po

x = p

s B domain c
s B type c, for any x and any c, we derive:

sB po

sB po
p

s B domain = po

s B type

and similarly:

s B type
while for the last inference rule, we obtain:

s B range = po

oB po
p

s B po
po
using the same kind of algebra.

s B subPropertyOf = po

s B,

Since our design choice leads to a multi-linear relation be-
tween the three statement elements, by construction we can
also easily deﬁne reiﬁcation, i.e., deﬁne the statement s it-
self, via a relation of the form:

s = spoB x p y
explicitized,

to be

which is
rdf:subject,
rdf:predicate, or rdf:object from the calculated
statement, by further algebraic combinations.

and then obtain the

4.3 Other intrinsic OWL properties
Beyond our objective of integrating RDFS axioms, it ap-
pears that the properties of the chosen algebraic VTB struc-
ture allows us to directly implement some of the OWL
features. We already mentioned in 4.2 the possibility of
inferring any predicate chaining, allowing to implement
OWL2 property chains (owl:ObjectPropertyChain,
i.e., property deﬁned by appending two other properties)
which generalizes this mechanism. Further OWL features
include:
• −c the complement of a class c (owl:complementOf)
• c1 + c2 the union of two classes c1 and c2,
this

deﬁnition may be extended to any number of classes
(owl:unionOf)

• the inverse p∼ of a property p (owl:inverseOf)
• if a property p is encoded as a vector p such that the ma-
trix Bp is symmetric, we can directly conclude that this
property is symmetric. Indeed, the transpose of a sym-
metric matrix is equal to itself; since the matrices are con-
sidered orthogonal, the transpose is also the approximate
inverse, therefore the approximate inverse of a symmetric
matrix is equal to itself (owl:SymmetricProperty)
Further investigating how other intrinsic OWL properties

can be integrated is a perspective of the present work.

5

Introducing data and data structure

5.1 Representing literals
A step further, we not only deﬁne relationships between en-
tities (i.e., “ObjectProperty”) but also relate an entity to a
literal (i.e., “DataProperty”), i.e., a quantitative or qualita-
tive value.

Regarding numerical values, it appears that we can easily

deﬁne multidimensional numerical values of the form:

v = ek1

2 + · · ·

1 + ek2
where ei is a identiﬁer representing the ki-th component and
eki
i stands for the i-th iterate of the binding operator (Komer
et al. 2019). This allows to deﬁne integer values, and even
more.

If the binding is performed using a convolution operator,
it is easily shown that this generalizes to complex numbers.6
In our case, we propose an alternative and consider only
“physical” numerical values r, i.e., bounded in [min, max]
and with a ﬁnite precision ε, which is practice, the case for
any physical value. We must thus write:

r def= k

ε

max − min + min, k ∈ {0 · · · (cid:98) max − min

ε

(cid:99)}

considering that two values v1, v2 with |v1 − v2| < ε are
indistinguishable. Such strong speciﬁcation is particularly
useful for numerical calculus (normalized estimations, spu-
rious value detection, estimation precision threshold, . . . )
and in our case, it allows to consider only the related pos-
itive integer value k.

Regarding string literals or qualitative values (e.g,.
Boolean “true” or “false” value), at this stage we simply pro-
pose to deﬁne one identiﬁer (i.e. a string literal and a random
unit vector) for each value.

5.2 Data container
In RFD and in any knowledge speciﬁcation, we need to de-
ﬁne data containers (i.e., rdfs:Container), mainly ei-
ther unordered (i.e., rdf:Bag or “set”) or ordered (i.e.,
rdf:Seq or “list”).

More precisely, RDF/RDFS provides several classes to

represent collections:
(i) rdf:Container, which includes the following sub-

classes:

6In a nutshell, because it can be related to a numerical exponen-

(i.i) rdf:Seq, which is an ordered container,
(i.ii) rdf:Bag, which is an unordered container,
(i.iii) rdf:Alt, which contains a set of “alternatives”, the
ﬁrst element of which being the default, and the other
elements constituting an unordered collection

(ii) rdf:List completed by:

• the properties rdf:first and rdf:rest
• the instance rdf:nil corresponding to the empty list
representation, ordered sets (rdf:List or
In our
rdf:Seq) could all be represented as lists in the sense of
RDF, in a recursive manner.7

Furthermore, unordered sets of value are obviously repre-

sented by simple addition, i.e.:

bag = x1 + x2 + · · ·
allowing to easily implement membership property, element
addition and deletion.

A step ahead, “Alternative” containers (i.e., rdf:Alt in
the RDFS sense) are simply implemented by a combination
of the two previous representations, where rdf:first
points to an addition. Similarly, we could deﬁne vectors like
indexed containers like in (Eliasmith 2013).

6 Effective neuronal implementation: an

illustrative example

In order to illustrate these developments and validate the
fact the proposed formalism is effectively compatible with
a biologically inspired approach, we have considered a very
small example of ontology (schematized in Fig. 2) inspired
by the so-called “pizza” tutorial ontology 8 (Horridge 2011).
To that end, we used the Nengo simulator (Bekolay et al.
2014) considering two associative memories as schematized
in Fig. 3. 9

The Nengo platform provides an effective implementation
of the Neural Engineering Framework (NEF) and the Se-
mantic Pointer Architecture (SPA). It enables building large-
scale bio-inspired models of spiking neurons by connecting
together reusable modular components (see (Bekolay et al.
2014) for more details). Therefore, instead of directly ma-
nipulating the neurons, we can implement our architecture

7This is done by storing the following associations:

l →B(rdf:List, rdf:type)
+ B(l1, rdf:first)
+ B(l(cid:48), rdf:rest)

l(cid:48) →B(rdf:List, rdf:type)
+ B(l2, rdf:first)
+ B(l(cid:48)(cid:48), rdf:rest)

l(n−1) →B(rdf:List, rdf:type)

+ B(ln, rdf:first)
+ B(rdf:nil, rdf:rest)

...

8https://github.com/owlcs/pizza-ontology
9The code is openly shared at https://gitlab.inria.fr/line/

tiation via a Fourier transform (Komer et al. 2019).

aide-group/onto2spa

using only those modular neural networks and the available
connecting operations. Among other features, it encom-
passes binding and unbinding operations deﬁned within the
VTB algebra, as well as associative memories which allow
us to store and recall patterns organized into SPA vocabu-
laries. Theoretical details underlying the implementation of
such associative memories are available in (Stewart, Tang,
and Eliasmith 2011).

To illustrate the use of these features with an example,
we created a Nengo vocabulary containing all the resources
of our ontology encoded as vectors, and we stored asserted
memberships and relationships between these resources into
a ﬁrst associative memory (AM1).
In a second associa-
tive memory (AM2), we stored the RDFS entailment rule
referred to in our paper as the class inheritance (rdfs9 de-
scribed in 4.1). Such an architecture could store more rules
based on predicate chaining but, for the sake of simplicity,
we restricted our example to this one only.

The idea is to query the knowledge base (stored into
AM1) against the entailment rules (stored into AM2) ac-
cording to some speciﬁc cues: our network can therefore be
seen as a question answering system, where the question to
answer is raised by these cues. For instance, in our example,
the system receives a subject cue (thisPizza) and a predicate
cue (type), which induce the question “what is the type of
thisPizza”. A third cue indicates which rule to use: in this
case, the class inheritance entailment (in practice, instead
of a third cue, the network could keep a buffer of several
rules to test successively). In order to visualize the differ-
ent steps of the data processing, we plotted the similarities
of the module outputs at several points of the architecture,
compared against the symbols in our vocabulary (Fig 4).

Figure 2: An example of a very simple ontology with two indi-
viduals, black arrows correspond to factual statements input in the
data base and red arrows to inferred statements. Rectangular boxes
stand for individuals, round boxes for classes and arrows are la-
beled by properties.

7 Discussion and conclusion
What has been presented here is a proof of concept of map-
ping between RDFS (with some OWL extensions) ontology
speciﬁcation and a biologically plausible numerical imple-
mentation based on VSA, with a preliminary partial numer-
ical experimentation. This mainly aims at opening new re-
search perspectives on the uniﬁcation of symbolic and nu-
merical approaches for cognitive modeling and Artiﬁcial In-
telligence.

Here the key point is to demonstrate that any symbolic
knowledge, as possibly expressed in languages such as
RDFS and OWL, can be also represented and manipulated
with a numerical neuronal formalism of associative mem-
ory. Regarding the genericity of this demonstration, it is
worth mentioning that (Mandler 2011) proposes that human
(neuronal) memory can represent three classes of structures
(in short, associations, sequences and relational structures),
the three of them being discussed here. This is especially
interesting for our team, since we study human learning us-
ing this kind of symbolic approaches (Mercier et al. 2021).
Therefore the biological plausibility of ontology represen-
tation and reasoning is a key issue. By plausibility, we in-
deed do not claim that this is directly coded “as is” in the
brain, but that what corresponds to symbolic processing in
the brain may be represented by such processing, exactly as
discussed by (Eliasmith 2013). Beyond that, this kind of rep-
resentation allows us to inject prior knowledge into learning
systems, and opens new perspectives on symbol emergence.
Of course, several alternative implementations of on-
tologies in VSA are possible. While we chose a non-
commutative, non-associative algebra to differentiate the
functions (subject, predicate, object) of the resources in a
statement, we could also achieve that in a commutative and
associative algebra such as HRR, thus using a slightly differ-
ent representation similar to what has been done in (Craw-
ford, Gingerich, and Eliasmith 2016) to represent the Word-
net database. 10 However, this formalism is quite heavy and
we would miss some of the interesting properties induced by
binding directly the predicate to the object.

This preliminary study is to be completed at different lev-
els. At a technical level, the tensorial generalization in sec-
tion 4.2 has been stated at an abstract level, and has yet
to be effectively implemented in order to target all RDFS
mechanisms. The fact that we rely on Description Logic
Programming (DLP) restrains the possibility to take into ac-
count the whole decidable part OWL2 speciﬁcation and we
aim to surpass this limit by further studying the intrinsic al-
gebraic properties of our representation. We will also, as
in human cognitive processes, consider “approximate” rea-

10In that case, the “work-around” consists in adding new vectors
predicate and object accounting for the functions in the triple,
as well as an index ti (represented by a vector ti) to each statement
and store the triples as:

s1 → τ1 t1 (cid:126) (p1 (cid:126) predicate + o1 (cid:126) object)

s1 → τ2 t2 (cid:126) (p2 (cid:126) predicate + o2 (cid:126) object)
+ τ3 t3 (cid:126) (p3 (cid:126) predicate + o3 (cid:126) object)

Figure 3: A proposition of architecture to implement our ontology and inference rules in a neuronal system, using the modules provided by
the Nengo simulator. A subject cue (thisPizza) and a predicate cue (type) prompt the question “what is the type of thisPizza”; a third cue
indicates the rule to use (class inheritance entailment). The network retrieves information in each associative memory and combines them to
infer that thisPizza is not only a MargheritaPizza but also, through class inheritance, a Pizza. (Data is in square boxes and their processing in
round boxes. Rectangular purple boxes account for associative retrievals. The circled numbers are markers locating which signals are plotted
on Fig. 4.)

Figure 4: The numerical simulation results. Each signal is visualized through its cosine similarity against the vocabulary. The cues guiding
the reasoning (under the form of square inputs) are plotted on the left, and processing steps are on the right (refer to Fig. 3 for the location
of each signal, marked by the circled numbers). We can observe the correctness of the inferred object and predicate (respectively PIZZA and
TYPE, plotted on 5 and 6) bound together to characterize the subject THIS PIZZA.

AM1 (knowledgebase)type ⊘ subClassOfthisPizzatypeB(margherita,type)margheritatypemargheritaB(pizza,subClassOf)pizzaB(pizza, type)B(margherita, type) +B(B(pizza,subClassOf),type)SUBJECTCUEPREDICATECUEUnbindingUnbindingINFERREDSTATEMENTWORKINGMEMORYBindingRULE CUEASSOCIATIVEMEMORIESB(B(pizza,subClassOf),type)BindingSuperposition312654AM1 (knowledgebase)AM2(entailment rules)soning about undecidable facts, by better considering the se-
mantic offered by the possibility/necessity score used here.
Finally, we made explicit how literal representation is easily
possible, but a lot of work is still to be done to have this part
fully operational.

We are also aware that, in our representation, the ontology
and inference rules are “hard-coded” within the associative
memories; the long-term perspective would be to learn those
associations. As new observations come along, we wish to
store the newly observed relationships between individuals.
A step further, logical inference as we proposed is a way to
learn new associations, both memberships and relationships,
within the ontology itself: for now, the newly inferred state-
ments are only kept into the working memory, but we could
store them for future use. Last but not least, we also wish
to learn the inference rules themselves, by spotting some
patterns that are repeated within the ontology. We presume
that this vector formalism will facilitate learning, as it is the
representation used by most learning algorithms, although
a concrete implementation for this feature is still an open
question. In order to remember inferred relationships, the
naive approach would be to change the connections in the
associative memory or even add new ensembles by hand.
However, the neuroscience and machine learning ﬁelds pro-
vide more sophisticated and biologically plausible solutions
to this problem. For example, (Voelker, Crawford, and Elia-
smith 2014) used a combination of unsupervised and super-
vised learning to learn new key-value associations in spiking
neurons online.

Acknowledgments. Margarida Romero is highly thanked
for powerful ideas regarding the use of this formalism to
model human learning which is at the origin of this technical
work. We also had a great pleasure discussing with Terrence
Stewart whom we deeply thank for his insightful feedback.

References

Alexandre, F. 2019. De quelles fac¸ons l’intelligence artiﬁ-
cielle se sert-elle des neurosciences ? The Conversation.
Allemang, D.; Hendler, J.; and Gandon, F. 2020. Semantic
Web for the Working Ontologist. ACM.

Ayadi, A.; Samet, A.; de Beuvron, F. d. B.; and Zanni-Merk,
C. 2019. Ontology population with deep learning-based
NLP: a case study on the Biomolecular Network Ontology.
Procedia Computer Science 159:572–581.

Bekolay, T.; Bergstra, J.; Hunsberger, E.; DeWolf, T.; Stew-
art, T. C.; Rasmussen, D.; Choo, X.; Voelker, A. R.; and
Eliasmith, C. 2014. Nengo: a Python tool for building large-
scale functional brain models. Front. Neuroinform. 7.

Chui, C. K., and Mhaskar, H. N. 2018. Deep Nets for Local
Manifold Learning. Front. Appl. Math. Stat. 4. Publisher:
Frontiers.

2017.
Cohen, W. W.; Yang, F.; and Mazaitis, K. R.
TensorLog: Deep Learning Meets Probabilistic DBs.
arXiv:1707.05390 [cs]. arXiv: 1707.05390.

Crawford, E.; Gingerich, M.; and Eliasmith, C. 2016. Bi-
ologically Plausible, Human-Scale Knowledge Representa-
tion. Cogn Sci 40(4):782–821.
Denœux, T.; Dubois, D.; and Prade, H. 2020. Represen-
tations of Uncertainty in AI: Probability and Possibility. In
Marquis, P.; Papini, O.; and Prade, H., eds., A Guided Tour
of Artiﬁcial Intelligence Research: Volume I: Knowledge
Representation, Reasoning and Learning. Cham: Springer
International Publishing. 69–117.
Eidoon, Z.; Yazdani, N.; and Oroumchian, F. 2008. On-
In Macdonald, C.;
tology Matching Using Vector Space.
Ounis, I.; Plachouras, V.; Ruthven, I.; and White, R. W.,
eds., Advances in Information Retrieval, Lecture Notes in
Computer Science, 472–481. Berlin, Heidelberg: Springer.
Eliasmith, C., and Anderson, C. H. 2002. Neural Engineer-
ing:Computation, Representation, and Dynamics in Neuro-
biological Systems. A Bradford Book. The MIT Press. Pub-
lisher: The MIT Press.
Eliasmith, C. 2013. How to Build a Brain: A Neural Ar-
chitecture for Biological Cognition. OUP USA. Google-
Books-ID: BK0YRJPmuzgC.
Garcez, A. d., and Lamb, L. C. 2020. Neurosymbolic AI:
The 3rd Wave. arXiv:2012.05876 [cs]. arXiv: 2012.05876.
Gayler, R. 2003. Vector Symbolic Architectures answer
Jackendoff’s challenges for cognitive neuroscience. In Fron-
tiers in Artiﬁcial Intelligence and Applications. Journal Ab-
breviation: ICCS/ASCS International Conference on Cog-
nitive Science Publication Title: ICCS/ASCS International
Conference on Cognitive Science.
Gosmann, J., and Eliasmith, C. 2019. Vector-Derived Trans-
formation Binding: An Improved Binding Operation for
Deep Symbol-Like Processing in Neural Networks. Neural
Computation 31(5):849–869. Publisher: MIT Press.
Grosof, B. N.; Horrocks, I.; Volz, R.; and Decker, S. 2003.
Description logic programs: combining logic programs with
description logic. In Proceedings of the 12th international
conference on World Wide Web, WWW ’03, 48–57. New
York, NY, USA: Association for Computing Machinery.
Guo, S.; Wang, Q.; Wang, L.; Wang, B.; and Guo, L. 2016.
Jointly Embedding Knowledge Graphs and Logical Rules.
In Proceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, 192–202. Austin,
Texas: Association for Computational Linguistics.
Hohenecker, P., and Lukasiewicz, T.
Reasoning with Deep Neural Networks.
1808.07980.
Horridge, M. 2011. A Practical Guide To Building OWL
Ontologies Using Prot´eg´e 4 and CO-ODE Tools Edition 1.3.
Jim´enez, A.; Elizalde, B.; and Raj, B. 2018. Sound event
classiﬁcation using ontology-based neural networks. In Pro-
ceedings of the Annual Conference on Neural Information
Processing Systems, 9.
Komer, B.; Stewart, T. C.; Voelker, A. R.; and Eliasmith,
C. 2019. A neural representation of continuous space using
fractional binding. In 41st Annual Meeting of the Cognitive

2020. Ontology
jair 68. arXiv:

Sauerwald, T., and Zanetti, L. 2019. Random Walks on
Dynamic Graphs: Mixing Times, HittingTimes, and Re-
turn Probabilities. arXiv:1903.01342 [cs, math].
arXiv:
1903.01342.
Schlegel, K.; Neubert, P.; and Protzel, P. 2020. A compar-
ison of Vector Symbolic Architectures. arXiv:2001.11797
[cs]. arXiv: 2001.11797.
Shi, S.; Chen, H.; Ma, W.; Mao, J.; Zhang, M.; and Zhang,
In Proceedings of
Y. 2020. Neural Logic Reasoning.
the 29th ACM International Conference on Information &
Knowledge Management, CIKM ’20, 1365–1374. New
York, NY, USA: Association for Computing Machinery.
Smith, L. 1994. The development of modal understanding:
Piaget’s possibility and necessity. New Ideas in Psychology
12(1):73–87.
Stewart, T. C.; Tang, Y.; and Eliasmith, C. 2011. A biolog-
ically realistic cleanup memory: Autoassociation in spiking
neurons. Cognitive Systems Research 12(2):84–92.
Sun, R., and Alexandre, F. 2013. Connectionist-Symbolic
Integration: From Uniﬁed to Hybrid Approaches. Taylor &
Francis Group, 3rd edition edition.
Tettamanzi, A.; Zucker, C. F.; and Gandon, F. 2017. Possi-
bilistic testing of OWL axioms against RDF data. Interna-
tional Journal of Approximate Reasoning.
Tous, R., and Delgado, J. 2006. A Vector Space Model
for Semantic Similarity Calculation and OWL Ontology
In Bressan, S.; K¨ung, J.; and Wagner, R.,
Alignment.
eds., Database and Expert Systems Applications, Lecture
Notes in Computer Science, 307–316. Berlin, Heidelberg:
Springer.
Voelker, A.; Crawford, E.; and Eliasmith, C. 2014. Learning
large-scale heteroassociative memories in spiking neurons.
Wang, M.; Qiu, L.; and Wang, X. 2021. A Survey on
Knowledge Graph Embeddings for Link Prediction. Sym-
metry 13(3):485. Number: 3 Publisher: Multidisciplinary
Digital Publishing Institute.
Xiao, H.; Huang, M.; and Zhu, X. 2015. From one point
to a manifold: Knowledge graph embedding for precise link
prediction. arXiv preprint arXiv:1512.04792.
Zhu, W.; Qiu, Q.; Huang, J.; Calderbank, R.; Sapiro, G.; and
Daubechies, I. 2017. LDMNet: Low Dimensional Mani-
fold Regularized Neural Networks. arXiv:1711.06246 [cs].
arXiv: 1711.06246 version: 1.

Science Society, 6. Montreal, Canada: Cognitive Science
Society.
Lallement, Y.; Hilario, M.; and Alexandre, F. 1995. Neu-
rosymbolic Integration: Cognitive Grounds and Computa-
tional Strategies. In Proceedings World Conference on the
Fundamentals of Artiﬁcial Intelligence, 12.
Levesque, H. J. 1986. Knowledge Representation and Rea-
soning. Annu. Rev. Comput. Sci. 1(1):255–287. Publisher:
Annual Reviews.
Levy, S. D., and Gayler, R. 2008. Vector Symbolic Ar-
chitectures: A New Building Material for Artiﬁcial General
Intelligence. In Frontiers in Artiﬁcial Intelligence and Ap-
plications, 6.
Mandler, G. 2011. From Association to Organization. Curr
Dir Psychol Sci 20(4):232–235. Publisher: SAGE Publica-
tions Inc.
McClelland, J. L., and Rogers, T. T. 2003. The parallel
distributed processing approach to semantic cognition. Nat
Rev Neurosci 4(4):310–322.
Mercier, C.; Roux, L.; Romero, M.; Alexandre, F.; and
Vi´eville, T. 2021. Formalizing Problem Solving in Com-
In 2021
putational Thinking : an Ontology approach.
IEEE International Conference on Development and Learn-
ing (ICDL), 1–8.
Petrucci, G.; Ghidini, C.; and Rospocher, M. 2016. Ontol-
ogy Learning in the Deep. In Blomqvist, E.; Ciancarini, P.;
Poggi, F.; and Vitali, F., eds., Knowledge Engineering and
Knowledge Management, Lecture Notes in Computer Sci-
ence, 480–495. Cham: Springer International Publishing.
Phan, N.; Dou, D.; Wang, H.; Kil, D.; and Piniewski, B.
2017. Ontology-based deep learning for human behavior
prediction with explanations in health social networks. In-
formation Sciences 384:298–313.
Plate, T. 1995. Holographic reduced representations. IEEE
Trans. Neural Netw. 6(3):623–641.
Pulverm¨uller, F. 2013. How neurons make meaning: brain
mechanisms for embodied and abstract-symbolic semantics.
Trends in Cognitive Sciences 17(9):458–470.
Riegel, R.; Gray, A.; Luus, F.; Khan, N.; Makondo, N.;
Akhalwaya, I. Y.; Qian, H.; Fagin, R.; Barahona, F.;
Sharma, U.; Ikbal, S.; Karanam, H.; Neelam, S.; Likhyani,
A.; and Srivastava, S. 2020. Logical Neural Networks.
arXiv:2006.13155 [cs]. arXiv: 2006.13155.
Romero, M.; David, D.; and Lille, B. 2019. CreaCube,
a Playful Activity with Modular Robotics. In Gentile, M.;
Allegra, M.; and S¨obke, H., eds., Games and Learning Al-
liance, volume 11385. Cham: Springer International Pub-
lishing. 397–405. Series Title: Lecture Notes in Computer
Science.
Rusawuk, A. L. 2018. Possibility and Necessity: An Intro-
duction to Modality.
Sajjad, H. P.; Docherty, A.; and Tyshetskiy, Y. 2019. Ef-
ﬁcient Representation Learning Using Random Walks for
Dynamic Graphs.
arXiv:
1901.01346.

arXiv:1901.01346 [cs, stat].

