A Simple Reservoir Model of Working Memory with
Real Values
Anthony Strock, Nicolas P. Rougier, Xavier Hinaut

To cite this version:

Anthony Strock, Nicolas P. Rougier, Xavier Hinaut. A Simple Reservoir Model of Working Mem-
ory with Real Values. Third workshop on advanced methods in theoretical neuroscience, Jun 2018,
Göttingen, Germany. ￿hal-01861784￿

HAL Id: hal-01861784

https://inria.hal.science/hal-01861784

Submitted on 25 Aug 2018

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

A Simple Reservoir Model of Working Memory with Real Values

MNEMOSYNE Team

first-name.last-name@inria.fr

Anthony Strock2,1,3, Nicolas Rougier1,2,3, Xavier Hinaut1,2,3

1. Inria Bordeaux Sud-Ouest, Talence, France.
2. LaBRI, UMR 5800, CNRS, Bordeaux INP, Université de Bordeaux, Talence, France.
3. Institut des Maladies Neurodégénératives, UMR 5293, CNRS, Université de Bordeaux, Bordeaux, France.

Abstract
Prefrontal cortex is known to be involved in many high-level cognitive 
functions, in particular working memory. Here, we study to what extent 
a group of randomly connected units can store and maintain (as output) 
an arbitrary real value from a streamed input, i.e. how such system act 
as a sustained working memory module without being distracted by the 
input stream. 

Furthermore, we explore to what extent such an architecture can take 
advantage of the stored value in order to produce non-linear 
computations. Systematic comparison between different architectures 
(with and without feedback, with and without a working memory unit) 
shows that explicit memory is required.

With Principal Component Analyses (PCA) we show that the reservoir 
state is encoding time and the memorized value in different ways 
depending if a supplementary task is required. Moreover, theses 
memory states are similar to attractors in an input-driven system [3], and 
in particular, similar to a noisy line attractor [6].

In this study, we did not try to find the optimal number of reservoir units 
needed for each task. Conversely, we voluntary limited the size of the 
reservoir to 100 neurons in order to see if such rather small reservoirs 
were sufficiently competitive.

Materials & Methods
Echo State Networks [1]
Update equation of the reservoir (recurrent layer) and 
the readout (output layer):

Matrices Win and W are randomly generated. 

Training of the output weights with ridge regression

Results: Model comparison

Main results for task 2 (RMSE)

No WM unit: 
With WM unit:

3.03e-1
7.26e-4

Detailed results for tasks 1 & 2

Task 1: store value sync. with last trigger

Task 2: scale input with last trigger sync. value

Materials & Methods

Value

V

Trigger

T

ﬁxed
plastic

Working
Memory

M

Value

V

Trigger

T

ﬁxed
plastic

Working
Memory

M

Global Results

Testing the simple architecture for untrained scenarios

Need for WM unit (see model comparison panel for details)

(Left) PCs explaining most variance. (Right) PCs most correlated with WM unit.

(Left) PCs explaining most variance. (Right) PCs most correlated with WM unit.

PCA analyses

Discussion

We have shown how a small group of randomly connected units is able 
to maintain an arbitrary value at an arbitrary time from a streamed 
input. It is to be noted that the model has not been trained at 
memorizing every possible value since there is virtually an infinite 
number of values between -1 and +1. What the model has actually 
learned is to gate an input value into a placeholder, a.k.a. a working 
memory (WM) unit. After training, this WM unit act like a gated 
memory: information enters while the gate is opened and is kept 
constant once the gate is closed. It can then be used to solve more 
complex tasks.

We have also demonstrated how such explicit working memory is 
critical in solving a scaling (i.e. multiplication) task. The same model 
deprived of the explicit working memory fails at solving the task and 
exhibits very bad performances (error of 3.03e-1 instead of 7.26e-4). 
This demonstrates the criticality of the presence of the working memory 
unit.

Future work will investigate similar mechanisms when online learning 
algorithms are used (e.g. reward-modulated rules [4] and FORCE 
learning variants [6, 7]), and investigate how temporal patterns such as 
conceptors [2] could be stored in WM units in a similar way.

xxxxx

References
[1] H. Jaeger, “The “echo state” approach to analysing and training recurrent neural networks” Bonn, 
Germany: German National Research Center for Information Technology GMD Technical Report, vol. 148, 
p. 34, 2001.
[2] H. Jaeger. Controlling recurrent neural networks by conceptors. arXiv preprint arXiv:1403.3369, 2014.
[3] R Pascanu and H Jaeger. A neurodynamical model for working memory. Neural networks, 24(2):199–
207, 2011.
[4] G M Hoerzer, R Legenstein, and W Maass. Emergence of complex computational structures from 
chaotic neural networks through reward-modulated hebbian learning. Cerebral cortex, 24(3):677–690, 
2012.
[5] Barak, O., & Tsodyks, M. (2014). Working models of working memory. Current opinion in neurobiology, 
25, 20-24.
[6] Beer, C., & Barak, O. (2018). Dynamics of dynamics: following the formation of a line attractor. arXiv
preprint arXiv:1805.09603.
[7] Sussillo, D., & Abbott, L. F. (2009). Generating coherent patterns of activity from chaotic neural 
networks. Neuron, 63(4), 544-557.
[8] J. Bergstra, D. Yamins, and D. D. Cox, “Hyperopt: A python library for optimizing the hyperparameters
of machine learning algorithms,” in Proceedings of the 12th Python in Science Conference, pp. 13–20. 
2013.
[9] P S Goldman-Rakic. Circuitry of primate prefrontal cortex and regulation of behavior by 
representational memory. Comprehensive Physiology, 1987.
[10] M Rigotti, O Barak, M R Warden, X-J Wang, N D Daw, E K Miller, and S Fusi. The importance of mixed 
selectivity in complex cognitive tasks. Nature, 497(7451):
585, 2013.
[11] H Jaeger. Long short-term memory in echo state networks: Details of a simulation study. Technical 
report, Jacobs University Bremen, 2012.
[12] J Dambre, D Verstraeten, B Schrauwen, and S Massar. Information processing capacity of dynamical 
systems. Scientific reports, 2:514, 2012.
[13] P F Dominey. Complex sensory-motor sequence learning based on recurrent state representation and 
reinforcement learning. Biological cybernetics, 73(3):265–274, 1995.

Links
Paper: 

https://hal.inria.fr/hal-01803594v1

Source code: 

https://github.com/anthony-strock/ijcnn2018

Related paper
Anthony Strock, Nicolas Rougier, Xavier Hinaut. A Simple Reservoir Model 
of Working Memory with Real Values. IJCNN 2018 - International Joint 
Conference on Neural Networks, Jul 2018, Rio de Janeiro, Brazil. 
〈hal-01803594〉

