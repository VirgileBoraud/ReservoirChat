Speech Taskonomy: Which Speech Tasks are the most
Predictive of fMRI Brain Activity?
Subba Reddy Oota, Veeral Agarwal, Mounika Marreddy, Manish Gupta, Raju

Surampudi Bapi

To cite this version:

Subba Reddy Oota, Veeral Agarwal, Mounika Marreddy, Manish Gupta, Raju Surampudi Bapi.
Speech Taskonomy: Which Speech Tasks are the most Predictive of fMRI Brain Activity?.
IN-
TERSPEECH 2023 - 24th INTERSPEECH Conference, Aug 2023, Dublin, Ireland. pp.5167–5171,
￿10.21437/Interspeech.2023-121￿. ￿hal-04131475￿

HAL Id: hal-04131475

https://hal.science/hal-04131475

Submitted on 16 Jun 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Speech Taskonomy: Which Speech Tasks are the most Predictive of fMRI
Brain Activity?

Subba Reddy Oota1∗, Veeral Agarwal2∗, Mounika Marreddy2, Manish Gupta2,3, Raju Bapi2

1Inria Bordeaux, France, 2IIIT Hyderabad, India, 3Microsoft, India
subba-reddy.oota@inria.fr,{veeral.agarwal,mounika.marreddy}@research.iiit.ac.in
gmanish@microsoft.com,raju.bapi@iiit.ac.in

Abstract

Self-supervised speech based models have been found to be suc-
cessful in predicting brain recordings of subjects experiencing
naturalistic story listening. Inspired by the recent progress on
deep learning models for various speech-processing tasks, exist-
ing literature has leveraged pretrained speech Transformer mod-
els for brain encoding. However, there is no work on exploring
the efficacy of task-specific finetuned Transformer representa-
tions for this task. Hence, in this paper, we explore transfer
learning from representations finetuned for eight different tasks
from Speech processing Universal PERformance Benchmark
(SUPERB) for predicting brain responses. Encoding models
based on task features are used to predict activity in different re-
gions across the whole brain, and also in language and auditory
brain regions. Our experiments on finetuning the Wav2Vec2.0
model for these eight tasks show that the model finetuned on au-
tomatic speech recognition (ASR) yields the best encoding per-
formance for the whole brain, language and auditory regions.
Index Terms: speech recognition, human-computer interac-
tion, computational paralinguistics

1. Introduction

In computational cognitive science, brain encoding is the prob-
lem of predicting brain activations from stimuli. For the past
two decades, researchers have focused on mapping stimulus
representations to brain activations through encoding models
for text and vision. For text, researchers have explored both
syntactic as well as semantic representations,
the most re-
cent ones using Transformer-based deep learning (DL) meth-
ods [1, 2, 3, 4]. For vision, the encoding models have leveraged
our algorithmic understanding of visual hierarchy (V1, V2, V4,
IT) in the visual cortex and hence follow the convolutional neu-
ral network-based design [5, 6]. Recent advancements in deep
learning models for speech [7, 8, 9, 10] have motivated neuro-
science researchers to leverage them for auditory brain encod-
ing [11, 12, 13, 14] starting early 2022.

Millet et al. [11] used a pretrained deep learning model
Wav2Vec2.0 [8] to learn latent representations of the speech
waveform similar to those of the human brain. They find that
the functional hierarchy of its transformer layers aligns with the
cortical hierarchy of speech in the brain, and reveals the whole-
brain organisation of speech processing. Similarly, Vaidya et
al. [12] experimented with four pretrained speech representa-
tion methods (APC [7], Wav2Vec [15], Wav2Vec2.0 [8], and
HuBERT [9]) and found that Wav2Vec2.0 aligns best with the
human auditory system. Further, Oota et al. [14] and Tuckute et
al. [13] extended this analysis to more such deep learning based

The first two authors made equal contribution.

speech models. However, no existing work has investigated the
implications of finetuning speech pretrained models for various
speech-processing tasks for speech encoding in the brain.

Although pretrained speech models can understand broad
aspects of speech in general, their representations are not specif-
ically tuned towards capturing distinctive characteristics of
speech tasks. When participants are listening to speech narra-
tives, it is plausible that they are engaged in various speech tasks
such as phoneme and speech recognition, emotion recognition,
comprehending the intent, keyword identification, attending to
speaker characteristics, etc. Thus we expect that the brain ac-
tivation recorded during narrative listening would be related to
various speech tasks. Hence, toward designing a better brain
speech-processing pipeline when subjects listened to naturalis-
tic stories, in this paper, we finetune a pretrained speech model
(Wav2Vec2.0) on eight different speech-processing tasks. Our
goal in this paper is to find which of these eight finetuned mod-
els best captures distinctive characteristics of story listening and
hence leads to the best encoding accuracy.

Indeed explorations around which finetuned models lead to
better encoding accuracy compared to pretrained ones, have al-
ready been done rigorously for text and vision. For text, sev-
eral researchers [3, 16, 17, 18, 19] finetuned a pretrained lan-
guage model (BERT [20]) for multiple tasks from General Lan-
guage Understanding Evaluation (GLUE) benchmark [21], and
found that using a finetuned BERT leads to improved brain pre-
dictions. For visual stimuli, Wang et al. [22] finetuned a pre-
trained vision model (ResNet50 [23]) for multiple 2D and 3D
computer vision tasks. They found that models finetuned for
3D vision tasks lead to better encoding accuracy compared to
pretrained models alone.
Inspired by the success of finetun-
ing in the language and vision fields, we build neural speech
taskonomy models for brain encoding and aim to find speech-
processing tasks that have the most explanatory capability of
brain activation during naturalistic story listening experiments.
Wav2Vec2.0 [8] is a popular state-of-the-art model for sev-
eral speech-processing tasks. The robust pretraining helped
the model to achieve the state-of-the-art word error rate on the
benchmark LibriSpeech speech-to-text dataset requiring fine-
tuning with just one hour worth of labeled data [8]. Hence, we
experiment with Wav2Vec2.0 as our pretrained model. Further,
Speech processing Universal PERformance Benchmark (SU-
PERB) [24] is a collection with a wide range of speech pro-
cessing tasks that captures different abilities of human speech
processing related to how humans produce, perceive, and un-
derstand speech. Therefore, in the hope of discovering a task
that best captures distinctive characteristics of story listen-
ing, we finetune Wav2Vec2.0 on the following eight SUPERB
tasks, and evaluate their brain encoding performance: Phoneme
Recognition (PR), Automatic Speech Recognition (ASR), Key-

word Spotting (KS), Intent Classification (IC), Speaker Diariza-
tion (SD), Speaker Verification (SV), Speaker Identification
(SID), and Emotion Recognition (ER). We chose these eight
tasks of the thirteen available in SUPERB as these would benefit
from finetuning of pretrained models and are also more relevant
for the speech encoding models we plan to investigate.

Overall, we make the following contributions in this paper.

• Given Transformer models finetuned for eight speech tasks,
we propose the problem of finding which of these are the
most predictive of fMRI brain activity for story listening.
• We show that task-specific (ASR, ER, SID and IC) speech
representations lead to a significant improvement in brain
alignment compared to the pretrained Wav2Vec2.0 model for
specific brain regions. Finetuning on ER, SID and IC leads to
the best alignment for the early auditory cortex; finetuning on
ASR provides the best encoding for the auditory associative
cortex and language regions.

• Layer-wise analysis of the effect of each speech task on the
alignment with whole brain activity shows that (a) the ASR
task is better aligned in middle layers, and (b) performance
degrades drastically for later layers, specifically for SD and
PR tasks.

2. Task Descriptions

We experiment with the eight SUPERB tasks briefly described
as follows. Phoneme Recognition (PR) transcribes an ut-
terance into the smallest content units. Automatic Speech
Recognition (ASR) transcribes utterances into words. Key-
word Spotting (KS) detects preregistered keywords by clas-
sifying utterances into a predefined set of words. Intent Clas-
sification (IC) classifies utterances into predefined classes to
determine the intent of speakers. Speaker Diarization (SD)
classifies each utterance for its speaker identity as a multi-class
classification, where speakers are in the same predefined set for
both training and testing. Speaker Verification (SV) verifies
whether the speakers of a pair of utterances match as a binary
classification, and speakers in the testing set may not appear in
the training set. Speaker Identification (SID) predicts who is
speaking when for each timestamp, and multiple speakers can
speak simultaneously. Emotion Recognition (ER) predicts an
emotion class for each utterance. These tasks check for per-
formance across several cognitive speech perception skills like
recognition (PR and ASR), detection (KS), semantics (IC, SF,
and ST), speaker-related (SV, SD, and SID), and paralinguistics
(ER). Several speech tasks, such as Query by Example (QbE),
Slot Filling (SF), Speech Enhancement (SE), and Speech Sep-
aration (SS), do not require fine-tuning. Hence, we have not
chosen these models for our study.

Our selection of these tasks was based on the following de-
sign principles: (1) We wanted to select a set of tasks covering
diverse cognitive speech perception skills. (2) We wanted to se-
lect tasks that are a part of popular speech benchmark like SU-
PERB [21]. (3) We selected tasks for which Wav2vec2.0-base
finetuned models were available.

3. Dataset and Experiments

The “Narratives” collection aggregates a variety of fMRI
datasets collected while human subjects listened to real spo-
ken stories [25]. We analyze data from 82 subjects listening to
the story titled ‘PieMan’ with 282 TRs (repetition time – fMRI
recorded every 1.5 sec.). The dataset is in English and contains

957 words across 67 sentences. The dataset was already prepro-
cessed and projected on the surface space (“fsaverage6”).

We use the multi-modal parcellation of the human cerebral
cortex (Glasser Atlas: consists of 180 ROIs (regions of interest)
in each hemisphere) to display the brain maps [26], since the
Narratives dataset contains annotations tied to this atlas. The
data covers both auditory and language brain ROIs with the fol-
lowing subdivisions: (i) early auditory cortex (EAC: A1, A2,
LBelt, MBelt, PBelt); (ii) auditory association cortex (AAC:
STSda, STSva, STSdp, STGa, TE1a, TE2a); and (iii) inferior
frontal gyrus (IFG: 44, 45, IFJa, IFSp) [27, 28, 29]. The dataset
has been made available freely without restrictions by [25].

The input audio story is first segmented into clips corre-
sponding to 1 TR. Each audio clip is input to the speech mod-
els one by one to obtain stimulus representations per clip. The
representations are obtained by probing the pretrained speech
model and taking the output from different encoder layers.
For all the models, we used the checkpoints provided by the
huggingface library. Since we extracted the features at each
TR, downsampling is not required for further sampling the fre-
quency of each feature dimension.

BOLD (Blood oxygenation level-dependent) fMRIs mea-
sure brain activity by detecting changes associated with blood
flow. When an area of the brain is in use, blood flow to that
region also increases. It takes a while for the vascular system to
respond to the brain’s need for glucose. Thus, blood flow lags
the neuronal events triggering by a few seconds. This hemo-
dynamic response is typically modeled using a finite response
filter (FIR) per voxel. We model this for each subject sepa-
rately with eight temporal delays corresponding to around 12
secs. This means that we predict the brain activations at the tth
time point based on the concatenation of speech representation
for audio clips corresponding to (t − 8)th to tth time point.

4. Methodology

Voxelwise Encoding Model The main goal of each fMRI en-
coder model is to predict brain responses associated with each
brain voxel given a stimuli. To explore how speech signals
are encoded in the brain when listening to stories, we use lay-
erwise pretrained Wav2Vec2.0 features in a voxelwise encod-
ing model to predict brain responses. We train fMRI encoding
models using Banded ridge regression [30] on stimuli repre-
sentations from various feature spaces. Before doing regres-
sion, we first z-scored each feature channel separately for train-
ing and testing. This was done to match the features to the
fMRI responses, which were also z-scored for training and test-
ing. The solution to the banded regression approach is given
by f ( ˆβ) = argmin
F , where Y denotes

F + λ∥β∥2

∥Y − Xβ∥2

β

the voxels matrix across TRs, β denotes the learned regression
coefficients, and X denotes stimulus representations.
Cross-Validation We follow 4-fold (K=4) cross-validation. All
the data samples from K-1 folds were used for training, and the
model was tested on samples of the left-out fold.
Evaluation Metric: We evaluate our models using popular
brain encoding evaluation metric, Pearson Correlation (PC), as
described in the following. Given a subject and a brain region,
i=1 and { ˆYi}N
let N be the number of samples. Let {Yi}N
i=1
denote the actual and predicted voxel value vectors for the ith
sample. Thus, Y ∈ RN ×V and ˆY ∈ RN ×V where V is the
number of voxels in that region. Let { ˆEi}N
i=1 denote the stim-
uli representation for the ith sample. Thus, E ∈ RN ×D where
D is the dimensionality of the encoded representation. PC is

Figure 1: Brain alignment of pretrained Wav2Vec2.0 (blue) and different finetuned tasks. The left plot compares the average Pearson
correlation across all layers of pretrained Wav2Vec2.0 and all voxels, and the same quantity for each downstream task. Error bars
indicate the standard error of the mean across participants and ‘\*’ indicates that the particular task PC is significantly higher than
pretrained Wav2Vec2.0. The right plot compares the layer-wise performance of pretrained Wav2Vec2.0 and the finetuned speech tasks.

Figure 2: Voxel-wise correlation values for the brain alignment
of pretrained Wav2Vec2.0 and ASR task across all the layers for
the ASR finetuned model.

(cid:80)N

i=1 corr[Yi, ˆYi] where corr is the

then computed as PC= 1
N
correlation function.
Statistical Significance To estimate the statistical signifi-
cance of the performance differences, we performed two-tailed
paired-sample t-tests on the mean correlation values for the
subjects. Further,
the Benjamni-Hochberg False Discovery
Rate (FDR) correction [31] is used for all tests (appropri-
ate because fMRI data is considered to have positive depen-
dence [32]). The correction is performed by grouping all the
subject-level p-values (i.e., across each speech task and pre-
trained Wav2Vec2.0) and choosing one threshold for all results.
Implementation Details for Reproducibility All experiments
were conducted on a machine with 1 NVIDIA GEFORCE-GTX
GPU with 16GB GPU RAM. We used banded ridge-regression
with the following parameters: MSE loss function, and L2-
decay (λ) varied from 10−1 to 10−3; best λ was chosen by
tuning on validation data.

5. Results

In order to assess the performance of the fMRI encoder models
learned using the representations from a variety of speech tasks,
we computed the Pearson correlation coefficient between the

Figure 3: Brain alignment of pretrained Wav2Vec2.0 (blue)
and different finetuned tasks. Plot compares the average Pear-
son correlation across all layers of pretrained Wav2Vec2.0 and
brain ROIs. Error bars indicate the standard error of the mean
across participants. A ‘\*’ at a particular bar indicates that the
task is significantly better than pretrained Wav2Vec2.0, whereas
∧ denotes that the task performance is significantly lower.

predicted and true responses across whole brain voxels, various
auditory and language ROIs, and sub-ROIs.
Whole Brain Results In Fig. 1 (left), we present the aver-
age brain alignment across all layers of pretrained Wav2Vec2.0
and for each speech task. In comparison to Wav2Vec2.0, ASR
shows improvement in correlation, whereas tasks including ER,
SID and IC report similar correlation performance. Further,
tasks such as SD and SV yield lower correlation, which sug-
gests that speaker diarization and verification are not important
in listening to stories. This result suggests that there are cer-
tain speech tasks that are important for improved brain align-
ment over pretrained Wav2Vec2.0. The effect of the two-tailed
test was significant for the tasks with pretrained Wav2Vec2.0,
p-values are as follows: ASR (0.0287), ER (0.0341), SID
(0.0341), and IC (0.0341). For the remaining tasks, the p-
values with pretrained Wav2Vec2.0 as follows: PR (0.893), KS
(0.862), SV (0.944), and SD (0.944). Overall, ASR, ER, SID
and IC are statistically significant across all speech tasks with
pretrained Wav2Vec2.0.

In Fig. 1 (right), we also report the layer-wise performance
for Wav2Vec2.0 and all speech tasks. Similar to previous
work [11], we observe that pretrained Wav2Vec2.0 has the best
brain alignment in the early and in later layers. On the other

\*\*\*\*Wav2Vec2.0PRASRKSERSVSDSIDIC0.060.080.1Average across all the layersAvg Pearson Correlation1234567891011120.080.09Wav2Vec2.0PRASRKSERSDLayer DepthAvg Pearson CorrelationASRER PR SD \*\*\*\*\*\*^^^\*\*^\*\*^EACAACIFG0.10.150.20.250.30.350.40.45Wav2Vec2.0PRASRKSERSVSDSIDICAvg Pearson CorrelationFigure 4: Brain alignment of pretrained Wav2Vec2.0 (blue) and different finetuned tasks. Plot compares the average Pearson correlation
across all layers of pretrained Wav2Vec2.0 and sub-ROIS. The error bars indicate the standard error of the mean across participants.

hand, the ASR task has the best brain alignment in the middle
layers. We further observe that the alignment after finetuning on
a speech task is significantly worse, mainly for later layers. This
pattern holds across speech tasks, including PR and SD. These
results provide direct evidence that the later layers of finetuned
task representations have more task-specific information. Since
phoneme parsing is an early processing step in speech under-
standing, as expected, the early layers of PR (phoneme recogni-
tion) finetuned model are well aligned with brain activity. Over-
all, these results demonstrate that when listening to a story, in-
formation processing operations related to recognizing words
(ASR), intent present in the sentence (IC), the emotion of the
story (ER), and speaker identification (SID) processing may be
engaged.

Fig. 2 presents the voxel-wise correlation values for the
brain alignment of pretrained Wav2Vec2.0 and each speech task
across all the layers for the ASR task. Low correlations in some
regions indicate that finetuning changes predictions for those re-
gions. We observe that the correlation is high in temporal lobes
but not in language regions and parietal regions. Thus, ASR
leads to better language understanding compared to pretrained
Wav2Vec2.0. Perhaps that is why, like language models, the
ASR model also has the best performance for middle layers [2].

ROI Level Results We further examine the effect on the align-
ment specifically in a set of ROIs that are thought to underlie
speech and language comprehension, as shown in Fig. 3. We
make the following observations: (1) All speech tasks are better
aligned with EAC compared to AAC and IFG regions. (2) ASR
task has a higher brain Pearson correlation than other tasks in
AAC and IFG ROIs (3) Since ASR task handles both phoneme
and word recognition, we observe that ASR task is responsible
for the better brain alignment in language ROIs, AAC and IFG.
On the other hand, the KS task is based on keyword extraction;
hence, IFG has a better correlation, whereas EAC has a lower
correlation. Similarly, PR is better aligned with EAC which is
responsible for the processing of phonemes and sounds. (4) ER,
SID, IC and pretrained Wav2Vec2.0 showed similar correlation
for whole brain (Fig. 1). But Fig. 3 shows that these sub-regions
have a higher correlation compared to Wav2Vec2.0 in the EAC
region. (5) Finally, across all ROIs, the pretrained Wav2Vec2.0
model has a worse correlation compared to at least five other
speech task models.

Sub-ROI Level Results We further demonstrate the prediction
performance of the encoder model for sub-ROIs across speech

tasks, as shown in Fig. 4. It can be observed that the sub-ROI of
early auditory cortex (EAC: A1) has a higher Pearson correla-
tion than other sub-ROIs. On the other hand, sub-ROIs A2 from
AAC, 44 and 45 from IFG display similar correlation perfor-
mance. However, the sub-ROIs in the AAC (STGa and STSdp)
yield a lower correlation. The language ROIs 44 and 45, to-
gether with STSda and STSdp in the AAC, are part of the well-
known language network associated with narrative comprehen-
sion [33], and it is heartening to see that task features from ASR
task show the best correlation in these regions.

6. Discussion and Conclusion

We evaluated the processing of finetuned representations of
eight speech tasks and their alignment with brain responses. We
showed that the representations of multiple speech task fine-
tuned models (ASR, PR, ER, SID and IC) compared to pre-
trained Wav2Vec2.0 lead to a significant increase in brain align-
ment across auditory regions. To understand the contribution
of each speech task to the brain alignment better, we performed
multiple analyses: layer-wise correlations, and analyses at ROI
and sub-ROI levels. We find that ASR is better aligned than
Wav2Vec2.0 in all analyses – whole brain, ROI and sub-ROI
level. Our results (Figs. 3 and 4) also seem to support that ASR
best captures activity in both auditory and language regions.

Recently, Shah et al. [34] used 43 probing tasks to un-
cover the parts of the two popular self-supervised speech mod-
els (Mockingjay and Wav2Vec2.0) that encode specific speech
properties like (i) audio features: total duration, local jitter, (ii)
fluency features: filled pause rate, mean silence, (iii) pronun-
ciation features: stressed syllable percent, mean stress distance
syllable, and (iv) semantic level text features: total nouns and
total adjectives. These techniques have revealed a hierarchy of
information processing in multi-layered speech models that pro-
gresses from simple to complex with increased depth. To better
understand the reasons for the better alignment between task-
specific speech models and the brain, future work can focus
on investigating the correspondence between the detailed pro-
cessing of underlying speech properties by the human brain vs.
self-supervised speech models, similar to alignment between
linguistic properties of language models and human brains [35].
This work was done on native English speakers’ data re-
lated to English stories only. More work needs to be done to
verify which of these insights hold for datasets in other lan-
guages, and for non-native English speakers.

A1A2STGaSTSdaSTSdp44450.10.20.30.40.5Wav2Vec2.0PRASRKSERSVSDSIDICAvg Pearson CorrelationER7. References
[1] J. Gauthier and R. Levy, “Linking artificial and human neural rep-

resentations of language,” arXiv:1910.01244, 2019.

[2] M. Toneva and L. Wehbe, “Interpreting and improving natural-
language-

language processing (in machines) with natural
processing (in the brain),” arXiv:1905.11833, 2019.

[3] D. Schwartz, M. Toneva, and L. Wehbe, “Inducing brain-relevant
bias in natural language processing models,” NeurIPS, vol. 32, pp.
14 123–14 133, 2019.

[4] S. R. Oota, F. Alexandre, and X. Hinaut, “Long-term plausibil-
ity of language models and neural dynamics during narrative lis-
tening,” in Proceedings of the Annual Meeting of the Cognitive
Science Society, vol. 44, no. 44, 2022.

[5] R. Beliy, G. Gaziv, A. Hoogi, F. Strappini, T. Golan, and M. Irani,
“From voxels to pixels and back: Self-supervision in natural-
image reconstruction from fmri,” arXiv:1907.02431, 2019.

[6] C. Du, C. Du, L. Huang, and H. He, “Conditional generative neu-
ral decoding with structured cnn feature prediction,” in Proceed-
ings of the AAAI Conference on Artificial Intelligence, vol. 34,
no. 03, 2020, pp. 2629–2636.

[7] Y.-A. Chung, H. Tang, and J. Glass, “Vector-quantized autoregres-
sive predictive coding,” Interspeech, pp. 3760–3764, 2020.

[8] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:
A framework for self-supervised learning of speech representa-
tions,” NeurIPS, vol. 33, pp. 12 449–12 460, 2020.

[9] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhut-
dinov, and A. Mohamed, “Hubert: Self-supervised speech repre-
sentation learning by masked prediction of hidden units,” TASLP,
vol. 29, pp. 3451–3460, 2021.

[10] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli,
“Data2vec: A general framework for self-supervised learning in
speech, vision and language,” arXiv:2202.03555, 2022.

[11] J. Millet, C. Caucheteux, P. Orhan, Y. Boubenec, A. Gramfort,
E. Dunbar, C. Pallier, and J.-R. King, “Toward a realistic model
of speech processing in the brain with self-supervised learning,”
arXiv:2206.01685, 2022.

[12] A. R. Vaidya, S. Jain, and A. G. Huth, “Self-supervised models
of audio effectively explain human cortical responses to speech,”
arXiv preprint arXiv:2205.14252, 2022.

[13] G. Tuckute, J. Feather, D. Boebinger, and J. H. McDermott,
“Many but not all deep neural network audio models capture
brain responses and exhibit hierarchical region correspondence,”
bioRxiv, pp. 2022–09, 2022.

[14] S. R. Oota, K. Pahwa, M. Marreddy, M. Gupta, and B. S. Raju,
“Neural architecture of speech,” in ICASSP 2023-2023 IEEE In-
ternational Conference on Acoustics, Speech and Signal Process-
ing (ICASSP).

IEEE, 2023, pp. 1–5.

[15] S. Schneider, A. Baevski, R. Collobert, and M. Auli, “wav2vec:
Unsupervised pre-training for speech recognition,” Proc. Inter-
speech 2019, pp. 3465–3469, 2019.

[16] S. Kumar, T. R. Sumers, T. Yamakoshi, A. Goldstein, U. Hasson,
K. A. Norman, T. L. Griffiths, R. D. Hawkins, and S. A. Nastase,
“Reconstructing the cascade of language processing in the brain
using the internal computations of a transformer-based language
model,” BioRxiv, pp. 2022–06, 2022.

[17] K. L. Aw and M. Toneva, “Training language models for
deeper understanding improves brain alignment,” arXiv preprint
arXiv:2212.10898, 2022.

[18] G. Merlin and M. Toneva, “Language models and brain align-
beyond word-level semantics and prediction,” arXiv

ment:
preprint arXiv:2212.00596, 2022.

[19] S. R. Oota, J. Arora, V. Agarwal, M. Marreddy, M. Gupta, and
B. R. Surampudi, “Neural language taskonomy: Which nlp tasks
are the most predictive of fmri brain activity?” arXiv preprint
arXiv:2205.01404, 2022.

[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-
training of deep bidirectional transformers for language under-
standing,” arXiv preprint arXiv:1810.04805, 2018.

[21] A. Wang, A. Singh,

J. Michael, F. Hill, O. Levy, and
S. R. Bowman, “Glue: A multi-task benchmark and analy-
sis platform for natural language understanding,” arXiv preprint
arXiv:1804.07461, 2018.

[22] A. Wang, M. Tarr, and L. Wehbe, “Neural taskonomy: Inferring
the similarity of task-derived representations from brain activity,”
NeurIPS, vol. 32, pp. 15 501–15 511, 2019.

[23] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
for image recognition,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2016, pp. 770–778.

[24] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,
Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang,
W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W.
Li, S. Watanabe, A. Mohamed, and H. yi Lee, “SUPERB: Speech
Processing Universal PERformance Benchmark,” in Interspeech,
2021, pp. 1194–1198.

[25] S. A. Nastase, Y.-F. Liu, H. Hillman, A. Zadbood, L. Hasenfratz,
N. Keshavarzian, J. Chen, C. J. Honey, Y. Yeshurun, M. Regev
et al., “The “narratives” fmri dataset for evaluating models of nat-
uralistic language comprehension,” Scientific data, vol. 8, no. 1,
pp. 1–22, 2021.

[26] M. F. Glasser, T. S. Coalson, E. C. Robinson, C. D. Hacker,
J. Harwell, E. Yacoub, K. Ugurbil, J. Andersson, C. F. Beckmann,
M. Jenkinson et al., “A multi-modal parcellation of human cere-
bral cortex,” Nature, vol. 536, no. 7615, pp. 171–178, 2016.

[27] C. M. Baker, J. D. Burks, R. G. Briggs, A. K. Conner, C. A.
Glenn, K. N. Taylor, G. Sali, T. M. McCoy, J. D. Battiste, D. L.
O’Donoghue et al., “A connectomic atlas of the human cere-
the lateral parietal lobe,” Operative Neuro-
brum—chapter 7:
surgery, vol. 15, no. suppl 1, pp. S295–S349, 2018.

[28] C. K. Milton, V. Dhanaraj, I. M. Young, H. M. Taylor, P. J.
Nicholas, R. G. Briggs, M. Y. Bai, R. D. Fonseka, J. Hormovas,
Y.-H. Lin et al., “Parcellation-based anatomic model of the se-
mantic network,” Brain and behavior, vol. 11, no. 4, p. e02065,
2021.

[29] R. H. Desai, U. Tadimeti, and N. Riccardi, “Proper and common
names in the semantic system,” Brain Structure and Function, vol.
228, no. 1, pp. 239–254, 2023.

[30] A. N. Tikhonov, V. J. Arsenin, and V. Arsenin, Solutions of ill-

posed problems. V H Winston, 1977.

[31] Y. Benjamini and Y. Hochberg, “Controlling the false discovery
rate: a practical and powerful approach to multiple testing,” Jour-
nal of the Royal statistical society: series B (Methodological),
vol. 57, no. 1, pp. 289–300, 1995.

[32] C. R. Genovese, “A bayesian time-course model for functional
magnetic resonance imaging data,” Journal of the American Sta-
tistical Association, vol. 95, no. 451, pp. 691–703, 2000.

[33] S. A. Nastase, Y.-F. Liu, H. Hillman, K. A. Norman, and U. Has-
son, “Leveraging shared connectivity to aggregate heterogeneous
datasets into a common response space,” NeuroImage, vol. 217,
p. 116865, 2020.

[34] J. Shah, Y. K. Singla, C. Chen, and R. R. Shah, “What all
do audio transformer models hear? probing acoustic represen-
tations for language delivery and its structure,” arXiv preprint
arXiv:2101.00387, 2021.

[35] S. R. Oota, M. Gupta, and M. Toneva, “Joint processing of lin-
guistic properties in brains and language models,” arXiv preprint
arXiv:2212.08094, 2022.

