Which Hype for my New Task? Hints and Random
Search for Reservoir Computing Hyperparameters
Xavier Hinaut, Nathan Trouvain

To cite this version:

Xavier Hinaut, Nathan Trouvain. Which Hype for my New Task? Hints and Random Search for
Reservoir Computing Hyperparameters. ICANN 2021 - 30th International Conference on Artificial
Neural Networks, Sep 2021, Bratislava, Slovakia. ￿hal-03203318v2￿

HAL Id: hal-03203318

https://inria.hal.science/hal-03203318v2

Submitted on 15 Dec 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Which Hype for my New Task?
Hints and Random Search for
Echo State Networks Hyperparameters

Xavier Hinaut1,2,3,∗[0000−0002−1924−1184] and Nathan
Trouvain1,2,3[0000−0003−2121−7826]

1 INRIA Bordeaux Sud-Ouest, France.
2 LaBRI, Bordeaux INP, CNRS, UMR 5800.
3 Institut des Maladies Neurod´eg´en´eratives,
Universit´e de Bordeaux, CNRS, UMR 5293.
\*Corresponding author: xavier.hinaut@inria.fr

Abstract. In learning systems, hyperparameters are parameters that
are not learned but need to be set a priori. In Reservoir Computing, there
are several parameters that needs to be set a priori depending on the
task. Newcomers to Reservoir Computing cannot have a good intuition
on which hyperparameters to tune and how to tune them. For instance,
beginners often explore the reservoir sparsity, but in practice this param-
eter is not of high inﬂuence on performance for ESNs. Most importantly,
many authors keep doing suboptimal hyperparameter searches: using
grid search as a tool to explore more than two hyperparameters, while
restraining the spectral radius to be below unity. In this short paper,
we give some suggestions, intuitions, and give a general method to ﬁnd
robust hyperparameters while understanding their inﬂuence on perfor-
mance. We also provide a graphical interface (included in ReservoirPy)
in order to make this hyperparameter search more intuitive. Finally, we
discuss some potential reﬁnements of the proposed method.

Keywords: Reservoir Computing · Echo State Networks · Hyperparam-
eters · Random Search · Grid Search · Eﬀective Spectral Radius

1

Introduction

1.1 Disclaimer

This short paper aims to give some keys to reservoir newcomers to know how to
set hyperparameters. It does not aim to be comprehensive like [12] or to discuss
studies on hyperparameter search, but rather to give a few useful practical hints
to always keep in mind. The main originality lies in the general method proposed,
supplemented with ReservoirPy graphics, applied on two common tasks.

1.2 You have a new task

Suppose you have already some practical experience with Reservoir Computing
(RC) [13], in particular with Echo State Networks [8]. For instance, you read the

2

N. Trouvain and X. Hinaut.

nice guide of Lukoˇseviˇcius [12] and you want to apply RC on a new task. How
do you choose your HyperParameters (HPs) for that new task (for which you
have no assumption of which HP values would work)?

If you only want good results and do not want to know how the HPs inﬂuence
the performance of your task, you can use an optimizer (e.g. a Bayesian [17] or
evolutionary one [1]) to set your HPs once and that’s it. The problem is that if
you want to know if you have found robust HPs with your optimized search, you
will run the optimizer a few times more and will probably end up with several sets
of HP values that are all diﬀerent. Unfortunately, making an average of values
for each HP is probably not a good idea given the interdependencies between
HPs (see subsection 2.6). In the end, you would probably want to know a little
more on the robustness of the HPs you found. Moreover, knowing which HPs
are important will help you to adapt them if needed. For example if at some
point you obtain more diverse data (e.g. on a classiﬁcation task) and you would
like to increase the reservoir size to gain performance. You will need to perform
again the optimization but this time with more data and an increased size of
reservoir: this will cost you much more time to optimize.

Conversely, you want to ﬁnd good HPs for your task, and this time you
are willing to know more about the robustness of the HP values found and
understand which HPs are the most important for your task. Thus, you will
probably do a grid search because that’s what other people usually do, and
because it provides you nicely discretized maps showing gradients of performance
depending on your HPs. The problem is, by doing this, you will probably miss
some of the best performances you could expect. Moreover, it will cost you a lot
of computational time, much more than what you would need with an optimizer:
because by exploring v values for each of your p HPs, you will need to perform
pv evaluations4.

Thus, what should you do to disentangle this seemingly impossible trade-oﬀ?
This is what we will argue in this paper. First, we discuss theoretical aspects of
reservoir computing and interpret their impact on practical purposes while ex-
plaining why grid search is not a good idea. Then, we provide a general practical
method for hyperparameter exploration along with graphical tools. Afterwards,
we show how this method applies to two tasks of time series prediction. Finally,
we discuss some potential reﬁnements of the proposed method.

2 From theory to practice

2.1 At the edge of chaos

Like other dynamical systems (e.g. cellular automata [9]), reservoirs have rich
and “interesting” dynamics at the edge of chaos, or edge of stability [2]. For most
tasks, you want to have reservoir dynamics with contractive property in order not
to have diverging dynamics that could destroy the behavior of your outputs. In
other words, contractive property means that the reservoir will forget its current

4 We call evaluations the training of instances of reservoirs for sets of HP values.

Reservoirs only need random search

3

state and inputs after some time. While in chaotic regime small diﬀerences in
states or inputs are ampliﬁed dramatically. Legenstein et al [10] have proposed
an interesting computational predictive measure to ﬁnd these edge of stability
dynamics in the HP space. The measure is based on the idea that, in order to
generalize well, a reservoir needs a mixture of two properties: (1) the ability to
represent similar inputs in a similar way, and (2) the ability to separate inputs.
Taking into account the fact that dynamics with contractive properties enable
property (1), and chaotic dynamics enable property (2), what we need for a good
generalization ability is to have both (1) and (2) at the same time, namely, at
the intersection of contractive and chaotic dynamics: at the edge of stability. As
they show in their computational study, their predictive generalization measure
often overlaps with regions of the HP space with edge of stability dynamics.

That being said, remember that there is no such optimal reservoir dynamics
for all tasks. Indeed, the desired dynamics depend heavily on your task. Thus,
such computational predictive measures are probably useful only for some class
of tasks.

2.2 Why you shouldn’t care about the Echo State Property

In his seminal paper, Jaeger deﬁnes the Echo State Property (ESP) [8]. To
simplify shortly, ESP states that a spectral radius (SR) following SR < 1 is a
theoretical condition to have a contracting system in the absence of inputs. One
should remember that originally this theoretical condition is interpolated from
linear systems. However, we usually do not use linear reservoirs, as most people
use hyperbolic tangent (tanh) activation function. Thus, even if reﬁnements of
the ESP were made, this condition should not be a constraining rule but only a
rough guide, because reservoir dynamics depend on the inputs fed; it was shown
that for some tasks optimal SR could be superior to 1.

Knowing that ESP is mainly a theoretical guide to set the SR is not enough.
If you use reservoir with leaky neurons (i.e. with a leak rate inferior to one),
then you should also know what the eﬀective spectral radius is: it is what you
should care about instead of the SR itself. Jaeger et al [3] introduced the notion
of eﬀective spectral radius for reservoirs with leaky-integrator neurons. The idea
is roughly the following: the leak rate has a kind of “cooling” eﬀect on the
dynamics, thus for a constant spectral radius if one decreases the leak rate, the
dynamics will “slow down”: the same happens for the eﬀective spectral radius.
Consequently, the more your decrease the leak rate, the more you can increase
the spectral radius, while still having the ESP.

2.3 The myth of grid search

Often, if people do not tune HPs by hand, they use grid search, and we have done
it for too long too. Grid search means that you explore each HP by changing its
values in a predeﬁned grid, such as for each possible value of one HP you will
have tested all possible values of the other HPs.

4

N. Trouvain and X. Hinaut.

Bergsta et al. [6] explained why grid search is suboptimal compared to ran-
dom search. In Figure 1 we can see a schematic explanation of why random
search performs better. As Bergsta et al. [6] pointed out: “Grid search allocate
too many trials to the exploration of dimensions that do not matter and suﬀer
from poor coverage in dimensions that are important.” Thus, you should prefer
random exploration over grid search. One advantage of grid search is to be able
to guide intuition by visualizing a map of the performance depending on the HPs
values. As we will show in section 3, similar maps could be obtained with random
search. Moreover, random search is particularly adapted to reservoir because of
the little computation cost of the training compared to other methods based on
back-propagation. A good view of the hyperspace can be obtained at low cost.
In fact, grid search wastes evaluations to explore unimportant HPs. Whereas,
random search do not waste evaluations because for all HP no value is explored
twice (i.e. no need to evaluate the same value several time for one HP in order to
make a grid to explore other HPs). With grid search, by exploring v values for
each of p HPs, one needs to perform pv evaluations. While for random search,
the number of values explored v is equal to the total number of evaluations.

Fig. 1. Random search samples more eﬃciently than grid search. Image from [6].

2.4 Pieces of advice and hints on HPs choice

Like in other computing paradigms, all HPs do not have the same inﬂuence
on the performance of your task. The most important HPs, like Lukoˇseviˇcius
suggests [12] are the spectral radius, input scaling, leaking rate (or leak rate),
number of units in the reservoir and feedback scaling (in case you have feedback
from the readout units to the reservoir). If you do not want to explore too many
HPs you should care only about them. For instance, newcomers often consider
the sparsity of the reservoir to be important, but it is not, in particular if you
generate the weights with a Gaussian distribution, many weights will be close
to zero in any case. A rule of thumb that we use for sparsity is to keep 20%
of non-zero connections in input and reservoir weights matrices, whatever the
size of your reservoir is. For most HPs, in particular the ones recommended by

Reservoirs only need random search

5

Lukoˇseviˇcius, one should not use a uniform exploration, but rather a logarithmic
exploration. This means that you will be likely to explore low values as much as
high values of your searching ranges (with the same sampling at a given order
of magnitude). For your ﬁrst HPs search on a new task, we recommend to use
to explore HPs with three levels of magnitude between the smallest value and
the highest value (e.g. 1 and 1000).

2.5 A general method with random search

We consider a simple ESN with leaky integrator units, without feedback weights
and with readout weights regularization. The main steps of the proposed method
are (including two preliminary steps):

– Pre1: Take the minimal reservoir size N you would like to use (you can change

N after the HP search)

– Pre2: Fix one of the following HP: IS (Input Scaling), SR (Spectral radius)

or LR (Leak Rate) (to avoid conﬂicting interdependencies)

– A: 1st random search (large range): explore all HP within a wide range of

values.

– B1: 2nd random search (narrow range): remove unimportant HPs and narrow
the ranges of HP values to the x% best values (e.g. 10% best values is a good
choice when doing more than 100 evaluations.).

– B2: Choose median or best HP values (but the ridge) depending on loss vs. HP

cloud shape

– C: (optional) Check robustness of previously ﬁxed HPs (in 2nd step) and

adapt if needed

– D1: (optional) Explore the trade-oﬀ between loss and computation time when

varying the reservoir size N

– D2: Find best regularization parameter (ridge) for chosen N
– E: Evaluate test set on chosen HPs for 30 reservoir instances

2.6 ReservoirPy HP interdependency plot

The proposed method uses HP interdependency plots in order to have a visual
evaluation of the loss obtained as a function of all HP explored along with the
interactions between HPs. Implementation details are provided here5.

3 An illustrated example

We will make the exploration for two well known tasks of chaotic time series
prediction: Mackey-Glass [14] and Lorenz [11] time series prediction. Both time
series where generated using ReservoirPy library, and details about their char-
acteristics can be found in the documentation.

5 https://hal.inria.fr/hal-03203318/document

6

N. Trouvain and X. Hinaut.

(a) Lorenz task.

(b) Mackey-Glass task.

Fig. 2. (a) Small search dependence plot for Lorenz task. Two diﬀerent patterns of
interaction between error (or loss) and parameter value can be observed : the U-shaped
clouds (top-left scatter plot and bottom right scatter plot) and the skewed-linear clouds
(middle scatter plot). (b) Small search dependence plot for Mackey-Glass task. Here,
if the dependence to the leak rate seems to be the same as in the Lorenz task, we can
see that the SR has little inﬂuence on the error value, unlike in the Lorenz task, and
is therefore task dependent.

For the other parameters that need to be set we do as follows. We set the
washout (i.e. number of timesteps ignored at the beginning) to the size of the
reservoir N. After N timesteps, it is indeed expected that all neural nodes in the
reservoir graph have received signals from inputs and surrounding units. The
random seeds used to initialize the 5 instances of the model used during cross
validation are kept ﬁxed for all evaluations. We ﬁnally ﬁxed the reservoir size N
to 100, to shorten the computation time during the searches.

We chose 5 parameters to explore: spectral radius (SR), leak rate (LR), con-
nectivity probability of W (W proba), connectivity probability of Win (Win
proba), and ridge regularization parameter. We included known “unimportant”
HPs like the connectivity probability of matrices in order to illustrate how to
deal with these kinds of parameter throughout the method we propose. We also
chose to ﬁx at least one of the more important HPs, to reduce the complexity
of the search: IS will be kept constant and equal to 1 during this ﬁrst step.

If we would have liked to do a grid search on 5 HPs and limit our search
to about 100 evaluations, we could have taken 3 values per HP for a total of

102101Loss value (min: 0.001129)10% best5% bestBestR2 best populationNormalized R220406080100102101100102101lossLoss min.101100102101100105103101102101100102101100101100LR101100102101105103101101100102101100SR106105104103102101ridge101100LR106105104103102101105103101ridge10210110110010% best R2parameter distribution101100100105102Hyperopt trials summary - Mackey-Glass , 125 trials102101100Loss value (min: 0.007891)10% best5% bestBestR2 best populationNormalized R220406080102101100102101100lossLoss min.101100102101100105103101102101100102101100101100LR101100102101100105103101101100102101100SR106105104103102101ridge101100LR106105104103102101105103101ridge10210110010110010% best R2parameter distribution101100100105102Hyperopt trials summary - Mackey-Glass , 125 trialsReservoirs only need random search

7

53 = 125 evaluations. For grid search exploring only 3 values is very small,
that is why exploring more than 2 HPs with grid search is too computationally
expensive. Whereas for random search, 125 evaluations is enough to have a ﬁrst
idea of the good HP ranges. Taking 25 evaluations per HP explored is a good
rule of thumb.

3.1 1st random search

For both tasks, we used a “task agnostic” range of HPs to be explored (with the
type of distribution chosen a priori, speciﬁed in parentheses): SR = [10−2, 101]
(log-uniform); LR = [10−3, 1] (log-uniform); W proba = [10−2, 1] (uniform);
Win proba = [10−2, 1] (uniform); ridge = [10−3, 10−9] (log-uniform). IS is kept
ﬁxed and equal to 1. A minimum error of 1.2 × 10−3 is obtained at this stage of
the evaluation for the Lorenz task, and 7.8 × 10−3 for the Mackey-Glass task6.

3.2 2nd random search

We can then use this information to redeﬁne the range of possible values of
parameters. One has to include the x% best results inside this range. In other
words, the new range must include previous x% best values, with an error mar-
gin set depending on the trend observed in the inﬂuence of the parameter over
performance, e.g. if it seems that expanding a bit the range towards positive
side of a parameter axis would allow to explore possibly interesting values, we
recommend expanding it. With 125 evaluations, x = 10 seems to be a good com-
promise. Also, one can ﬁx parameters that seem to have no particular inﬂuence
7). We can therefore ﬁx
on the model performance (e.g. sparsity of W and Win
these parameters to a speciﬁc value for the rest of the exploration. In both tasks,
we chose to ﬁx W proba and Win to the median value of the top x = 10% trials.
Lorenz task For Lorenz task, we redeﬁned the range of HPs as follows: SR
= [10−2, 3] (log-uniform); LR = [10−1, 1] (log-uniform); W proba = 0.6 (ﬁxed);
Win proba = 0.5 (ﬁxed); ridge = [10−7, 1] (log-uniform).

Mackey-Glass task For Mackey-Glass task, we redeﬁned the range of HPs
as follows: SR = [10−2, 3] (log-uniform); LR = [10−1, 1] (log-uniform); W proba
= 0.6 (ﬁxed); Win proba = 0.5 (ﬁxed); ridge = [10−6, 10] (log-uniform). Figure
2 summarizes the results of this second random search for both tasks.

3.3 Best values obtained

During this second stage of search, the minimum error reached for Lorenz task
lowered to 1.1 × 10−3, while the minimum error reached for Mackey-Glass re-
mained close to the value obtained during the large search.

6 A ﬁgure summarizing the results of this ﬁrst random search on the Lorenz time series

prediction task is available here: https://hal.inria.fr/hal-03203318/document

7 W proba and Win proba have little inﬂuence on the error distribution (diagonal
plots) and no linear dependence with the other parameters can be seen on ﬁgure
available here: https://hal.inria.fr/hal-03203318/document

8

N. Trouvain and X. Hinaut.

Fig. 3. Restricted search on the IS parameter, for the Lorenz task. With a value of 1,
we can see that the IS yields the best results and reaches the minimum of the U-shaped
error cloud that can be seen in the top-left plot.

Fig. 4. Error obtained with the best ridge for sizes of reservoir in {100, 200, 400, 800},
on the Mackey-Glass task (left) and Lorenz task (right). Note that variations in error
are really small for both tasks.

Looking at the graphs, we select values given the shape of the estimated dis-
tribution of the error metrics. Two patterns generally arise: the U-shape clouds
and the skewed clouds. For the parameters provoking U-shaped clouds of error,
we set the value of the parameter to the median of the x = 10% best values,

102Loss value (min: 0.001177)10% best5% bestBestR2 best populationNormalized R220406080100101100102lossLoss min.106105104103102101100101100IS106105104103102ridge106105104103102ridge10210010010010% best R2parameter distribution105104103102Hyperopt trials summary - Lorenz , 125 trialsReservoirs only need random search

9

e.g. SR = 7.10−2 for the Lorenz prediction task, as observed in ﬁgure 2. For the
HPs clearly skewed with a linear trend (still on a logarithmic scale), we take the
best parameter found, e.g. LR = 0.98, in the case of the Lorenz prediction task.
We also double checked this values on the coupled parameters plots around the
diagonal of plots, to make sure that no picked value would poorly interact with
another parameter. We can also see in ﬁgure 2 that the spectral radius seems to
play a not so important role in the Mackey-Glass task. The error distribution
regarding to this parameter is quite ﬂat, with unclear dependence compared to
the Lorenz task. This shows that the SR can be a task dependent parameter,
and be robust against variations for a wide range of values.

Lorenz task For Lorenz task, using the method described above, we ﬁnally
chose the following parameters: SR = 7.10−2 (ﬁxed); LR = 0.96 (ﬁxed); W proba
= 0.6 (ﬁxed); Win proba = 0.5 (ﬁxed); ridge = [10−6, 10−1] (log-uniform).

Mackey-Glass task For Mackey-Glass task, we ﬁnally chose the following
parameters: SR = 0.12 (log-uniform); LR = 0.97 (log-uniform); W proba = 0.6
(ﬁxed); Win proba = 0.5 (ﬁxed); ridge = [10−6, 10] (log-uniform).

The ridge was not ﬁxed until the end of the process. We only kept narrowing
its explored range. Regularization parameters should not be ﬁxed until all other
parameters are found: they are meant to optimize the model regarding to its
complexity, in our case regarding to the number of neuronal units of the reservoir,
the parameter N . We assess the impact of this parameter on the performance in
section 3.5.

3.4 Checking the input scaling

Before evaluating the model using the parameters found, we recommend as-
sessing the interdependence of the chosen parameters and the parameters kept
constant during evaluation. In our case, the input scaling (IS) was ﬁxed to 1, but
as it controls the amplitude of the signals fed to the reservoir, it can have great
inﬂuence on the model performance. We performed a simple check consisting in
deﬁning a range of one or two order of magnitude around the ﬁxed value of the
IS, and running a random search on this range. The ridge will also be kept vari-
able, as it has not been ﬁxed yet. Results of this evaluation for the Lorenz task
can be found in ﬁgure 3 and on ﬁgure 2 for the Mackey-Glass task. Although it
appears that a maximum of performance can be reached with a ﬁxed value of 1
in the case of the Lorenz task, with this value placed just below the minimum
of the error, we can see that the distribution of error is skewed towards the left
of this value in the case of the Mackey-Glass task. We therefore redeﬁne the IS
value for Mackey-Glass to 0.3.

3.5 Searching for the best ridge

Finally, we propose to explore the dependence of the ridge on the reservoir size
N. We explored the ridge in a range of [10−5, 10−1], and used four diﬀerent
reservoir sizes N = {100, 200, 400, 800}. By selecting the most adapted ridge for
each size, i.e. the ridge corresponding to the minimum error produced with each

10

N. Trouvain and X. Hinaut.

reservoir size, we can see the relation existing between the error, the ridge and
the reservoir size in ﬁgure 4. Using this information, one can then select the pair
{ridge, N} that suits the most their needs and resources in terms of performance
and computational power. Assuming that we have enough computational power
to use a 800 neurons reservoir at least, we select the ﬁnal values for ridge and N:
Lorenz task Because a size of 400 seems to already give good performance
as shown in ﬁgure 4, we will keep this value instead of the maximum of 800. We
ﬁnally chose: N = 400; ridge = 2 × 10−4.

Mackey-Glass task In the case of Mackey-Glass, we obtained best results
with a size of 100, as shown in ﬁgure 4. Even if counter-intuitive, this result is
probably due to the simplicity of the task performed, or to a low dependency
between the reservoir size and the chosen parameters. We ﬁnally chose: N = 100;
ridge = 5 × 10−4.

To conclude this experimentation, we evaluated the selected HPs on a test
data set, with 30 diﬀerent random initializations. The test data set is deﬁned as
the 2000 time steps following the 6000 time steps used during cross validation.
This 6000 time steps are used as training steps during the evaluation. The results
obtained are presented in table 1:

Table 1. Results averaged over 30 random instances on the test set with the chosen
HPs.

R2
Task
0.99999 (±2 × 10−6)
Lorenz
Mackey-G. 9.78 × 10−3 (±4.56 × 10−3) 0.99805 (±2 × 10−3)

NRMSE
4.5 × 10−4 (±1 × 10−4)

Our ﬁnal results outperformed the previously reached error values in both
tasks. However, the diﬀerence in performance between previously reached values
and ﬁnal values remains relatively low, and could be attributed to chance. A
better evaluation could be done using more folds during cross validation, and
more random initializations to avoid any bias from both the data and the initial-
ization. Nevertheless, our method helped ﬁnding ranges of HPs able to produce
satisfying models in terms of performance, and without relying on too many
assumptions and hypothesis, as the ones generally made about SR value for in-
stance. Moreover, we evaluated the model regarding the inﬂuence of almost all
possible parameters, ensuring robustness of the results.

4 Discussion

We ﬁrst discussed theoretical aspects of reservoir computing and which impact
they should have on practical purposes. Hopefully, we convinced the reader not
to perform grid search for more than two hyperparameters (HPs), given the

Reservoirs only need random search

11

high number of evaluations that such suboptimal method would require8. Then,
we provided a general practical method for HP exploration. Finally, we showed
how this method applies to the Lorenz and Mackey-Glass chaotic time series
prediction tasks, and provided several ﬁgures illustrating the inﬂuence of HPs
on performance. To our knowledge, neither grid search nor other optimizer pro-
vide such an amount of information and intuitions for reservoir computing HP
exploration. In summary, we proposed a method that is better than a compro-
mise between the graphical intuitions that grid search would provide, and the
optimum performance that an optimizer provides. Our method probably uses
more evaluations than most eﬃcient optimizers (even if evolutionary methods
are generally demanding in terms of number of evaluations). Given the precision
of the performance obtained throughout the paper, the number of evaluations
used could be reduced.

Our study illustrates that the importance of HPs depends on the task. Indeed,
we showed in ﬁgure 2 that the spectral radius is more robust to changes (i.e. it
oﬀers a wider range of eligible values) for the Mackey-Glass than for the Lorenz
task. Moreover, the interdependencies between HPs are visible, for instance on
ﬁgure 2 for SR vs. ridge subplot we can see a red-orange diagonal: if the SR is
changed, the ridge has to be adapted, and vice versa9. In future work, we would
like to integrate these interdependencies within the HP search. For example by
deﬁning a HP Y as a linear function of a HP X. Searching the best values for
Y would thus be redeﬁned as ﬁnding the best values of a and b in the following
equation: Y = a.X + b.10 One can argue that this adds more HPs, but one can
assume that ﬁnding such interdependencies in such a way is less demanding in
number of evaluations needed.

The tasks we have chosen are famous tasks for reservoir benchmarking, how-
ever they are rather simple to solve, given that a 100-unit reservoir already solves
the task well. In future work, we will apply the proposed method to more complex
and more computationally demanding tasks which would not enable hundreds
of evaluations to be performed. Indeed, in one of our recent study [7] we used
parts of the method proposed here. We applied it to a more diﬃcult task (i.e.
language processing) and among other things, in supplementary material of [7]
we showed the eﬀect of the regularization on a loss vs. reservoir size plot. Given
the point-clouds drawn, we could see the best loss given the reservoir size both
with optimal regularization and without any regularization at the same time.

The method we proposed in this study is probably applicable to other ma-
chine learning paradigms than RC. Thus reducing the time and computational
resources needed for HP search: this is a step towards more environment-friendly
methods. In one of our recent studies [16], we explored a new way of viewing the
inﬂuence of HPs. With UMAP visualizations (Uniform Manifold Approximation

8 With grid search, by exploring v values for each of p HPs, one needs to perform pv
evaluations. While for random search, v is equal to the total number of evaluations.
9 For interdependency for IS vs. ridge see https://hal.inria.fr/hal-03203318/document
10 Of course as we plot many variables with log scales, the equation would often look

like log(Y ) = a.log(X) + b.

12

N. Trouvain and X. Hinaut.

and Projection [15]) we explored the inﬂuence of HPs on the internal dynamics
of the reservoir. For instance, increasing the spectral radius or the leak rate is
fragmenting the UMAP representations of the internal states.

Finally, we would like to enjoin authors to think about simple rules while
writing their next paper on Reservoir Computing: (1) please provide all the HPs
you have used with the details of how you found them, in order to enable others
to have a chance to reproduce your work; (2) demonstrate that the HPs you
have found or chosen are robust again perturbations or slight modiﬁcations (like
in the supplementary material of [7]), instead of just picking the results from a
black-box optimization search.

References

1. Ferreira et al., A.: An approach to reservoir computing design and training. Expert

systems with applications 40(10), 4172–4182 (2013)

2. Schrauwen et al., B.: An overview of reservoir computing: theory, applications and

implementations. In: Proc. of ESANN. pp. 471–482 (2007)

3. Jaeger et al., H.: Optimization and applications of echo state networks with leaky-

integrator neurons. Neural Networks 20(3), 335–352 (2007)

4. Bergstra et al., J.: Hyperopt: A python library for optimizing the hyperparameters

of machine learning algorithms. In: Proc. Scipy. pp. 13–20. Citeseer (2013)

5. Trouvain et al., N.: Reservoirpy: an eﬃcient and user-friendly library to design

echo state networks. In: ICANN. pp. 494–505. Springer (2020)

6. Bergstra, J., Bengio, Y.: Random search for hyper-parameter optimization. Journal

of Machine Learning Research 13(Feb), 281–305 (2012)

7. Hinaut, X.: Which input abstraction is better for a robot syntax acquisition model?

phonemes, words or grammatical constructions? In: ICDL-EpiRob (2018)

8. Jaeger, H.: The “echo state” approach to analysing and training recurrent neural

networks. Bonn, Germany: GNRCIT GMD Tech. Rep. 148, 34 (2001)

9. Langton, C.G.: Computation at the edge of chaos: Phase transitions and emergent

computation. Physica D: Nonlinear Phenomena 42(1-3), 12–37 (1990)

10. Legenstein, R., Maass, W.: Edge of chaos and prediction of computational perfor-

mance for neural circuit models. Neural Networks 20(3), 323–334 (2007)

11. Lorenz, E.N.: Deterministic Nonperiodic Flow. J. Atmo. Sci. 20(2), 130–141 (1963)
12. Lukoˇseviˇcius, M.: A practical guide to applying echo state networks. In: Neural

Networks: Tricks of the Trade, pp. 659–686. Springer (2012)

13. Lukoˇseviˇcius, M., Jaeger, H.: Reservoir computing approaches to recurrent neural

network training. Computer Science Review 3(3), 127–149 (2009)

14. Mackey, M.C., Glass, L.: Oscillation and chaos in physiological control systems.

Science 197(4300), 287–289 (Jul 1977)

15. McInnes, L., Healy, J., Saul, N., Großberger, L.: Umap: Uniform manifold approx-
imation and projection. Journal of Open Source Software 3(29), 861 (2018)
16. Variengien, A., Hinaut, X.: A journey in ESN and LSTM visualisations on a lan-

guage task. arXiv preprint arXiv:2012.01748 (2020)

17. Yperman, J., Becker, T.: Bayesian optimization of hyper-parameters in reservoir

computing. arXiv preprint arXiv:1611.05193 (2016)

Reservoirs only need random search

13

5 Supplementary Material

5.1 Implementation details

We used the ReservoirPy library [5], which has been interfaced with hyperopt [4],
to make the following experiments and ﬁgures in order to dive into the explo-
ration of HPs. We illustrated our proposed method with two diﬀerent tasks of
chaotic time series prediction. These tasks consist in predicting the value of the
series at time step t+1 given its value at time step t. To asses the performance of
our models on these tasks, we performed cross validation. Each fold is composed
of a train and a validation data set. The folds are deﬁned as continuous slices of
the original series, with an overlap in time, e.g. the validation set of the ﬁrst fold
would be the training set of the second, and the two sets would be two adjacent
sequences of data in time in the series. For the last fold of the time series, the
train set is deﬁned as the last available slice of data, while the train set of the
ﬁrst fold is used as validation set.

For all tasks, we used a 3-fold cross validation measure on a time series
composed of 6000 time steps, i.e. each fold is composed of 4000 time steps, with
a train set and a validation set of 2000 time steps each. We used two metrics
to perform this measure: the Normalized Root Mean Squared Error, deﬁned in
equation 1, and the R2 correlation coeﬃcient, deﬁned in equation 2:

(cid:113)

NRMSE(y, ˆy) =

(cid:80)N −1

t=0 (yt − ˆyt)2

1
N
max y − min y

R2(y, ˆy) = 1 −

(cid:80)N −1

t=0 (yt − ˆyt)2
t=0 (yt − ¯y)2

(cid:80)N −1

(1)

(2)

where y is a time series deﬁned over N time steps, ˆy is the estimated time
series predicted by the model, and ¯y is the average value of the time series y over
time. NRMSE was used as an error measure, which we expect to reach a value
near 0, while R2 was used as a score, which we expect to reach 1, its maximum
possible value. All measures were made by averaging this two metrics across all
folds, with 5 diﬀerent initializations of the models for each fold.

14

N. Trouvain and X. Hinaut.

Fig. 5. Restricted search on the IS parameter, for the Mackey-Glass task. The ﬁxed
value of 1 deﬁned at the beginning of the search for IS might not be the optimal value
given all the other parameters chosen. In the case of the Mackey-Glass task, we can
clearly see in the top-left and bottom-left plots that better results are achieved with
lower values, with the top 10% trials distribution being placed around a median of 0.3.

102101Loss value (min: 0.006859)10% best5% bestBestR2 best populationNormalized R220406080100102101100102101lossLoss min.105104103102101100102101100102101100IS106105104103102101100ridge105104103102101100ridge10210110210110010010010% best R2parameter distribution104102100Hyperopt trials summary - Mackey-Glass , 125 trialsReservoirs only need random search

15

Fig. 6. Large search dependence plot for Lorenz task, with 125 trials. Diagonal of the
plot matrix displays interactions between all parameters explored and the error value,
also referred to as loss value. Top 10% trials in terms of score (here, R2) are represented
using colors from yellow (top 10) to red (top 1) in all plots. Other plots on the matrix
display the interactions of all possible couple of parameters. In these plots, the value of
the error is represented using shades of purple and blue, on a logarithmic scale, while
score is represented using diﬀerent circle sizes. Circle size is normalized regarding to
the score values. Because R2 can take values between −∞ and 1, the smallest dots
represents negative values. Finally, the bottom row of plots display the parameter
distribution of the top 10% of trials, in terms of score, and for each parameter.

101101Loss value (min: 0.001272)10% best5% bestBestR2 best populationNormalized R220406080102101100101101lossLoss min.1021011021011000.00.51.01021011000.00.51.0102101100106103102101100102101100103102101LR1021011011010.00.51.01031021010.00.51.01031021011061031031021011021011000.000.250.500.751.00Win\_proba1021010.000.250.500.751.000.250.500.751011010.00.51.00.000.250.500.751.001061030.000.250.500.751.001021011000.000.250.500.751.00W\_proba1021010.000.250.500.751.000.00.51.00.000.250.500.751.000.250.500.751011011061030.000.250.500.751.00102101100SR107105103ridge102101LR1071051030.00.51.0Win\_proba1071051030.00.51.0W\_proba107105103105ridge10110110110010% best R2parameter distribution1021010.250.500.750.250.500.75108105102Hyperopt trials summary - Lorenz , 125 trials