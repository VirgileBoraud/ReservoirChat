apprentissage de séquences et extraction de règles de
réseaux récurrents : application au traçage de schémas
techniques.
Ikram Chraibi Kaadoud

To cite this version:

Ikram Chraibi Kaadoud. apprentissage de séquences et extraction de règles de réseaux récurrents :
application au traçage de schémas techniques.. Autre [cs.OH]. Université de Bordeaux, 2018. Français.
￿NNT : 2018BORD0032￿. ￿tel-01771685￿

HAL Id: tel-01771685

https://theses.hal.science/tel-01771685

Submitted on 19 Apr 2018

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

THÈSE
PRÉSENTÉE A

L’UNIVERSITÉ DE BORDEAUX
ÉCOLE DOCTORALE DE MATHÉMATIQUES ET
D’INFORMATIQUE

par Ikram Chraibi Kaadoud

POUR OBTENIR LE GRADE DE

DOCTEUR

SPÉCIALITÉ: INFORMATIQUE

Apprentissage de séquences et extraction de règles
de réseaux récurrents : application au traçage de
schémas techniques.

Date de soutenance : Mars 2018

Devant la commission d’examen composée de :
Frédéric Alexandre Directeur de recherche, Inria . . . . . . . . . . . . . . . . . . . . . . . . Directeur
Nicolas Rougier . . . . Chargé de recherche HDR, Inria . . . . . . . . . . . . . . . . . . . . Co-Directeur
Michel Paindavoine
Professeur des Universités, Université de Bourgogne . Rapporteur
Arnaud Revel . . . . . . Professeur des Universités, Université de La Rochelle Rapporteur
Anke Brock . . . . . . . . Enseignante-chercheuse, ENAC . . . . . . . . . . . . . . . . . . . . . Examinatrice
Helene Sauzeon . . . . Professeure des Universités, Université de Bordeaux . Examinatrice
Hervé Frezza-Buet
Professeur, Centrale Supelec . . . . . . . . . . . . . . . . . . . . . . . . Examinateur

2018

Résumé Les connaissances implicites d’individu s’acquièrent selon deux moyens.
Le premier consiste en la répétition de séquences, ce qui permet à l’individu
d’extraire implicitement des régularités. Le second moyen est une migration
de connaissances explicites en connaissances implicites dut au développement
d’une expertise. Il s’agit dans les deux cas d’apprentissage implicite. Dans nos
travaux, nous avons souhaité nous pencher sur les séquences de composants
électriques et notamment la problématique d’extraction des règles implicites
dans ces séquences, aspect important de l’extraction de l’expertise métier à
partir des schémas techniques. Nous nous plaçons dans le domaine connexion-
niste, et nous avons en particulier considéré des modèles neuronaux capables
de traiter des séquences. Nous avons implémenté deux réseaux de neurones
récurrents : le modèle de Elman, le Simple Recurrent Network, et un modèle
doté d’unités LSTM (Long Short Term Memory). Nous avons évalué ces deux
modèles sur diﬀérentes grammaires artiﬁcielles (grammaire de Reber et ses va-
riations) au niveau de l’apprentissage, de leurs capacités de généralisation de
celui-ci et leur gestion de dépendances séquentielles. Finalement, nous avons
aussi montré qu’il était possible d’extraire les règles encodées (issues des sé-
quences) dans le réseau récurrent doté de LSTM, sous la forme d’automate. Le
domaine électrique est particulièrement pertinent pour cette problématique car
il est plus contraint (combinatoire plus réduite) que la planiﬁcation de tâches
dans des cas plus généraux comme la navigation par exemple, qui pourrait
constituer une perspective de ce travail.

Title Sequence learning and rules extraction from recurrent neural networks:
application to the drawing of technical diagrams

Implicit knowledge is acquired in two ways. The ﬁrst consists in
Abstract
the repetition of sequences, which allows the individual to extract implicitly
regularities. The second way is a migration of explicit knowledge into implicit
knowledge during the development of an expertise. In both cases, it is im-
plicit learning. In our work, we endeavor to observe sequences of electrical
components and in particular the problem of extracting rules hidden in these
sequences, which are an important aspect of the extraction of business exper-
tise from technical drawings. We place ourselves in the connectionist domain,
and we have particularly considered neuronal models capable of processing
sequences. We implemented two recurrent neural networks: the Elman model
and a model with LSTM (Long Short Term Memory) units. We have eva-
luated these two models on diﬀerent artiﬁcial grammars (Reber’s grammar
and its variations) in terms of learning, their generalization abilities and their
management of sequential dependencies. Finally, we have also shown that it
is possible to extract the encoded rules (from the sequences) in the recurrent
network with LSTM units, in the form of an automaton. The electrical domain
is particularly relevant for this problem. It is more constrained with a limited

Extraction de règles de réseaux récurrents

iii

combinatorics than the planning of tasks in general cases like navigation for
example, which could constitute a perspective of this work.

Keywords Recurrent Neural Networks, LSTM, Sequence Learning, Rules
Extraction, Technical diagrams

Mots-clés Réseaux de neurones récurrents, LSTM, Apprentissage de sé-
quences, Extraction règles, schémas techniques

Laboratoire d’accueil
Tour, 33405 Talence

Inria Bordeaux Sud-Ouest, 200 Avenue de la Vieille

iv

Ikram Chraibi Kaadoud

Remerciements

Nous piétinerons éternellement aux frontières de
l’inconnu, cherchant à comprendre ce qui restera toujours
incompréhensible. Et c’est précisément cela qui fait des
nous des hommes

Isaac Asimov

A la ﬁn de la thèse, un exercice diﬃcile survient : celui de rendre hommage
aux personnes qui ont contribué au succès de celle-ci, des travaux menés et de
la rédaction du manuscrit. C’est donc maladroitement que je m’essaie à cet
exercice ici, dans les quelques lignes qui suivent . . .

A mes parents, qui m’ont accompagné dans cette aventure, à mes sœurs,
mes modèles de persévérance, à mes tantes, qui se sont déplacées pour ma
soutenance malgré la peur de l’avion, à ma famille bordelaise, aux êtres qui
nous ont quitté, et à qui je penserais toujours. . .Et à tous ceux encore là, je dis
un grand merci ! Il va de soi que je ne serais pas là où j’en suis sans vous, sans
les valeurs que vous m’avez transmises et le courage que vous m’avez insuﬄé.

A ma force tranquille qui était tout simplement là et qui m’a soutenu tout

au long de mes péripéties, merci.

Je remercie également les membres passés de Mnemosyne, parmi lesquels
Charlotte, Fabien et Maxime qui m’ont accueilli et donné envie de faire de la
recherche.
Je remercie également les membres actuels de Mnemosyne pour leur présence
et leurs accompagnements, Thalita, Bhargav, Silvia, Antony, Remya, Aurélien,
Basile et les autres.
Je remercie spécialement, André Garenne et Chrystel Plumejeau, pour m’avoir
si souvent écouté et conseillé ! A chaque question, respectivement, sur le deve-
nir des chercheurs ou les démarches administratives, vous avez été là !
Aux membres de l’INRIA BSO et de l’IMN avec qui j’ai eu le plaisir de discuter
et d’échanger, je dis merci ! Les petites rencontres du quotidien font la beauté
de la vie !

v

A tous les doctorants de l’association Aquidoc, grâce à qui je me suis sentie
moins seule dans les moments d’incertitude, je vous remercie pour le soutien
psychologique et les échanges formidables sur la recherche et la vie des docto-
rants que nous avons pu avoir.

A tous ceux qui me font le bonheur de leur amitié, je ne vous oublie pas et

à ceux-là je dis merci !
A tout ceux que j’ai oublié de citer, mais qui ont été là, je m’excuse et je vous
remercie.

A Alexandra, Maud, Brice, Meryl et Laure avec qui j’ai traversé le master

de sciences cognitives et ces trois années, je vous dois énormément. Merci !

J’adresse mes remerciements les plus sincères à l’entièreté de l’équipe Al-
go’Tech de Bidart et de Paris, passé et présent : Anne-Cécile, France, Lize, An-
doni, Jean-philipe, Maher (et Neila), Sylvain, et tous les autres ! Mais surtout
merci à Didier et M. Jean-Michel Petolat, pour leur aide dans la compréhen-
sion des problématiques métier et la qualité des échanges que nous avons eu !
Votre accueil à tous, vos explications, votre patience, votre accompagnement
dans cette aventure, et surtout votre conﬁance ont grandement contribué au
succès de ma thèse !

Enﬁn, il va de soi que je n’en serais pas là sans mes 3 Senseis ! Que dis-je
mes mentors : Frédéric Alexandre, Nicolas Rougier, mes directeurs de thèse,
et Thierry Vieville, mon maitre Yoda ! Je pense que des remerciements ne
sauraient exprimer ma gratitude pour l’opportunité que vous m’avez oﬀert
de réaliser ma thèse auprès de vous dans le domaine de l’informatique cogni-
tive ! Votre conﬁance, votre ouverture d’esprit, et votre encadrement et votre
patience, ont fait de moi la scientiﬁque et l’humaine accomplie que je suis au-
jourd’hui !

Si l’on était dans un jeu vidéo, je m’assurerais de sauvegarder ma partie à
ce stade, aﬁn de ne jamais oublier les personnes qui ont fait ces trois années . . .

A vous tous je dis tout simplement merci.

Saving game . . .

Loading next level. . .

vi

Ikram Chraibi Kaadoud

Table des matières

Introduction générale

1
2
Mémoire et connaissances dans la fonction de raisonnement . . . . . .
4
Mémoire et apprentissage implicite . . . . . . . . . . . . . . . . . . .
7
Evolution des connaissances chez l’expert . . . . . . . . . . . . . . . .
9
. . . . . . . . . . . . . . . . . . . . . .
Algo’Tech, contexte industriel
Extraction des connaissances et de l’expertise en entreprise . . . . . . 12
Application à l’analyse de documents . . . . . . . . . . . . . . . . . . 14
Contexte informatique actuel
. . . . . . . . . . . . . . . . . . . . . . 17
Problématique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Structure du document . . . . . . . . . . . . . . . . . . . . . . . . . . 21

1 Extraction et préparation des données issues de schémas élec-

triques
23
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
1.2 Contexte environnemental et informatique . . . . . . . . . . . . 24
. . . . . . . . . . . . . . . 25
1.3 Les données : les schémas électriques
Structure d’un Folio . . . . . . . . . . . . . . . . . . . . 25
1.3.1
. . . . . . . . . . . . . . 27
1.3.2 Lecture d’un schéma électrique
1.3.3 Les données brutes : les PDF et DXF . . . . . . . . . . . 28
1.4 Première approche : extraction des concepts . . . . . . . . . . . 29
1.4.1 Transformation d’un schéma en un graphe . . . . . . . . 29
1.4.2 Abstraction des graphes par étude de similarité des mots 30
1.4.3 Fusion de nœuds et concaténation de nœuds puis de

graphes

. . . . . . . . . . . . . . . . . . . . . . . . . . . 31
1.4.4 Analyse et résultat . . . . . . . . . . . . . . . . . . . . . 32

1.5 Seconde approche : extraction de l’empreinte implicite par data

mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
1.5.1 Une approche par le contenu . . . . . . . . . . . . . . . . 34
1.5.2 Processus de Data mining : le clustering hiérarchique . . 35
1.6 Analyse des résultats précédents . . . . . . . . . . . . . . . . . . 38
1.7 Présentation du prototype . . . . . . . . . . . . . . . . . . . . . 40
. . . . . . . . . . . . . . . 44
1.8 Conclusion des travaux préliminaires

vii

2 Réseaux de neurones récurrents : De l’apprentissage séquentiel

TABLE DES MATIÈRES

à l’extraction de règles de grammaire
2.1

Introduction : Des RNN à l’extraction des règles, quelques déﬁ-
nitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
. . . . . . . . . . . 51
2.2 Le temps dans les systèmes connexionnistes
2.3 Deux modèles de RNN : SRN et LSTM . . . . . . . . . . . . . . 59
2.3.1 Le SRN . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
2.3.2 Long Short Term Memory, LSTM . . . . . . . . . . . . . 70
2.4 Extraction des règles et de la grammaire à partir d’un RNN . . 85
2.4.1 Les cinq étapes de l’extraction des règles à partir d’un

47

réseau de neurones récurrents

. . . . . . . . . . . . . . . 86
2.4.2 Les étapes critiques du procédé . . . . . . . . . . . . . . 90
2.4.3 Grammaire connue versus grammaire inconnue . . . . . . 90
2.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91

3 Implémentation des RNNs et extraction de règles

3.1
3.2 Les grammaires utilisées

93
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
. . . . . . . . . . . . . . . . . . . . . . 95
3.2.1 La grammaire manuelle . . . . . . . . . . . . . . . . . . . 96
3.2.2 La grammaire de REBER (RG) . . . . . . . . . . . . . . 97
3.2.3 Les variantes de la grammaire de REBER . . . . . . . . 98
3.3 Comparaison du SRN et RNN-LSTM . . . . . . . . . . . . . . . 99
3.3.1
SRN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
3.3.2 RNN-LSTM . . . . . . . . . . . . . . . . . . . . . . . . 115
3.4 Extraction de règles à partir du RNN-LSTM . . . . . . . . . . . 121
3.4.1 Patterns d’activités de la couche cachée . . . . . . . . . . 122
3.4.2 Algorithme d’extraction des règles
. . . . . . . . . . . . 124
3.4.3 Application à la grammaire de REBER et à ses variations127
3.4.4 Application à une grammaire électrique (EL) . . . . . . . 133
3.4.5 Discussion sur l’extraction de règles à partir de RNN-

3.5 Discussion

LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141

Conclusion générale

145
Résumé des travaux . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
Résultats
3.5.1 Au niveau industriel
. . . . . . . . . . . . . . . . . . . . 147
3.5.2 Au niveau machine learning . . . . . . . . . . . . . . . . 148
. . . . . 149
3.5.3 Au niveau gestion des connaissances implicites
Perspectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
3.5.4 Amélioration de l’approche actuelle . . . . . . . . . . . . 150
3.5.5 Application à d’autres domaines . . . . . . . . . . . . . . 153

viii

Ikram Chraibi Kaadoud

TABLE DES MATIÈRES

A Manuel utilisateur prototype

B Dictionnaire des mots communs

C Dictionnaire des catégories de composants électriques

D Algorithme d’apprentissage de SRN

155

183

185

187

E Algorithme d’apprentissage de RNN-LSTM d’après Gers et

Schmidhuber [2001]

Bibliographie

189

193

Extraction de règles de réseaux récurrents

ix

TABLE DES MATIÈRES

x

Ikram Chraibi Kaadoud

Introduction générale

Table des matières

2
Mémoire et connaissances dans la fonction de raisonnement . . . . . .
4
Mémoire et apprentissage implicite . . . . . . . . . . . . . . . . . . .
7
Evolution des connaissances chez l’expert . . . . . . . . . . . . . . . .
9
. . . . . . . . . . . . . . . . . . . . . .
Algo’Tech, contexte industriel
Extraction des connaissances et de l’expertise en entreprise . . . . . . 12
Application à l’analyse de documents . . . . . . . . . . . . . . . . . . 14
. . . . . . . . . . . . . . . . . . . . . . 17
Contexte informatique actuel
Problématique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Structure du document . . . . . . . . . . . . . . . . . . . . . . . . . . 21

La gestion des connaissances (KM pour Knowledge Management) pour une
organisation se déﬁnit comme "le management des activités et des processus
destinés à ampliﬁer l’utilisation et la création des connaissances au sein d’une
organisation selon deux ﬁnalités complémentaires fortement imbriquées : une
ﬁnalité patrimoniale (préservation, réutilisation et actualisation des connais-
sances) et une ﬁnalité d’innovation durable (création active des connaissances
individuelles et de leur intégration au niveau collectif dans une organisation)"
[Grundstein, 1995]. Axe de développement stratégique pour les entreprises,
l’essor du KM ces dernières années est dû majoritairement à celui des tech-
niques de Machine Learning allié à celui du Big Data, faisant de la collecte des
données un réel enjeu économique et sociétal [Serrour et al., 2006; Wirtz et al.,
2014; Fischer et al., 2015; Spiekermann et al., 2015; Jin et al., 2015; Zaoui et
Mebarki, 2017].

Toutefois, il faut souligner deux points. Premièrement, la bonne récolte des
données, ainsi que le fait d’avoir un volume suﬃsant, n’implique pas une bonne
exploitation ou un résultat pertinent pour une entreprise. En eﬀet, l’enjeu ma-
jeur actuel des entreprises est celui du choix de la bonne donnée parmi toutes
celles accumulées, à savoir celle qui permet de synthétiser l’information perti-
nente ou de prendre la bonne décision. Deuxièmement, les systèmes d’informa-
tion actuels (ensemble organisé de ressources qui permet de collecter, stocker,

1

Mémoire et connaissances dans la fonction de raisonnement

traiter et distribuer de l’information) ignorent souvent la dimension humaine
du processus de création des données [Crié, 2003]. Or, la connaissance n’est pas
dans les données mais dans l’utilisateur [Malhotra, 2000] puisque quelles que
soient les conclusions des algorithmes de fouille de données, c’est l’opérateur
humain qui les valide ou non [Alvarez-Macias et al., 2004].

Parmi les industriels confrontés à cette problématique de gestion des connais-
sances se trouve l’entreprise Algo’Tech, éditrice de logiciel de conception et de
dessin assisté par ordinateur dans le domaine de la schématique électrique.
Analyser et comprendre les données aﬁn d’extraire le métier (comprendre les
règles, les missions et les contraintes du métier) de ses clients sont des tâches
auxquelles cette entreprise est confrontée quotidiennement. Dans ce domaine
très technique où les méthodes de Big Data ne peuvent pas être mise en place,
pour des raisons que nous expliquerons ultérieurement, Algo’Tech comme beau-
coup d’autres entreprises, se doit de trouver des solutions innovantes pour res-
ter en course. Dans nos travaux ci dessous, et à travers le cas de l’entreprise
Algo’Tech, nous proposons une approche centrée sur la dimension humaine
et l’organisation cognitive des connaissances, aﬁn d’extraire les connaissances
métier de documents techniques.

Mémoire et connaissances dans la fonction de rai-
sonnement

Le raisonnement fait partie des fonctions exécutives chez l’humain. Il s’agit
d’un processus cognitif de haut niveau qui permet à un individu de s’adapter
à son environnement en fonction des informations perçues, de son expérience
et de son objectif [Dubuc, 2002]. Cela permet par exemple de faire face à des
situations complexes, d’interagir avec son environnement, ou encore d’établir
des plans. Lors du processus de raisonnement, deux types de connaissances
sont sollicitées : les connaissances explicites et implicites [Dienes et Perner,
1999].

Les connaissances explicites [Smith, 2001; Dienes et al., 1991] sont l’en-
semble des connaissances qu’un individu peut mémoriser et récupérer consciem-
ment puis exprimer par le langage. Elles sont souvent apprises sous forme
verbale. Elles sont associées à la mémoire explicite (ﬁgure 1) qui concerne l’en-
semble des événements biographiques (mémoire épisodique) et la mémoire des
concepts et des objets (mémoire sémantique) [Squire, 1992; Squire et Zola,
1996]. Ces connaissances, formalisables et transmissibles, sont exprimables par
l’individu et peuvent donc être stockées sur un format extérieur (document,
rapports, etc.). Elles sont considérées facilement extractibles [Nonaka et Ta-
keuchi, 1995].

2

Ikram Chraibi Kaadoud

Introduction générale

Les connaissances implicites sont non exprimables et sont celles dont l’in-
dividu n’est pas conscient [Schacter, 1987]. Elles sont liées à la mémoire im-
plicite (ﬁgure 1). L’apprentissage de telles connaissances est implicite : c’est
par l’imitation, la pratique, le vécu et l’expérience, qu’un individu acquiert ce
type de connaissances. Elles regroupent les connaissances procédurales (liées
par exemple aux capacités motrices comme faire du vélo), celles liées à la mé-
moire perceptive (mémoires des formes, des couleurs, des odeurs, des sons),
celles des réﬂexes et des apprentissages non-associatifs comme l’habituation et
la sensibilisation [Squire, 1992; Squire et Zola, 1996]. Ces connaissances sont
importantes car elles sont associées au vécu d’une personne et ont une incidence
directe sur le raisonnement. Dienes et Berry [1997] présentent la connaissance
implicite comme inﬂexible, inaccessible et liée aux caractéristiques du matériau
(i.e. la nature de l’information) utilisé lors de l’apprentissage.

Figure 1 – Une taxonomie des diﬀérentes mémoires. Image adaptée de Squire
et Zola [1996]

Comme nous le verrons par la suite, la compréhension de ce type de connais-
sance, son acquisition et son extraction ont fait et font encore l’objet de
nombreux travaux de recherche. Il existe notamment un paradoxe dans cette
connaissance implicite, qu’il convient de souligner : c’est à la fois la plus recher-
chée (liée à l’individu, elle porte plus d’information que des données [Malhotra,
2000; Crié, 2003], mais c’est aussi la moins accessible. Or dans le domaine de
l’extraction de l’expertise, qui nous intéresse dans le cadre de nos travaux, c’est
la gestion de l’ensemble des connaissances implicite et explicite qui représente
un axe stratégique pour les entreprises. L’extraction de l’expertise est d’autant
plus complexe qu’il existe trois stades d’acquisition des compétences (que nous
détaillerons ultérieurement) en fonction desquels, les experts peuvent ou non
verbaliser leurs connaissances [Hoﬀman, 2014]. Ainsi avant de décrire plus en
profondeur cette problématique d’extraction de l’expertise, il faut d’abord s’in-
téresser à la mémoire implicite et l’apprentissage implicite dans leur ensemble.

Extraction de règles de réseaux récurrents

3

Mémoire et apprentissage implicite

Nous les présentons plus en détail dans la section suivante.

Mémoire et apprentissage implicite

La mémoire implicite se distingue de la mémoire explicite d’une part au
niveau de l’apprentissage utilisé pour acquérir les connaissances et d’autre part
au niveau de l’utilisation de ces connaissances. La majorité des preuves com-
portementales d’un système de mémoire implicite est basée sur des travaux
dans lesquelles l’expérience conduit à une modiﬁcation des performances sur
une tâche donnée sans que les participants soient conscients d’avoir appris quoi
que ce soit [Ettlinger et al., 2011].
Le langage est un exemple typique d’apprentissage implicite : c’est par la pra-
tique (répétition et imitation) que le langage s’apprend chez l’enfant (langue
maternelle), et non par l’apprentissage explicite des règles grammaticales [Ou-
deyer, 2009]. Ce phénomène d’apprentissage implicite a été étudié pour dif-
férentes tâches appliquées à diﬀérents types d’informations (grammaire ou
musique) et diﬀérentes populations (adulte et enfants)[Reber, 1967; Servan-
Schreiber et al., 1988; Cleeremans et McClelland, 1991; Cleeremans, 1993;
Gasparini, 2004; Gombert, 2006; Nadeau et Fisher, 2011; Reber, 2013].

Witt [2010b], qui a réalisé une revue des diﬀérentes déﬁnitions proposées à
ce phénomène, suggère ainsi de considérer que le terme d’apprentissage impli-
cite recouvre toutes les formes d’apprentissage qui opèrent à l’insu du sujet,
sans que ce dernier soit conscient du fait qu’il est en train de modiﬁer de ma-
nière stable son comportement. Il s’appuie pour cela sur la déﬁnition de Per-
ruchet et Vinter [1998]. Selon eux, "l’apprentissage désigne un mode adaptatif
par lequel le comportement des sujets se montre sensible aux caractéristiques
structurales d’une situation à laquelle ils ont été préalablement exposés, sans
que l’adaptation qui en résulte soit due à une exploitation intentionnelle de la
connaissance explicite des sujets concernant ces caractéristiques".
Les circonstances de cet apprentissage sont considérées accidentelles plutôt
que intentionnelles [Berry et Dienes, 1991; Dienes et Berry, 1997; Perruchet et
Nicolas, 1998]. Considéré robuste et indépendant des mécanismes explicites,
l’apprentissage implicite possède un bon maintien dans le temps, mais ne per-
met pas de transfert des connaissances assimilées à de nouvelles situations
[Gasparini, 2004].

Les travaux relatifs à l’apprentissage implicite dérivent de trois domaines de
recherche distincts : premièrement, la linguistique et psycholinguistique [Re-
ber, 1967], deuxièmement, l’ergonomie [Broadbent, 1977], et troisiémement,
l’étude du rôle de l’attention dans la mémoire et dans l’acquisition d’habiletés
sensori-motrices [Nissen et Bullemer, 1987]. Reber [1967] a été le premier à

4

Ikram Chraibi Kaadoud

Introduction générale

désigner le processus d’acquisition passif et automatique sous le terme d’ap-
prentissage implicite. Il s’est inspiré pour cela des travaux du linguiste Miller et
du psycho-linguiste Chomsky, portant sur les grammaires (ensemble de règles)
à états ﬁnis pour élaborer une tâche standard de grammaire artiﬁcielle nom-
mée désormais, la tâche de Reber. La grammaire représentée sous forme de
graphe orienté se compose d’arcs et de nœuds avec la particularité que chaque
arc est associé à la production d’un symbole. La ﬁgure 2 est un exemple de
grammaire artiﬁcielle. Ainsi, eﬀectuer un parcours de la grammaire artiﬁcielle
selon les arcs dans l’ordre des ﬂèches produit une séquence dite grammaticale
(les séquences non grammaticales ne respectant pas les transitions de la gram-
maire). Dans ces travaux, Reber soumet ces séquences à un panel de sujets,
qui doivent les classer selon qu’elles respectent la grammaire ou non. Les sujets
avaient été informés que les séquences étaient générées par une grammaire. Il
met en évidence que les performances pour cette tâche de classiﬁcation sont
supérieures au niveau de hasard bien que les sujets étaient incapables de jus-
tiﬁer leur décision ou de décrire les règles [Chambaron-Ginhac, 2005].

Figure 2 – Exemple de grammaire artiﬁcielle : B désigne "Begin" et E désigne
"End". Image inspirée de Reber [1967], extraite Servan-Schreiber et al. [1988]

Parmi les nombreux travaux réalisés qui découlent de ceux de Reber, nous
en soulignerons deux : le premier correspond à une tâche comportementale ins-
pirée de celle de Reber et le second consiste à proposer un modèle de réseaux
de neurones mimant le comportement observé. Les deux travaux sont abordés
dans les articles suivants : Servan-Schreiber et al. [1988] et Cleeremans et Mc-
Clelland [1991]. Pour la tâche comportementale, Cleeremans mesure le temps
de réaction mis par les sujets pour faire un choix. Les sujets étaient face à un
ordinateur sur lequel étaient présentés 6 éléments lumineux en ligne horizon-
tale. Ils avaient 6 boutons à disposition, un correspondant à chaque élément
lumineux. La tâche consistait à presser aussi vite et aussi précisément que pos-
sible l’une des six touches correspondantes pour reproduire l’élément présenté.

Extraction de règles de réseaux récurrents

5

Mémoire et apprentissage implicite

Les stimuli étaient générés en utilisant la grammaire à états ﬁnis utilisée par
Reber. Certains d’entres eux n’étaient cependant pas "grammaticaux" et à
chaque essai, il y avait 15% de chances de substituer un stimulus prescrit par
la grammaire, par un autre choisi aléatoirement. Bien que les sujets ignoraient
la nature séquentielle des stimuli, Cleeremans a observé une diminution du
temps de réaction dans cette tâche de choix pour l’ensemble des sujets. Ces
derniers devenaient ainsi de plus en plus sensibles à la structure séquentielle du
matériel (lorsque la séquence était grammaticale) au cours de l’entraînement.
Tout comme pour la tâche de Reber, les sujets étaient incapables de formaliser
ou d’expliciter un quelconque apprentissage.

Les travaux de Reber et ceux de Cleeremans montrent que les individus
soumis ou observant de manière répétée des régularités temporelles (pouvant
correspondre à ce que l’on appelle des invariants ou des règles) ﬁnissent par
apprendre ces dernières et les incluent aussitôt dans leur raisonnement et leur
interaction avec l’environnement. Le tout sans en être conscient. Cela s’ex-
plique par le fait que l’apprentissage implicite extrait la structure statistique
de l’environnement (ici les séquences) : les mécanismes d’apprentissage im-
plicite produisent donc des performances améliorées à la suite de répétitions
[Reber, 2013]. Par conséquent, il existe chez l’humain une capacité d’apprentis-
sage séquentiel implicite qui se manifeste dès lors que l’individu est soumis de
manière répétée à une même séquence d’événements et qui permet une amé-
lioration de la performance sans que le sujet en soit conscient [Cleeremans,
1993]. Gombert [2006] aﬃrme que "le moteur des apprentissages implicites
est de nature fréquentielle" : la fréquence d’exposition à des éléments associés
est fondamentale pour que le phénomène d’apprentissage implicite ait lieu et
cela indépendamment de l’intention de l’apprenant [Perruchet et Pacton, 2004;
Gombert, 2006; Lété, 2006; Deacon et al., 2008; Nadeau et Fisher, 2011]. Forts
de ces résultats, les auteurs de ces articles ont proposé un réseau de neurones
récurrent simple (SRN pour simple recurrent network), pour comprendre ce
phénomène d’apprentissage implicite. Ce modèle à trois couches, que nous dé-
crirons par la suite dans nos travaux, leur a permis de mettre en évidence
la capacité du réseau de neurones à encoder une représentation implicite des
règles de la grammaire utilisée pour générer les séquences. Le SRN a ainsi été
adopté comme cadre d’étude de l’apprentissage implicite.

La compréhension et l’étude de la mémoire implicite et des connaissances
implicites sont aussi des questions qui se posent dans le domaine de l’expertise.
En eﬀet, l’acquisition de cette dernière implique une évolution des connais-
sances de l’individu du domaine de l’explicite vers l’implicite. Nous allons ex-
pliciter cela dans la section suivante.

6

Ikram Chraibi Kaadoud

Introduction générale

Evolution des connaissances chez l’expert

Un expert, en général, est une personne sur-entrainée, qui a acquis des
habilités particulières par son expérience et qui a notamment connu une mi-
gration de ses connaissances explicites à un savoir-faire implicite (ﬁgure 3) . Par
exemple, un conducteur expérimenté ayant plusieurs années de conduite der-
rière lui est un expert en conduite par rapport à un jeune conducteur qui vient
de décrocher son permis. Ce dernier fera appel à ses connaissances explicites du
code de la route et du fonctionnement global de la voiture (positionnement des
vitesses, positionnement des diﬀérentes pédales du véhicule, etc.) pour utiliser
son véhicule. A contrario, le conducteur expérimenté conduira son véhicule de
manière plus automatique ayant développé des habitudes de conduites [Ma-
thern, 2012]. De nombreux travaux en psychologie considèrent ainsi l’expert
comme un individu possédant un savoir-faire (connaissances et compétences
particulières liés à ses expériences) et des ressources tacites par rapport à un
novice [Coulet, 2014].

Figure 3 – Evolution du temps nécessaire à la résolution de tâche au fur
et à mesure de la montée en compétence (pratique) selon trois étapes, basée
sur la théorie de Fitts et Posner [1967], Anderson [1983],Rasmussen [1986] et
VanLehn [1996]. Image adaptée de Kim et al. [2013]

Hoﬀman [2014] décrit 3 étapes d’acquisition de compétences (i.e de montée
en expertise), proposées en premier lieu par Fitts et Posner [1967] et mises à
jour par Anderson [1983] et Anderson [1987]. Le premier stade est qualiﬁé de
stade cognitif : les individus cherchent et récoltent des connaissances explicites
de plusieurs sources sur le domaine considéré. Dans le cadre de la conduite, ces
sources explicites sont le code de la route et les cours dispensés par une auto-
école. Pour réaliser une tâche, l’individu sollicite ses connaissances explicites
issues de sa mémoire à long terme en rapport avec la tâche à accomplir, sur

Extraction de règles de réseaux récurrents

7

Evolution des connaissances chez l’expert

lesquelles il applique des procédures générales (i.e. des procédures qui peuvent
être appliquées à n’importe quel domaine). Lors de cette première phase, les
processus de prise de décision et de résolution de problèmes sont lents, fasti-
dieux et sujets à erreurs [Anderson, 2005].

Le second stade est nommé le stade associatif et intervient au fur et à me-
sure qu’un individu devient de plus en plus compétent dans un domaine. L’uti-
lisation répétée de connaissances explicites dans des situations données abou-
tit à la formation de connaissances procédurales spéciﬁques au domaine. Elles
consistent en l’association directe entre des conditions spéciﬁques du domaine
et l’action résultante, ce qui aboutit à l’apprentissage implicite de règle. La né-
cessité d’analyser des connaissances explicites est progressivement contourné :
lorsque les conditions dans l’environnement correspondent aux conditions de la
règle procédurale, l’action est automatiquement invoquée. Les processus longs
et fastidieux de rappel (souvenir) des connaissances explicites et d’application
de procédures générales à ces connaissances sont ainsi contournés [Charness et
Campbell, 1988].

Le troisième stade est le stade autonome : les procédures deviennent auto-
matisées. Les associations se renforcent, se spécialisent et/ou s’adaptent à des
types particuliers de situations. Les connaissances procédurales deviennent très
rapides et automatiques [Anderson, 1983; Charness et Campbell, 1988; Hoﬀ-
man, 2014]. Durant ce stade, les connaissances procédurales simples deviennent
composées ou remplacées par des connaissances procédurales plus complexes et
inclusives. Ce dernier type de connaissances compressant (contenant) un large
nombre de conditions d’instanciations et d’actions résultantes, induit une perte
de capacité des individus à verbaliser leurs connaissances [Neves et Anderson,
1981]. Ainsi au fur et à mesure que les procédures deviennent composées et
automatisées, la capacité de verbaliser les connaissances liées à la compétence
décroit [Anderson, 2005]. Quand la réalisation d’une tâche devient compléte-
ment automatisée, le traitement ne nécessitant plus de ressources cognitives est
autonome et inconscient (i.e implicite) [Dumais, 1981; Anderson, 1983; Carr
et al., 1982; Logan, 1988; Hoﬀman, 2014]. L’individu est alors un expert dans
la résolution de tâche dans le domaine où il est monté en compétence. Dans le
domaine de l’entreprise, ce processus est aussi pris en compte par Nonaka et
Takeuchi [1995] qui le désigne par le terme d’internalisation (ou appropriation)
des connaissances explicites en implicites (ﬁgure 4). La ﬁgure 3 illustre les trois
stades d’évolution d’un expert et leur impact sur la capacité de verbaliser ses
connaissances.

En résumé, un expert, dans un domaine donné, possède des connaissances et
des procédures implicites qui lui permettent de choisir rapidement et automa-
tiquement la bonne option dans diﬀérents contextes en analysant une situation

8

Ikram Chraibi Kaadoud

Introduction générale

courante en fonction de ses expériences antérieures. Il évite ainsi l’utilisation
d’un processus itératif et laborieux de prise de décision [Bootz et Schenk, 2014;
Bootz, 2015, 2016].

Dans le domaine industriel où la gestion des connaissances est un axe
concurrentiel pour les entreprises (la connaissance est l’équivalent d’un trésor
pour ces dernières), la question de l’expert et de la transmission des connais-
sances représente un double enjeu pour les entreprises : au niveau des collabo-
rateurs internes des entreprises, mais aussi au niveau des clients et les prospects
(clients potentiels). Nous allons présenter dans la section suivante notre parte-
naire industriel Algo’Tech et sa problématique d’extraction des connaissances
implicites. Nous reviendrons ensuite sur la problématique de l’extraction de
l’expertise dans le contexte de entreprise.

Algo’Tech, contexte industriel

Nos travaux s’inscrivent dans le cadre d’une thèse CIFRE en partenariat
avec l’entreprise Algo’Tech. Il s’agit d’un éditeur, distributeur et intégrateur
de solutions logicielles dédiées aux métiers de l’électricité, de la gestion des
documents techniques et de la gestion patrimoniale. Cette entreprise conçoit
des logiciels de dessins et de conception assistés par ordinateur (respective-
ment désigné par DAO et CAO) pour des clients acteurs du bâtiment et de
l’industrie - qui travaillent dans des domaines diﬀérents tels que la santé, l’aé-
ronautique, la fabrication de machines (grues, convoyeurs) - avec pour but de
leur proposer des solutions performantes de plus en plus automatisées.
Elle permet à ses clients qui sont des entreprises, grandes structures ou PME,
de se constituer une base de données centralisant leur expertise métier à partir
de l’étude de précédents dossiers. Cela leur facilite la maintenance, la mise à
jour et l’utilisation de leurs connaissances pour répondre à leurs propres clients
ou pour la transmission en interne (lors d’un recrutement par exemple). Ses
solutions permettent de numériser ou migrer les précédents schémas électriques
aﬁn de les intégrer à la base de données et de les maintenir facilement par la
suite. Les nouveaux schémas des clients sont ensuite générés en prenant en
compte les spéciﬁcités de chaque client : non seulement les composants élec-
triques, le matériel, mais aussi les habitudes de conception.
Actuellement, sur l’ensemble du processus d’extraction, c’est l’extraction du
dernier élément - les habitudes de dessins - qui est le plus laborieux et énergi-
vore en terme de temps et de ressources humaines. Pour chaque client souhai-
tant automatiser la génération de ses schémas électriques, le plus important
est de retrouver son empreinte, (ou signature) dans les nouveaux schémas.
Autrement dit, le but de l’entreprise est que les schémas soient tracés automa-
tiquement, comme leurs clients l’auraient fait manuellement. En eﬀet, chaque

Extraction de règles de réseaux récurrents

9

Algo’Tech, contexte industriel

client est capable de reconnaitre les schémas émis par sa société du fait de
l’agencement des composants électriques. Assimilées à une signature impli-
cite, retrouver ces habitudes de conception dans les nouveaux schémas est une
condition nécessaire pour l’adoption des outils par les utilisateurs ﬁnaux (les
collaborateurs de l’entreprise-cliente).

Pour réaliser l’extraction de cette signature et pour chaque nouveau client,
un collaborateur d’Algo’Tech étudie les anciens schémas électriques, aﬁn de les
analyser et de préparer les bases de données de leurs outils. La diﬃculté ma-
jeure réside alors dans l’agencement des composants électriques. Le domaine
électrique, industriel ou domestique, bénéﬁcie d’un grand nombre de normes
déﬁnies au niveau national mais aussi au niveau international. Cependant, et
aussi surprenant soit-il, il n’y a pas de normes concernant la structure et la
conception des schémas électriques, même si les symboles basiques et les no-
tations restent sensiblement les mêmes. Chaque entreprise de ce domaine est
de fait libre de tracer ses schémas selon sa propre expérience, son expertise et
son savoir-faire. Les collaborateurs d’Algotech doivent donc passer en revue les
schémas électriques, présentés souvent sous la forme de documents PDF, four-
nis par leurs clients et identiﬁer ce qui constitue leurs signatures. Actuellement,
ce processus d’étude et d’analyse se fait manuellement. Plusieurs facteurs ex-
pliquent cela : tout d’abord les documents fournis ne sont pas assez nombreux
pour pouvoir mettre en place une technique de fouille de données automatisée.
Le nombre de PDF fourni par des clients varie énormément en fonction de leur
domaine : de quelques dizaines à une centaine, mais leur nombre reste diﬃcile-
ment compatible avec une approche de type Big Data. Par ailleurs, un schéma
électrique pouvant être dessiné sur plusieurs feuilles au format A4 (chacune
représentant alors une portion du circuit global), le nombre de feuilles, appe-
lées folios, varie aussi entre une vingtaine de pages à plusieurs centaines pour
certains clients. Ainsi il n’y a pas de garantie en termes de volume de données.

Un autre paramètre justiﬁant l’étude manuelle des schémas réside dans
le type de PDF fourni par les clients. Les schémas électriques étaient tradi-
tionnellement réalisés sur machine puis imprimés sur support papier car ce
format est plus simple à annoter pour les techniciens de terrains à chaque mise
à jour des installations physiques (par exemple, la modiﬁcation d’une instal-
lation électrique avec l’ajout de nouveaux composants). Pour une meilleure
gestion de leur projets, de nombreuses entreprises-clientes se sont retrouvées
à numériser leurs anciens schémas électriques papiers. Ainsi, parmi les PDF
transmis à Algo’Tech pour analyse, nombre d’entre eux se composent d’en-
semble d’images numérisées à partir d’anciens schémas. L’exploitation de tels
documents nécessite alors l’utilisation de techniques de reconnaissance optique
de caractères (OCR pour optical character recognition) dont les résultats dé-
pendent fortement de la qualité des images numérisées. En fonction de celle-ci,

10

Ikram Chraibi Kaadoud

Introduction générale

il sera possible d’extraire le texte contenu dans les images du PDF ou non
[Fataicha, 2005; Belaïd et Cecotti, 2006]. Cette qualité d’image étant non ga-
rantie, les documents PDF restent complexe à traiter automatiquement. Enﬁn,
un paramètre important est la nécessité d’extraire à la fois le texte et les sym-
boles des schémas électriques : c’est l’alliance de ces deux informations qui
permet d’extraire la connaissance métier du client : à la fois les habitudes de
conception et les règles électriques.

La mise en évidence des habitudes de dessins nécessite une lecture atten-
tive des schémas électriques. Au sein d’Algo’Tech, le collaborateur en charge de
cette tâche, commence par le premier folio comportant une portion du circuit
et se charge d’étudier la séquence de composants électriques sur chacun des
ﬁls (les ﬁls s’étendant chacun sur un nombre variable de pages). C’est donc
par une étude minutieuse des diﬀérents schémas électriques (agencement des
portions de circuits, agencement des composants électriques, recensement des
séquences de composants électriques) d’un même client, qu’Algo’Tech réussi
à extraire les connaissances implicites et explicites contenues dans les docu-
ments techniques de leurs clients. Pour mener à bien ce processus d’extraction
des connaissances techniques, des réunions d’analyses avec leurs clients sont
nécessaires aﬁn de conﬁrmer, inﬁrmer ou ajuster les informations extraites
par Algo’Tech. Ce processus d’échange verbal amène l’entreprise-prestataire à
faire face à la problématique d’extraction de l’expertise que nous décrirons par
la suite. En résumé, Algo’tech fait donc régulièrement face à deux probléma-
tiques. La première est la compréhension des processus de planiﬁcation et de
raisonnement de leurs clients via l’étude des schémas électriques. La seconde
est d’amener les experts de leurs clients à expliquer leur travail, autrement
dit, de les faire expliciter leurs habitudes de travail implicites. Ce travail est
donc diﬃcile car cela implique d’accompagner les experts dans l’exploration de
leurs propres habitudes et connaissances. Ce processus, est souvent l’opportu-
nité pour le client de faire le point sur son expertise et ses méthodes de travail,
en validant ou réformant certaines d’entre elles.

En quoi l’approche inspirée des processus cognitifs chez l’humain est-elle
pertinente pour répondre à ces problématiques ? Notre hypothèse est la sui-
vante : les schémas électriques et le processus de planiﬁcation (impliquant le
raisonnement) chez l’humain partagent des caractéristiques communes. Par
exemple, lors de la réalisation d’un schéma électrique d’un bâtiment, il est
possible d’approcher le problème selon une vision hiérarchique : d’abord un
concept global (l’alimentation du bâtiment), puis ses sous-concepts (l’alimen-
tation des diﬀérentes salles) et les sous-sous-concepts (l’alimentation des équi-
pements dans chaque salle). Mais il est aussi possible d’adopter une vision
modulaire du schéma et considérer tous les composants électriques relatifs à
l’alimentation, puis tous ceux relatifs à la sécurité et enﬁn ceux relatifs aux

Extraction de règles de réseaux récurrents

11

Extraction des connaissances et de l’expertise en entreprise

équipements. Enﬁn chaque schéma est réalisé en fonction de paramètres impli-
cites (tel les règles et habitudes de travail) spéciﬁques à l’entreprise émettrice :
une même alimentation pour un même équipement peut être représenté de ma-
nières diﬀérentes selon les habitudes d’entreprises diﬀérentes. L’ensemble de ces
éléments permettent de faire un parallèle avec la fonction de planiﬁcation chez
l’humain. En eﬀet, lors du processus de planiﬁcation d’une tâche chez l’humain,
ce dernier peut choisir d’adopter une approche hiérarchique (décomposer une
tâche en sous-tâche et sous-sous-tâches) ou adopter une approche séquentielle
et modulaire de la tâche (en regroupant les sous-tâches de même type puis
en exécutant un type de sous-tâche puis l’autre). Mais surtout, il existe tout
une dimension implicite qui va venir impacter son processus de planiﬁcation
et ses choix : ses expériences, son vécu et ses connaissances acquises. Deux
personnes, mettront en place deux plans d’actions diﬀérents pour atteindre un
même objectif. Ce parallèle entre les spéciﬁcités des schémas électriques et la
fonction de planiﬁcation chez l’humain justiﬁe l’approche proposée dans nos
travaux.

A travers la présentation de l’entreprise Algo’Tech, nous avons soulevé la
question de l’extraction de l’expertise en entreprise. Dans le contexte industriel,
il s’agit d’une problématique omniprésente liée à la pérennité des entreprises
et à leur solidité [Barney, 1991; Prahalad et Hamel, 1999; Cowan et al., 2000].
Nous allons expliquer cela dans la section suivante.

Extraction des connaissances et de l’expertise en
entreprise

Figure 4 – Les quatre transformations des connaissances au sein d’une or-
ganisation d’après Nonaka et Takeuchi [1995]. Image adaptée de Viola [2005]

Une expertise partagée à tous les collaborateurs au sein d’une entreprise,

12

Ikram Chraibi Kaadoud

Introduction générale

garantit le bon fonctionnement de cette dernière et prévient un ralentissement
ou arrêt soudain lorsqu’un collaborateur clé s’absente. Il est donc essentiel
que la raison des choix puisse être accessible et explicite. D’après Nonaka
et Takeuchi [1995], les entreprises créent et utilisent des connaissances selon
quatre moyens illustrés dans la ﬁgure 4. (1) La socialisation (ou le transfert
direct) permet le partage des connaissances tacites par l’expérience (observa-
tion et imitation). (2) L’externalisation (ou explicitation) permet de rendre des
connaissances implicites, explicites par le recueil des savoir-faires des individus
de l’entreprise-cliente. (3) La combinaison consiste à recueillir les connaissances
explicites en les décomposant en connaissances élémentaires, (4) l’internalisa-
tion (ou appropriation) intervient lorsque la connaissance explicite devient une
connaissance implicite par exemple lors de l’acquisition d’une formation suivie
de pratique [Serrour et al., 2006; Viola, 2005; Hoﬀman, 2014]. Dans ce qui suit,
nous nous focaliserons surtout sur les processus d’externalisation selon deux
moyens : un échange directement avec les experts métier du domaine ciblé,
et/ou via l’échange d’une documentation informant sur le domaine, ses procé-
dés, ses règles et ses normes [Ermine, 2001].

Dans un contexte industriel, un expert a pour fonction d’analyser, de
conseiller et de faire des recommandations de façon à améliorer l’eﬃcience
des solutions mises en places. Il est ainsi souvent l’interlocuteur à privilégier
pour s’informer d’un domaine. Capable d’identiﬁer les risques mais aussi les
opportunités liées à son domaine de spécialité, il peut préconiser des plans d’ac-
tions et mener des projets complexes dans son domaine. Deux types d’experts
se distinguent dans un même domaine : l’expert-débutant et l’expert-senior.
Si le premier a acquis son expertise par l’apprentissage d’un enseignement, le
second, pratiquant son métier depuis plusieurs années, a accumulé expériences
de terrain et cas d’études concrets. Si cette distinction est importante à souli-
gner, c’est qu’elle impacte aussi la représentation mentale des experts de leur
métier. Lowe [1993] a démontré l’existence de diﬀérences entre ces deux types
d’experts en utilisant une série de tâches de dessin. Il a ainsi conﬁrmé qu’un
expert-débutant possède une conception qui se concentre plus sur des aspects
superﬁciels et généraux alors qu’un expert-senior, se focalise plus sur la struc-
ture sémantique profonde des éléments. Ces résultats ont été conﬁrmés ensuite
par Aimé et Charlet [2013]. Pour un domaine donné, les experts-débutants ont
une conceptualisation de type plutôt textuelle, ce qui implique des concepts
détaillés, analytiques, contrôlés et de bas niveau, alors que les experts-seniors
ont une conceptualisation plutôt de type essentielle, plus ﬂoue, conceptuelle,
intuitive, utilisant des prototypes et des concepts de haut niveau. Cela implique
donc qu’une même connaissance peut être décrite et représentée diﬀéremment.

Cette diﬀérence de représentation possède surtout des répercussions sur
les processus d’externalisation (processus 2) décrit par Nonaka et Takeuchi

Extraction de règles de réseaux récurrents

13

Application à l’analyse de documents

[1995] (ﬁgure 4). En eﬀet, parmi les moyens classiques d’extraction de l’ex-
pertise, l’entretien entre l’entreprise-prestataire (celle qui fournit le service) et
l’entreprise-cliente (celle qui paie pour la prestation) arrive en tête. Cette ren-
contre, qu’elle soit virtuelle, téléphonique ou physique dans les locaux d’une des
entreprises, est nécessaire pour échanger autour de l’expertise métier. Fonctions
des moyens des entreprises, en terme de ressources ﬁnancières et humaines, ces
processus sont énergivores au niveau ﬁnancier et humain, chronophages et po-
tentiellement porteurs d’erreurs. Pour aider cette passation de connaissances,
l’échange de documents techniques (portant sur l’aspect technique du métier),
emails et compte-rendus est aussi privilégié. Si les deux derniers éléments sont
importants pour apporter des précisions et répondre à des questionnements,
l’information majeure se trouve surtout dans les documents techniques, car ce
sont eux qui représentent des exemples concrets d’application de l’expertise à
des problématiques données. Nous allons décrire dans la section suivante les
moyens utilisés pour extraire la connaissance à partir de ce type de documents.

Application à l’analyse de documents

L’analyse des documents techniques est une partie importante du processus
d’extraction de l’expertise. Ces documents sont souvent conﬁés à l’entreprise-
prestataire pour analyse. La qualité et la quantité de ces documents techniques
impactent fortement la capacité de compréhension du métier ciblé : ils doivent
être exploitables, pertinents, d’actualité (respectant les dernières normes en
vigueur) et surtout en relation avec la problématique ciblée. Ces documents
techniques sont importants car ils sont porteurs d’informations. Il est impor-
tant de distinguer une donnée d’une information à ce stade : d’après Chaudet
[2009] une donnée est un élément brut qui n’a pas encore été interprété et qui
est dénué de contexte, alors qu’une information est une donnée interprétée,
à savoir, mise en contexte. Porteuse d’une sémantique, une information, lors-
qu’elle est comprise est alors considérée comme une connaissance. Ainsi le but
de l’analyse des documents techniques est d’extraire les connaissances qu’ils
contiennent : les données mises en contexte deviennent donc des informations,
qui une fois assimilées et comprises deviennent des connaissances.

Nonaka et Takeuchi [1995], deux experts de la question du management
de la connaissance en entreprise, diﬀérencient deux formes de connaissances :
tacite et explicite. Nous retrouvons ici les mêmes catégories abordées précédem-
ment pour l’organisation des connaissances au niveau cognitif. Dans le domaine
de la gestion des connaissances au sein des entreprises, les connaissances ta-
cites désignent les compétences, les expériences, l’intuition ou encore les secrets
de métiers qu’une personne acquiert au cours de sa vie professionnelle. Leurs

14

Ikram Chraibi Kaadoud

Introduction générale

mises en pratique se fait sans intention et leurs transmissions se fait par l’imi-
tation et l’imprégnation. La connaissance explicite (connaissance formalisée et
transmissible sous forme de documents réutilisables) concerne généralement
les projets, les clients, les fournisseurs et les processus. Il faut souligner cepen-
dant que la complexité majeure dans l’analyse de documents techniques réside
dans la sélection des données : lesquelles sont pertinentes ? lesquelles sont dé-
terminantes ? Selon le volume des documents techniques, la complexité de leur
contenu, diﬀérentes approches de fouilles de données existent. Nous distinguons
deux axes de fouilles des documents techniques : la fouille des images (l’image
mining) [Haddad et Mulhem, 2001; Korczak et al., 2005; Pham et al., 2011] et
la fouille du texte (le text mining) [Cherﬁ et al., 2003; Nassirtoussi et al., 2014].

L’image mining consiste en l’analyse et la reconnaissance d’images. Cette
approche est généralement plébiscitée dans des domaines tels que l’ingénie-
rie mécanique, l’étude des cartes de villes, les ﬁgures dessinées à la main, les
cartes géographiques et les circuits électriques. Ses deux utilités sont la fouille
de grandes collections d’images (exemple : mise en évidence des similitudes
dans des images de satellites météorologiques) et l’extraction d’informations
par l’analyse de grandes collections d’images et de données alphanumériques
associées (exemple : les documents liés à l’imagerie médicale et les données
des dossiers des patients). Pour plus de détails techniques sur les techniques
liées à l’image mining, se référer à Zhang et al. [2001]. Cette approche, bien
que répandue, possède cependant des contraintes qui limitent son adoption
par toutes les entreprises, toutes tailles confondues. Parmi elles, le volume du
corpus d’images et la qualité de celles-ci en sont les principales.

Le second axe de fouille de données applicable à des documents techniques
est le text mining. Cette approche consiste à fouiller le texte extrait des do-
cuments aﬁn d’extraire des connaissances selon un objectif donné [Neto et al.,
2000]. Les domaines d’applications sont par exemple la recherche d’informa-
tion, le ﬁltrage de documents, la sécurité [Gegick et al., 2010] ou encore le
domaine médical [Fang et al., 2008]. Pour utiliser les techniques de text mi-
ning, deux étapes sont nécessaires : l’analyse des données, et l’interprétation de
l’analyse [Tan et al., 1999]. La première étape consiste à analyser les données se-
lon l’objectif à atteindre aﬁn de savoir lesquelles cibler : relation entre les mots,
syntaxe recherchée, etc. La seconde étape consiste à identiﬁer, sélectionner et
extraire les textes correspondant à l’analyse. Par exemple, l’identiﬁcation et la
mise de coté des courriers indésirables dans les emails. Parmi les applications
du text mining, nous soulignons l’importance de la recherche d’information
au sein de documents ou de données selon les besoins d’un utilisateur [Tan
et al., 1999; Mooney et Bunescu, 2005]. Cette approche sera abordée dans nos
travaux.

Extraction de règles de réseaux récurrents

15

Application à l’analyse de documents

Tout comme pour l’image mining, le text mining est un domaine très actif
où de nombreux travaux ont vu le jour ces dernières années [Mostafa, 2013; He
et al., 2013; Nassirtoussi et al., 2015; Goyal, 2017]. La profusion des documents
au format numérique (sites web, emails, contenu numérique, etc.) a engendré
de nouveaux besoins en terme de capacité d’analyse des documents et de rapi-
dité de traitement. La fouille de documents est par extension un domaine très
dynamique [Singhal, 2001; Roche, 2011]. Il existe ainsi d’importants travaux
dans les domaines du text mining et de l’image mining mais séparément. Or
dans le cas de l’étude de documents techniques tels des schémas, le texte et
l’image sont tous les deux porteurs d’informations.

Ces dernières années, de nouveaux axes d’analyse des documents techniques
sont apparus. L’idée étant de ne plus se focaliser sur les données mais sur l’en-
semble des documents : l’analyse syntaxique et l’analyse sémantique [Kembhavi
et al., 2016]. Par analyse syntaxique, nous désignons l’analyse de l’ensemble
des règles régissant les documents techniques et par sémantique, l’analyse de la
signiﬁcation (le sens) des documents. L’analyse syntaxique consiste à mettre
en évidence la structure d’un texte ou document. Le but est de mettre en
évidence l’ensemble des règles de syntaxe combinées dans un texte. Dans le
cas de diagrammes, il s’agira de détection des constituants et leurs relations
syntaxiques. L’analyse ou interprétation sémantique vise à mettre en évidence
le sens du document, son sujet (thématique). Pour un diagramme, cela consis-
terait à mettre en évidence les tâches de cartographie des constituants et leurs
relations avec des entités sémantiques ou des entités réelles (concepts de mots).
Appliquée aux documents techniques, l’alliance de ces deux méthodes permet
de faire de l’interprétation de diagramme. Récemment, Kembhavi et al. [2016]
se sont intéressés à ce problème d’interprétation en proposant une approche
globale des diagrammes, indépendamment du domaine. Ils ont pour cela ex-
ploité un volume très important de données. Leur algorithme d’apprentissage
de Deep learning a appris sur la base de 5 000 schémas. De tels travaux sont
possibles grâce aux techniques de Deep learning et au Big Data. L’alliance de
grands volumes de données avec des algorithmes et machines puissants rend
possible l’interprétation de documents techniques. Or c’est ce qui en repré-
sente aussi les limites d’utilisation : le volume de données utilisé pour l’étude
de Kembhavi et al. [2016] est bien plus important que ceux que la plupart des
PME peuvent prétendre (environ entre 100 et 200 schémas pour leurs clients).

Lorsqu’il s’agit de documents techniques et notamment de schémas et dia-
grammes, les approches traditionnelles d’extraction des connaissances ne sont
pas adéquates : la nécessité d’avoir de gros volumes de données à disposition
ainsi que les moyens de les analyser ont induit la nécessité de rechercher de
nouvelles solutions. Récemment de nouvelles approches ont émergées, notam-
ment celles liées à une ingénierie centrée autour de l’homme : interface homme-

16

Ikram Chraibi Kaadoud

Introduction générale

machine, ergonomie cognitive, management des connaissances, etc. Revenir à
l’humain et comprendre son raisonnement aﬁn de mieux comprendre la ma-
nière dont il le transpose dans les documents et diagrammes techniques semble
être une solution prometteuse.

Contexte informatique actuel

Ces dernières années ont vu naître un engouement croissant pour l’intelli-
gence artiﬁcielle. En 2013, IBM présentait un outil cognitif Watson, inspiré et
mimant le raisonnement humain. Ce dernier a édité un livre de cuisine en 2015.
En 2014, Amazon lance son assistant personnel intelligent nommé Alexa. En
2016, le logiciel de Google DeepMind a battu le champion européen de Go. En
2017, de nombreux constructeurs automobiles tel Tesla ou Toyota, travaillent
sur le concept de voitures autonomes. Ces quelques événements ne représentent
qu’une partie inﬁme de l’ensemble des travaux et tests menés par des labora-
toires et entreprises dans le domaine de l’Intelligence Artiﬁcielle (IA). Nous
souhaitons ici illustrer le contexte informatique actuel dans lequel nous avons
réalisé nos travaux. Nous aborderons les domaines qui nous ont inspirés et
ceux où l’apprentissage séquentiel et l’extraction des connaissances sont im-
portants : les réseaux de neurones, la gestion des connaissances et le langage
naturel.

Face à la montée en puissance des diﬀérentes formes IA, une question re-
vient souvent : que se passe-t-il dans les réseaux de neurones ? Ces IA sont vues
comme des boites noires puissantes en terme de calcul. Capables d’apprendre et
de traiter de très gros volume de données, elles ne "comprennent" néanmoins
pas leurs données. Le fonctionnement de ces réseaux (les algorithmes et les
mathématiques) est connu et compréhensible par la communauté scientiﬁque,
mais il est néanmoins diﬃcile de comprendre ce qui se passe au sein des réseaux
de neurones. Nombreux sont ceux qui s’interrogent sur les raisons des perfor-
mances de ces réseaux. Par exemple, DARPA, l’agence du département de la
Défense des États-Unis chargée de la recherche et développement des nouvelles
technologies destinées à un usage militaire, a lancé son programme "Explai-
nable Artiﬁcial Intelligence" (XAI) pour tenter d’apporter des réponses. Parmi
les travaux qui s’intéressent à ces questions on retrouve notamment ceux de
Geoﬀrey Hinton, un des pères du Deep Learning [Frosst et Hinton, 2017].
"Est-il possible d’observer et d’extraire ce qui se passe dans les réseaux de
neurones ?" est ainsi un exemple de questions auxquelles des travaux, parmi
lesquels les nôtres, tentent de répondre. Ce questionnement sur les IA, fait
écho à celui sur le cerveau humain : que se passe-t-il au sein du cerveau lors
de certaines tâches comportementales ? On observe de nombreuses tentatives
d’expliquer le biologique par l’informatique. Par exemple, le SRN a été proposé

Extraction de règles de réseaux récurrents

17

Contexte informatique actuel

comme cadre d’étude pour modéliser l’apprentissage séquentiel chez l’humain
[Servan-Schreiber et al., 1989], ou encore des algorithmes de Deep learning
ont été proposés pour étudier le cortex visuel [Cherﬁ et al., 2003; Nassirtoussi
et al., 2014].

Autre domaine dynamique, celui de la gestion des connaissances. Comme
souligné précédement, cette dernière est un capital non négligeable pour les
entreprises. L’explosion des volumes de données ces dernières années avec le
Big Data, alliée au développement d’algorithmes de machine learning de plus
en plus performants (réseaux de neurones profonds liés au Deep Learning) ont
précipité une course aux informations. Si la gestion des connaissances est un
domaine stratégique c’est parce qu’il permet la création, le partage, la réutili-
sation et la représentation des connaissances au sein des structures, ce qui est
sensé améliorer les performances et accroitre les innovations. Le développement
rapide ces dernières années du domaine des ontologies (modèle de données re-
présentatif d’un ensemble de concepts et des relations entre ces concepts dans
un domaine)[Ding et Foo, 2002] et celui de la visualisation des données [Wit-
ten et al., 2016] est la preuve de l’importance et l’intérêt grandissant pour
le domaine [O’Neill, 2016]. La recherche d’informations est aussi un domaine
très dynamique. Parmi les domaines d’applications, nous citerons l’analyse de
sentiments à partir de tweets sur internet pour évaluer la réaction du public
pour des ﬁlms ou des événements en tout genre [He et al., 2013; Goyal, 2017]
ou encore les moteurs de recherches tels que Google et Bing [Amini et Gaus-
sier, 2013]. Nous citerons aussi le PDF mining (fouille de documents PDF) : il
s’agit d’un domaine de recherche qui traite de la problématique d’extraire des
informations à partir de ﬁchiers PDF.

Dans le domaine de l’apprentissage séquentiel, un domaine d’intérêt est
celui du langage naturel. De nombreux travaux en recherche et en industrie
s’intéressent à la question de l’interprétation (la sémantique) des textes. Com-
prendre un langage revient à décoder et interpréter des séquences de mots, qui
eux même sont des séquences de symboles. Par ailleurs, chaque langage obéit à
des règles explicites, les règles de grammaires (la syntaxe), mais aussi des règles
implicites (les us et coutumes de chaque région, les expressions, le contexte,
etc.). Cette proximité entre le langage naturel et notre problématique d’ex-
traction des connaissances nous a souvent amené à regarder les travaux de ce
domaine de plus près et à nous en inspirer. C’est donc dans ce contexte infor-
matique riche, que nos travaux s’inscrivent aﬁn de répondre à la problématique
que nous décrivons dans la section suivante.

18

Ikram Chraibi Kaadoud

Introduction générale

Problématique

A l’image de la programmation informatique, le traçage des schémas élec-
triques est un domaine normé où il existe des règles établies. Pourtant dans le
premier cas, il a été mis en évidence qu’il était possible de déterminer préci-
sément l’auteur d’un code informatique en étudiant les caractéristiques impli-
cites contenues dans ce dernier [Rosenblum et al., 2011]. Dans le domaine des
schémas électriques, nous savons qu’il existe une connaissance (l’expertise en
l’occurence) implicite au vu des connaissances extraites par les collaborateurs
Algo’Tech. Notre hypothèse est qu’il est possible d’extraire les connaissances
implicites dans les schémas électriques d’un client donné, parmi lesquelles les
règles de connectivité à partir des séquences de composants électriques. Nous
nous focaliserons ainsi dans nos travaux sur cela et présenterons notre approche
pour expliciter ces règles implicites.

D’un point de vue scientiﬁque, la question qui se pose est celle de l’extrac-
tion sous un format explicite de connaissances qui ont été apprises implicite-
ment. Comme nous l’avons vu précédemment, connaissance et expertise sont
des notions transversales à de nombreux domaines de la recherche : cognition,
knowledge management, IA et data mining. Répondre à la question posée, né-
cessite ainsi d’aborder précisément chacune de ces notions dans chacun de ces
diﬀérents domaines, aﬁn de déterminer les possibles ponts que l’on peut établir
entre eux. Autrement dit cela revient à s’intéresser, selon diﬀérents axes, au
cerveau en tant que modèle capable de manipuler et gérer à la fois l’informa-
tion explicite et implicite, ainsi que de gérer l’apprentissage de séquences, et
enﬁn d’extraire des régularités (règles) à partir de séquences.

Le premier axe, à partir duquel nous avons puisé nos connaissances pour
étayer notre étude, est celui de la cognition : La question posée est celle de l’or-
ganisation et de l’évolution des connaissances liées à l’expertise d’un individu
au fur et à mesure de sa pratique. Plus précisément, au niveau de la mémoire,
c’est la capacité cognitive du cerveau à apprendre des régularités (règles) à par-
tir de plusieurs séquences qui a été étudiée (mémoire implicite, apprentissage
implicite et séquentielle). Le second axe d’étude est celui du knowledge mana-
gement : la question était celle des moyens existants pour identiﬁer et extraire
les connaissances dans le but de les partager. Au niveau humain, l’organisation
de l’expertise métier en entreprise et les moyens de l’extraire est une question
qui s’est posée dès le début de nos travaux. Au niveau informatique, dans un
premier temps, parmi les moyens existants dans l’IA notamment ceux relatifs
au data mining, c’est la question des données qui s’est posée : quelles sont les
données à prendre en compte ? et quels sont les moyens envisageable pour les
extraire ? ont été deux questions importantes à soulever, sans oublier celle qui
consiste à se demander comment transformer et représenter des données brutes

Extraction de règles de réseaux récurrents

19

Contributions

en connaissances. Dans un second temps, c’est le sujet de la mise en évidence
de connaissances implicites à partir des données séquentielles que nous avons
souhaité traiter en nous penchant sur les moyens automatiques actuels et fu-
turs qu’il est possible de mettre en oeuvre pour cela.

La transdisciplinarité de la question scientiﬁque de la mise en évidence de la
dimension implicite des connaissances, nous amène à une approche connexion-
niste qui semble donc légitime pour répondre aux questions soulevées dans le
domaine de la mémoire (apprentissage séquentiel) et dans le domaine de l’ex-
traction des connaissances. Plus précisément, l’étude de réseaux de neurones
récurrents, modèle bio-inspiré, semble pertinente. Parmi les modèles réalisant
l’apprentissage séquentiel, ces réseaux de neurones ont été (et sont encore)
l’objet de nombreux travaux visants à extraire les règles apprises qui serviront
de base à nos travaux. Ils véhiculent en eﬀet l’image de "boite noire" (les in-
formations apprises ne sont pas explicites) ce qui rejoint notre problématique
de mémoire implicite non accessible.

En résumé, les trois points importants de notre approche sont :

1. la transformation des données brutes en connaissances et l’identiﬁcation
de celles pertinentes pour l’apprentissage séquentiel : lesquelles sont
porteuses de la connaissance implicite ?

2. le choix du modèle de réseau de neurones récurrent qui permet un
apprentissage séquentiel en gérant les caractéristiques des séquences
(contexte temporel et dépendances séquentielles) quelles que soit leurs
tailles

3. les moyens d’extraire à partir des réseaux récurrents les règles (régula-

rités) implicites contenues dans les séquences apprises

Ce travail de thèse aura une application au contexte industriel pour valider

la transposabilité des travaux à des problématiques industrielles.

Contributions

Pour répondre aux questions soulevées précédemment, nous proposons trois

contributions dans nos travaux.
La première vise à établir un procédé d’extraction et de représentation des
connaissances techniques dans les schémas électriques. Nous décrirons com-
ment construire une représentation selon diﬀérents niveaux d’abstraction à
partir de ces documents techniques. Il s’agira de notre phase de fouille et de
préparation des données.
Notre seconde contribution consiste à proposer un modèle neuronal d’appren-
tissage séquentiel. Nous décrirons particulièrement les diﬀérentes composantes

20

Ikram Chraibi Kaadoud

Introduction générale

des séquences et leurs importances et nous réaliserons une comparaison entre
diﬀérents modèles de réseaux de neurones.
Enﬁn, notre troisième contribution concernera l’extraction des règles encodées
dans un modèle neuronal. Nous présenterons les diﬀérentes étapes qui visent
à l’extraction et la représentation de manière explicite des règles implicites
présentes dans des séquences.
Ce travail sera validé et mené grâce à des études expérimentales dont certaines
utiliseront des données fournies par Algo’Tech. A travers ces trois approches,
nous souhaitons ainsi proposer une solution générique pour la réalisation d’un
processus d’extraction des connaissances à partir de schémas électriques.

Nous souhaitons aussi souligner ce que nos travaux de thèse n’ont pas pour
but de faire. L’extraction des règles de connectivité électrique peut laisser pen-
ser que nous souhaitons potentiellement créer une ontologie (i.e. un modèle de
données représentant les connaissances d’un domaine [Teulier et al., 2005]) du
domaine électrique global à partir de toutes les données extraites des diﬀérents
clients (composants et relations entre eux) ou une ontologie du domaine élec-
trique d’un client donné à partir des données extraites de ses schémas, comme
cela a pu être le cas dans des travaux dans d’autres domaines [Aimé et al.,
2010; Charlet et al., 2012; Bouzidi et al., 2010]. Dans les faits, bien que ces
deux idées soient très intéressantes, elles ne correspondent pas à la question
que nous ciblons ici. Notre but est de trouver le moyen d’extraire une ontologie
du métier d’une entreprise-client telle qu’elle l’utilise quotidiennement, avec les
règles métier et ses habitudes de travail.

Structure du document

Dans le premier chapitre, nous présentons l’ensemble du travail de pré-
paration et d’extraction des données réalisées en deux temps. En eﬀet, plu-
sieurs problématiques ont été soulevés : Comment exploiter les documents
PDF au mieux ? Parmi l’ensemble des données extraites, quelles sont celles
qui contiennent l’information implicite recherchée ? et enﬁn comment extraire
les habitudes de travail à partir de plusieurs exemples de séquences ? Pour y
répondre, dans un premier temps nous décrivons l’approche adoptée pour la
mise en évidence de la structure hiérarchique et modulaire des connaissances
contenues dans les schémas électriques. Ensuite, dans un second temps, nous
décrirons le travail réalisé proprement dit : celui qui nous permet d’extraire les
règles métiers explicites et implicites contenues dans les séquences de compo-
sants électriques.

Dans le second chapitre, nous présenterons une classiﬁcation des réseaux de
neurones selon leur prise en compte du paramètre temporel. Nous détaillerons

Extraction de règles de réseaux récurrents

21

Structure du document

surtout deux réseaux de neurones récurrents à couches : le modèle de Elman,
appelé aussi Simple Recurrent Network, qui a une représentation interne et
implicite du temps et les LSTM, qui ont une représentation interne et expli-
cite du temps. Chacun de ces modèles a marqué une rupture avec les modèles
précédents en terme d’architecture ou de performance. Après avoir décrit ces
deux exemples nous aborderons les procédés listés dans la littérature permet-
tant d’extraire les règles encodées dans un réseau de neurones récurrent, après
un apprentissage de séquences.

Dans le troisième chapitre, nous décrirons l’ensemble de nos travaux expéri-
mentaux menés : nous détaillerons nos implémentations des modèles de Elman
et des LSTM, ainsi que leurs performances selon diﬀérentes séquences issues de
diﬀérentes grammaires artiﬁcielles. Nous détaillerons alors le nouveau proces-
sus d’extraction des connaissances encodées dans les séquences à partir d’un
réseau de LSTM. Nous montrerons notamment un exemple innovant mené sur
des séquences électriques issues d’une grammaire électrique adaptée d’un cas
concret pour établir la transposabilité de nos travaux à des problématiques
industrielles.

Enﬁn nous discuterons de l’ensemble de nos travaux, en détaillant notam-

ment les diﬀérents objectifs atteints, et les perspectives qu’ils ouvrent.

22

Ikram Chraibi Kaadoud

Chapitre 1

Extraction et préparation des
données issues de schémas
électriques

Table des matières

1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
1.2 Contexte environnemental et informatique . . . . . . . . . . . . 24
. . . . . . . . . . . . . . . 25
1.3 Les données : les schémas électriques
Structure d’un Folio . . . . . . . . . . . . . . . . . . . . 25
1.3.1
1.3.2 Lecture d’un schéma électrique
. . . . . . . . . . . . . . 27
1.3.3 Les données brutes : les PDF et DXF . . . . . . . . . . . 28
1.4 Première approche : extraction des concepts . . . . . . . . . . . 29
1.4.1 Transformation d’un schéma en un graphe . . . . . . . . 29
1.4.2 Abstraction des graphes par étude de similarité des mots 30
1.4.3 Fusion de nœuds et concaténation de nœuds puis de

graphes

. . . . . . . . . . . . . . . . . . . . . . . . . . . 31
1.4.4 Analyse et résultat . . . . . . . . . . . . . . . . . . . . . 32

1.5 Seconde approche : extraction de l’empreinte implicite par data

mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
1.5.1 Une approche par le contenu . . . . . . . . . . . . . . . . 34
1.5.2 Processus de Data mining : le clustering hiérarchique . . 35
1.6 Analyse des résultats précédents . . . . . . . . . . . . . . . . . . 38
1.7 Présentation du prototype . . . . . . . . . . . . . . . . . . . . . 40
. . . . . . . . . . . . . . . 44
1.8 Conclusion des travaux préliminaires

1.1

Introduction

Dans ce chapitre, nous abordons le contexte industriel plus en profondeur
et nous décrivons les données fournies par l’entreprise ainsi que leurs caractéris-

23

1.2. Contexte environnemental et informatique

tiques. Nous présentons notre première approche qui nous permet de transfor-
mer et représenter ces données brutes en concepts métier. Nous verrons qu’il
est possible de représenter chaque schéma électrique en un graphe unique.
Nous décrirons ensuite nos travaux sur l’extraction de l’empreinte implicite
d’une entreprise-cliente. Nous aborderons pour cela les techniques de cluste-
ring tel que le clustering hiérarchique et les diﬀérentes représentations possibles
des résultats. Nous mettrons ainsi en évidence la structure hiérarchique et sé-
quentielle des composants électriques, et leur importance. Nous présenterons
notamment le prototype réalisé et qui découle des travaux menés dans ce cha-
pitre.

1.2 Contexte environnemental et informatique

La première étape de nos travaux a consisté en la constitution du corpus
de données. Nous avons observé et échangé avec les experts de l’entreprise
Algo’Tech et les entreprises-prestataire. Ces dernières utilisent, pour paramé-
trer la base de données de leurs logiciels, des informations issues des schémas
électriques. Ce travail d’extraction réalisé via un expert-Algo’Tech n’était, à
notre arrivée, pas formalisé : le "pourquoi" et le "comment" étaient donc es-
sentiellement présents dans l’esprit des opérateurs humains (ceux qui tracent
les schémas) et les experts-Algo’Tech (ceux qui les analysent), et le "quoi"
résidait dans les schémas électriques (au format PDF la plupart du temps).
Nous avons donc dû réaliser tout un processus d’observation, d’entretien et
d’analyse des procédés de travail existants aﬁn de savoir quelles étaient les
données sur lesquelles nous devions nous focaliser, et ainsi constituer nos cor-
pus de données. Cette étape était nécessaire, car sans elle, nous n’aurions pas
pu comprendre la complexité du travail d’extraction des connaissances réa-
lisé par les experts de l’entreprise Algo’Tech. Néanmoins nous étions sûr de
l’existence d’une structure implicite dans les schémas électriques, puisque les
experts Algo’Tech arrivaient à formaliser les connaissances extraites dans une
base de données existante structurée. Dans cette section nous allons donc dé-
tailler l’ensemble du travail mené pour la mise en évidence de cette structure
implicite directement à partir des schémas électriques.

Pour cela, nous avons entamé nos travaux en nous intéressant au processus
de gestion de projets informatiques existant au sein de l’entreprise Algo’Tech :
Nous avons pu participer, avec les experts, à des réunions d’analyse des besoins
et d’extractions des connaissances réalisées entre Algo’Tech et leurs clients et
cela sur diﬀérents projets. Dans un premier temps, nous décrirons les schémas
électriques et leur structure. Nous présenterons ensuite l’ensemble du travail
préliminaire visant à extraire les concepts métiers globaux d’un client donné
aﬁn de vériﬁer l’existence de motifs communs dans la structure de diﬀérents

24

Ikram Chraibi Kaadoud

1. PREPARATION DES DONNÉES

schémas d’un même client. Enﬁn nous aborderons le processus de découverte
des connaissances dans les données, avant de décrire d’un point de vue infor-
matique le prototype réalisé.

1.3 Les données : les schémas électriques

Si le domaine électrique est très normé en France, la représentation des sché-
mas reste libre et à l’appréciation de l’opérateur humain qui trace le schéma.
Nous avons donc étudié ces schémas du point de vue des opérateurs humains,
aﬁn de comprendre les codes et les usages qui dictent leur travail. x Dans le
jargon électrique, une page d’un PDF représentant un schéma est nommée un
folio. Nous allons détailler la structure et les informations apportées par ces
documents dans la section suivante.

1.3.1 Structure d’un Folio

Figure 1.1 – Organisation et contenu du folio F\_004 : A - Le titre du folio,
B - Le numéro de folio, C - Les renvois folios, D - Les composants électriques,
E - Le cartouche

Quelle que soit l’entreprise-client émettrice, les folios de diﬀérents schémas
électriques partagent les mêmes caractéristiques sur le fond et la forme. D’un

Extraction de règles de réseaux récurrents

25

1.3. Les données : les schémas électriques

point de vue structurel, un folio se compose de deux parties : le cartouche
(Figure 1.1, label E) et le circuit dessiné.

Le cartouche désigne le cadre extérieur, au bord de la feuille A4. Il permet
de délimiter la zone de dessin, de fournir une échelle de mesure (en haut dans
Figure 1.1) et d’apporter un complément d’information textuelle sur la por-
tion de circuit représenté. C’est au niveau du cartouche que l’on retrouve par
exemple, le logo de l’entreprise, le nom et numéro du projet, ainsi que le titre
du folio et le numéro de folio. Le titre du folio permet d’apporter une infor-
mation sémantique sur ce que représente la portion du circuit. Le numéro de
folio se compose d’une partie alphabétique et une partie numérique. La partie
alphabétique dépend des usages en entreprise (exemple : Folio 004, F\_004).
La partie numérique encodée sur 3 chiﬀres, est plus standard et s’incrémente
tout au long des pages d’un schéma électrique, mais pas toujours de manière
continue (par exemple, la page 5 peut être numérotée F\_004 et la page 6,
F\_200, selon les conventions en vigueur dans l’entreprise-cliente ).

Le circuit représente schématiquement une portion d’une installation élec-
trique réelle : les ﬁls (câbles) électriques sont généralement désignés par des
lignes droites sur le folio (en rouge et violet sur la ﬁgure 1.1) et les composants
électriques sont représentés par des symboles et des noms appelés repères.
Dans la ﬁgure 1.1, les repères des trois composants électriques sont de gauche
à droite : Q100, Q101 et Q102. La structure des repères dépend beaucoup
des conventions de nommage en vigueur dans l’entreprise cliente. Ils se com-
posent, toutefois, souvent d’une partie alphabétique (désignant l’appartenance
d’un symbole électrique à une catégorie) et d’une partie variable (chiﬀres et
symboles). L’alliance des deux permet de créer un repère unique pour le sym-
bole électrique sur toute l’installation. Parmi les autres indications textuelles,
on retrouve aussi les numéros ou identiﬁants de chacun des ﬁls (en bleu sur
la ﬁgure 1.1), ainsi que des informations sur leur diamètre (XX mm2). Les
renvois folios complètent le dessin du circuit électrique en donnant une infor-
mation sur la connectivité des diﬀérentes portions et donc des folios. Si dans
le ﬁchier PDF, deux portions peuvent être dessinées sur deux folios diﬀérents,
dans la réalité il n’y a pas d’interruption physique dans le ﬁls électrique lui-
même. Un folio peut aussi être décrit comme deux ensembles d’informations
complémentaires mais distincts, d’une part se trouve le tracé du circuit (sym-
boles et ﬁls électriques), et d’autre part se trouve l’ensemble des indications
textuelles (nom des équipements et les indications sur les ﬁls électriques). Ces
ensembles peuvent être présentés, chacun indépendamment de l’autre, à un
expert client du domaine qui reconnaitra alors la partie du circuit représenté,
car il existe dans le tracé et dans le texte de chaque folio une partie de la
connaissance globale du domaine du client.

26

Ikram Chraibi Kaadoud

1. PREPARATION DES DONNÉES

1.3.2 Lecture d’un schéma électrique

La lecture d’un schéma électrique se fait folio par folio en suivant des ren-
vois folios. Les usages les plus communs en schématique font que le premier
folio représente l’alimentation de l’installation électrique et les derniers repré-
sentent les équipements électriques. En suivant les ﬁls et les renvois folios, il est
possible de lire le schéma et de connaitre quel câble alimente quel équipement
en passant par quel composant. En d’autres termes, c’est en suivant les ﬁls
électriques que l’on arrive à extraire une séquence de composants électriques.

Comment lit-on un renvoi folio ? La lecture de la ﬁgure 1.1 informe de la
présence de six renvois folios : trois indiquent 005-02 et trois autres indiquent
200-03. Les trois premiers renvois folios indiquent que les ﬁls électriques conti-
nuent dans le folio 005 (F\_005) et que leur représentation commencera, sur ce
folio, colonne 02 selon l’échelle de mesure représentée (règle située en haut de
la ﬁgure 1.1). De même, les trois derniers renvois folios indiquent une conti-
nuation des ﬁls dans le folio 200, colonne 03.

Figure 1.2 – Détail des renvois folios du folio 004

Figure 1.3 – Agrandissement des renvois folios du folio 200

La diﬀérence entre trois renvois folios similaires réside dans le numéro du
câble associé à chaque renvoi. Ce numéro représente un identiﬁant unique dans
un schéma qui permet d’établir le lien entre chacun des trois renvois folios du
folio 004 et ceux des folio 200 et 005, comme on peut le constater sur les ﬁgures
1.2 et 1.3

Extraction de règles de réseaux récurrents

27

1.3. Les données : les schémas électriques

1.3.3 Les données brutes : les PDF et DXF

Les schémas électriques peuvent se présenter, entre autres, sous deux for-
mats : les ﬁchiers PDF décrits précédemment et/ou des ﬁchiers DXF. Le sigle
PDF, signiﬁant Portable Document Format, est un format utilisé pour pré-
senter et échanger des documents de manière ﬁable et cela indépendamment
du logiciel, du système d’exploitation ou du matériel utilisé pour le générer.
L’ensemble des schémas électriques que nous avons reçus se sont trouvés sous
ce format là. Nous avons donc en premier cherché à exploiter ce type de docu-
ment. Lorsque le format le permettait, il s’avère qu’il nous a été facile d’extraire
l’entièreté du texte présent dans les folios des documents PDF. Après analyse,
et pour chaque client, nous avons choisi d’extraire les informations suivantes
de chacun des folios des schémas électriques :

— les titres des folios,
— les numéros des folios,
— les renvois folios
— les repères.

Le signe DXF, signiﬁant Drawing eXchange Format, est quant à lui un
format d’échange ouvert standard de plans. Créé par la société Autodesk, il
permet l’interopérabilité entre diﬀérents outils de conception assistée par ordi-
nateur. C’est un format considéré ouvert car il s’agit en fait d’un ﬁchier ASCII
qui peut être lu par n’importe quel éditeur de texte et qui est organisé par
sections (HEADER, BLOCKS, ENTITIES, etc.). Un ﬁchier DXF est donc une
traduction ASCII d’un schéma électrique : en analysant les blocs et leurs at-
tributs, il est possible d’extraire des données et méta données (coordonnées,
attributs, nom du bloc, etc.) relatives aux composants électriques.

Ainsi si le texte extrait d’un folio de PDF permet d’avoir le nom de com-
posant électrique par exemple, l’analyse du DXF permet de fournir les coor-
données (x, y) au sein du folio. C’est cette information qui permet l’analyse du
contenu que nous décrirons plus loin dans la section 1.5. La lecture et l’inter-
prétation d’un schéma électrique nécessite de prendre en compte le texte et les
symboles électriques utilisés. Comme notre souhait est d’explorer la valeur sé-
mantique du texte et son importance dans la représentation des connaissances
des dessinateurs, nous avons choisi d’opter pour une alliance de l’analyse des
PDF et des DXF. Cette dernière permet ainsi une analyse quasi-complète des
schémas électriques et une interprétation plus exhaustive sans avoir à utiliser
des techniques de reconnaissance d’image, qui est un sujet complexe en soi.

28

Ikram Chraibi Kaadoud

1. PREPARATION DES DONNÉES

1.4 Première approche : extraction des concepts

Dans cette section nous présentons les travaux préliminaires réalisés aﬁn
de mettre en évidence l’organisation sémantique des schémas, représentation
abstraite du métier du client présente dans l’esprit des experts du domaine.
A travers la fouille et l’analyse des textes issues des schémas, nous mettons
en évidence les concepts métiers, ainsi que les relations entre eux. Ces travaux
ont été menés sur des schémas électriques d’un même client ayant été réalisés
sur une période d’une dizaine d’années.

1.4.1 Transformation d’un schéma en un graphe

En nous basant sur les informations structurelles d’un folio (les renvois fo-
lios) nous avons transformé chaque schéma électrique en graphe. Lors de la
lecture d’un schéma électrique, un expert suit les ﬁls électriques et les renvois
folios aﬁn de naviguer d’un folio à un autre. Nous avons mimé algorithmi-
quement ce raisonnement : Pour chaque folio de chaque schéma, le numéro
de folio est désigné comme identiﬁant d’un nœud et le titre du folio comme
étiquette du même nœud. Ensuite les renvois folios sont utilisés aﬁn de créer
des liens entre les diﬀérents nœuds. On obtient ainsi un graphe représentant, à
un bas niveau d’abstraction, le schéma électrique d’origine. Nous appellerons
ce graphe, "graphe original" dans la suite. La ﬁgure 1.4 présente le graphe
original d’un schéma client que nous qualiﬁerons de schéma A. Cette nouvelle
représentation des schémas électriques a permis d’une part, de transformer un
PDF linéaire de plusieurs pages en une seule représentation tenant sur une
seule feuille, et d’autre part, elle a permis l’identiﬁcation de motifs au sein
de plusieurs schémas d’un même client. En comparant les graphes d’un même
client, on retrouve en eﬀet souvent une organisation similaire des nœuds et des
arcs dans les graphes issus d’une même entreprise-cliente, mais aussi un même
vocabulaire.

Figure 1.4 – Graphe original du schéma électrique A de 43 pages : seuls les
folios comportant des renvois folios et donc des portions de circuits électriques
sont représentées.

Extraction de règles de réseaux récurrents

29

1.4. Première approche

1.4.2 Abstraction des graphes par étude de similarité des

mots

Par habitude, les experts des entreprises-clients dessinent souvent de la
même façon leurs schémas. Ces habitudes se retrouvent aussi dans le vocabu-
laire choisi. Les experts Algo’Tech sont ainsi capables de savoir qui de leur
client a pour habitude de nommer leur premier folio alimentation "Alimen-
tation Générale" ou "Distribution puissance". Pour mettre en évidence ces
habitudes de vocabulaire et s’en aﬀranchir, nous avons procédé à l’analyse des
titres des folios. Pour chaque schéma, après avoir extrait l’ensemble des titres,
nous avons créé un sac de mots sur lesquels nous avons appliqué les ﬁltres
suivants, en utilisant des expressions régulières : les noms des équipements tel
que MV1, les mots de liaison de la langue française (de, des, l’, a, etc.), ainsi
que les chaines numériques ayant été éliminés, seules les chaines de caractères
alphabétiques de plus de 4 symboles ont été conservées. Une fois les sacs de
mots ﬁltrés obtenus (un sac de mot par schéma électrique) nous avons calculé
l’intersection entre eux aﬁn d’obtenir les mots communs. Le résultat obtenu
présenté à un expert métier Algo’Tech et comparé aux titres issus des schémas
client, nous a permis de déduire que ces mots communs constituent les piliers
de la logique métier du client, et donc les concepts globaux de son métier. En
d’autres termes, ce sont les termes qui sont habituellement les plus utilisés chez
l’entreprise cliente. L’étape suivante a été d’utiliser ces mots communs sur les
graphes générés précédemment aﬁn d’étudier les relations sémantiques entre
les concepts, mais tels qu’ils sont présents dans chacun des diﬀérents schémas
électriques. Nous avons donc mis en place un dictionnaire python (liste dont
les index sont des clés, à savoir des valeurs autres que numériques) dans un
ﬁchier JSON contenant pour chaque concept global, les déclinaisons et syno-
nymes possibles, comme présenté ci-dessous :

’ALIMENTATION’ : [’ALIMENTATION’, ’ALIMENTATIONS’, ’DISTRI-

BUTION’, ’PUISSANCE’] ’BORNE’ : [’BORNE’, ’BORNES’]

Dans l’exemple ci-dessus, les premiers mots sont les clés (ce sont les concepts
métiers récurrents d’un client), alors que ceux entre " [ ] " sont les valeurs
proches du mot commun. Une intervention des experts Algo’Tech a été néces-
saire à ce niveau-là pour notamment déterminer les synonymes et valider les
termes techniques. L’annexe B présente un exemple de dictionnaire de concepts
métiers complet. Nous avons ensuite parcouru chacun des graphes d’origine et
en appliquant le dictionnaire, remplacé chaque titre de folio, par le mot com-
mun qui lui correspond. Grâce à cette approche, nous avons créé un graphe
modiﬁé, copie du graphe original, pour chaque schéma électrique, où la dif-
férence réside dans les étiquettes des nœuds. La ﬁgure 1.5 présente le graphe
modiﬁé du schéma A obtenu avec ce procédé. Nous avons ainsi obtenu un en-

30

Ikram Chraibi Kaadoud

1. PREPARATION DES DONNÉES

semble de graphes avec des étiquettes (labels des nœuds) standardisées.

Figure 1.5 – Graphe modiﬁé du schéma électrique A de 43 pages : Les éti-
quettes des nœuds ont été remplacées grâce au dictionnaire des mots communs.

1.4.3 Fusion de nœuds et concaténation de nœuds puis

de graphes

Uniformiser les étiquettes a pour but de fournir des graphes avec une base
commune : leurs étiquettes. Le but recherché lors des développements est de
pouvoir obtenir, à partir de chaque graphe modiﬁé, un graphe plus abstrait
représentant la logique métier globale du schéma électrique de départ.

Pour cela, en premier lieu, à la lecture des graphes modiﬁés obtenus, les
nœuds portant la même étiquette ont été fusionnés tout en conservant les liai-
sons des nœuds originaux. Nous obtenons ainsi un graphe abstrait représentant
un réseau sémantique valide du point de vue du métier du client. A la ﬁn de ce
processus, nous avons obtenu autant de réseaux sémantiques que de schémas
électriques. En reprenant l’hypothèse de départ, à savoir qu’un schéma est une
partie de la connaissance globale du client dans son domaine, chaque réseau
sémantique est alors une instanciation d’un réseau sémantique global. La ﬁgure
1.6 présentant le graphe abstrait du schéma A, est donc une instanciation de
la connaissance globale du client sous la forme d’un réseau sémantique.

En second lieu, nous avons fusionné tous les réseaux sémantiques : autre-
ment dit, nous avons regroupé toutes les instances aﬁn de former le réseau

Extraction de règles de réseaux récurrents

31

1.4. Première approche

sémantique global du client, soit un graphe global de concepts métier. La ﬁ-
gure 1.7 présente un exemple de la fusion de 3 graphes abstraits issus de 3
schéma électriques et leur fusion en un seul réseau sémantique représentant les
concepts métiers présents dans ces 3 schémas et les relations.

Figure 1.6 – Représentation du schéma électrique A de 43 pages sous forme
de graphe de concepts métiers : Les nœuds portant des étiquettes similaires
ont été fusionnés.

1.4.4 Analyse et résultat

Cette première approche bien que simple, a permis de conﬁrmer l’existence
d’une structure cachée dans les schémas électriques d’un même client. Le fait
que l’on ait mis en évidence des répétitions dans le vocabulaire et que l’on
arrive à extraire un graphe global de concept métier à partir des schémas,
conﬁrme l’existence de règles implicites qui dictent la manière de tracer ces
schémas. Par ailleurs, cette approche a aussi permis de regrouper plusieurs
folios de diﬀérents schémas électriques sous la bannière d’un concept métier.
En eﬀet, en réalisant le chemin inverse, à savoir du graphe global de concepts
métier au graphe original, il est possible de constituer des groupes de folios
par concepts tels que présenté dans la ﬁgure 1.8. Ce regroupement est exploité
dans la section suivante. Pour la suite de notre approche, nous avons choisi
d’explorer le contenu de chaque folio : le circuit électrique.

32

Ikram Chraibi Kaadoud

1. PREPARATION DES DONNÉES

Figure 1.7 – Exemple de génération d’un graphe de concept métier d’un
client à partir de 3 graphes de logique métier issus de l’analyse de 3 schémas
électriques (le schéma A est le même que celui des ﬁgures 1.4, 1.5 et 1.6)

Extraction de règles de réseaux récurrents

33

1.5. Seconde approche : extraction de l’empreinte implicite par data mining

1.5 Seconde approche : extraction de l’empreinte

implicite par data mining

Les experts d’une entreprise-cliente sont capables de reconnaitre, en re-
gardant un circuit électrique (folio sans le cartouche), si celui-ci porte leur
empreinte ou non et donc s’il provient de leur entreprise. Cette information
implique qu’il existe, dans le tracé d’un schéma et donc dans l’agencement
des composants électriques sur un folio, une empreinte implicite et donc une
connaissance implicite de l’entreprise-cliente dans ce tracé. Le processus de dé-
couverte de connaissances à partir des données, ou Knowledge discovery of data
(KDD) en anglais, a été décrit par Fayyad et al. [1996]. Itératif et interactif,
ce processus implique de nombreuses étapes nécessitant une prise de décision
de la part d’un opérateur humain. Dans la section suivante, nous utilisons ce
processus pour la mise en évidence de l’empreinte implicite et par extension,
pour découvrir quelles données sont porteuses de cette connaissance.

1.5.1 Une approche par le contenu

La donnée centrale considérée ici est le composant électrique (Figure 1.1, la-
bel D). Les ﬁchiers DXF nous fournissant des informations sur les composants
électriques sous la forme d’un symbole dessiné dans un folio, permettent aussi
d’extraire ses coordonnées (x, y). Ainsi, pour une entreprise-cliente donnée,
pour chaque folio de chaque schéma électrique, nous utilisons du text mining
sur les PDF, pour extraire les repères des diﬀérents composants électriques.
Puis grâce à cette liste de repères, nous extrayons depuis les DXF relatifs
aux PDF, les coordonnées de chacun des composants électriques. Cette opé-
ration réalisée sur chaque folio, permet ainsi l’obtention d’autant de groupes
de composants électriques que de folios, tout schémas électriques confondus.
Puisque c’est l’arrangement interne des composants dans un folio qui nous in-
téresse, comme première étape, nous avons choisi d’ordonner les composants
à l’intérieur de chaque groupe par ordre croissant selon leurs coordonnées (x,
y) respectives (tri par x croissant, puis y croissant). En appliquant le même
procédé à tous les ensembles de composants électriques par folio, il sera ainsi
plus aisé de les comparer et d’étudier leurs similarités. Nous utiliserons à partir
d’ici le terme de séquence de composants électriques (la séquence étant une no-
tion très importante pour la suite de nos travaux) pour désigner un ensemble
de composants électriques ordonnés selon leurs coordonnées (x, y) croissantes
respectives. Ainsi nous obtenons autant de séquences que de folios pour une
entreprise-cliente donnée, ces séquences représentant l’agencement des compo-
sants électriques au sein de chaque folio. Par exemple, dans la ﬁgure 1.1, la
séquence est Q100, Q101, Q102.

Nous avons analysé ces séquences de composants électriques issus de diﬀé-
rents schémas aﬁn de mettre en évidence les motifs existants dans l’agencement

34

Ikram Chraibi Kaadoud

1. PREPARATION DES DONNÉES

des composants électriques. Pour cela nous avons entamé nos travaux par l’ana-
lyse des repères. La comparaison des séquences issues de schémas distincts mais
de folios ayant la même thématique (par exemple les folios Alimentations de
2 schémas diﬀérents) a mis en évidence l’existence de points communs dans
les repères des composants. Ceux nommés avec le même préﬁxe (Q100, M2,
M1T, G2, V1T) représentent des composants ayant les mêmes fonctions et sont
souvent représentés par le même symbole. Nous avons ainsi identiﬁé plusieurs
groupes de repères (ceux commençant par M, ceux commençant par Q, etc)
que nous avons soumis aux experts.

Il existe dans les repères des règles de nommage spéciﬁques à chaque
entreprise-cliente qu’il est aussi possible de mettre en évidence. Nous exploi-
tons l’existence de cette généricité aﬁn d’étudier l’existence de corrélation dans
le contenu de diﬀérents folios. Nous avons ainsi abordé avec les experts-clients
mais aussi les experts-Algo’Tech la notion de famille de composants électriques
et leur signiﬁcation. Ensemble, nous avons répertorié 25 familles (catégories)
de composants électriques. Ces familles sont assez courantes dans le domaine
global des schémas électriques : comme elles ont été déﬁnies une fois, elles
peuvent être utilisées pour tous les schémas dans le domaine électrique. Le
tableau 1.1 présente un exemple de repères de composants électriques et de
leurs familles. La liste des familles de composants est fournie en annexe C.

Désignation de la famille
Générateurs dispositifs
alimentation
Moteurs
Appareils mécanique de connexion
pour circuits de puissance

Préﬁxe associé Exemple de repères

G

M
Q

G2

M1T, M3
Q100, Q1T

Table 1.1 – Exemples des diﬀérentes familles de composants électriques

Nous utilisons ainsi dans les étapes suivantes ces deux éléments - les sé-
quences de composants électriques et la liste des catégories de composants
électriques - pour la construction de nos ensembles de données et le processus
d’exploration de ces données décrit ci-dessous.

1.5.2 Processus de Data mining : le clustering hiérar-

chique

Dans sa description, Fayyad et al. [1996] distingue le processus de décou-
verte des connaissances du data mining : il présente en eﬀet le second comme
une étape du premier et non l’équivalent. Il déﬁnit le data mining comme l’ap-
plication d’un algorithme particulier aﬁn d’extraire des patterns à partir des

Extraction de règles de réseaux récurrents

35

1.5. Seconde approche

données. Il est en eﬀet rejoint par Ramos et al. [2007] qui décrit le KDD comme
un ensemble de 3 étapes :

— Pre-processing : la préparation des données
— Data mining : la fouille des données
— Post processing : l’analyse des résultats
Ainsi, nous allons aborder, dans ce qui suit, nos travaux selon ces 3 étapes.

Preprocessing

Deux traitements sont réalisés sur les séquences : la création de 2 corpus
de données et leur transformation. Le premier corpus de données regroupe
toutes les séquences, indépendamment des schémas électriques d’origines. Le
second corpus de données se compose quant à lui de sous-corpus de données,
chacun regroupant les séquences selon le concept dominant auxquels elles ap-
partiennent. En eﬀet, ayant déterminé précédemment des groupes de folios
selon leur concept métier d’appartenance, (voir section 1.4.4), nous avons re-
pris ces groupes et traité leurs folios de sorte à obtenir des groupes de séquences
selon le concept d’appartenance.

Le second traitement réalisé est celui de la transformation. Ayant déﬁni 25
familles de composants électriques, nous avons traduit chaque séquence en un
vecteur binaire de taille 25, chaque unité désignant la présence ou l’absence
d’une famille de composants électriques dans le folio considéré. Nous obtenons
à la ﬁn de cette partie deux corpus de vecteurs binaires.

Data mining : Clustering hiérarchique

Aﬁn d’observer d’une part les clusters résultants et aﬁn d’autre part, de
mettre en évidence l’empreinte présente dans les séquences, nous nous sommes
inspirés pour cette partie, des travaux de Servan-Schreiber et al. [1988] et de
Servan-Schreiber et al. [1991]. Tous deux utilisent le clustering hiérarchique
aﬁn de mettre en évidence l’encodage d’une grammaire (ensemble de règles et
de patterns) dans un réseau de neurones récurrent qui aurait appris plusieurs
séquences de symboles issues de cette même grammaire.

Le clustering hiérarchique est une méthode d’analyse de clusters qui vise
à créer une hiérarchie de clusters. Ce dernier terme désigne un ensemble de
données qui possèdent une ou plusieurs caractéristiques communes (exemple :
tous les individus de plus de 1,70 m, ou tous les documents qui sont des ar-
ticles de 2016). Le clustering hiérarchique possède deux fonctions importantes :
une distance (distance euclidienne, distance de hamming [Hamming, 1950] ou
distance de Jaccard [Jaccard, 1901], par exemple) et une méthode de liaison
(linkage method en anglais) qui calcule la distance entre deux clusters selon

36

Ikram Chraibi Kaadoud

1. PREPARATION DES DONNÉES

Figure 1.8 – Présentation des étapes de constitution des corpus. Les éléments
qui partagent une même couleur appartiennent au même concept métier. Dans
le 1er corpus, les pointillés indiquent les schémas d’origines dont sont issus les
vecteurs.

la méthode (distance) choisie. Initialement, chacune des données constitue un
cluster (que nous appellerons cluster-simple pour la suite). Ensuite, à chaque
itération, l’algorithme fusionne les clusters les plus proches en un nouveau clus-
ter, c’est à dire ceux qui partagent le plus de caractéristiques communes, selon
les deux paramètres cités ci-dessus, jusqu’à ce qu’il ne reste plus qu’un seul
cluster.

Pour choisir la meilleure combinaison "distance-méthode de liaison", il est
conseillé de calculer le coeﬃcient de corrélation de Cophenetic [Sokal et Rohlf,
1962] : plus sa valeur est proche de 1, plus le clustering préserve les distances
originales. Le résultat d’un clustering hiérarchique peut être graphiquement
représenté sous la forme d’un dendrogramme (la représentation standard) ou
d’un arbre de cluster (un graphe) avec les séquences (les cluster-simples) en tant
que feuille et les clusters obtenus comme nœud. Hees [2015] présente de ma-
nière concise et explicite le clustering hiérarchique et les dendrogrammes. Dans
notre cas, la combinaison ayant donné le meilleur coeﬃcient de corrélation de
Cophenetic est la distance de Jaccard [Jaccard, 1901] alliée à la méthode de
liaison moyenne (average linkage method). Nous avons ainsi appliqué ces para-
mètres sur l’ensemble des données de nos deux corpus et nous avons à chaque
fois représenté cela sous la forme de dendrogrammes et d’arbres de clusters.
La ﬁgure 1.9 représente un dendrogramme résultant du clustering hiérarchique
avec les paramètres cités ci-dessus, et les ﬁgures 1.11 et 1.12 représentent des
arbres de cluster des concepts Alimentation et Equipement. Ces résultats sont
issus de l’analyse de 3 schémas électriques d’une même entreprise-cliente.

Extraction de règles de réseaux récurrents

37

1.6. Analyse des résultats précédents

Post-processing

Pour mieux comparer les clusters obtenus entre le premier corpus de vec-
teurs (toutes les séquences de tous les schémas électriques confondus) et les
clusters des sous-corpus de vecteurs, nous avons aussi choisi de décrire les clus-
ters obtenus par le clustering hiérarchique par des vecteurs binaires. Chaque
vecteur binaire de chaque cluster-nœud, résultant de l’union des vecteurs bi-
naires des clusters-ﬁls. La ﬁgure 1.10 illustre cette opération. Grâce à cette
normalisation, nous avons comparé les arbres de clusters des diﬀérents corpus
entre eux : nous avons comparé les arbres de clusters de chaque concept (sous-
corpus du 2nd corpus de la ﬁgure 1.8) à l’arbre de clusters du premier corpus
(de la ﬁgure 1.8). Ainsi nous avons observé ce qui suit sur les arbres de clusters :
à l’exception de quelques folios, nous avons retrouvé dans l’arbre de cluster du
premier corpus de vecteurs, l’ensemble des arbres de cluster des sous-corpus
de vecteurs du second corpus. Autrement dit, nous avons une concordance
entre les clusters des concepts et les clusters tous folios confondus. De plus,
nous avons observé que des folios dessinés à diﬀérentes périodes (années dif-
férentes), et ne portant pas le même titre se retrouvent groupés avec souvent
une distance de 0 : cela implique que la séquence de composants électriques est
restée inchangé pendant la période séparant le dessin des deux schémas d’ori-
gines. En d’autres termes, les habitudes de dessins sont restées les mêmes au
cours du temps. Enﬁn, l’utilisation d’arbre de clusters, a aussi permis la mise
en évidence de cas particulier (ou d’erreur) dans un schéma électrique, lorsque
par exemple, un cluster-simple de folio est dessiné à un endroit inattendu.
L’analyse des résultats, lors de cette étape a donc permis la mise en évidence
d’une récurrence - et donc d’habitudes de travail - dans l’agencement des sym-
boles électriques. C’est bien dans les séquences de composants électriques des
folios des schémas électriques, que l’on trouve encodée la connaissance liée aux
habitudes de travail de l’entreprise cliente.

1.6 Analyse des résultats précédents

La seconde approche développé dans cette section a permis de conﬁrmer
l’existence d’une empreinte cachée dans les séquences de composants élec-
triques. L’application d’un algorithme de clustering hiérarchique sur des sé-
quences de composants électriques issues de folios diﬀérents a mis en évidence
l’existence de motifs dans ces séquences. Cela est vrai pour des folios issus de
schémas d’une même année, mais aussi pour des folios de schémas avec plu-
sieurs années de diﬀérence. Il existe ainsi des habitudes de représentation qui
sont pérennes au sein des entreprise-clientes.

38

Ikram Chraibi Kaadoud

1. PREPARATION DES DONNÉES

Figure 1.9 – Un dendrogramme, résultat d’un clustering hiérarchique appliqué
sur les séquences issues de 3 schémas électriques d’un même client. Les chiﬀres
présents sur l’axe x identiﬁent les séquences et les chiﬀres sur le dendrogramme
désignent les distances calculées entre les clusters.

Extraction de règles de réseaux récurrents

39

1.7. Présentation du prototype

Figure 1.10 – Portion d’un arbre de cluster résultant du clustering hiérar-
chique : les vecteurs binaires des clusters-nœuds, résultent de l’union des vec-
teurs binaires des clusters-ﬁls.

Il est important de souligner la diﬀérence entre les séquences utilisées dans
cette section et celles qui seront utilisées dans la suite des travaux et d’expli-
quer les raisons de ce choix. Dans le cas présent, les séquences sont générées
grâce aux coordonnées (x,y) de chaque composant électrique, alors que dans
le chapitre 3, les séquences seront générées par la lecture d’un graphe orienté
(ce qui sera détaillé par la suite). Le processus d’extraction des séquences de
composants électriques à partir de la lecture des ﬁls électriques n’ayant pas
été opérationnel dans les temps impartis de la thèse, nous ne pouvions pas
extraire les séquences directement depuis les schémas électrique. Nous avons
donc pris le parti d’exploiter des séquences ordonnées selon les coordonnées
(x,y) des composants électriques pour la mise en évidence de l’empreinte im-
plicite dans la section ci-dessus. Notre but étant ici de souligner et mettre en
évidence l’existence de règles et connaissances implicites au niveau des com-
posants électriques.

1.7 Présentation du prototype

L’ensemble des travaux décrits dans cette section ont été réalisé avec le
langage Python. Aﬁn de permettre un transfert facilité des travaux réalisés
dans le cadre de la thèse, l’ensemble du code réalisé a été regroupé dans un
prototype aﬁn de permettre la réalisation des tests supplémentaires sur diﬀé-
rents clients et prospects par les experts de l’entreprise Algo’Tech sans mon
intervention.

Voici quelques précisions techniques :
— le prototype a été réalisé pour le système d’exploitation Windows
— une installation a été fournie comportant l’exécutable, un manuel d’uti-

lisation ainsi qu’un exemple de test avec les résultats associés.

— le prototype a pour fonctionnalités principales : la fouille de documents
PDF et DXF, l’export des résultats sous Excel, et la mise en place
d’algorithmes de clustering hiérarchique avec pour but de fournir les
dendrogrammes et arbres de clusters associés.

40

Ikram Chraibi Kaadoud

1. PREPARATION DES DONNÉES

Figure 1.11 – Arbre de cluster résultant du clustering hiérarchique sur toutes
les séquences rattachées au concept métier ALIMENTATION pour 3 schémas
électriques d’un même client (2 réalisés en 2010 et un en 2014)

Extraction de règles de réseaux récurrents

41

1.7. Présentation du prototype

Figure 1.12 – Arbre de cluster résultant du clustering hiérarchique sur toutes
les séquences rattachées au concept métier EQUIPEMENT pour 3 schémas
électriques d’un même client (2 réalisés en 2010 et un en 2014)

42

Ikram Chraibi Kaadoud

1. PREPARATION DES DONNÉES

Figure 1.13 – Interface d’accueil du prototype d’extraction des connaissances
réalisé

La ﬁgure 1.13 présente une capture écran de l’interface du prototype et le
tableau 1.2 liste les librairies majeures en python utilisées pour la réalisation
du logiciel. Ce prototype a pour but d’accompagner l’entreprise lors de son
analyse du métier de ses prospects et clients, aﬁn qu’elle puisse plus rapidement
s’approprier le métier, les concepts et les habitudes de représentation de ses
clients. Pour cela, le prototype est actuellement en cours de test au sein de
l’entreprise Algo’Tech aﬁn d’évaluer son utilité sur diﬀérents cas clients (connus
en interne mais aussi nouveaux), ses limites mais aussi imaginer ses futures
fonctionnalités. Il sera évalué en fonction du gain de temps qu’il procure aux
experts-Algotech et l’exactitude des données extraites des schémas clients. Le
manuel utilisateur décrivant l’ensemble du prototype et des fonctionnalités a
été mis en annexe A.

Librairie

Version

Description

TKinter
PDFminer
Graphviz

Pour la réalisation de l’interface

20140328 Pour la fouille de PDF
0.5.2

Matplotlib 1.4.3
0.16.0
Scipy
2.4.0
Openxyl

Pour la représentation graphique des arbres
de clusters sous forme de graphes
Pour la réalisation des calculs
Pour les techniques de data mining
Pour l’export excel

Table 1.2 – Liste des librairies pythons utilisées pour la réalisation du proto-
type : versions et objectifs de chacune d’entre elles

Extraction de règles de réseaux récurrents

43

1.8. Conclusion des travaux préliminaires

1.8 Conclusion des travaux préliminaires

Figure 1.14 – Structure hiérarchique et séquentielle dans un schéma élec-
trique :
A - Séquence de concepts métier,
B - Séquence de folios,
C - Séquences de composants électriques

Grâce aux travaux décrits dans les sections précédentes (1.4 et 1.5), nous
avons mis en évidence l’existence d’une connaissance implicite dans l’arrange-
ment des composants électriques au sein de chaque folio. Celle-ci est à la fois
hiérarchique et séquentielle dans chaque schéma électrique. Lors de la première
approche, la représentation d’un schéma électrique sous la forme d’un graphe
original permet de décliner un schéma électrique sous la forme de plusieurs
séquences de folios liés entre eux par des renvois folios (ﬁgure 1.14, label A).
Cette même approche, en permettant d’abstraire un schéma sous la forme d’un
graphe de concepts métiers globaux permet de transformer les séquences de
folios en séquence de concepts métiers (ﬁgure 1.14, label B). Enﬁn, la seconde
approche permet elle de représenter chaque folios par sa séquence de compo-
sants électrique. Chaque séquence de folios, peut être représentée une suite
de séquences de composants électriques mises bout à bout. Il s’agit du niveau
le plus bas et le plus détaillé pour décrire un schéma électrique (ﬁgure 1.14,
label C). La hiérarchie va donc dans le sens suivant, du plus abstrait au plus
détaillé : du concept métier, au folio, à la séquence de composants électriques.

Ces travaux préliminaires nous ont permis de conﬁrmer l’existence d’une
dimension implicite dans les schémas électriques et de localiser au moins une
partie de cette connaissance au niveau des séquences de composants électriques.
Au niveau technique, le prototype résultant de ces travaux permet de vériﬁer
cette dimension chez d’autres clients et prospects. Des développements sont
encore nécessaires pour avoir une version ﬁnalisée de l’outil (aﬁn qu’il puisse
correspondre à plus de cas clients) et le hisser à un niveau de prototype ﬁni.

44

Ikram Chraibi Kaadoud

1. PREPARATION DES DONNÉES

Pour la suite des travaux, nous avons choisi de nous focaliser sur l’analyse des
séquences de composants électriques puisqu’elles sont porteuses de connais-
sances explicites et implicites, à savoir, les règles de connectivité électriques, et
les habitudes de représentation d’une entreprise-cliente. De plus, étant donné
le niveau le plus détaillé de description d’un schéma, nous pensons qu’elles
peuvent conduire à la reconstruction de la grammaire d’un client. Nous nous
appuyons pour cela sur le domaine spéciﬁque du machine learning qui étudie
l’apprentissage de séquences par des réseaux de neurones récurrent. Nous nous
intéressons notamment à l’extraction des règles cachées dans ces séquences à
partir de ces réseaux de neurones récurrent.

Extraction de règles de réseaux récurrents

45

1.8. Conclusion des travaux préliminaires

46

Ikram Chraibi Kaadoud

Chapitre 2

Réseaux de neurones récurrents :
De l’apprentissage séquentiel à
l’extraction de règles de
grammaire

Table des matières

2.1

Introduction : Des RNN à l’extraction des règles, quelques déﬁ-
nitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
2.2 Le temps dans les systèmes connexionnistes
. . . . . . . . . . . 51
2.3 Deux modèles de RNN : SRN et LSTM . . . . . . . . . . . . . . 59
2.3.1 Le SRN . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
2.3.2 Long Short Term Memory, LSTM . . . . . . . . . . . . . 70
2.4 Extraction des règles et de la grammaire à partir d’un RNN . . 85
2.4.1 Les cinq étapes de l’extraction des règles à partir d’un

réseau de neurones récurrents

. . . . . . . . . . . . . . . 86
2.4.2 Les étapes critiques du procédé . . . . . . . . . . . . . . 90
2.4.3 Grammaire connue versus grammaire inconnue . . . . . . 90
2.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91

L’extraction de règles à partir de réseaux de neurones apprenant des sé-
quences nous conduit à décrire deux éléments : un réseau de neurones récur-
rents (RNN pour Recurrent Neural Network ) qui apprend les séquences et une
extraction des règles encodées par le réseau de neurones. Le choix du premier
est déterminant pour la qualité des résultats du second. La ﬁgure 2.1 présente
les diﬀérentes étapes d’extraction de règles à partir d’un RNN : de l’appren-
tissage de séquences, au processus d’extraction des règles sous la forme d’un
automate. Dans ce chapitre, nous allons déﬁnir dans un premier lieu quelques
concepts importants dans nos travaux. Cette brève introduction nous per-

47

2.1. Introduction : Des RNN à l’extraction des règles, quelques déﬁnitions

mettra d’aborder de manière sommaire les notions essentielles de nos travaux,
avant de les détailler. Nous aborderons aussi la question de la gestion du temps
dans les systèmes connexionnistes et des diﬀérentes méthodes existantes pour
cela. Nous présenterons ensuite les caractéristiques des RNN, avant de décrire
en détail deux modèles qui ont chacun marqué une rupture avec les modèles
antérieurs : le Simple Recurent Network (SRN) et les Long Short Term Me-
mory(LSTM). En dernière partie, nous aborderons la question de l’extraction
des règles et patterns encodés dans un RNN, une fois un apprentissage séquen-
tiel réalisé. Nous passerons alors en revue les diﬀérents moyens d’observer a
posteriori ce qui se passe dans un RNN.

Figure 2.1 – Présentation schématique du processus d’extraction des règles à
partir d’un RNN

2.1

Introduction : Des RNN à l’extraction des
règles, quelques déﬁnitions

Dans cette première partie de ce chapitre, nous allons déﬁnir ce qu’est
une séquence, ce que sont les règles que nous souhaitons extraire. Nous allons
aborder les notions de grammaire formelle et d’automates et expliquer notre
intérêt pour ces notions. Nous déﬁnirons ensuite les notions de contexte et de
dépendances séquentielles, qui constituent les diﬀérentes composantes des sé-
quences en les accompagnant d’exemples. Notre représentation du temps dans
les séquences sera aussi explicitée. Enﬁn, nous présenterons les étapes de notre
approche avant d’aborder dans les parties suivantes de ce chapitre la littéra-
ture existante pour chacune d’elles.

Une séquence est déﬁnie comme une suite d’éléments mis et traités les uns
à la suite des autres. Sa longueur peut être ﬁnie (ex : des chaines de carac-
tères ou des mots) ou inﬁnie (ex : ﬂux), et la position d’un élément dans la
séquence est désignée par son index. Les éléments peuvent être des chiﬀres, des
caractères, des mots, des symboles, etc. Nous souhaitons à travers nos travaux,

48

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

exploiter les séquences de composants électriques, telles qu’elles sont présentes
dans les schémas, aﬁn d’extraire les règles de connectivité électrique (i.e. l’ex-
pertise) qui y sont cachées. Par règle, nous entendons la formulation simple
d’une convention ou d’un principe vériﬁé, comme par exemple les règles de suc-
cession des composants d’une grammaire. A travers l’extraction de ces règles,
nous souhaitons extraire la grammaire formelle de connectivité électrique d’un
client.

Une grammaire formelle est un formalisme qui déﬁnit une syntaxe, en-
semble des règles régissant l’usage d’éléments et qui déﬁnit un langage formel.
Une grammaire formelle se compose d’un ensemble de symboles ("lettres" du
langage) qui sont soit terminaux (aux extrémités) soit non-terminaux, d’un
point de départ et d’un ensemble de règles. Il existe diﬀérentes grammaires
formelles (générale, contextuelle, algébrique et régulière) que nous ne détaille-
rons pas ici. Pour plus d’informations, le lecteur pourra se référer à Chomsky
[1956]. Dans la littérature, une grammaire formelle est souvent représentée
sous la forme d’un automate. Il s’agit d’un graphe orienté composé de nœuds
et de transitions. Les transitions ont toutes un sens : allant d’un nœud source
vers un nœud destination. On fera référence au graphe par le terme graphe-
automate dans les parties suivantes. Une grammaire formelle permet de déﬁnir
un langage formel, à savoir un ensemble d’éléments agencés de telle sorte à
respecter les règles (la syntaxe) de cette grammaire. La lecture des transitions
d’un graphe-automate permet de générer des séquences qui représentent les
instances de ce langage formel [Jacobsson, 2005]. Ces séquences sont qualiﬁées
de grammaticales car leur agencement respecte les règles de la grammaire for-
melle. Dans le cas où une séquence ne respecte pas une grammaire, elle est dite
non-grammaticale. Ces notions de respect ou non des règles sont importantes
et seront notamment utilisées lors du chapitre 3. A ce stade, nous n’irons pas
plus loin dans la déﬁnition de la grammaire et du langage formel. Nous don-
nerons plus de détails par la suite, lorsque nous aborderons les grammaires
utilisées pour nos travaux en chapitre 3.

Si la syntaxe représente la forme des séquences, il est important d’aborder
et de déﬁnir ici la notion de sémantique, le fond des séquences. Le terme de
"sémantique" désigne le sens (la signiﬁcation) qui émerge de l’agencement des
éléments au sein de séquences. En linguistique, la sémantique désigne les si-
gniﬁés (éléments dont on parle) qui représentent la représentation mentale des
concepts associés aux éléments dont on parle [De Saussure, 1989].
La sémantique globale d’une séquence dépend de deux notions. La première est
celle de dépendances séquentielles (l’ordre des éléments). Il s’agit des contraintes
d’intégrité qui déterminent les dépendances entre deux ensembles d’attributs
en termes d’ordre [Golab et al., 2009]. Par exemple les séquences A-B-C et A-C-
B sont diﬀérentes du fait de leur ordre. La seconde notion est celle de contexte

Extraction de règles de réseaux récurrents

49

2.1. Introduction : Des RNN à l’extraction des règles

temporel. Ce terme désigne l’ensemble des éléments passés (aux objets ou en-
tités) ayant lieu avant un élément à l’instant t et qui ont une inﬂuence sur
son interprétation. La distance de l’élément donné en terme de pas de temps
avec les éléments précédents permet de fournir une mesure de ce contexte. Le
contexte temporel peut porter sur l’ensemble des éléments de la séquence :
il peut être local (sur un petit ensemble d’éléments qui se suivent), s’étendre
sur plusieurs éléments, ou encore porter sur les extrémités d’une séquence. En
voici deux exemple ci dessous.

t0

t2
séquence 1 B P X
séquence 2 B T V

t1

Dans ce premier exemple, le contexte temporel est local : Si le but est de
prédire l’occurence d’un élément, la diﬀérence au temps t2 entre X pour la
première séquence et V pour la seconde, découle de la diﬀérence au pas de
temps

1.

−

t0

t5
t1
séquence 1 A C C C C F
séquence 2 B C C C C E

t3

t2

t4

Dans ce second exemple, le contexte temporel s’étend entre les extrémités
d’une même séquence, soit cinq pas de temps. A l’instant t5, la séquence 1
reçoit F si elle a commencé par A au temps t0 et la séquence 2 reçoit E si elle a
commencé par B au temps t0. Dans cet exemple, ce sont les éléments à t0 qui
déterminent l’élément à t5. Nous souhaitons souligner l’importance de la notion
de "contexte temporel" dans notre étude des séquences de composants élec-
triques. Ces dernières obéissent à des règles de connectivité électrique. Le choix
d’un composant peut donc dépendre de l’élément précédent direct, mais aussi
de composants placés au tout début d’une séquence (4 ou 6 folio en amont).
La notion de contexte est très importante dans nos travaux, puisqu’elle est
une partie de la connaissance métier cachée dans les séquences de composants
électriques.

Extraire des règles cachées dans des séquences implique donc d’être capable
de prendre en compte le contexte temporel, tout en détectant les ambiguïtés
et en reconnaissant les symboles. Nous allons aborder dans la section suivante
la gestion du temps dans les systèmes connexionnistes et notamment les diﬀé-
rentes architectures permettant de gérer le contexte temporel. Nous aborderons
ensuite plus en détail la solution que nous avons choisie, à savoir les réseaux
de neurones récurrents.

50

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

2.2 Le temps dans les systèmes connexionnistes

Les réseaux de neurones artiﬁciels (ANN pour Artiﬁcial Neural Network )
ont été introduits comme modèle rudimentaire du traitement de l’information
dans le cerveau humain [Gelly, 2017]. Depuis leur émergence dans les années
40, de nombreuses évolutions et architectures ont vu le jour dans des domaines
très variés tels que la reconnaissances d’image, de la parole, ou encore la tra-
duction automatique. Diﬀérents types d’apprentissage pour les ANN existent
selon la tâche considérée. Nous déﬁnissons ici brièvement trois d’entre eux :
l’apprentissage supervisé, non-supervisé et par renforcement. L’apprentissage
supervisé consiste à utiliser un couple de données (entrée-sortie attendue). Le
réseau ajuste son apprentissage (ses poids) en utilisant l’erreur entre la sortie
fournie et la sortie attendue. On parle d’apprentissage supervisé car le réseau se
voit fournir le résultat attendu. Dans un apprentissage non-supervisé, le réseau
cherche à extraire des corrélations ou régularités parmi les données fournies en
entrée. Il n’a pas d’indication sur le résultat à obtenir. Il modiﬁe ses poids
sur la seule base des entrées reçues. Dans le cas d’un apprentissage par ren-
forcement, le réseau apprend à partir d’expériences à réagir selon diﬀérentes
situations. Son but est de parvenir à optimiser une récompense quantitative
au cours du temps.

Diﬀérentes architecture de réseaux de neurones, utilisant ces diﬀérents ap-
prentissages, ont été conçus pour gérer et intégrer le temps. Ce sont alors des
modèles temporels. Nous allons décrire ci dessous sans être exhaustif, les dif-
férentes catégories de réseaux de neurones artiﬁciels existantes selon leur prise
en compte du paramètre temporel. Pour chacune d’entre elles, nous déﬁni-
rons la classe de ANN, décrirons leur dynamique concernant le traitement de
l’information et fournirons des exemples.

Classiﬁcation des modèles temporels

Pour le choix de la classiﬁcation des modèles temporels, nous nous inspirons
des travaux de Durand [1995]. Bien que datée, nous estimons que la classiﬁca-
tion proposée par cet auteur correspond bien à notre approche du problème de
prise en compte du temps par les réseaux de neurones. Pour un complément
d’information, le lecteur pourra se référer à la thèse plus récente de Khouzam
[2014] qui traite aussi de la question de l’encodage du temps dans les réseaux
de neurones dynamiques. Celle-ci sera brièvement abordée par la suite. Du-
rand [1995] distingue deux grandes classes de réseaux de neurones temporels :
ceux qui représentent la séquence dans sa totalité à l’extérieur du réseau, et
ceux qui intègrent le temps à l’intérieur de leur architecture. Dans le premier
cas, cela revient à un problème de classiﬁcation, car le temps est une donnée
supplémentaire fournie en entrée. Les réseaux impliqués sont acycliques. Dans

Extraction de règles de réseaux récurrents

51

2.2. Le temps dans les systèmes connexionnistes

Figure 2.2 – Présentation de la classiﬁcation des modèles temporels avec
exemple de modèle pour chaque type

le second cas, le réseau est récurrent (comporte des boucles) et encode impli-
citement le temps au fur et à mesure qu’il traite les informations. Dans cette
seconde catégorie il faut distinguer les réseaux de neurones récurrents qui ont
une représentation implicite du temps et ceux qui ont en une représentation
explicite. Les modèles ci-dessous sont énumérés selon cette classiﬁcation.

Modèles à représentation externe : le Percetron et TDNN

Dans ce type de modèle, le temps est traité comme une dimension équi-
valente aux autres. On dit alors qu’il est "spatialisé". Par exemple, pour la
tâche de classiﬁcation de séquences, une séquence évoluant dans le temps sera
représentée par une image. C’est cette dernière qui est fournie au réseau.

Parmi les réseaux utilisés pour cette tâche, nous allons présenter le perce-

tron, et le TDNN (Time Delay Neural Network ).

Le Perceptron simple et Perceptron multi-couches

•

Le Perceptron simple est un réseau dit feed-forward car il propage l’infor-
mation (l’activation des unités) de la couche d’entrée à la couche de sortie
[Rosenblatt, 1958]. C’est un réseau acyclique car son graphe de connexion ne
comporte pas de boucle, et dont la dynamique est déclenchée par la réception
en entrée d’information. Ce réseau se compose de deux couches : l’entrée et la
sortie, ce qui implique une seule matrice de poids [Martinez, 2011].

Un perceptron multi-couches est une extension du perceptron simple. Dési-
gné par le sigle MLP (pour Multi-layer Perceptron en anglais), il est l’exemple
le plus utilisé quand il s’agit de présenter un réseau de neurones acyclique feed-
forward. Un percetron multi-couches est doté de plusieurs couches : l’entrée,
la sortie et une ou plusieurs couches cachées. Si le réseau possède n couches,

52

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

Figure 2.3 – Classiﬁcation de séquences pour les modèles à représentation
externe du temps.
A - Représentation de quatre séquences dans un espace de stimuli en deux
dimensions (x,y).
B - Représentation de quatre séquences dans un espace de stimuli en trois
dimensions (x,y, t), t codant le temps aﬁn d’en dégager une image.
C - Représentation d’une image (portion de B) selon des vecteurs codant pour
des intensités de niveaux de gris. Image extraite de Durand [1995]

Figure 2.4 – Exemples d’un perceptron simple, à gauche et d’un perceptron
multi-couches à droite. Image adaptée de Rougier [2000]

Extraction de règles de réseaux récurrents

53

2.2. Le temps dans les systèmes connexionnistes

−

il possède alors n
1 matrice de poids (une entre chaque suite de couches).
Le MLP à une couche cachée est théoriquement un approximateur universel
de fonctions. En théorie, il suﬃt d’ajouter un nombre de neurones suﬃsant
au niveau de la couche cachée pour approximer n’importe quelle fonction non
linéaire [Gelly, 2017].

L’algorithme le plus répandu pour l’apprentissage d’un perceptron (simple
ou multi-couches) est la méthode de rétro-propagation de l’erreur (error back-
propagation en anglais) de Rumelhart et al. [1985]. Cet algorithme consiste à
partager l’erreur commise par le réseau en sortie à l’ensemble des neurones du
réseau. Il modiﬁe tous les poids du réseau selon la fraction de l’erreur commise
par chaque neurone. Tout le réseau apprend de l’erreur, et non seulement les
couches supérieures. Cet algorithme sera détaillé ultérieurement à l’occasion
de la description du SRN, section 2.3.1. La ﬁgure 2.4 présente des exemples de
perceptron simple et de perceptron multi-couches.

Dans la question de la modélisation temporelle qui nous préoccupe, le MLP
ne possède pas de gestion du temps. Si un MLP doit apprendre la séquence
"1,2,3,4,5,4,3,2,1" en associant chaque élément à son successeur, il ne saura
pas faire la distinction entre les deux occurrences du chiﬀre 3, et ne réussira
donc pas à prédire les chiﬀres 4 ou 2. Néanmoins, il possède la capacité de
généraliser son apprentissage par rapport à des données déjà apprises. Il peut
donc reconnaitre des séquences, qui sont sensiblement similaire à celles déjà
apprises, comme par exemple les données présentées en ﬁgure 2.3, label C, où
le temps est a été spatialisé. La faiblesse de ce réseau réside dans les données
apprises : si celles-ci comportent des distorsions temporelles, alors les vecteurs
présentés au réseau en entrée peuvent être très diﬀérents de ceux appris. Ce
réseau ne peut s’appliquer aux cas où les données sont soumises à de fortes
variations temporelles comme c’est le cas pour la parole.

Time Delay Neural Network, TDNN

•

Le réseau neuronal à retardement ( TDNN ) est un réseau dont le but prin-
cipal est de classer les motifs de façon invariante, c’est-à-dire sans détermina-
tion préalable explicite du début et de la ﬁn du motif. Le TDNN a d’abord été
proposé pour classiﬁer les phonèmes dans les signaux vocaux pour la reconnais-
sance vocale automatique, où la détermination automatique de segments précis
ou de limites de caractéristiques est diﬃcile ou impossible. Le TDNN reconnaît
les phonèmes et leurs caractéristiques acoustiques / phonétiques sous-jacentes,
indépendamment des décalages temporels, c’est-à-dire de la position dans le
temps. Un signal d’entrée est augmenté de copies retardées en tant qu’autres
entrées, le réseau de neurones est invariant au décalage temporel puisqu’il n’a
aucun état interne. Il s’agit d’un réseau multi-couches dont la composante de

54

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

Figure 2.5 – Architecture du TDNN. Image issue de Durand [1995]

base est le MLP. Ce réseau possède la capacité de reconnaissance de décalage
temporel au niveau du signal d’entrée, mais conserve les limitations des MLP
au niveau de la distorsion temporelle. Tout comme pour le MLP, l’algorithme
pour l’apprentissage est celui de la rétropropagation de l’erreur. Une alterna-
tive à ces modèles a été envisagée : elle consiste à fournir aux réseaux un ﬂot
d’information, évènement par événement. Le système fournit ainsi une réponse
au bout d’un temps ﬁni. C’est ce que nous allons voir dans ce qui suit.

Modèles à représentation interne et implicite : le modèle de Hopﬁeld
et les RNNs à couches

Figure 2.6 – Le modèle de
Hopﬁeld. Image extraite de Du-
rand [1995]

Figure 2.7 – Le modèle de Jordan. Image
extraite de Servan-Schreiber et al. [1991]

Les RNNs sont des réseaux de neurones qui comportent un cycle dans leur
graphe de connectivité. Ces cycles permettent au réseau d’entretenir une in-
formation en se l’envoyant à lui même. Cela change la dynamique du réseau de

Extraction de règles de réseaux récurrents

55

2.2. Le temps dans les systèmes connexionnistes

neurones et l’amène à s’auto-entretenir. Ces modèles étaient souvent plébiscités
notamment pour le traitement automatique de la parole, et plus généralement
de séquences, car leurs caractéristiques leur permettent d’apprendre, de stocker
et de prendre en compte l’information contextuelle passée lors de traitement
de l’information à l’instant présent. Dans le cas de RNN bidirectionnels, ils
prennent aussi en compte l’information contextuelle future [Gelly, 2017]. Un
système dynamique est un système dont l’état courant dépend des états précé-
dents. Autrement dit, le passé est codé par des états internes du système. Les
RNNs que nous allons décrire obéissent à cette règle. Nous allons les décrire
et expliquer comment le temps est pris en compte en leur sein.

Le modèle de Hopﬁeld

•

Le modèle de Hopﬁeld [Hopﬁeld, 1982], ﬁgure 2.6, est un réseau qui se com-
pose d’une seule couche où toutes les unités sont interconnectées. Cette couche
représente à la fois l’entrée du réseau et sa sortie. Ce réseau est assimilé à
une mémoire adressable par son contenu : les connaissances mémorisées étant
distribuées dans le réseau et non localisées à une adresse, il est possible de
récupérer l’entièreté d’une donnée, juste en présentant une version dégradée
(partielle ou bruitée) [Rougier, 2000]. Du point de vue temporel, le modèle de
Hopﬁeld est un système dynamique, puisque ses états à t + 1 sont dépendants
des états à t [Durand, 1995]. C’est ce qui fait de lui un réseau temporel. Il est
à noter que le modèle d’Hopﬁeld possède quelques limites : quelle que soit sa
taille, le réseau ne peut stocker qu’un nombre ﬁni d’états stables. Si le seuil est
dépassé, le phénomène d’"oubli catastrophique" apparait : l’ajout d’exemples
supplémentaires conduit à la suppression d’exemples précédemment appris.
D’autres systèmes ont été développé en suivant le même type de formalisme
que Hopﬁeld, il s’agit de réseaux de neurones aléatoires [Samuelides et Cessac,
2007]

Les réseaux de neurones récurrents à couches

•

Les réseaux de neurones à couches sont une catégorie particulière des ré-
seaux de neurones récurrents qui ont pour spéciﬁcité d’être composés de couches
(couche d’entrée, couche(s) cachée(s) et couche de sortie) et apprennent des
couples (d’entrées, sorties) comme les perceptrons. Ces réseaux reprennent à
la fois l’idée de la propagation avant et de la rétropropagation de l’erreur, et
l’idée de la récurrrence de Hopﬁeld [Hopﬁeld, 1982]. L’entrée globale du réseau
se propage à la fois en avant dans le réseau de couche en couche, mais elle
contient aussi une information sur le passé [Martinez, 2011]. Plusieurs archi-
tectures ont été proposées pour ce type de réseau mais nous n’allons aborder
ici que le modèle de Jordan. Nous souhaitons à travers la présentation de ce
modèle illustrer la particularité et l’adéquation de cette catégorie de réseaux

56

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

pour notre tâche.

Le modèle de Jordan, ﬁgure 2.7, a été proposé en 1986 par Jordan [1986].
Son architecture est similaire à celle d’un perceptron multi-couches doté d’une
seule couche cachée et dont les neurones sont tous interconnectés entre les
couches successives. Sa particularité réside dans le fait que sa couche d’en-
trée comporte des unités de contexte. Ces dernières reçoivent à chaque pas de
temps une copie de l’activation de la couche de sortie et une information due
à leur boucle récurrente. Au pas de temps t, le réseau reçoit ainsi à la fois
l’entrée courante de l’instant t et une partie de l’information passée à t
1.
Cette connexion récurrente d’une unité de contexte à elle-même donne une
dynamique (mémoire) individuelle à l’unité. Autre particularité importante à
souligner, les données représentant les séquences sont présentées de
manière ordonnée au réseau. Lors du traitement d’une séquence, à chaque
pas de temps, le réseau reçoit les informations relatives à l’élément courant.
Les connexions récurrentes lui permettent de garder une trace en interne (i.e.
d’encoder un état) qui reﬂète implicitement les évolutions passées du réseau.
C’est donc ce traitement séquentiel des données et la présence de connexions
récurrentes qui permettent au réseau de développer sa représentation interne
et sa gestion du temps.

−

Le SRN est une variante du modèle de Jordan qui possède aussi des unités
de contexte. Ces dernières se distinguent à deux niveaux : elles reçoivent une
copie de l’activation de la couche cachée (et non la couche de sortie comme
pour le modèle de Jordan) et n’ont pas de connexion récurrente. Ce modèle
sera décrit plus en détail par la suite dans la section 2.3.1.

Le comportement d’un RNN peut être assimilé à celui d’un automate à
états ﬁnis [Durand, 1995; Casey, 1998]. Dans le cas des RNN à couches, la
conﬁguration d’activité des unités de contexte en entrée code l’état du système.
L’autre partie de l’entrée, fournit l’information courante. C’est en fonction de
ces deux informations, que le réseau ﬁnit par tomber dans un nouvel état. Ce
comportement mime celui d’un automate à états ﬁnis et permet par le suivi
des éléments reçus à chaque pas de temps de pouvoir reconstituer ﬁnement
l’automate. Servan-Schreiber et al. [1991] ont proposé une application de ce
type de réseau : faire apprendre une grammaire à un RNN à couches via des
séquences de symboles ordonnés, aﬁn d’observer l’automate extrait du RNN.

Modèles à représentation interne et explicite

Les LSTM représentent un modèle qui possède une représentation interne
et explicite du temps. Les LSTM ont été proposés initialement par Hochreiter
et Schmidhuber [1997]. L’inspiration à leur origine était le souhait de trouver

Extraction de règles de réseaux récurrents

57

2.2. Le temps dans les systèmes connexionnistes

un nouveau modèle permettant de pallier les faiblesses des réseaux de neu-
rones récurrents traditionnels. Les auteurs ont proposé un nouveau système
avec une unité complexe, comparable à un automate, dotée d’une circuiterie
interne reliant et exploitant des éléments simples, à savoir des neurones arti-
ﬁciels utilisant la rétropropagation pour leur apprentissage. Le modèle LSTM
se distingue de ses prédécesseurs par une nouveauté : les cellules mémoires.
Ces unités spéciales sont utilisées, en plus d’unités standards (neurone artiﬁ-
ciel standard), au niveau de la couche cachée d’un réseau de neurones, aﬁn de
permettre le maintien d’information en mémoire pour de longues périodes de
temps. Ce sont ces cellules mémoires qui permettent de garder une représen-
tation explicite du passé. Ces modèles seront détaillés dans la section 2.3.2.

Parmi les ressources les plus utiles pour comprendre les RNNs et avoir
une vision globale, ﬁgurent une thèse de doctorat [Gers, 2001] et une revue
portant sur les dernières évolutions des RNN [Lipton et al., 2015]. Ces deux
travaux présentent à la fois les RNNs ainsi que les dernières innovations dans
le domaine des LSTM.

Autre classiﬁcation et autres réseaux de neurones

Dans ses travaux de thèse, Khouzam [2014] aborde la question du temps
dans les réseaux de neurones en s’intéressant à ceux qui peuvent traiter les
séquences temporelles. Il distingue quatre types de réseaux pour cela : les
réseaux feed-forward, des réseaux récurrents, (que nous avons présentés pré-
cédemment), les réseaux dynamiques avec lignes à retard et les réseaux auto-
organisés. Inspirées du cortex cérébral, les cartes auto-organisées de Kohonen
(SOMs)[Kohonen, 1982] utilisent des techniques d’apprentissage non supervisé
possèdant diﬀérentes variantes. Elles permettent de visualiser notamment la
connectivité locale en deux dimensions et de distinguer des groupes dans des
ensembles de données. Cette caractéristique permet le traitement de données
qui possèdent des relations spatiales entre elles (comme par exemple les pixels
d’une image [Martinez, 2011]) ou des données "statiques" (non temporelles).
Le SOM est un réseau de neurones compétitif dont le fonctionnement se base
sur un apprentissage non-supervisé. Chaque neurone est relié à l’ensemble des
entrées. A chaque nouvelle entrée, il y a un neurone déclaré gagnant : le Best
Matching Unit. C’est celui dont la valeur est la plus proche de la donnée pré-
sentée. Ce neurone et ses voisins sont modiﬁés (changement de valeur) aﬁn
de s’adapter au mieux à l’entrée courante. Cette spécialisation de la région de
la carte permettra ensuite à ces neurones de mieux répondre aux prochaines
entrées similaires. En fonction de la distance des voisins au neurone gagnant,
l’impact du changement décroît. La carte est ainsi "déformée" au fur et à me-
sure des entrées aﬁn de s’adapter aux données reçues [Rougier, 2000; Hinaut,
2013]. De nombreuses variantes temporelles des SOMs existent et sans entrer

58

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

dans les détails nous n’en citerons que deux : Temporal Kohonen Map et Re-
cursive SOM [Khouzam, 2014].

Nous sommes conscients qu’il existe diﬀérents angles sous lesquels la ques-
tion de l’encodage du temps dans les réseaux de neurones peut être traitée et
qu’il existe aussi une multitude de réseaux de neurones (certains plus récents)
que ceux abordés ici. Nous n’avons donc pas la prétention d’établir une liste
exhaustive des réseaux de neurones. Notre but est de donner ici les axes de
réﬂexion principaux concernant la question du traitement de séquences.

2.3 Deux modèles de RNN : SRN et LSTM

Parmi les solutions plébiscitées ces dernières années pour l’apprentissage
séquentiel, on trouve le SRN et les LSTM. Le SRN a marqué une rupture
dans les années 90, en faisant partie des premiers réseaux récurrents à couche
prenant en compte une information au cours du temps aﬁn de fournir une
séquence d’informations en sortie. Quant aux LSTM, ils brillent par leurs per-
formances et leur capacité à résoudre des tâches complexes qui n’avaient pas
encore été résolues par les précédents algorithmes de RNNs [Hochreiter et al.,
2001]. Nous allons aborder en profondeur ces deux modèles dans la section
suivante en détaillant leur architecture, les algorithmes d’apprentissage et sur-
tout leur apport dans notre problématique d’apprentissage implicite de règles
à partir de séquences. Le lecteur familier avec le SRN peut passer à la section
suivante car cette partie peut lui sembler simple. Néanmoins au vu du contexte
CIFRE de la thèse et aﬁn d’assurer la bonne transmission des connaissances à
l’entreprise partenaire, nous avons souhaité expliciter en détail le principe et
fonctionnement de ce modèle.

2.3.1 Le SRN

C’est en 1990, que Elman [1990] a proposé un réseau récurrent simple
(SRN, Simple Recurrent Network (SRN) en anglais). Il s’agit d’un RNN à
couches qui représente le temps de manière interne et implicite grâce à ses
connexions récurrentes, qui lui permettent d’encoder le contexte temporel des
entrées [Cleeremans et McClelland, 1991].

Architecture du modèle

Le SRN est un réseau de neurones récurrent qui se compose de 3 couches :
une couche d’entrée, une couche cachée et une couche de sortie. La particularité
du réseau réside dans la couche d’entrée : celle-ci se compose d’unités d’entrée
et d’unités de contexte. Si les premières ont pour fonction de transmettre
une donnée (une lettre, un caractère ou un mot) au réseau à un instant t,

Extraction de règles de réseaux récurrents

59

2.3. Deux modèles de RNN : SRN et LSTM

Figure 2.8 – Réseau de neurones récurrent simple de Elman décrit par Servan-
Schreiber et al. [1988]

les secondes permettent de sauvegarder une trace des représentations internes
du réseau lors des précédentes entrées. Pour cela, dans un SRN, il y a autant
d’unités de contexte que d’unités dans la couche cachée. Lors de la propagation
en avant, à chaque pas de temps, la couche de contexte reçoit une copie du
motif d’activation de la couche cachée.

Ainsi au pas de temps suivant, la couche cachée reçoit un motif global :
concaténation de celui des unités d’entrée et celui des unités de contexte. Au-
trement dit, à chaque pas de temps, la couche cachée reçoit deux informations :
celle de l’instant t (le présent) et celles des représentations internes passées du
réseau dues à l’entrée à t-1.

Au niveau des poids, l’ensemble est modiﬁé lors de l’apprentissage, à l’ex-
ception de ceux de retour (permettant la copie) entre la couche cachée et les
unités de contexte.

Apprentissage du modèle : Algorithme de rétropropagation

Apprentissage et fonctionnement
•

Le SRN apprend des couples (entrée, sortie). Lors de l’apprentissage d’une
séquence, l’entrée du couple correspond à l’élément courant reçu par le réseau à
l’instant t, et la sortie correspond à l’élément suivant que le réseau doit traiter
à l’instant t+1 (voir l’algorithme 1). Pour son apprentissage, et après la phase
de propagation avant où l’information est propagée à travers les couches, le ré-
seau utilise l’algorithme de rétropropagation du gradient pour ajuster ses poids
[Rumelhart et al., 1985]. L’idée derrière cet algorithme est de partager l’erreur
globale en sortie du réseau à l’ensemble de neurones, de la couche de sortie à
la couche d’entrée. Les fractions d’erreurs permettent ainsi d’ajuster les poids
entre les diﬀérentes couches du réseau. Pour plus de détails techniques, Gelly
[2017] réalise une explication approfondie de l’algorithme de rétropropagation.
L’algorithme 1 présente l’algorithme de rétropropagation (aussi présent en an-
nexe D).

A ce stade, il nous faut souligner l’importance de certains paramètres dans

60

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

l’apprentissage du SRN : le taux d’apprentissage et le momentum. Le premier
se déﬁnit comme le pas d’apprentissage du réseau qui détermine la vitesse à
laquelle le SRN apprend. Généralement compris entre 0 et 1, le taux d’appren-
tissage lorsqu’il est proche de 1, permet un apprentissage rapide du réseau (car
les poids varieront sur de grandes amplitudes). Néanmoins, il peut aussi induire
trop d’oscillations dans les valeurs des poids. Ces derniers ne convergeant pas,
empêche le réseau d’apprendre. A contrario, lorsque le taux d’apprentissage
est plus proche de 0 que de 1, le réseau apprend lentement en faisant converger
petit à petit les poids vers de bonnes valeurs. Dans ce cas aussi, la valeur est
importante : Si le taux d’apprentissage est trop petit, le nombre d’itérations
peut être trop grand et le réseau peut aussi être bloqué dans un minimum
local. Quant au momentum, il s’agit d’un paramètre inertiel, compris entre 0
et 1, qui permet de stabiliser l’apprentissage en évitant les oscillations ou de
rester coincé dans un minimum local. Il permet la prise en compte des erreurs
précédentes : si sa valeur est faible, les erreurs précédentes ne sont pas prises
en compte lors de l’apprentissage à l’instant t. Si sa valeur est élevée, le calcul
des poids à l’instant t prend alors en compte les erreurs précédentes.

D’autres algorithmes d’apprentissage existent telle la rétropropagation dans
le temps (BPTT pour Back Propagation Through Time) et l’apprentissage
récurrent en temps réel (RTRL pour Real Time Recurrent Network). Nous
ne donnerons qu’une brève déﬁnition ici. La rétropropagation dans le temps
(BPTT) a pour idée centrale le dépliement du réseau neuronal récurrent dans
le temps, aﬁn qu’à chaque instant il puisse être assimilé à un réseau de neu-
rone feed-forward comme le percetron [Rumelhart et al., 1985]. La ﬁgure 2.9
illustre un exemple de RNN déplié. Conceptuellement, à chaque pas de temps,
le réseau est copié en une nouvelle instance, il y a donc autant d’instances du
réseau que de pas de temps traité dans une séquence. Pour chaque instance,
le réseau reçoit une entrée et l’état interne du pas de temps précédent, puis
fournit une sortie. Les erreurs sont calculées et les poids mis à jour en utilisant
la rétropropagation standard, avec une restriction cependant : les poids équi-
valents dans l’ensemble des instances doivent être les mêmes. Spatialement,
chaque pas de temps du RNN déroulé peut être vu comme une couche sup-
plémentaire puisque le réseau dépend de l’ordre des données présentées et que
l’état interne du pas de temps précédent est pris comme entrée à chaque pas
de temps.

L’apprentissage récurrent en temps réel (RTRL) [Williams et Zipser, 1989]
est un algorithme d’apprentissage qui calcule le gradient d’erreur exact à
chaque pas de temps. Autrement dit, cet algorithme calcule les dérivées des
états et des sorties par rapport à tous les poids pendant la propagation avant
à chaque pas de temps. Il est donc adapté aux tâches d’apprentissage en ligne.
Mathématiquement plus simple que BPTT, il est néanmoins très lent : plus

Extraction de règles de réseaux récurrents

61

2.3. Deux modèles de RNN : SRN et LSTM

Algorithme 1 Algorithme d’apprentissage du SRN

Pour tout séquence de taille n faire

Pour t = 0 à (n

1) faire

−
# Etape 1 : propagation avant
entrée\_réseau = séquence[t]
sortie\_attendue = séquence[t + 1]
sortie\_obtenue = SRN.propagation\_avant(entrée\_réseau)
Erreur\_SRN = sortie\_attendue - sortie\_obtenue
# Etape 2 : propagation arrière
# Etape 2a : calcul de l’erreur pour chaque unité de la couche de sortie
# σ0 : dérivée de la fonction d’activation σ des unités du SRN
Pour tout unité k de la couche de sortie faire
δk = Erreur\_SRN[k] \* σ ’(sortie\_obtenue[k])

Fin Pour
# Etape 2b :Calcul de l’erreur au niveau de la couche cachée pour
chaque unité j et au niveau de la couche d’entrée
Pour tout couche du réseau, de la couche cachée à la couche d’entrée
faire

Pour tout unité j de la couche courante c faire

# σ0 : dérivée de la fonction d’activation σ des unités du SRN
# w(k,j) : poids provenant de j et allant vers k
# yj : somme des entrées de l’unité j
δj = σ ’(yj) \* Pk(w(k,j) \* (δk))

Fin Pour

Fin Pour
# Etape 2c : Mise à jour des poids
Pour tout poids w(j,i) faire

# xji : entrée associée au lien entre l’unité i vers l’unité j
Variation du poids = Taux d’apprentissage \* δj \* xji + momentum
\* (Variation du poids(t-1))
Nouveau poids = Ancien poids + Variation du poids

Fin Pour

Fin Pour

Fin Pour

62

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

Figure 2.9 – Représentation d’un RNN déplié. Image issue de Deloche [2017]

le réseau comporte d’unités, plus il comporte de poids, et plus les calculs sont
longs et complexes. Cet inconvénient font que l’algorithme ne peut être uti-
lisé que sur de très petits réseaux. Par ailleurs, cet algorithme appliqué au
SRN nécessite un nouveau paramétrage à chaque nouveau corpus de données
[Cernansky et Benuskova, 2003]. Cette contrainte rend donc la combinaison
SRN-RTRL diﬃcile à utiliser dans diﬀérents contextes comme nous souhai-
tons le faire dans travaux présentés par la suite. Une version détaillée du RTRL
est fournie par Giles et al. [2008]. Pour un descriptif détaillé des diﬀérents al-
gorithmes d’apprentissage des RNN, le lecteur pourra se référer à Jaeger [2002].

Exemple : apprentissage de la séquence BTXSE

•

Figure 2.10 – Etapes de la propagation avant à un instant t dans un SRN

Nous allons décrire l’apprentissage du SRN pour la tâche suivante : l’ap-
prentissage de la séquence BTXSE issue de la grammaire de REBER. Nous
donnerons plus de détails sur cette grammaire ultérieurement dans la section
3.2.2. A chaque pas de temps, le réseau doit apprendre à prédire le symbole
suivant, selon le symbole reçu à l’instant t mais aussi les symboles précédem-
ment vus. Il reçoit un vecteur binaire en entrée, représentant pour le symbole

Extraction de règles de réseaux récurrents

63

2.3. Deux modèles de RNN : SRN et LSTM

Figure 2.11 – Etapes de la propagation avant dans un SRN pour toute une
séquence. Chaque symbole de la séquence est représenté par une couleur. Les
symboles dotés d’une couleur plus claire représentent la version encodée des
symboles présentés en entrée au réseau.

64

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

courant, et fournit un vecteur de valeurs réelles, représentant les probabilités
de succession de chacun des symboles. Selon ces valeurs, le réseau peut prédire
un ou plusieurs successeurs à un symbole à l’instant t. Les tables 2.1 et 2.2
montrent un exemple de l’état du réseau après la présentation du symbole B
à t0 et du symbole T à t1. Dans le premier cas, le réseau prédit le symbole
T (sa valeur d’activation est de 53%). Dans le second cas, le réseau prend en
compte l’état interne du réseau à t0 grâce à la couche de contexte et prédit X
(sa valeur d’activation est de 56%).

Symbole
Sortie du réseau

B
00

T S
00
53

P X V E
00
38

01

02

Couche cachée
Couche de contexte

01
00

00
00

10
00

Symbole
Entrée du réseau

B
100

T S
00
00

P X V E
00
00

00

00

Table 2.1 – Etat du réseau après la présentation du symbole ”B” (après ap-
prentissage) de la séquence BTXSE. Les valeurs d’activation en interne (com-
prises entre 0 et 1) sont aﬃchées ici sur une échelle de 0% à 100%. Exemple
adapté de Servan-Schreiber et al. [1991].

Symbole
Sortie du réseau

B
00

T
01

Couche cachée
Couche de contexte

Symbole
Entrée du réseau

B
00

T
100

S
39

84
01

S
00

P X V E
00
00

56

00

00
00

28
10

P X V E
00
00

00

00

Table 2.2 – Etat du réseau après la présentation du symbole ”T” (après ap-
prentissage) de la séquence BTXSE. Les valeurs d’activation en interne (com-
prises entre 0 et 1) sont aﬃchées ici sur une échelle de 0% à 100%. Exemple
adapté de Servan-Schreiber et al. [1991]

L’algorithme d’apprentissage étant supervisé, le réseau a besoin pour ap-
prendre, d’une entrée et d’une sortie attendue. A chaque pas de temps, le SRN
compare la sortie qu’il a fournie avec celle attendue aﬁn de calculer l’erreur
et d’ajuster son apprentissage. Il a donc besoin du couple d’éléments : (élé-
ment entrée, élément suivant attendu). Pour chaque séquence de taille N, il y
a N-1 couples d’éléments. Nous nommerons ces couples des échantillons. Pour

Extraction de règles de réseaux récurrents

65

2.3. Deux modèles de RNN : SRN et LSTM

la séquence BTXSE, les échantillons sont : (B,T) (T,X), (X,S) et (S,E) et
chaque symbole est encodé selon un vecteur binaire de taille 5 (car nous avons
7 symboles) ne comportant qu’une seule unité non nulle. Lors de la phase de
l’apprentissage, le SRN apprend la séquence BTXSE selon le schéma suivant :

Instant t0, échantillon (B,T) : le SRN traite le premier couple d’élé-
ments. Il reçoit en entrée le vecteur binaire désignant B en entrée. Seuls les
neurones de la couche d’entrée voient leur activité changer. Celles des unités
de la couche de contexte restent nulles car celles-ci ne sont pas impactées par
les entrées, mais par la couche cachée dont elles reçoivent une copie du pattern
d’activation. L’activation de la couche d’entrée dans son intégralité (couche
d’entrée + couche de contexte) est alors propagée à la couche cachée (Figure
2.10, étape 1). Les neurones de la couche cachée voient leur activité changer. Le
pattern d’activation de la couche cachée encode alors l’état interne du réseau
selon l’entrée reçue (Figure 2.10, étape 2). Le pattern d’activation de la couche
cachée représente alors un encodage des caractéristiques des patterns d’entrée
qui sont pertinents pour la tâche de reconnaissance d’une portion de séquence
Cleeremans et McClelland [1991]. Le pattern d’activation de la couche cachée
est alors simultanément, copié dans la couche de contexte pour l’itération sui-
vante et propagé à la couche de sortie (Figure 2.10, étape 3). Cette dernière
traite l’activation reçue. Le réseau émet une réponse sous la forme d’un pattern
d’activation qui déﬁnit un élément ou plusieurs éléments selon les successeurs
prédits (Figure 2.10, étape 4). Le SRN compare alors la sortie obtenue (sa
prédiction) avec la sortie attendue (le vecteur binaire décrivant l’élément T),
et calcule l’erreur. Cette dernière est rétropropagée en utilisant l’algorithme
de rétropropagation pour ajuster les poids des connexions du SRN ( à l’excep-
tion de ceux entre la couche cachée et la couche de contexte) et cela avant la
présentation de l’élément suivant au réseau.

Instant t1, échantillon (T,X) : A l’instant t1, le SRN traite le second
couple d’élément (T, X). Il reçoit donc en entrée le vecteur binaire désignant T
en entrée et suit les mêmes étapes décrites dans l’étape t0. La diﬀérence réside
ici dans le pattern d’activation de la couche cachée qui n’est plus nul. En eﬀet,
il contient une copie du pattern d’activation de la couche cachée à l’instant
précédent t0. Lorsque la couche cachée reçoit une information, elle reçoit à la
fois l’activation de la couche d’entrée à l’instant courant, mais aussi celle de
la couche de contexte qui contient une information sur la représentation du
réseau à l’instant précédant. Ainsi la couche cachée peut encoder des infor-
mations à propos des caractéristiques pertinentes des 2 premiers symboles. La
couche cachée réalise donc des calculs selon une information présente et passée.

Instant t2, échantillon (X,S) : la couche de contexte contient une infor-
mation à propos de l’élément à t0 et t1 (qu’elle communique en même temps

66

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

que la couche d’entrée à la couche cachée) et s’apprête ensuite à recevoir celle
à propos de t2. Le SRN doit prédire le symbole suivant, sachant qu’il vient de
voir successivement les lettres BTX.

Instant t3, échantillon (S,E) : la couche de contexte contient à présent
une information à propos de l’élément à t0, t1 et t2 et s’apprête ensuite à rece-
voir celle à propos de t3 et du dernier échantillon.

La ﬁgure 2.11, illustre l’apprentissage de la séquence BTXSE aux diﬀérents
pas de temps décrits ci-dessus. Arrivé à la ﬁn de l’apprentissage d’une séquence,
le réseau accumule deux types de ”mémoires” : sa couche de contexte informe
sur le passé récent qu’il vient de traiter, c’est la mémoire sur le court terme. Et
ses poids, contiennent la mémoire sur le long terme : leur ajustement permettra
au réseau de faire la bonne prédiction la prochaine fois qu’il sera soumis à la
même séquence. Si le SRN apprend plusieurs séquences, il apprend "plusieurs
contextes" qui le rendent capable, de distinguer entre deux séquences proches
mais diﬀérentes. Si le réseau apprend les séquences BTSXSE et BPTVVE, il
prédira X après avoir "vu" BTS et V après avoir "vu" BPT.

Limites des modèles RNN actuels

Les RNN souﬀrent de 3 problèmes, que nous allons détailler ici, qui nuisent

à leurs performances pour l’apprentissage de séquences longues.

Le premier problème est celui de l’atténuation du contexte temporel. Lors
du fonctionnement du réseau SRN par exemple, celui ci accumule, au fur et à
mesure du temps, une copie des patterns d’activité passés dans sa couche de
contexte. Or, cette couche ﬁnit à un moment par être saturée, du fait de sa
taille limitée, ce qui génère un oubli des passés lointains au privilège de passé
plus récents [Hinaut, 2013]. Dans le cas de Hopﬁeld, le phénomène porte le
nom d’ "oublie catastrophique". et représente par conséquent un frein à l’ap-
prentissage de dépendances sur le long terme.

Le second problème est celui du dépliement des RNN lors notamment de
l’utilisation de la rétropropagation dans le temps (BPTT) (que nous avons pré-
senté précédemment) comme algorithme d’apprentissage (section 2.3.1). Pour
chaque séquence, à chaque pas de temps, une nouvelle instance du réseau est
créée. Un RNN ﬁnit par ressembler une fois déplié, à un réseau profond doté de
plusieurs couches. Si les séquences sont longues (un millier de pas de temps),
le nombre d’instances (de couches) à prendre en compte pour les calculs sera
tout aussi important et il faudra autant de dérivées à calculer pour chaque mise
à jour des poids. La complexité de calcul engendré par cela, peut avoir pour
conséquence l’évanouissement du gradient (que nous expliquerons ci dessous)

Extraction de règles de réseaux récurrents

67

2.3. Deux modèles de RNN : SRN et LSTM

ou l’explosion des poids et rendre l’apprentissage lent et ineﬃcace. Cet algo-
rithme d’apprentissage, porteur d’une complexité spatiale et de calcul, n’est
donc pas adéquat pour l’apprentissage de séquences avec des contextes tem-
porels portant sur un nombre important de pas de temps.

Enﬁn, le troisième problème est celui de l’évanouissement du gradient lors
de l’apprentissage qui survient dans les réseaux profonds (dotés de plusieurs
couches comme le MLP). Ce problème est dû aux méthodes d’apprentissage
utilisant certaines fonctions d’activation. Nombres d’entres elles (par exemple,
sigmoïde ou tangente) projetant leur entrée dans une très petite plage de sortie
([0, 1] par exemple pour la fonction sigmoïde) d’une manière non linéaire,
aboutissent à l’obtention d’un gradient faible.

Lorsque plusieurs couches sont empilées, le gradient transitant par ces
couches devient de plus en plus faible. Par conséquence, même un grand chan-
gement dans les paramètres de la première couche n’a pas un grand eﬀet sur
les couches de sortie (couches profondes). Or les méthodes basées sur le gra-
dient apprennent la valeur d’un paramètre en comprenant comment une petite
modiﬁcation de la valeur de ce paramètre aﬀectera la sortie du réseau. Si une
modiﬁcation de la valeur du paramètre entraîne une très faible modiﬁcation
de la sortie du réseau, le réseau ne peut tout simplement pas apprendre le
paramètre de manière eﬃcace, ce qui pose problème. Dans le problème de
l’évanouissement du gradient, les gradients de la sortie du réseau, par rapport
aux paramètres dans les premières couches, devenant extrêmement faibles, em-
pêchent l’apprentissage et l’ajustement des paramètres des couches profondes
du réseau. Ce problème tend à s’aggraver à mesure que le nombre de couches
dans l’architecture augmente.

Pour ﬁnir, un rappel plus qu’une limite : les performances des RNN dé-
pendent des paramètres choisis. Nous avons présenté précédemment les notions
de taux d’apprentissage et de momentum. Si le choix de ces paramètres peut
nuire ou améliorer les performances du SRN, cela est vrai pour les RNN. Nous
listerons aussi le nombre d’unités du RNN en général et le nombre d’unités de
la couche cachée dans le cas du SRN comme autres paramètres importants. Ce
dernier, avec le taux d’apprentissage et le momentum sont les paramètres que
nous avons testés et fait varier dans notre étude des performances du SRN en
section 3.3.1.

Apport du SRN dans l’apprentissage implicite

Dans le cadre de l’étude de l’apprentissage implicite, Reber défend l’idée
que cet l’apprentissage s’explique par un processus inconscient d’abstraction
des règles qui régissent une situation [Witt, 2010a]. D’après lui, les sujets ar-
rivent à discriminer les stimuli qui correspondent aux règles, de ceux qui ne

68

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

les respectent pas, sans être capables d’expliquer ces règles. Il est rejoint en-
suite par Axel Cleeremans [Cleeremans et al., 1989; Cleeremans et McClel-
land, 1991]. Ce dernier introduit un cadre théorique qui uniﬁe les données et
les modèles existants sur l’apprentissage implicite (i.e inconscient), ainsi qu’un
modèle informatique détaillé de la performance humaine dans les situations
d’apprentissage de séquences : le SRN. Dans une série d’expériences de mesure
du temps de réaction lors de la réalisation d’un choix, Cleeremans a ainsi mon-
tré, à l’aide d’un SRN capable de prédire parfaitement les éléments successifs
de séquences générées à partir de grammaires à états ﬁnis, que des sujets hu-
mains développaient une sensibilité à la structure temporelle des séquences qui
leur ont été présenté. Et cela sans que leur connaissance explicite de la séquence
soit modiﬁée. Il s’intéresse alors au processus d’abstraction des règles dans le
SRN pour tenter de comprendre celui qui a lieu de manière inconsciente au
niveau cognitif chez les sujets humains et démontre ainsi la capacité du SRN
à encoder des règles cachées dans des séquences [Servan-Schreiber et al., 1988,
1991]. En utilisant des techniques de clustering hiérarchiques réalisées à dif-
férents moments de l’apprentissage du SRN sur les patterns d’activité de la
couche cachée, les chercheurs ont démontré une modiﬁcation de l’organisation
interne de l’encodage des éléments.

En d’autres termes, le SRN modiﬁe sa représentation interne des règles abs-
traites des séquences au fur et à mesure de l’apprentissage, jusqu’à ce qu’elles
lui permettent de ne faire que des prédictions correctes.

Apport du SRN dans le domaine des RNN

L’utilisation de l’algorithme d’apprentissage BPTT pour les RNN induit
des réseaux dotés de plusieurs couches tel un réseau profond. Ces réseaux
ont souvent de faibles performances dues au problème de l’évanouissement du
gradient, sans oublier la complexité de calcul que cela engendre (deux limites
que nous avons détaillés précédemment section 2.3.1). Par son architecture, et
notamment la présence d’une récurrence dans son architecture, le SRN (doté
de seulement 3 couches) permet d’apprendre des séquences temporelles avec
un algorithme standard de back propagation. En évitant d’utiliser le BPTT, le
SRN permet d’éviter les problèmes liés au dépliement (la complexité de calcul
lors de l’apprentissage de longues séquences) et à l’évanouissement du gradient
(qui survient si le dépliement aboutit à l’obtention de réseaux profonds).

Discussion

D’un point de vue technique, parmi les modèles connexionnistes, le SRN
semble particulièrement adapté pour le matériel séquentiel grâce notamment
à son architecture dotée de récurrence. En eﬀet, le modèle développe une sen-
sibilité au contexte temporel à partir du moment où cela est pertinent pour

Extraction de règles de réseaux récurrents

69

2.3. Deux modèles de RNN : SRN et LSTM

l’amélioration de ses performances et pour la tâche de prédiction de l’élément
suivant [Cleeremans et McClelland, 1991]. De plus, le modèle ne nécessite pas
de grandes ressources d’analyse : son architecture élémentaire a pour consé-
quence des mécanismes d’apprentissages élémentaires car tous les calculs sont
locaux à l’élément courant (pas de représentation explicite des éléments pré-
cédents).

Néanmoins et malgré ses apports dans le domaine de l’implicite et des
RNN, le SRN est un modèle qui souﬀre d’une limite importante. Son architec-
ture qui lui permet de stocker le passé, est aussi sa faiblesse : Dût à sa taille
non-extensible, la couche de contexte ﬁnit par être saturée en information sur
le passé. Cela implique que chaque nouvelle entrée encodée par le réseau pren-
dra peu à peu le pas sur les informations les plus anciennes. Un passé récent
ﬁnit par écraser le passé plus ancien, ce qui limite l’utilisation du modèle pour
l’apprentissage de séquences longues avec des dépendances sur le long terme
[Sutton, 1988; Gers et al., 1999; Hinaut, 2013].

Dans la partie suivante, nous allons aborder le modèle des LSTM, ou Long
Short Term Memory, qui a connu un regain d’intérêt ces dernières années et
qui oﬀre un nouveau contexte pour l’étude des séquences et des règles cachées
les régissant.

2.3.2 Long Short Term Memory, LSTM

Nous avons évoqué brièvement les LSTM précédemment comme des au-
tomates complexes utilisant des neurones artiﬁciels pour pallier les faiblesses
des RNNs traditionnels. Ces réseaux, nous le verrons plus en détail par la
suite, possèdent une architecture complexe mais réalisent des calculs simples,
ce qui leur permet essentiellement deux éléments. D’une part, ils explicitent
le temps grâce à leur architecture : ils sont considérés proches du fonctionne-
ment du BPTT dans leur manière de gérer les entrées séquentielles. D’autre
part, grâce au "Carroussel d’erreur constante", unité linéaire de calcul que
nous présenterons ultérieurement, ils peuvent maintenir l’erreur sur plusieurs
pas de temps sans que celle-ci disparaisse ou ne soit altérée. Cette dernière
caractéristique permet aux LSTM de gérer ainsi de longues séquences sans
souﬀrir du problème de l’évanouissement du gradient ou aucun autre problème
lié au dépliement. Dans cette section, nous décrivons plus en détail ce mo-
dèle en présentant l’architecture d’un réseau LSTM, mais aussi en détaillant la
structure d’une unité LSTM. Nous aborderons ensuite les calculs réalisés lors
de la propagation avant et l’algorithme recommandé pour la rétropropagation
lors de l’apprentissage. Nous présenterons brièvement aussi les variations les
plus connues et utilisées de ces dernières années. Enﬁn, à la lumière de l’en-
semble de ces détails techniques, nous expliquerons l’apport des LSTM dans

70

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

l’apprentissage séquentiel.

Présentation des LSTMs

Le concept des LSTM, pour Long Short Term Memory, provient de l’idée
suivante : dans un RNN, l’information est stockée sous deux formats. L’acti-
vation des unités représente l’historique récent du réseau et donc une mémoire
sur le court terme du réseau, alors que le poids des liaisons entre les neurones
représente l’expérience accumulée du RNN lors de son apprentissage et donc
une mémoire sur le long terme. Forts de ce constat, Hochreiter et Schmidhuber
[1997] ont introduit les cellules mémoires, une forme de mémoire intermédiaire
permettant de stocker des informations importantes sur une période de temps
plus longue que les RNN existants [Lipton et al., 2015]. D’un point de vue
technique, les LSTM sont motivées par l’envie de proposer un modèle qui ne
souﬀre pas du problème du débordement ou de l’évanouissement de l’erreur
lors de l’apprentissage. En eﬀet, dans un RNN traditionnel tel que Elman,
l’apprentissage ﬁnit à un moment par induire un signal d’erreur qui soit ex-
plose (ce qui induit des poids qui augmentent), soit s’évanouit (surtout lorsque
le réseau doit apprendre sur de longs délais ce qui prend un temps prohibitif
ou qui ne fonctionne pas du tout). Or sans erreur, il n’est plus possible de
faire apprendre au RNN [Hochreiter et Schmidhuber, 1997; Hochreiter et al.,
2001]. Depuis 1997, la recherche liée au LSTM est un domaine très actif et de
nombreuses variations ont été proposées. Nous allons présenter dans la section
suivante la version des LSTM proposée par Gers et al. [1999] et qui est la
première à avoir créé une rupture en termes de performances avec le modèle
d’origine. Nous reprendrons les mêmes notations que celles de l’article de Gers
et al. [1999] (similaires à celle de Hochreiter et Schmidhuber [1997]) pour la
suite de nos travaux.

Architecture d’une unité LSTM

Bien souvent, on retrouve dans la littérature la notion de "cellule LSTM"
(LSTM cell) pour désigner une unité LSTM dans sa globalité. Pour éviter
toute confusion, nous utiliserons le terme d’unité LSTM lorsque l’on désignera
l’ensemble : bloc et cellule. Nous allons ainsi décrire ci-dessous l’architecture
globale d’un RNN-LSTM avant de détailler les composants bloc et cellule sé-
parément.

Comment s’insèrent les unités LSTM dans un RNN-LSTM ?
•

Dans un RNN tel que Elman, le réseau se compose de 3 couches : une
couche d’entrée, une couche cachée et une couche de sortie. La couche cachée

Extraction de règles de réseaux récurrents

71

2.3. Deux modèles de RNN : SRN et LSTM

Figure 2.12 – Présentation d’un réseau doté d’unités LSTM au niveau de la
couche cachée. Image issue de Lapalme [2006]

se compose de neurones artiﬁciels, du même type que ceux de la couche d’en-
trée et ceux de la couche de sortie. Dans un RNN-LSTM, l’architecture du
réseau reste la même, à savoir 3 couches. La diﬀérence réside essentiellement
dans la couche cachée. Les unités utilisées sont alors des unités LSTM com-
posées de blocs et de cellules. La couche cachée reçoit donc ainsi à l’instant
t l’information provenant de la couche d’entrée, mais aussi l’information pro-
venant de la couche cachée à l’instant t-1. Le pattern d’activité de la couche
cachée se compose de l’ensemble des sorties des diﬀérentes cellules de chaque
unité LSTM. La ﬁgure 2.12 présente un exemple d’un RNN-LSTM.

Les Blocs
•

Deux notions sont importantes à distinguer lorsqu’il s’agit d’un LSTM : les
blocs et les cellules. Si le premier terme désigne le contenant, celui qui apprend
à laisser entrer, sortir les informations reçues ou qui choisit de les oublier, le
second terme désigne le contenu, celui qui traite l’information.
Un bloc LSTM peut avoir une ou plusieurs cellules : il leur distribue l’informa-
tion de manière égale. Une cellule au contraire n’appartient qu’à un seul bloc.
La ﬁgure 2.13 illustre le cas d’un bloc doté de 2 cellules et la ﬁgure 2.14 celui
d’un bloc à une seule cellule. Un bloc LSTM j a la particularité d’avoir (3 + c)
voies d’entrées de l’information, c désignant le nombre de cellules dans le bloc.
Les voies d’entrée de l’information sont la porte d’entrée inj, la porte d’oubli

72

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

Figure 2.13 – Présentation d’une unité LSTM dotée d’un bloc composé de 2
cellules

φj et la porte de sortie outj, et les entrées de chaque cellule. Chaque cellule de
chaque bloc reçoit la même information. Nous détaillerons cela dans la partie
suivante dédiée à la présentation des cellules.

— la porte d’entrée : Quand sa valeur est proche de zéro, cette porte em-
pêche les informations extérieures de parvenir aux cellules des blocs et
donc de mettre à jour leur valeur. L’information provenant des couches
inférieures n’est donc pas transmise aux couches supérieures.

— la porte d’oubli : Quand sa valeur est proche de zéro, la cellule oublie
son passé. Les cellules du bloc ne prennent alors en compte que le pré-
sent.

— la porte de sortie : Cette porte détermine si une cellule peut envoyer une
valeur aux autres cellules ou non. En eﬀet, quand sa valeur est proche
de zéro, la cellule ne fournit aucune information en sortie (valeur proche
de 0 aussi)

Chacune des portes reçoit à un pas de temps t, l’information associée au
pas de temps t en provenance de la couche d’entrée, mais aussi celle associée
au pas de temps t-1 en provenance des cellules de la même couche cachée. Bien

Extraction de règles de réseaux récurrents

73

2.3. Deux modèles de RNN : SRN et LSTM

que l’algorithme d’apprentissage soit détaillé par la suite, il semble important
de préciser que les poids arrivant sur ces portes sont tous modiﬁés durant l’ap-
prentissage. Cela implique que chaque bloc apprend à reconnaitre le moment
où il faut prendre en compte l’information extérieure, mais aussi les moments
où il faut communiquer avec l’extérieur. Soulignons aussi que les portes de sor-
tie décrites ci-dessus, ne faisaient pas partie du modèle original. Elles ont été
proposées par Gers et al. [1999] et sont depuis proposées dans la majorité des
implémentations au vue de l’amélioration des performances [Lipton et al., 2015]

Figure 2.14 – Présentation d’une unité LSTM doté d’un bloc, lui même doté
d’une cellule. Image issue de Gers et al. [1999]

Les cellules
•

Une cellule c, appartenant à un bloc j et dont le numéro est v, et que nous
noterons cv
j , possède aussi une voie d’entrée et une voie de sortie (en plus des
3 voies liées aux 3 portes.)

En plus de cela, chaque cellule possède une unité linéaire de calcul nommée
"Carrousel d’erreur constante" ou "Constant error carousel" en anglais (CEC),
dont l’activation est notée scv
C’est cette cellule qui résout le problème de l’évanouissement du gradient qui
empêche l’apprentissage. En l’absence de nouvelles entrées ou de signal d’er-
reur (lors de la rétropropagation) parvenant à cette cellule, le retour d’erreur
local (induit par la boucle récurrente) du CEC reste constant. Il peut être donc

j . On parle alors d’état de la cellule.

74

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

maintenu sur de long pas de temps (nous y reviendrons par la suite).
En eﬀet, grâce aux portes du bloc, l’état de la cellule n’est pas mis à jour à
chaque pas de temps. En termes d’informations reçues, une cellule CEC prend
en compte diﬀérents types d’information : l’information calculée au pas de
j (t), les valeurs des
temps t (l’entrée de la cellule, celle de l’état de la cellule scv
diﬀérentes portes yinj(t), yφj(t) et youtj(t)) mais aussi de l’information relative
au pas de temps t-1 (l’ensemble de valeurs de sorties de l’ensemble des cellules
de la couche cachée ycv
j (t-1) ). Ce fonctionnement
en deux temps est ce qui confère aux LSTMs leur performance.

j (t-1) et l’état de la cellule scv

Fonctionnement d’une unité LSTM

Une unité LSTM, comme nous l’avons vu, se compose de plusieurs élé-
ments : 3 portes et autant d’entrées et de sorties que de cellules. Malgré la
complexité de l’architecture, les performances sont au rendez vous grâce no-
tamment à la simplicité des calculs réalisés. En eﬀet, en dehors des sorties des
cellules, l’ensemble des autres composants (porte et entrée de cellules) sont
des neurones artiﬁciels, qui utilisent la rétropropagation standard pour leur
apprentissage. Et en interne, les calculs sont des produits et des sommes, ce
qui rend l’ensemble facilement dérivable et manipulable pour l’apprentissage.
Une autre manière de représenter une unité LSTM est de la considérer comme
une représentation schématique de 3 algorithmes "si alors sinon" (pour les 3
portes) décrit ci dessous (Algorithme 2).

Nous allons décrire dans cette partie le fonctionnement d’une unité LSTM

et les diﬀérentes équations nécessaires à cela.
Notations et fonctions d’activations
•

Pour commencer, il est nécessaire de déﬁnir ici quelques notations :
Pour les blocs et cellules :
— j : bloc d’une couche cachée donnée
— v : cellule dans un bloc donné
— cv
— inj : la porte d’entrée du bloc j
— φj : la porte d’oubli du bloc j
— outj : la porte de sortie du bloc j
— scv

j (t)) : état s de la cellule v du bloc j

j : cellule v du bloc j

Pour la couche de sortie du réseau :
— k : neurone au niveau de la couche de sortie du réseau
— ek : erreur calculée au niveau de l’unité k de la couche de sortie du

réseau

Extraction de règles de réseaux récurrents

75

2.3. Deux modèles de RNN : SRN et LSTM

Algorithme 2 Algorithme de fonctionnement des portes selon trois algo-
rithmes "si alors sinon"

Si V aleur\_porteEntree

0 alors

Porte ouverte : Transmission de l’information extérieure à l’intérieur du
bloc
Si V aleur\_porteOubli

0 alors

L’état précédent du CEC impacte le calcul de l’état présent
Si V aleur\_porteSortie
0 alors

Porte ouverte : Transmission de l’information intérieure vers l’exté-
rieur du bloc

"

"

"

Sinon

Porte fermée : Pas d’information fournie en sortie

Fin Si

Sinon

la cellule oublie son passé

Fin Si

Sinon

Porte fermée : Information bloquée

Fin Si

— tk : sortie (valeur) attendue au niveau de l’unité k

Autres notations :
— m : neurone possédant une liaison avec une unité LSTM
— netx : activation reçue par l’unité de calcul x (que ce soit une porte ou

une cellule ou un neurone artiﬁciel)
— ym : activité de l’unité de calcul m
— w(l,m) : poids provenant de m et allant vers l’unité de calcul l

Trois fonctions d’activation seront citées dans la section ci-dessous :
f (x) : Sigmoïde, fonction logistique entre [0,1] telle que
— f : x

!

f (x) =

1
1 + e−x

— h : x

— g : x

!

!

h(x) : Sigmoïde centrée entre [-1,1] telle que

h(x) = 2

f (x)

1

−

⇤

g(x) : Sigmoïde centrée entre [-2,2] telle que

Nous noterons leurs dérivées respectives f 0, h0 et g0

g(x) = 4

f (x)

2

−

⇤

(2.1)

(2.2)

(2.3)

76

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

Figure 2.15 – Représentation schématique des étapes de la propagation avant
dans un RNN-LSTM pour toute une séquence. Chaque symbole de la séquence
est représenté par une couleur. Les symboles dotés d’une couleur plus claire
représentent la version encodée des symboles présentés en entrée au réseau.

Propagation avant
•

La réception par le réseau d’une information au pas de temps t, initie
la propagation avant de l’information dans le réseau. Celle-ci doit alors être
transmise de la couche d’entrée à la couche cachée puis à la couche de sortie.
A l’instant t, chaque bloc de la couche cachée reçoit 2 informations : le pattern
d’activité de la couche d’entrée de l’instant t, et le pattern d’activité de la
couche cachée de l’instant t-1. Autrement dit, la couche cachée renvoie vers
elle-même son propre pattern d’activité et cela grâce à l’information que chaque
cellule renvoie vers elle-même, les autres cellules, et les portes des diﬀérents
blocs. La ﬁgure 2.15 présente schématiquement les étapes de la propagation
avant dans un RNN-LSTM doté d’une unité LSTM au niveau de sa couche
cachée.
Cette information passée et présente est reçue au même moment par l’entrée
de la cellule du bloc, la porte d’entrée, la porte d’oubli et la porte de sortie du
bloc. Les calculs des activations reçues sont détaillés ci-dessous :

Au niveau des entrées de l’unité LSTM, bloc et cellule

Soit m le nombre de neurones 1 qui possèdent une liaison avec une unité

1. m désigne les neurones de la couche d’entrée, mais aussi les cellules des blocs de la
couche cachée. Il s’agit de l’ensemble des neurones artiﬁciels qui envoient une information à

Extraction de règles de réseaux récurrents

77

2.3. Deux modèles de RNN : SRN et LSTM

Figure 2.16 – Etapes de la propagation avant dans une unité LSTM dotée
d’un bloc d’une cellule. Les informations entrantes dans une unité sont en bleu,
celle sortante en vert. Les calculs au sein d’une cellule sont en orange.
A - L’unité LSTM reçoit les activations des autres unités du réseau puis calcule
les activités des portes du bloc.
B - La cellule calcule l’activité entrante reçue (input squashing) et la module
selon la valeur de la porte d’entrée du bloc (input gating).
C - Mise à jour de la valeur du CEC (memorizing) selon l’activité entrante
modulée de la cellule et l’activité du CEC au temps précédent modulée par la
valeur de la porte d’oublie du bloc (forgetting).
D - La cellule calcule l’activité résultant du CEC (output squashing) et la
module selon la valeur de la porte de sortie du bloc (output gating). Selon le
résultat, la cellule envoie une activation aux autres unités du réseau.
Images adaptées de Gers et al. [1999] (ﬁgure 2.14).

78

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

LSTM et qui envoient une information au temps t, p le nombre de neurones qui
possèdent une liaison avec une unité LSTM et qui envoient une information au
1 et soit netx les activations calculées pour une unité de calcul x :
temps t

−

Pour la cellule, x = cv

j :

netcv

j = X
m

(w(cv

j ,m) ·

ym(t)) + X

p

(w(cv

j ,p) ·

yp(t

1))

−

Pour la porte d’entrée, x = in :

netinj = X
m

(w(inj ,m) ·

ym(t)) + X

p

(w(inj ,p) ·

yp(t

1))

−

Pour la porte d’oubli, x = φ :

netφj = X
m

(w(φj ,m) ·

ym(t)) + X

p

(w(φj ,p) ·

yp(t

1))

−

Pour la porte de sortie, x = out :

(2.4)

(2.5)

(2.6)

netoutj = X
m

(w(outj ,m) ·

ym(t)) + X

p

(w(outj ,p) ·

yp(t

1))

−

(2.7)

La ﬁgure 2.16, label A, présente les diﬀérentes activations reçues par une
unité LSTM dotée d’un bloc d’une cellule. Pour chacune des portes, la fonction
d’activation f est appliquée sur les entrées reçues telle que :

yinj (t) = finj (netinj (t))

yφj (t) = fφj (netφj (t))

youtj (t) = foutj (netoutj (t))

(2.8)

(2.9)

(2.10)

Pour la cellule, c’est la fonction d’activation g qui est appliquée sur les

entrées reçues :

ycv

j (t) = g(netcv

j (t))

(2.11)

Dans l’unité LSTM, au niveau de la cellule

La mise à jour de l’état de la cellule peut être décrite selon la séquence de

calcul suivante :

l’instant t et qui sera pris en compte dans le calcul de l’activation de l’entrée de la cellule

Extraction de règles de réseaux récurrents

79

2.3. Deux modèles de RNN : SRN et LSTM

— Etape 1 (ﬁgure 2.16, label B) : L’activité de l’entrée de la cellule g(netcv

j (t))

est multipliée par la valeur de la porte d’entrée du bloc yinj (t). C’est à
ce niveau là que le bloc peut empêcher une information non pertinente
de venir modiﬁer l’état interne de la cellule. Lorsque la porte d’entrée
est close (activation proche de zéro), les entrées non pertinentes, ainsi
que le bruit, n’entrent pas dans l’unité LSTM.

ycv

j (t)

·

yinj (t) = g(netcv

j (t))

yinj (t)

·

(2.12)

— Etape 2 (ﬁgure 2.16, label C) : la porte d’oubli joue le rôle de bouton de
remise à zéro du passé. Par exemple, lorsque l’étude d’une nouvelle sé-
quence débute, son activité est proche de zéro. Le produit de l’équation
2.13 étant alors nul, la cellule "oublie son passé" et peut commencer à
stocker un nouveau passé.

scv

j (t

1)

yφj (t)

(2.13)

−
— Etape 3 (ﬁgure 2.16, label C) : Il s’agit de la somme des deux étapes
précédentes : autrement dit, la prise en compte du présent connaissant
le passé (ou l’absence de passé lorsqu’il s’agit du début d’une séquence).

·

g(netcv

j (t))

·

yinj (t) + scv

j (t

yφj (t)

1)

·

−

(2.14)

L’équation de mise à jour de l’état interne de la cellule à l’instant t est

donc la suivante :

scv

j (t) = g(netcv

j (t))

yinj (t) + scv

j (t

yφj (t)

1)

·

−

·

(2.15)

Il est important de souligner que l’équation 2.15 résume à elle seule la puis-
sance des LSTM puisqu’en une équation dérivable, 3 algorithmes "si alors si-
non" sont représentés (voir algorithme 2). Cette équation, réalisée pour chaque
cellule, a permis de gagner du temps de calcul. L’utilisation d’algorithmes de
conditions "si alors sinon" aurait été plus consommatrice en calcul et moins
ﬁable. Cette équation n’est réalisée que si l’ensemble des portes ont des valeurs
supérieurs à 0 (si toutes les conditions "Si alors" de l’algorithme 2 sont rem-
plis). Si les deux premières conditions (porte entrée et porte d’oubli) ne sont
pas remplis, le CEC ne reçoit pas d’entrée. N’étant pas mis à jour, l’informa-
tion en son sein est préservée pour ce pas de temps sans dégradation.

— Etape 4 (ﬁgure 2.16, label D) : Une fois l’état interne calculé, il ne reste
plus que le calcul de la sortie de la cellule, ce qui fait intervenir la porte
de sortie du bloc comme ce qui suit :

80

ycv

j (t) = h(scv

j (t))

youtj (t)

·

(2.16)

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

Il est important ici de souligner l’application de la fonction d’activation
h sur l’état de la cellule :

h(scv

j (t))

(2.17)

Propagation arrière
•

Dans cette section nous avons choisi de présenter une version de l’algo-
rithme de propagation arrière alliant BPTT et RTRL, telle que décrit par
Gers et al. [1999] et que nous avons choisi d’implémenter pour nos travaux.
Dans un RNN-LSTM, les unités de sortie utilisent BPTT pour la propagation
arrière du gradient. Les portes de sortie des blocs utilisent, elles, une version
tronquée de BPTT et enﬁn les poids allant aux cellules, aux portes d’entrées
et aux portes d’oubli des blocs seront mis à jour via une version tronquée de
RTRL. Ce choix se justiﬁe par le fait que grâce à la troncature, toutes les
erreurs seront coupées lorsqu’elles sortiront d’une cellule ou d’une porte, bien
qu’elles servent à modiﬁer les poids entrants. Ainsi le CEC d’une cellule devient
la seule partie du système à travers laquelle l’erreur peut être rétropropagée
et maintenue pour toujours. Cela rend les mises à jour des LSTM eﬃcaces
sans aﬀecter signiﬁcativement le pouvoir d’apprentissage : le ﬂux d’erreur à
l’extérieur des cellules a tendance à décroître exponentiellement de toute façon
[Hochreiter et Schmidhuber, 1997]. L’algorithme de propagation arrière sera
donné en entier en annexe D, mais nous abordons ici ses grandes étapes.

Calcul des dérivées durant la propagation avant :

Les calculs de la propagation arrière commencent pendant la propagation
avant. En eﬀet, à chaque pas de temps, après le calcul de la sortie du ré-
seau selon une entrée à l’instant t, le modèle RNN-LSTM calcule aussi des
dérivées partielles pour calculer le gradient et cela pour les portes d’entrées,
les portes d’oubli et les cellules de chaque bloc. L’idée est de calculer la dérivée
de la valeur des cellules par rapport aux poids du réseau pour ces voies d’entrée
de l’information là.

Ainsi pour chaque bloc j, les dérivées sont calculées comme ce qui suit :
Pour les cellules :

dSjv

c,m(t) = dSjv

cm(t

1)

⇤

−

yφj (t) + g0(netcv

j )(t)

Pour les portes d’entrées :

dSjv

in,m(t) = dSjv

in,m(t

1)

⇤

−

yφj (t) + g(netcv

j (t))

Pour les portes d’oubli :

⇤

⇤

yinj (t)

ym(t

1)

−

⇤

(2.18)

f 0
inj (netinj (t))

ym(t

⇤

1)
−
(2.19)

dSjv

φ,m(t) = dSjv

φ,m(t

1)

⇤

−

yφj (t) + h(scv

j (t))

f 0
φj (netφj (t))

ym(t

⇤

−

⇤

1) (2.20)

Extraction de règles de réseaux récurrents

81

2.3. Deux modèles de RNN : SRN et LSTM

avec au temps t = 0, dSjv

c,m(0) = 0, dSjv

in,m(0) = 0 et dSjv

φ,m(0) = 0

Rétropropagation pour une unité LSTM

L’étape suivante est la rétropropagation en elle même. Dans un RNN-
LSTM, une erreur est calculée au niveau de chaque unité k de la couche de
sortie telle que :

δk(t) = f 0

k(netk(t))

(tk(t)

⇤

−

yk(t))

(2.21)

Cette erreur δk(t) est ensuite rétropropagée et utilisée pour calculer l’erreur
au niveau des portes de sortie, d’oubli, d’entrée et des cellules des blocs comme
ce qui suit :

Pour les poids entre les unités k de la couche de sortie et les portes de

sortie des blocs j :

δoutj (t) = f 0

outj (netoutj (t))

Sj

(

X
v=1

⇤

h(scv

j (t))

(X
m

⇤

wkcv

j ⇤

δk(t)))

(2.22)

Pour les poids entre les unités k de la couche de sortie et les portes d’en-

trée, les portes d’oubli, ainsi que les cellules des blocs j :

ecv

j (t) = youtj

h0(scv

j (t))

⇤

(X
m

⇤

wkcv

j ⇤

δk(t)))

(2.23)

Mise à jour des poids d’un réseau RNN-LSTM

C’est lors de cette étape que l’on va utiliser les dérivées partielles calculées

précédemment :

Pour les unités k de la couche de sortie et les portes de sorties outj des

blocs j :

∆lm(t) = α

(δl(t))

⇤

ym(t

1)

−

⇤

avec l

k, i, out
Pour les cellules :

2

∆cv

j ,m(t) = α

ecv

j (t)

⇤

⇤

dSjvc,m(t)

Pour les portes d’entrée :

(2.24)

(2.25)

∆in,m(t) = α

Sj

(

X
v=1

⇤

ecv

j (t)

⇤

dSjvin,m(t))

(2.26)

82

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

Pour les portes d’oubli :

Sj

∆φ,m(t) = α

(

X
v=1

⇤

ecv

j (t)

⇤

dSjvφ,m(t))

(2.27)

Il faut souligner que selon les modèles, d’autres poids existent et qu’il faut
aussi les mettre à jour dans le RNN-LSTM. Ils seront alors mis à jour comme
pour les poids entre les unités de la couche de sortie et de la couche cachée.

Les variations les plus connues du modèle

Les connexions peepholes
•

Dans nos travaux, nous avons choisi de travailler sur le modèle de LSTM tel
que déﬁni par Gers et al. [1999]. Mais depuis, de nombreuses variations ont été
proposées comme notamment les connexions "peepholes". Proposées la même
année et par les mêmes auteurs, ces connexions lient ensemble et directement
le CEC aux portes d’entrée et d’oubli sans devoir d’abord être modulées par la
porte de sortie Gers et Schmidhuber [2000]. Ils rapportent que ces connexions
améliorent les performances sur les tâches de synchronisation où le réseau doit
apprendre à mesurer des intervalles précis entre les événements. Concrètement,
ces connexions impliquent que les portes d’entrée et d’oubli prennent l’état de
la cellule à l’instant t-1 en entrée supplémentaire. La porte de sortie ne prend
elle que l’état de la cellule à l’instant t puisque la nouvelle valeur de l’état de la
cellule a alors déjà été calculée. Ces informations supplémentaires induisent une
amélioration de la capacité des LSTM à apprendre des tâches qui nécessitent
un timing précis et une énumération des états internes [Lipton et al., 2015;
Greﬀ et al., 2017].

Gated Recurent Unit, GRU
•

Cho et al. [2014] ont proposé une simpliﬁcation de l’architecture LSTM sous le
nom de Gated Recurrent Unit (GRU). Ils n’utilisent ni connexions peepholes
ni fonctions d’activation de sortie, mais ont choisi de fusionner les portes d’en-
trée et d’oubli en une seule porte, dite de mise à jour. Enﬁn, ils ont modiﬁé la
fonction de la porte de sortie (renommée aussi porte de réinitialisation) aﬁn
qu’elle ne soit plus en charge que de la transmission des connexions récur-
rentes à l’entrée de bloc. Si les premiers résultats de comparaison entre GRU
et LSTM ont amené des résultats mitigés, depuis de nombreuses recherches
se sont focalisées sur cette variante des LSTM et l’amélioration de ses perfor-
mances [Chung et al., 2014].

D’autres implémentations ont été proposées (comme le full gradient de
Graves et Schmidhuber [2005]) et testées pour diﬀérentes tâches (reconnais-
sance de l’écriture manuscrite, modélisation acoustique) et diﬀérents corpus de

Extraction de règles de réseaux récurrents

83

2.3. Deux modèles de RNN : SRN et LSTM

données (corpus de parole TIMIT, base de données en ligne d’écriture manus-
crite IAM ou encore un jeu de données de modélisation musicale polyphonique
nommé JSB Chorales). Cependant, il en ressort que malgré les très bonnes
performances des diﬀérents modèles, il n’y a pas de modèle dominant : cha-
cun est performant pour une tâche donnée ou pour traiter un corpus donné
[Greﬀ et al., 2017]. Ces résultats nous ont donc convaincus d’étudier et d’incor-
porer les LSTM standards dans nos travaux, plutôt qu’une de ses variations.
La reproduction des expériences sur des variations pourra être envisagée en
perspective de ce travail de thèse.

Apport des LSTM dans l’apprentissage séquentiel sur le long terme

Tous les RNNs ont des boucles de rétroaction au niveau de la couche ré-
currente : c’est ce qui leur permet de maintenir l’information en mémoire sur
plusieurs pas de temps. Le problème, comme nous l’avons vu précédemment,
est que selon la longueur des séquences, la capacité des RNNs à conserver
le passé lointain est altéré puisque celui-ci est progressivement écrasé par un
passé plus récent (expliqué en section 2.3.1). Face à cette problématique les
LSTM semblent apporter une solution eﬃcace : grâce au système de mémoire
intermédiaire dû au mécanisme de portes d’une unité LSTM, l’information im-
portante est sauvegardée, mais surtout préservée, jusqu’à son utilisation. Elle
n’est ni impactée par le bruit, ni oubliée ou écrasée. Cela permet donc que les
dépendances séquentielles soient prises en charges par les LSTM et qu’elles ne
constituent plus un obstacle à leur utilisation. Gers et al. [1999] a notamment
montré que les LSTM étaient performants pour la prédiction des éléments
constituant des séquences de longueur ﬁnie de tailles variables, mais aussi de
ﬂux continus. Gers et Schmidhuber [2001] ont aussi montré la supériorité des
LSTM (leur modèle comportait des connexions peepholes) pour l’étude et l’ap-
prentissage de langages non-contextuels et contextuels. Ils ont d’ailleurs été les
premiers à utiliser avec succès un RNN-LSTM pour l’apprentissage de langage
contextuel. Au vu de ces résultats et de l’ensemble des détails techniques four-
nis dans cette partie, les LSTM semblent donc un dispositif de traitement de
séquence adaptatif très général et prometteur. Nous verrons dans le chapitre
3 notre implémentation et les travaux menés sur ce modèle.

Apport des LSTM dans l’évanouissement du gradient

Au delà de pallier la limite du SRN, les LSTM permettent de résoudre le
problème de l’évanouissement du gradient dont souﬀre la plupart des RNN
utilisant l’algorithme d’apprentissage BPTT (expliqué en section 2.3.1). Dans
un réseau multi-couches, les gradients pour les couches les plus profondes sont
calculés en tant que produits de nombreux gradients (des fonctions d’activa-
tion). Lorsque ces gradients sont petits ou nuls, ils disparaissent et lorsqu’il
sont supérieur à 1, ils tendent à exploser. Il devient donc très diﬃcile de les

84

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

calculer. Dans la récurrence du LSTM au niveau du CEC, la fonction d’ac-
tivation est la fonction d’identité avec une dérivée de 1.0. Ainsi, le gradient
rétropropagé ne s’annule pas et n’explose pas, mais reste constant. Lorsque la
porte d’oubli est activée (activation proche de 1.0), le gradient ne disparait
pas. Puisque l’activation de la porte d’oubli ne peut être supérieur à 1.0, le
gradient ne peut pas exploser non plus. Cette récurrence, alliée au système de
portes, explique pourquoi les LSTM sont adaptés et eﬃcaces pour apprendre
les dépendances à long terme, et cela indépendamment de la profondeur du
réseau ou la longueur de la séquence d’entrée.

2.4 Extraction des règles et de la grammaire à

partir d’un RNN

Il existe deux types d’algorithmes dans le domaine d’extraction des connais-
sances : ceux qui s’appliquent sur des modèles généraux de réseaux de neurones
et ceux qui raﬃnent des connaissances déjà connues [Remm, 1996]. Dans nos
travaux, nous nous intéressons surtout à la première catégorie, et plus précisé-
ment aux réseaux de neurones récurrents, puisque nous souhaitons accéder à
la mémoire implicite de ces réseaux et mettre en évidence les représentations
implicite qu’ils encodent. Pour plus de détails sur les diﬀérents algorithmes
d’extraction des connaissances à partir de diﬀérents modèles, tels les MLP, le
lecteur pourra se référer à Remm [1996].

L’extraction des règles à partir d’un RNN se déﬁnit comme le processus
consistant à rechercher, trouver et construire les grammaires encodées par les
RNN après un apprentissage séquentiel. Autrement dit, cela revient à extraire
un automate à état ﬁnis A tel que les séquences du langage L (généré par
une grammaire) acceptées par A soient équivalentes à celles acceptées par le
RNN [Jacobsson, 2005; Weiss et al., 2017]. Le but d’un tel processus vise,
entre autres, à éclaircir ce qui se passe dans un RNN lors de l’apprentissage
et résoudre la question des RNN "boites noires" [Omlin et Giles, 2000] en
mettant en évidence les règles encodées. De nombreux travaux ont été menés
dans ce sens pour extraire ces règles sous la forme d’un automate [Lawrence
et al., 2000; Omlin et Giles, 2000; Smith, 2003; Jacobsson, 2005; Zeng et al.,
1993; Bhargava et Purohit, 2011; Karpathy et al., 2015; Strobelt et al., 2016;
Wang et al., 2017; Murdoch et Szlam, 2017]. Nous allons aborder et décrire
dans cette section les étapes générales du procédé d’extraction des règles. Pour
cela nous utiliserons les concepts déﬁnis dans la section 2.1. Nous ﬁnirons cette
section en discutant les paramètres critiques qui peuvent impacter les résultats
recherchés. La ﬁgure 2.1 présente une vision globale du processus d’extraction
des règles à partir d’un RNN.

Extraction de règles de réseaux récurrents

85

2.4. Extraction des règles à partir d’un RNN

2.4.1 Les cinq étapes de l’extraction des règles à partir

d’un réseau de neurones récurrents

Parmi les références les plus complètes portant sur la question de l’extrac-
tion des règles, nous soulignerons celle de Jacobsson [2005] et plus récemment
celle de Weiss et al. [2017]. La première oﬀre une revue de l’ensemble des mé-
thodes d’extraction des règles à partir des RNN et propose une taxonomie
de ces diﬀérentes méthodes. Le point majeur de cet article est que Jacobson
réussit à déterminer ce qu’il désigne comme ”les quatre ingrédients clés” d’un
processus d’extraction. La seconde référence quant à elle, oﬀre une revue plus
récente, mise à jour et surtout montre le regain d’intérêt de la communauté
scientiﬁque pour ce domaine. Nous avons choisi de présenter ici cinq ingré-
dients et non pas quatre. Pour plus de clarté, nous ajoutons la génération des
patterns d’activations de la couche cachée en tant qu’ingrédient 0 aﬁn d’inclure
l’ensemble des éléments. Nous détaillerons dans ce qui suit le processus d’ex-
traction global à partir d’un RNN en commentant les diﬀérentes techniques
existantes pour chaque ingrédient.

Figure 2.17 – Itérations d’un algorithme d’extraction d’automate avec un
paramètre de quantiﬁcation q = 3 utilisé sur un RNN avec deux neurones,
entrainé sur un langage binaire. L’espace d’état est divisé en une région d’ac-
ceptation et une de rejet (respectivement en gris et en blanc). L’algorithme
développe le graphe jusqu’à ce que tous les nœuds aient deux arcs sortants.
A et B représentent les deux premières itérations, C et D représentent les 2
dernières. Exemples de Giles et al. [1991] extrait de Jacobsson [2005]

86

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

Figure 2.18 – Exemple d’extraction de règles à travers une recherche en lar-
geur dans un espace d’états groupés par k-means. A - Les états du RNN sont
échantillonnés pendant l’entraînement. B - Ces états sont regroupés en un
nombre prédéﬁni de clusters. C - Une recherche en largeur est eﬀectuée (voir
la ﬁgure 2.17 ). D - L’automate est construit. Images extraites de Jacobsson
[2005]

Ingrédient 0 : Les patterns d’activations de la couche cachée du RNN

Bien qu’il ne soit pas cité par Jacobsson [2005], il nous a semblé important
d’expliciter quelles sont les données nécessaires pour le processus d’extraction
sous la bannière "Ingrédient 0" pour la bonne compréhension des explica-
tions qui suivent. Ainsi nous considérons que l’élément initial incontournable
d’un processus d’extraction de règles est le RNN à partir duquel l’extrac-
tion est réalisé. Certains auteurs aﬃrment que le processus d’extraction peut
aussi avoir lieu durant l’apprentissage. Pour nos travaux, nous avons choisi de
nous focaliser sur le processus d’extraction une fois l’apprentissage réalisé et
la représentation interne stabilisée, car celle ci évolue durant l’apprentissage
[Servan-Schreiber et al., 1988]. Une fois l’apprentissage ﬁnalisé, il faut réaliser
la phase de test et enregistrer à chaque pas de temps, le pattern d’activation de
la couche cachée du RNN. En eﬀet, c’est l’ensemble des patterns d’activation
de la couche cachée à chaque pas de temps qui contient la représentation im-
plicite des règles que nous souhaitons extraire. Ce sont ces patterns d’activités
qui serviront de données à la suite du processus d’analyse.

Ingrédient 1 : La quantiﬁcation de l’espace d’états continu

Chaque pattern de la couche cachée d’un RNN peut être déﬁni comme une
représentation interne i du réseau à un instant t, en fonction de l’entrée reçue
par le réseau à l’instant t, mais aussi de celles traitées avant. Cette représen-
tation "se retrouve localisée" quelque part dans l’espace d’états du RNN. Cet
espace est considéré continu car les patterns de la couche cachée et de la couche
de sortie sont composés de valeurs réelles qui appartiennent à des intervalles
continus. Ainsi l’exploration d’un espace d’état d’un RNN peut théoriquement
et très probablement être de durée inﬁnie. Pour solutionner cela, la première

Extraction de règles de réseaux récurrents

87

2.4. Extraction des règles à partir d’un RNN

étape du processus consiste en la quantiﬁcation de cet espace d’états en un
ensemble d’états discrets. Le but est de cartographier l’ensemble des états dis-
crets correspondant aux états de l’automate que nous souhaitons extraire.

Depuis les années 90, de nombreuses stratégies ont été proposées pour
cela : Omlin et Giles [1996b] ont proposé l’équipartitionnement. Cette tech-
nique consiste à séparer chaque coordonnée de l’espace de sortie en intervalles
égaux nommés hypercubes et de traiter chacun d’entre eux comme un état.
Le paramètre central est alors le degré de quantiﬁcation q. Les auteurs ont
commencé avec q=2 et ont ensuite incrémenté ce paramètre, jusqu’a obtenir
un automate qui reconnaisse les séquences du corpus de test. La ﬁgure 2.17,
label A, présente un exemple d’équipartionnement avec un paramètre q=3 sur
un RNN de deux neurones entrainé sur un langage binaire. L’espace d’état
de chaque neurone est divisé en trois, ce qui forme neuf partitions égales, soit
les hypercubes. D’autres méthodes pour déterminer ce paramètre q ont été
explorées [Giles et al., 1991, 1992a; Omlin et Giles, 1996a], mais ont toutes
présenté des limites. En appliquant une quantiﬁcation uniforme sur l’ensemble
de l’espace de sortie, elle aboutit à une explosion du nombre des états (même
lorsque cela porte sur un petit réseau), ce qui rend l’exploration fastidieuse et
longue.

Une autre approche de quantiﬁcation de l’espace d’états se base sur des
approches de clustering (regroupement) telles que la quantiﬁcation vectorielle
avec l’algorithme des k-means [Zeng et al., 1993], les cartes auto-organisatrices
[Blanco et al., 2000] ou encore le clustering hiérarchique [Cleeremans et al.,
1989; Servan-Schreiber et al., 1989; Elman, 1990]. Dans ces approches, il est
considéré que chaque élément d’une séquence traitée par le réseau appartient à
un état via le pattern d’activité généré par le réseau à ce moment là. Ce sont ces
patterns qui sont soumis aux techniques de clustering aﬁn de dégager des clus-
ters. Ce partitionnement de l’espace d’états déﬁni par les clusters est ensuite
exploré de la même manière que celui décrit dans Omlin et Giles [1996b]. Sa
taille, beaucoup plus petite, le rend alors plus facilement exploitable. La ﬁgure
2.18 , label B, présente un espace d’état ayant été quantiﬁé par la technique de
clustering du k-means. Pour des revues sur les techniques d’extraction de règles
à partir de RNN, le lecteur se référera à Wang et al. [2017] et Jacobsson [2005].

Ingrédient 2 : La génération des états et des sorties

Une fois le partitionnement réalisé, le seconde étape consiste en la généra-
tion des états et des sorties (et de la classiﬁcation de sortie si nécessaire) en
donnant les patterns d’entrées au RNN. En reprenant le corpus de test, (le
même que celui utilisé pour la quantiﬁcation) et en le fournissant en entrée

88

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

au RNN, on obtient alors les mêmes patterns que précédemment. Dans le cas
où des techniques de clustering ont été utilisées pour la quantiﬁcation, chaque
pattern hi sera alors associé à une partition que nous avons préalablement
identiﬁé. Dans le cas où ce sont des techniques d’équipartionnement selon Om-
lin et Giles [1996b] qui ont été utilisées, c’est une recherche en largeur sur les
états qui est eﬀectuée. Chaque pattern pointant une partition, en alimentant le
réseau avec des patterns d’entrée, il est possible de déterminer quelles sont les
partitions qui correspondent aux données. Ce processus est réitéré jusqu’a ce
qu’aucune nouvelle partition ne soit visitée. Il est alors possible de regrouper
les patterns de la couche cachée selon la partition (l’état) auxquels ils sont
liés, mais aussi de trier ces états et ne garder que ceux qui correspondent aux
données. C’est la phase d’élagage.

Ingrédient 3 : La construction de l’automate

La construction de l’automate à états ﬁnis peut avoir lieu en même temps
que l’étape précédente, mais pour plus de clarté, ces deux étapes ont été sé-
parées conceptuellement. Le processus est le suivant : Lorsque les entrées (du
corpus de test) sont soumises au RNN, à chaque pas de temps, le pattern
d’activité de la couche cachée va indiquer un état donné. Au premier pas de
temps t0, un premier pattern d’activité de la couche caché h0, désignera un état
(ﬁgure 2.17, label A). Ce dernier devient le nœud de départ du graphe repré-
sentant l’automate : ce nœud initial représente alors l’état initial q0. Le second
pattern d’activité au pas de temps suivant t1 va fournir le second état qui sera
alors représenté par un second nœud dans le graphe-automate. La transition
entre les deux clusters (et donc les deux états) sera alors représentée par un
arc dans le graphe-automate et ainsi de suite, jusqu’a ce que tous les patterns
de la couche cachée ait été traités. Si un pattern d’activité hi, correspond à
un état déjà visité et dont le nœud existe déjà dans le graphe-automate, alors
seul un arc (une transition) vers ce nœud est ajouté à l’automate (ﬁgure 2.17,
label C, cas des transitions entre les nœuds 4 et 2). Si deux patterns hi et hj
correspondent aux mêmes clusters, et donc au même nœud, un arc récurrent
sur ce nœud est alors ajouté au graphe-automate. Cet état possèdera alors une
connexion récurrente sur lui même (ﬁgure 2.17, label C, cas des nœuds 3 et 5).

Ingrédient 4 : La minimisation de l’automate

Selon Jacobsson [2005], ce dernier ingrédient ne fait pas réellement partie du
procédé d’extraction des règles à partir des RNN. Cependant, il insiste pour le
présenter car de nombreux auteurs utilisent l’algorithme de minimisation stan-
dard Hopcroft et al. [1979] aﬁn de réduire l’automate obtenu à l’étape précé-
dente en le plus petit automate possible. En eﬀet, selon la grammaire encodée
dans les séquences, et la technique de partitionnement, l’automate extrait peut

Extraction de règles de réseaux récurrents

89

2.4. Extraction des règles à partir d’un RNN

s’avérer très dense et complexe à interpréter. Pour faciliter cette étape, cer-
tains ont donc recours à l’algorithme de minimisation standard qui permet
ainsi de réduire la complexité de l’automate-graphe en terme de nombres de
nœuds et de transitions. Une fois l’automate obtenu, il n’est pas garanti qu’il
soit correct. Il faut donc le tester pour le valider. Cette phase de test consiste
à utiliser à nouveau les séquences du corpus de test sur l’automate résultant
du processus d’extraction aﬁn de vériﬁer sa validité.

2.4.2 Les étapes critiques du procédé

Dans l’extraction de règles à partir d’un RNN, l’espace d’état continu est
approximé par un ensemble ﬁni d’états, et la dynamique du RNN est représen-
tée par les transitions entre cet ensemble discret d’états. Cependant, il existe
deux étapes critiques lors de ce processus d’extraction : La première concerne
l’étape de quantiﬁcation et la seconde l’étape de construction de l’automate
ou plutôt sa validation. Selon la stratégie de quantiﬁcation de l’espace d’état
d’un RNN qui a été choisie, le nombre de partition peut varier. Or c’est ce
dernier qui détermine le nombre d’états (de nœuds) de l’automate recherché.
Il n’existe pas de solutions préconisées concernant le choix de la méthode pour
l’étape de quantiﬁcation. De nombreux auteurs proposent des approches diﬀé-
rentes (quantiﬁcation, clustering, approche stochastique, etc.), chacune dotée
de ses limites, aﬁn de tenter de trouver l’automate qui approxime le mieux la
grammaire recherchée. Parmi ces méthodes, il faut souligner les approches de
clustering. Il s’agit d’un domaine de recherche en soi dans le machine learning
où de nombreux travaux sont en cours et voient régulièrement le jour. Par
ailleurs, l’un des problèmes essentiels en utilisant le clustering est qu’il n’existe
pas de manière ﬁable de relier les clusters entre eux selon un ordre temporel.
En eﬀet, si l’on se réfère à Elman, il a été prouvé et nous le verrons aussi
par la suite, que la relation temporelle entre les états ﬁnit par être perdue à
un moment, surtout dans le cas où les séquences d’entrées sont porteuses de
dépendances sur le long terme. Il devient donc diﬃcile de relier de manière
ﬁable les clusters entre eux selon un ordre temporel précis.

2.4.3 Grammaire connue versus grammaire inconnue

Dans l’ensemble des travaux cités et décrits ci-dessus, il est souvent fait
référence à l’extraction d’un automate que l’on compare à une grammaire
connue. Celle-ci, ayant servi à générer les séquences utilisées pour les phases
d’apprentissage et de tests, se retrouve encodée dans l’espace d’états du RNN
utilisé. Si en eﬀet, de nombreux travaux sont réalisés dans des domaines où les
grammaires sont connues, il en existe d’autres dans le but de mettre en évidence
des grammaires inconnues [Giles et al., 1992b]. Le procédé d’extraction reste
alors le même et celui de validation de l’automate aussi. La diﬀérence majeure

90

Ikram Chraibi Kaadoud

CHAPITRE 2 : RÉSEAUX DE NEURONES RÉCURRENTS

réside dans le fait qu’il n’est pas possible de comparer le ou les automates
obtenus à une grammaire aﬁn de déterminer le bon nombre de clusters par
exemple ou de partitions.

2.5 Conclusion

Dans ce chapitre nous avons souhaité présenter l’ensemble de connaissances
techniques nécessaires à la compréhension de nos travaux. Nous avons ainsi
déﬁni et décrit ce que sont des réseaux de neurones récurrents, ainsi que la
problématique à laquelle ils semblent répondre. Ensuite nous avons décrit plus
en détails deux réseaux en particulier. Le SRN est l’un des plus utilisés dans le
domaine de l’apprentissage implicite et a fait l’objet de nombreux travaux sur
l’extraction des règles. Les LSTM sont un modèle qui bien qu’ancien, connait
un véritable regain d’intérêt ces dernières années de la part de la communauté
scientiﬁque grâce à leurs nombreuses caractéristiques qui semblent pallier les
défauts des précédents modèles. Nous avons ensuite détaillé et décrit le pro-
cessus d’extraction des règles à partir des réseaux de neurones récurrents, en
soulignant les étapes critiques, aﬁn de rappeler qu’il existe dans ce domaine
encore de nombreuses questions à résoudre. En eﬀet, que ce soit au niveau
des RNNs, ou des procédés d’extraction des règles, de nombreux travaux sont
encore en cours (notamment dans le traitement du langage naturel) et la ques-
tion de l’adéquation "solution choisie - problématique considérée" reste donc
une question ouverte à discussion.

Dans le chapitre suivant nous allons présenter les travaux réalisés dans
le cadre de cette thèse : nous détaillerons nos implémentations d’un SRN et
d’un RNN-LSTM. Nous présenterons notamment, les travaux d’extraction des
connaissances que nous avons mis en place à partir des LSTM.

Extraction de règles de réseaux récurrents

91

2.5. Conclusion

92

Ikram Chraibi Kaadoud

Chapitre 3

Implémentation des RNNs et
extraction de règles

Table des matières

3.1
3.2 Les grammaires utilisées

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
. . . . . . . . . . . . . . . . . . . . . . 95
3.2.1 La grammaire manuelle . . . . . . . . . . . . . . . . . . . 96
3.2.2 La grammaire de REBER (RG) . . . . . . . . . . . . . . 97
3.2.3 Les variantes de la grammaire de REBER . . . . . . . . 98
3.3 Comparaison du SRN et RNN-LSTM . . . . . . . . . . . . . . . 99
3.3.1
SRN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
3.3.2 RNN-LSTM . . . . . . . . . . . . . . . . . . . . . . . . 115
3.4 Extraction de règles à partir du RNN-LSTM . . . . . . . . . . . 121
3.4.1 Patterns d’activités de la couche cachée . . . . . . . . . . 122
3.4.2 Algorithme d’extraction des règles
. . . . . . . . . . . . 124
3.4.3 Application à la grammaire de REBER et à ses variations127
3.4.4 Application à une grammaire électrique (EL) . . . . . . . 133
3.4.5 Discussion sur l’extraction de règles à partir de RNN-

3.5 Discussion

LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141

3.1

Introduction

Nous avons présenté dans le chapitre 1 les données porteuses d’informations
que nous avons mises en évidence via un travail de préparation et d’extraction.
Nous avons ainsi mis en avant l’importance des séquences de composants élec-
triques dans les schémas d’une entreprise cliente, car ce sont elles qui portent la
connaissance métier d’une entreprise : l’empreinte en matière de conception et
les règles de connectivité électrique. Dans nos travaux, nous avons présenté une

93

3.1. Introduction

séquence comme une suite de composants électriques ordonnés selon leurs co-
ordonnées (x,y) extraites des folios. On pouvait ainsi caractériser chaque folio
par une seule séquence, et étudier ensuite les similitudes entre diﬀérents folios
de diﬀérents schémas électriques. En réalité, les séquences lues et extraites par
les experts d’Algo’Tech sont plus complexes. Elles correspondent toujours à
une suite de composants électriques, mais ces derniers sont sur le même câble.
En eﬀet, à la lecture des schémas, les experts d’Algo’Tech parcourent les folios
d’un même schéma un à un, en suivant les ﬁls électriques. Or si ces derniers
commencent tous (schématiquement) depuis le même folio, ils ne font pas tous
le même chemin physique (dans la réalité) et ne sont pas destinés à la même
fonction. Cela implique donc qu’il existe plusieurs lectures à réaliser au sein
d’un même schéma électrique et qu’un folio peut être parcouru plusieurs fois,
selon le câble étudié.

Figure 3.1 – Deux renvois folios de 2 ﬁls diﬀérents mais allant à un même
folio

L’importance de ces séquences réside dans le fait qu’elles sont porteuses de
dépendances séquentielles implicites sur le long terme : en eﬀet, le choix des
équipements à la ﬁn du câble dépend de l’ensemble des choix réalisés en amont
et cela dès le premier folio. Ainsi, notre but est de trouver une approche pour
mettre en évidence à la fois les connaissances implicites (les règles de connecti-
vité électrique entre les composants) mais aussi les dépendances séquentielles.
La particularité de notre domaine de travail est qu’il n’existe pas de grammaire
explicite pour chaque entreprise-cliente, présentant les règles et habitudes de
travail de chacun. Vouloir extraire ces règles revient donc à reconstruire une
grammaire que nous n’avons pas et que nous ne pouvons valider sans l’échange
avec un expert du domaine concerné, qu’il soit d’Algo’Tech ou de l’entreprise-
cliente. Pour cela il aurait fallu réaliser la vériﬁcation des diﬀérentes règles
extraites (à chaque changement de paramètrage pour chacun de nos RNNs) en
comparant celles-ci aux schémas électriques et aux bases de données de chaque
client, ce qui engendre un travail volumineux trop chronophage pour solliciter
des experts d’Algo’Tech dessus.

Au vu des nombreux paramètres et contraintes que l’exploitation des don-
nées électriques imposait, nous avons choisi dans un premier temps, de tra-

94

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

vailler sur des séquences, non seulement similaires en termes de caractéris-
tiques, que nous détaillerons ensuite, mais aussi issues d’une grammaire connue.
Nous souhaitions ainsi fournir une preuve de concept valide dans un environ-
nement dont les paramètres sont maitrisés. En nous inspirant de la littérature
[Reber, 1967; Cleeremans, 1993], nous avons donc choisi pour nos travaux
d’utiliser comme base de travail, la grammaire de Reber, qui se rapproche le
plus du domaine électrique. En eﬀet, cette grammaire permet de générer des
séquences qui ont les mêmes caractéristiques que les séquences électriques :

— elles sont de tailles variables
— elles sont porteuses de contexte et donc de dépendances : le choix des

derniers éléments se fait en fonction des premiers

— l’occurrence de chaque élément dans chaque séquence est variable : une
occurrence ou plusieurs, une fois ou plusieurs fois dans une même sé-
quence.

— Les règles de connectivité entre les éléments d’une même séquence sont
ﬁxes, ce qui rejoint le coté normé, codiﬁé, réglementé du domaine élec-
trique.

Dans un second temps, une fois notre approche validée sur des grammaires
artiﬁcielles, nous la testerons sur des séquences électriques. Bien que le travail
d’extraction d’une grammaire électrique et de sa validation soit conséquent,
nous avons tenu à tester notre approche dans un cas réel aﬁn de la valider sur
une problématique industrielle concrète.

Pour aborder ce chapitre, nous allons dans un premier temps préciser les
diﬀérentes grammaires utilisées ainsi que leurs caractéristiques. Ensuite nous
présenterons nos résultats selon deux axes : le premier axe proposera un com-
paratif des performances du SRN et d’un RNN-LSTM lors de l’apprentissage
des séquences issues de la grammaire de Reber et de ses variantes. Le second
axe, quand à lui, décrira l’approche nouvelle que nous avons mis en place pour
l’extraction des règles encodées par un RNN-LSTM.

3.2 Les grammaires utilisées

Quatre grammaires ont été utilisées lors de nos travaux : une grammaire
manuelle créée par nos soins, la grammaire de Reber telle qu’il l’a proposé en
1967 [Reber, 1967], ainsi que deux de ses variantes. Chacune a permis de tester
un aspect particulier des réseaux de neurones : telles que les performances lors
de la prédiction, la résistance à la taille des séquences ou encore la bonne
rétention des informations lors de dépendances sur le long terme entre les
éléments. Les ﬁgures 3.2 et 3.4 présentent les quatre grammaires et la table
3.1 décrit leurs paramètres.

Extraction de règles de réseaux récurrents

95

3.2. Les grammaires utilisées

Figure 3.2 – Grammaire manuelle à gauche et Grammaire de Reber à droite
(Image extraite de Servan-Schreiber et al. [1988])

Grammaire

Nombre

de symboles de nœuds

Manuelle
Reber
Reber embarquée
Reber embarquée et continue

5
7
7
7

Nombre Nombre
d’arcs
6
10
24
26

4
6
14
14

Table 3.1 – Descriptif des diﬀérentes grammaires

3.2.1 La grammaire manuelle

Le but de la grammaire manuelle est de permettre l’étude des performances
des réseaux dans leur capacité à retenir des dépendances sur le long terme. Elle
permet en eﬀet de générer deux types de séquences dont les débuts et ﬁns sont
diﬀérents :

— Le type 1 est ACF
— Le type 2 est BCE

La diﬀérence entre des séquences d’un même type réside dans le nombre
de C intermédiaire entre les symboles de début et de ﬁn de séquence. Après
apprentissage de ces séquences, lors de la phase de test, en étudiant les patterns
d’activations de la couche de sortie à la ﬁn de chaque séquence (au moment
de prédire le dernier symbole), il est possible d’étudier la capacité du réseau à
conserver une information sur le long terme : En d’autres termes, nous testons
la résistance du réseau de neurones à prédire F ou E à chaque fois que la
séquence commence par, respectivement A ou B, et cela en faisant varier le
nombre de C intermédiaire. Le symbole D ne fait pas partie de la grammaire
manuelle, mais est utilisé dans les tests, notamment comme une mesure de
contrôle. Ne faisant pas partie du séquences générées par cette grammaire, il
ne devrait pas être prédit par le réseau.

96

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Figure 3.3 – Grammaire de Reber avec nœuds numérotés. Image extraite de
Servan-Schreiber et al. [1988]

3.2.2 La grammaire de REBER (RG)

La grammaire à états ﬁnis de Reber (RG pour Reber’s Grammar), présen-
tée dans la ﬁgure 3.3, a longuement été étudiée dans la littérature notamment
dans le cadre d’expériences d’apprentissage de grammaire artiﬁcielle pour étu-
dier les mécanismes d’apprentissage implicite [Cleeremans et McClelland, 1991;
Cleeremans et al., 1989]. Bien que nous l’ayons présenté précédemment, nous
allons maintenant détailler cette grammaire et présenter son intérêt pour nos
travaux. Cette grammaire à états ﬁnis est dans les faits un graphe orienté, se
composant de nœuds et d’arc de transitions. Ces derniers possèdent à la fois
un sens ainsi qu’une étiquette. Sur la ﬁgure 3.3 l’arc qui va du nœud #0 vers
le nœud #1 a pour étiquette le symbole T. Dans les expériences menées, les
séquences générées possèdent les particularités suivantes :

— elles sont de tailles variables
— elles commencent toutes par le symbole B pour "Begin", qui désigne

donc le début d’une séquence

— elles se terminent toutes par le symbole E pour "End", qui désigne donc

la ﬁn d’une séquence

Algorithme de génération des séquences

Les séquences sont générées comme suit : chaque séquence (vide au début)
est initialisée avec un B, et le nœud #0 est désigné comme nœud initial. Les
arcs dirigés (avec un sens) ayant tous une probabilité de 0.5 d’être choisis, c’est
de manière aléatoire qu’est sélectionné le nœud suivant parmi les successeurs
du nœud courant. Une fois le nœud sélectionné, l’étiquette de l’arc choisi est
ajoutée à la séquence. A ce stade, le résultat est soit BT ou BP. Une fois le
nœud suivant sélectionné, le processus de sélection aléatoire est répété parmi
les successeurs du nœud courant, et ainsi de suite jusqu’à ce que le nœud #5
soit atteint. Lorsque cela arrive, la séquence est considérée ﬁnie puisque le
nœud #5 ne possède pas d’arcs vers d’autres nœuds. Le symbole E est alors

Extraction de règles de réseaux récurrents

97

3.2. Les grammaires utilisées

ajoutée à la séquence, la concluant ainsi. La table 3.2 fournit un exemple de
séquences générées à partir de la grammaire.

Numéro séquence
1
2
3
4
5

séquence
BTXSE
BPVVE
BTSXXVPSE
BPTTTTVPSE
BTSSSSSSSSSSSXSE

Table 3.2 – Exemple de séquences générées à partir de la grammaire de Reber

L’importance du contexte dans la grammaire de Reber pour la mo-
délisation des séquences de composants électriques

La particularité de la grammaire de Reber est de proposer, à l’exception
des symboles de début et de ﬁn de séquence, chaque symbole deux fois, mais
pas dans le même contexte. En eﬀet, si l’on observe, par exemple le symbole T,
celui-ci apparait, à la fois après B et après P. Sauf que dans le premier cas, il
mène à S ou X et dans le second cas, il mène à lui-même (il s’agit d’une boucle)
ou V. Cette diﬀérence de symbole en amont et symbole en aval, peut être as-
similée à une diﬀérence de contexte. Or dans le contexte électrique, chaque
composant électrique possède ses propres contraintes dues à son ampérage, le
type d’alimentation et tout un ensemble de caractéristiques techniques élec-
triques. Une entreprise-cliente prendra donc l’habitude de connecter chacun
des composants à d’autres qui lui correspondent (par exemple même ampé-
rage) selon le contexte environnemental à l’instant t. Un même composant
peut donc se retrouver dans deux séquences de composants électriques dif-
férentes mais correctes électriquement car elles répondent à deux contextes
diﬀérents. Il est donc important d’être capable d’étudier dans notre approche
des séquences où l’on retrouve les mêmes éléments (symboles pour la gram-
maire ou composants électriques pour le domaine électrique) mais dans des
contextes diﬀérents. Notre choix de la grammaire de Reber se trouve donc
justiﬁé par l’ensemble des caractéristiques que partagent les séquences qu’elle
permet de générer avec les séquences du domaine électrique. Par ailleurs, le
nombre de séquences et de nœuds étant petit, cela permet de lancer des si-
mulations de calculs à faible coût, en terme de temps et de puissance de calcul.

3.2.3 Les variantes de la grammaire de REBER

Aﬁn d’éprouver les performances des réseaux de neurones récurrents, nous
avons choisi d’exploiter deux variantes de la grammaire de Reber que nous

98

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Figure 3.4 – Grammaire de Reber embarquée à gauche et Grammaire de
Reber embarquée et continue à droite. Images extraites de Servan-Schreiber
et al. [1988]

présentons ci dessous et dans la ﬁgure 3.4.

La grammaire de Reber embarquée (ERG)

La grammaire de Reber embarquée (ERG) a pour but, tout comme la
grammaire manuelle, de tester les performances du réseau dans le fait de retenir
des dépendances sur le long terme. La diﬀérence réside dans une grammaire
beaucoup plus complexe : celle-ci ajoute de l’ambiguïté au niveau des symboles
de début et de ﬁn de séquences (ils se retrouvent au sein des séquences aussi et
non plus exclusivement aux extrémités) et fournit des séquences plus longues.

La grammaire de Reber embarquée et continue (CERG)

La grammaire de Reber embarquée et continue (CERG) est une variante
très complexe de la grammaire d’origine car elle supprime la notion de début
et de ﬁn de séquence. Le réseau de neurones doit apprendre un ﬂux continu de
séquences ERG mises bout à bout.

3.3 Comparaison du SRN et RNN-LSTM

Dans cette section, nous allons aborder les diﬀérences de performances avec
le SRN et un RNN-LSTM, modèle doté d’unités LSTM. Les deux modèles pos-
sèdent une architecture commune avec néanmoins une variation. Tout deux se
compose d’une couche d’entrée, d’une couche cachée et d’une couche de sortie.
Les variations résident d’une part dans la nature des cellules de la couche ca-
chée (dans SRN, les cellules sont des neurones artiﬁciels et dans RNN-LSTM,
ce sont des unités LSTM qui composent la couche cachée) et dans la constitu-
tion de la couche d’entrée (dans SRN, la couche d’entrée comporte aussi des
unités de contexte). Pour chacune de nos implémentations, nous nous sommes

Extraction de règles de réseaux récurrents

99

3.3. Comparaison du SRN et RNN-LSTM

inspirés de la littérature existante pour guider nos choix concernant l’archi-
tecture de chaque modèle et les paramètres de tests. Pour le SRN, l’article de
référence est Servan-Schreiber et al. [1988], et pour le RNN-LSTM, l’article
de référence est Gers et al. [1999]. Nous allons ainsi dans un premier temps,
décrire deux variantes du SRN d’un point de vue technique, et les tests menés,
ensuite nous aborderons le modèle RNN-LSTM, ses performances et caracté-
ristiques avant de réaliser un comparatif entre les deux modèles. La table 3.3
présente les diﬀérents modèles implémentés et leurs caractéristiques.

Pour chaque test, à chaque fois que nous générons un corpus de séquences,
nous donnerons la taille de celui-ci, la longueur moyenne et la déviation stan-
dard des séquences composant le corpus. Nous fournirons aussi le nombre
d’échantillons moyen générés par ce corpus et analysé par le réseau. Pour
rappel, les échantillons (couples de symboles entrée-sortie) proviennent des
séquences générées. Chaque séquence de taille N engendre N-1 échantillons.

Paramètres
Nombre unités
couche entrée
Nombre unités
couche sortie
Nombre de
couches cachées
Nombre unités
couche cachée

Nombre unités
couche contexte
Nombre de poids modiﬁés
durant apprentissage

SRN 1 SRN 2

7

7

1

3

3

7

7

1

15

15

RNN-LSTM
7

7

1

8 cellules
(4 blocs de 2
cellules chacun)

51

435

424

Table 3.3 – Description des diﬀérentes architectures des réseaux de neurones
récurrents testés

3.3.1 SRN

Protocole

Servan-Schreiber et al. [1988] ont été parmi les premiers à se pencher sur la
problématique de l’encodage des règles de grammaire au sein d’un SRN. Nous
avons donc choisi de construire notre modèle de SRN selon les paramètres
fournis dans leur article. Le nombre d’unité de la couche cachée a été ﬁxé à 3
unités puis 15 unités, ce qui portait le nombre d’unités de la couche d’entrée

100

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

à 7 + 3 = 10 unités puis 7 + 15 = 22 unités. 7 désignant le nombre d’unités
en entrée (et en sortie) nécessaire pour encoder les symboles de la grammaire
de Reber. Dans ces deux cas, le nombre de poids paramétrés nbWRnn−Elman se
calcule comme ce qui suit :

nbWRnn−Elman = (nbentree + nbcontexte)

nbcachee + nbcache

nbsortie

(3.1)

⇤

⇤

avec : nbentree, le nombre d’unités de la couche d’entrée, nbcontexte, celui de
la couche de contexte, nbcachee celui de la couche cachée et nbsortie celui de la
couche de sortie.

Ainsi pour le SRN comportant 3 unités cachées, le nombre de poids est de :

nbWRnn−Elman = (7 + 3)

3 + 3

⇤

⇤

7 = 51

Et pour 15 unités cachées :

nbWRnn−Elman = (7 + 15)

15 + 15

⇤

⇤

7 = 435

(3.2)

(3.3)

Pour le choix des paramètres, nous avons utilisé la méthode de Grid Search
appliquée aux paramètres recommandés par la littérature. Le taux d’apprentis-
sage a été ﬁxé à [0.01, 0.015, 0.02] , le momentum a pris les valeurs suivantes :
[0.01, 0.1, 0.3, 0.5, 0.7, 0.9]. Pour la fonction d’activation, c’est la fonction
sigmoïde qui a été appliquée. La table 3.4 récapitule les paramètres du SRN
utilisés ainsi que les tailles des corpus de séquences. La table 3.6 présente les
36 conﬁgurations d’apprentissages utilisée pour SRN. Nous avons regroupé les
conﬁgurations trois par trois sous le nom de groupe aﬁn de faciliter la discus-
sion des résultats ci dessous.

Nombre d’unités de la couche cachée
Taux d’apprentissage
Momentum
Taille du corpus d’apprentissage
Taille du corpus de test généré à partir de RG
Taille du corpus de test généré aléatoirement

3, 15
0.01, 0.015, 0.02
0.01, 0.1, 0.3, 0.5, 0.7, 0.9
200 000
20 000
130 000

Table 3.4 – Descriptif des diﬀérents paramètres utilisés lors de l’implémenta-
tion et du test de SRN

Pour chacune des 36 conﬁgurations, un corpus d’apprentissage composé de
200 000 séquences (générées à partir de la grammaire de Reber) a été créé, en
accord avec l’expérience de Servan-Schreiber et al. [1988]. Sur les 36 corpus,
la longueur moyenne des séquences est 8.00 et la déviation standard moyenne

Extraction de règles de réseaux récurrents

101

3.3. Comparaison du SRN et RNN-LSTM

est 3.37. Le nombre moyen d’échantillons est de 1 402 360.

Pour tester les performances du réseau pour chaque conﬁguration, après
apprentissage, nous avons réalisé 10 simulations. Pour chacune d’entre elles,
nous avons généré deux corpus de tests : un corpus composé de séquences
grammaticales et un corpus composé de séquences aléatoires. Pour ces deux
types de séquences, les probabilités de choix des symboles étaient égales (0.5).

Le premier corpus est composé de 20 000 séquences générées à partir de la
RG. Sur l’ensemble des 360 corpus de test générés, la longueur moyenne des
séquences est de 7.94 et la déviation standard moyenne de 3.30. Le nombre
moyen d’échantillons de test est de 140 510.

Le second corpus se composant de 130 000 séquences, a été généré aléa-
toirement en utilisant les symboles de la RG. Ces séquences non grammati-
cales, dites aussi aléatoires, débutent toutes par B et ﬁnissent toutes par E
mais ne respectent pas les transitions de la grammaire. La table 3.5 présente
un exemple de séquences aléatoires. Sur l’ensemble des 360 corpus aléatoires
générés, la longueur moyenne de ces séquences est de 7.98 et leur déviation
standard moyenne de 6.48. Le nombre moyen d’échantillons traité se porte à
908 627. Nous avons aussi testé la probabilité que des séquences générées aléa-
toirement respectent la grammaire. Pour chaque séquence, nous avons testé
les successions des symboles : A chaque symbole d’une séquence, si le symbole
suivant est un successeur potentiel d’après la grammaire, alors l’analyse de
la séquence continue, sinon elle est interrompue. Une séquence (entièrement
analysée) dont la succession de symboles respecte la grammaire est considé-
rée grammaticale. Sur les 360 corpus générés aléatoirement, le pourcentage de
séquences non grammaticales moyen est de 99.89%.

Numéro séquence
1
2
3
4
5

séquence
BE
BVPXE
BTPPPPPE
BPSE
BSPPTTTTTTTTE

Table 3.5 – Exemple de séquences générées aléatoirement à partir des sym-
boles de la grammaire de REBER

102

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Groupe de
conﬁguration conﬁguration

Numéro

G0

G1

G2

G3

G4

G5

G6

G7

G8

G9

G10

G11

C0
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
C11
C12
C13
C14
C15
C16
C17
C18
C19
C20
C21
C22
C23
C24
C25
C26
C27
C28
C29
C30
C31
C32
C33
C34
C35

Nombre
d’unités de
la couche cachée
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
15
15
15
15
15
15
15
15
15
15
15
15
15
15
15
15
15
15

Taux
d’apprentissage

Momentum

0.01
0.01
0.01
0.015
0.015
0.015
0.02
0.02
0.02
0.01
0.01
0.01
0.015
0.015
0.015
0.02
0.02
0.02
0.01
0.01
0.01
0.015
0.015
0.015
0.02
0.02
0.02
0.01
0.01
0.01
0.015
0.015
0.015
0.02
0.02
0.02

0.01
0.1
0.3
0.01
0.1
0.3
0.01
0.1
0.3
0.5
0.7
0.9
0.5
0.7
0.9
0.5
0.7
0.9
0.01
0.1
0.3
0.01
0.1
0.3
0.01
0.1
0.3
0.5
0.7
0.9
0.5
0.7
0.9
0.5
0.7
0.9

Table 3.6 – Conﬁgurations d’apprentissage pour SRN

Extraction de règles de réseaux récurrents

103

3.3. Comparaison du SRN et RNN-LSTM

Apprentissage
•

Nous faisons apprendre au SRN 200 000 séquences RG avec 36 conﬁgura-
tions d’apprentissage diﬀérentes, présentées table 3.6. Pour évaluer l’appren-
tissage, nous calculons l’erreur quadratique moyenne commise à chaque pas de
temps. Puis nous lissons les résultats en calculant une moyenne à chaque pas
de temps sur les 5000 dernières erreurs enregistrées.

Les tables 3.7 et 3.8 représentent le comportement du réseau doté de 3
unités cachées selon les conﬁgurations de C0 à C17. Seules les 2000 premières
moyennes d’erreurs sont représentées, car c’est là que l’on observe les variations
les plus importantes. On observe que pour toutes les valeurs du momentum
inférieures à 0.5, les courbes représentant les moyennes des erreurs sont dé-
croissantes. Cela implique que l’erreur diminue et donc que le réseau apprend à
prédire la sortie attendue. Pour les valeurs de momentum de 0.7 et 0.9, l’erreur
croit rapidement dès le début de l’apprentissage. Le réseau n’apprend donc pas.

Les tables 3.9 et 3.10 représentent le comportement du réseau doté de 15
unités cachées selon les conﬁgurations de C18 à C35. De même que pour les
conﬁgurations précédentes, ici aussi seules les 2000 premières moyennes d’er-
reurs sont représentées, car c’est là que l’on observe les variations les plus im-
portantes. Pour toutes les valeurs testées du momentum inférieures ou égales à
0.3, le réseau réussit à apprendre puisque sa courbe d’erreur moyenne décroit.
Par contre, il s’avère qu’il n’arrive pas à apprendre une fois que la valeur du
momentum dépasse 0.5. Pour l’ensemble des conﬁgurations de C27 à C35, le
SRN aﬃche une erreur moyenne croissante, puis stagnante.

Sur les 36 conﬁgurations, celles qui présentent les meilleurs résultats sont les
conﬁgurations C18, C19, C21, C22, C24 et C25. Ce sont les conﬁgurations pour
lesquelles l’erreur représentée est la plus basse entre [0,6 ; 0,7]. Rappelons que
l’erreur représentée est l’erreur quadratique moyenne lissée sur les 5000 derniers
pas de temps (une erreur étant calculée à chaque pas de temps). Prenons le
cas où l’erreur calculée E = 0, 6, Cela implique que l’erreur se répartit sur
les 7 unités de la couche de sortie du réseau. Chaque unité de sortie possède
alors une erreur E0 = (0, 6/7) = 0, 085. Or cette erreur étant au carré, cela
induit pour chaque unité de sortie une erreur de e0 = pE0 = p0, 085 = 0, 29.
Cette valeur est importante pour des unités dont les valeurs d’activités sont
comprises dans l’intervalle [0,1]. A ce stade, nous n’avons pas pu explorer les
raisons de ces résultats et leur signiﬁcation. Néanmoins, nous pensons que
des travaux supplémentaires dans cette direction sont essentiels pour la bonne
compréhension de cet algorithme.

104

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

2
0
.
0

e
g
a
s
s
i
t
n
e
r
p
p
a
’
d

x
u
a
T

5
1
0
.
0

e
g
a
s
s
i
t
n
e
r
p
p
a
’
d

x
u
a
T

1
0
.
0

e
g
a
s
s
i
t
n
e
r
p
p
a
’
d

x
u
a
T

.

m
o
M

1
0
.
0

1
.
0

3
.
0

Table 3.7 – Evolution de la moyenne de l’erreur lors de l’apprentissage d’un
SRN doté de 3 unités de la couche cachée selon les conﬁgurations d’apprentis-
sage C0 à C8 (présentation des 2000 premières moyennes)

Extraction de règles de réseaux récurrents

105

3.3. Comparaison du SRN et RNN-LSTM

5
.
0

7
.
0

9
.
0

2
0
.
0

e
g
a
s
s
i
t
n
e
r
p
p
a
’
d

x
u
a
T

5
1
0
.
0

e
g
a
s
s
i
t
n
e
r
p
p
a
’
d

x
u
a
T

1
0
.
0

e
g
a
s
s
i
t
n
e
r
p
p
a
’
d

x
u
a
T

.

m
o
M

Table 3.8 – Evolution de la moyenne de l’erreur lors de l’apprentissage d’un
SRN doté de 3 unités de la couche cachée selon les conﬁgurations d’apprentis-
sage C9 à C17 (présentation des 2000 premières moyennes)

106

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

2
0
.
0

e
g
a
s
s
i
t
n
e
r
p
p
a
’
d

x
u
a
T

5
1
0
.
0

e
g
a
s
s
i
t
n
e
r
p
p
a
’
d

x
u
a
T

1
0
.
0

e
g
a
s
s
i
t
n
e
r
p
p
a
’
d

x
u
a
T

.

m
o
M

1
0
.
0

1
.
0

3
.
0

Table 3.9 – Evolution de la moyenne de l’erreur lors de l’apprentissage d’un
SRN doté de 15 unités de la couche cachée selon les conﬁgurations d’appren-
tissage C18 à C26 (présentation des 2000 premières moyennes)

Extraction de règles de réseaux récurrents

107

3.3. Comparaison du SRN et RNN-LSTM

5
.
0

7
.
0

9
.
0

2
0
.
0

e
g
a
s
s
i
t
n
e
r
p
p
a
’
d

x
u
a
T

5
1
0
.
0

e
g
a
s
s
i
t
n
e
r
p
p
a
’
d

x
u
a
T

1
0
.
0

e
g
a
s
s
i
t
n
e
r
p
p
a
’
d

x
u
a
T

.

m
o
M

Table 3.10 – Evolution de la moyenne de l’erreur lors de l’apprentissage d’un
SRN doté de 15 unités de la couche cachée selon les conﬁgurations d’appren-
tissage C27 à C35 (présentation des 2000 premières moyennes)

108

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Généralisation de l’apprentissage
•

Pour tester la performance du réseau et sa capacité à généraliser son ap-
prentissage, nous réalisons 10 simulations consécutives pour chaque conﬁgu-
ration d’apprentissage. A chaque simulation, comme dans l’article d’origine,
deux corpus de tests sont générés : le premier comporte 20 000 séquences RG
grammaticales, le second 130 000 séquences non grammaticales. Pour chacune
des simulations, le réseau a été alimenté avec ces deux corpus et la prédiction
réalisée par le réseau a été analysée à chaque pas de temps : Une prédiction est
jugée correcte si, pour chaque symbole d’une séquence donnée, l’activation de
son successeur est supérieure à 0,3 (le maximum étant 1) dans le pattern d’ac-
tivité de la couche de sortie du réseau selon Servan-Schreiber et al. [1988]. Si
ce critère n’est pas rempli, la présentation de la séquence au réseau est arrêtée
et la chaîne est considérée rejetée car jugée non grammaticale. Une séquence
"acceptée" est une séquence pour laquelle le réseau a su prédire correctement
le symbole suivant à chaque pas de temps. Pour chaque simulation, deux me-
sures ont été réalisées : le pourcentage de séquences grammaticales acceptées
et de séquences aléatoires acceptées. Un SRN performant devrait avoir les per-
formances suivantes : un taux élevé (proche de 100%) de reconnaissances des
séquences grammaticales et un taux faible (proche de 0%) de reconnaissances
des séquences non grammaticales. Voici un exemple : Soit BTXSE une séquence
obéissant à la grammaire de REBER. Au pas de temps t0, le réseau reçoit le
vecteur binaire représentant B en entrée. En fonction de l’apprentissage réa-
lisé, le réseau fournit un vecteur de taille 7 composé de valeurs réelles, chacune
d’entre elles indiquant l’activité d’une unité de la couche de sortie. Les unités
dont l’activité est supérieure à 0.3, sont celles désignant les successeurs poten-
tiels. Un réseau peut donc au pas de temps t, prédire plus d’un successeur. Si
le symbole suivant, dans ce cas T, fait partie des successeurs potentiels pré-
dits par le réseau, le processus d’analyse de la séquence peut continuer. Dans
le cas inverse, la séquence est rejetée. Si la séquence est présentée dans son
ensemble au réseau, cela implique qu’elle est acceptée. Les ﬁgures 3.5, 3.6, 3.7
et 3.8 présentent les performances moyennes sur 10 simulations du SRN selon
les diﬀérentes conﬁgurations présentées dans la ﬁgure 3.6.

Résultat du SRN à 3 unités cachées :
La table 3.5 montre qu’un SRN de 3 unités cachées ayant appris avec un
faible taux d’apprentissage et un momentum faible, présente de faibles per-
formances. Aucune des conﬁgurations de C0 à C8, ne dépassent les 25% de
séquences grammaticales reconnues, ce qui est relativement faible. A contra-
rio, le réseau semble bien rejeter les séquences aléatoires. La table 3.7 aﬃche
l’évolution de l’erreur du SRN pour ces diﬀérentes conﬁgurations. Celle-ci dé-
croit, ce qui induit que le réseau réussi à faire un apprentissage.

La ﬁgure 3.6 montre les performances d’un SRN de 3 unités cachées uti-

Extraction de règles de réseaux récurrents

109

3.3. Comparaison du SRN et RNN-LSTM

Figure 3.5 – Evolution des performances d’un SRN de 3 unités cachées pour
les conﬁgurations C0 à C8. Moyennes obtenues sur 10 simulations.

Figure 3.6 – Evolution des performances d’un SRN de 3 unités cachées pour
les conﬁgurations C9 à C17. Moyennes obtenues sur 10 simulations.

110

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

lisant une conﬁguration recommandée par Servan-Schreiber et al. [1988]. Si
l’on observe les conﬁgurations des groupes G3 (C9 à C11), G4 (C12 à C14) et
G5 (C15 à C17) (ﬁgure 3.6), on constate que l’augmentation du momentum
entraine une augmentation du taux de reconnaissance des séquences gramma-
ticales au sein de chaque groupe, tout en gardant un taux de reconnaissance de
séquences aléatoires acceptable (inférieur à 40%). Les performances semblent
meilleures que celles précédentes alors que le réseau n’apprend pas forcément
bien (ses courbes d’erreurs sont croissantes pour un momentum supérieur à
0.5). Seules les conﬁgurations C9, C12 et C15 permettent au réseau d’ap-
prendre. Sur l’ensemble des conﬁgurations testées pour un SRN de 3 unités
cachées, la conﬁguration C11 semble être celle qui présente les meilleurs résul-
tats de généralisation, bien que la courbe d’erreur associée soit croissante.

Résultat du SRN à 15 unités cachées :

Figure 3.7 – Evolution des performances d’un SRN de 15 unités cachées pour
les conﬁgurations C18 à C26. Moyennes obtenues sur 10 simulations.

La ﬁgure 3.7 montre les performances d’un SRN de 15 unités cachées. Dans
cette ﬁgure, l’augmentation du momentum au sein de chacun des groupes
G6 (C18 à 19), G7 (C21 à C26) et G8 (C24 à C26) induit une baisse des
performances. Le taux de reconnaissance des séquences grammaticales baisse
de plus de 75% en moyenne (entre la plus haute performance et la plus basse)
sur les 3 groupes. A contrario, la variation du momentum, n’a pas d’impact
signiﬁcatif au niveau du taux de reconnaissance des séquences aléatoires. Les
conﬁgurations C18, C21 et C24 sont avec le momentum le plus faible (0.01) et
présentent les meilleures performances. La table 3.9 montrent que les courbes
d’erreurs de ces conﬁgurations sont décroissantes. Les conﬁgurations C18, C21

Extraction de règles de réseaux récurrents

111

3.3. Comparaison du SRN et RNN-LSTM

et C24 sont donc celles où le réseau apprend bien, mais arrive aussi à généraliser
son apprentissage.

Figure 3.8 – Evolution des performances de Elman pour les conﬁgurations
C27 à C35. Moyennes obtenues sur 10 simulations.

La ﬁgure 3.8 présente les performances d’un SRN de 15 unités cachées avec
un taux d’apprentissage successivement de [0.01, 0.015, 0.02] et un momentum
prenant les valeurs successives de [0.5, 0.7, 0.9], pour chaque taux d’appren-
tissage. Sur 10 simulations, le taux moyen de reconnaissance des séquences
grammaticales et des séquences aléatoires est de 100%. Cela implique que le
réseau ne sait pas distinguer entre ces deux types de séquences. L’étude plus
précise de la prédiction a montré que le réseau prédisait à chaque pas de temps
l’ensemble des symboles possibles comme successeurs potentiels, ce qui justiﬁe
son incapacité à classer des séquences. Au niveau de l’erreur du réseau durant
son apprentissage, la table 3.10 montre qu’elle est croissante pour l’ensemble
des conﬁgurations de C27 à C35. Le réseau ne réalise pas d’apprentissage pour
ces conﬁgurations là.

112

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Résistance aux dépendances séquentielles sur le long terme
•

Figure 3.9 – Activité moyenne des unités de la couche de sortie pour un SRN
de 3 unités cachées. Apprentissage réalisé avec : taux d’apprentissage=0.01 et
momentum=0.01.

Figure 3.10 – Activité moyenne des unités de la couche de sortie pour un SRN
de 15 unités cachées. Apprentissage réalisé avec : taux d’apprentissage=0.01
et momentum=0.01.

Nous avons réalisé un test supplémentaire aﬁn de tester le comportement
des deux SRN face à des dépendances séquentielles. Nous avons pour cela,
utilisé deux séquences issues de la grammaire manuelle qui débutent et se ter-
minent diﬀéremment, mais qui comportent le même nombre i de symboles C
intermédiaires : AC iF et BC iE. Pour ce test, nous avons réalisé un apprentis-
sage en faisant varier le taux d’apprentissage comme ce qui suit [0.01, 0.015,
0.02] et le momentum comme cela [0.01, 0.5, 0.7, 0.9]. Les couches d’entrées et
de sorties se composent toutes deux de 6 unités pour les 6 symboles suivants :
[’A’, ’B’, ’C’, ’D’, ’E’, ’F’]. Ces deux séquences ont été apprises sur 1000 itéra-
tions, soit 500 fois chacune, l’une alternant avec l’autre. Le nombre de C a varié
selon les valeurs suivantes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 18, 30, 60]. Les courbes
représentant les moyennes d’erreurs sont toutes décroissantes (la moyenne a

Extraction de règles de réseaux récurrents

113

3.3. Comparaison du SRN et RNN-LSTM

été calculée sur les 50 dernières erreurs à chaque fois) signiﬁant que le réseau
apprend bien. Lors de la phase de test, les mêmes séquences ont été présentées
au RNN. Les patterns de la couche de sortie au dernier pas de temps de chaque
séquence, pour diﬀérentes occurrences de C, ont été enregistrés. Les ﬁgures 3.9
et 3.10 présentent l’activité moyenne des unités de la couche de sortie. Il s’agit
de la somme des activités de chaque unité de la couche de sortie au dernier pas
de temps de chaque séquence, divisé par le nombre de séquences. Chaque unité
étant associé à un symbole, ces ﬁgures permettent de visualiser les tendances
de prédiction du réseau au dernier pas de temps.

L’analyse de ces deux ﬁgures montrent que les réseaux se retrouvent rapi-
dement (dès 2C et dès 6C pour des SRN de respectivement 3 unités cachées et
15 unités cachées) à prédire majoritairement le symbole C à la ﬁn des deux sé-
quences et cela indépendamment du début. Dans la ﬁgure 3.9, le réseau prédit
de manière égale les symboles E et F pour les séquences comportant de 1 C.
Mais au delà de ce palier, le réseau change progressivement de comportement
et prédit le C comme dernier élément des séquences (indépendamment du type
de séquence traitée) : en parallèle de l’atténuation de l’activité moyenne des
unités prédisant E et F, on observe une augmentation progressive de l’activité
moyenne de l’unité prédisant C. Dans la ﬁgure 3.10, le réseau prédit de manière
égale les symboles E et F pour les séquences comportant de 1 à 4 C. Ensuite,
pour les séquences de 5 à 8 C, il prédit de manière sensiblement égale (au
dernier pas de temps) les symboles C, E et F. Puis à partir de 9 C répétés, le
réseau ne prédit plus que des symboles C. Le changement progressif de prédic-
tion observé pour les deux réseaux traduit un changement progressif dans la
capacité de ce dernier à conserver son passé lointain. Le souvenir des premiers
symboles (A et B) est progressivement écrasé par un passé récent, celui des
symboles C les plus récents. Ce phénomène s’accentue lorsque le réseau doit
traiter des séquences de plus en plus longues avec de plus en plus de symboles
C. Par ailleurs dans le cas du réseau à 15 unités cachées, nous observons une
variation ponctuelle des performances lorsque les séquences sont composées de
6C. Cette observation met en évidence une instabilité du réseau en terme de
performances.

Discussion

L’étude des performances du SRN selon diﬀérentes conﬁgurations d’ap-
prentissage met en évidence la sensibilité du réseau aux paramètres tels que
le taux d’apprentissage et le momentum. Par ailleurs, nous avons montré que
lorsque le réseau apprend (courbe d’erreur décroissante), cela n’implique pas
qu’il soit capable de généraliser son apprentissage. Malgré le fait que nous nous
sommes conformé aux paramètres selon la littérature existante, nous observons
des résultats qui ﬂuctuent. Le test utilisant la grammaire manuelle, quant à

114

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

lui, souligne d’une part la sensibilité du réseau au nombre d’unités de la couche
cachée et d’autre part, montre que celui-ci ﬁnit par perdre rapidement sa capa-
cité à distinguer le contexte (au bout de 2C pour un SRN de 3 unités cachées
et 6C pour un SRN de 15 unités cachées). De ce test, il en ressort que l’aug-
mentation du nombre d’unités de la couche cachée, permet de maintenir les
performances du réseau et notamment sa capacité à conserver le souvenir du
passé lointain. Néanmoins, cela laisse aussi penser qu’il existe un seuil au delà
duquel le phénomène d’oublie a lieu, et cela quelque soit le nombre d’unités
cachées du SRN. D’après Cleeremans et McClelland [1991], le SRN possède la
particularité d’apprendre les patterns pertinents pour distinguer un contexte
d’un autre. Or dans notre test, il est montré que lorsque celui ci est redondant,
et qu’il remonte loin dans le passé, le réseau ﬁnit par l’oublier, les informations
plus récentes venant écraser celles stockées et plus anciennes. Dans le cadre de
la question d’extraction des règles à partir d’un RNN, il nous faut souligner
une conséquence importante du phénomène d’oubli du passé lointain au proﬁl
du passé récent : celle de la perte des relations temporelles entre les états. Le
passé devenant de plus en plus ﬂou, cela implique qu’il sera de plus en plus
diﬃcile de reconstruire la ligne temporelle des séquences traitées (la succession
des symboles).

Nous venons de présenter les performances du SRN selon les expériences
de Servan-Schreiber et al. [1988]. Nous allons étudier dans la section suivante
les performances du RNN-LSTM lorsqu’il est soumis aux mêmes contraintes
d’apprentissage et aux mêmes tailles de corpus de données.

3.3.2 RNN-LSTM

Parmi l’ensemble des variations existantes du modèle LSTM, nous avons
choisi de nous focaliser sur la version standard proposée par Gers et al. [1999]
et que nous avons décrit section 2.3.2. Dans cet article, les auteurs proposent
l’association des LSTM de Hochreiter et Schmidhuber [1997] et d’une porte
de sortie aﬁn de permettre l’apprentissage de séquences mais aussi de ﬂux
(séquences mises bout à bout) issus des variations de la grammaire de Reber
(ERG et CERG). Ils ont ainsi montré l’eﬃcacité mais aussi la solidité de ces
nouveaux LSTM face à des corpus de données très volumineux (30 000 ﬂux
de taille 100 000). Nous présentons ici notre implémentation d’un RNN-LSTM
inspiré des travaux de Gers et des paramètres proposés. Nous décrirons les
expériences menées pour tester les performances du réseau.

Architecture du RNN-LSTM et apprentissage

Notre RNN-LSTM se compose de 3 couches : une couche d’entrée, une
couche cachée et une couche de sortie. Les couches d’entrée et de sortie se

Extraction de règles de réseaux récurrents

115

3.3. Comparaison du SRN et RNN-LSTM

composent toutes deux de 7 unités, du fait que les grammaires utilisées com-
portent 7 symboles. La couche cachée se compose de 4 blocs LSTM chacun
doté de 2 cellules LSTM et de 3 portes, soit 8 cellules et 12 portes en tout.
L’ensemble des unités de la couche d’entrée sont connectées à l’ensemble des
unités de la couche de sortie, à chacune des 8 cellules, et à chacune des portes
des 4 blocs. L’ensemble des cellules ont des connexions vers chacune des unités
de sorties, mais aussi vers l’entrée de toutes les cellules (une cellule a donc
une connexion récurrente avec elle même) et enﬁn, vers chacune des portes de
chaque bloc. Pour l’apprentissage, nous utilisons une combinaison du BPTT et
RTRL, décrite par Gers et al. [1999] et présentée en section 2.3.2. Un pseudo-
code décrivant l’apprentissage est fourni en annexe E.

Le nombre de poids paramétrés nbWRN N −LST M lors de l’apprentissage se cal-

cule donc comme ce qui suit :

nbWRN N −LST M = nbentree

nbbloc

nbcellulesP arBloc+

nbentree
(nbbloc

⇤
(nbbloc

⇤
nbbloc

⇤

nbporteP arBloc + nbentree
⇤
⇤
(nbbloc
nbcellulesP arBloc)
⇤
⇤
nbbloc
nbcellulesP arBloc
⇤
⇤
nbsortie + (nbbloc
⇤

nbsortie+
nbcellulesP arBloc)+
nbporteP arBloc)+
nbporteP arBloc) + nbsortie

⇤

⇤

⇤

(nbbloc

⇤

nbcellulesP arBloc)

(3.4)

Ainsi avec les paramètres de notre RNN-LSTM, nous obtenons un total

de :

nbWRN N −LST M = 7

4

⇤

⇤

2 + 7
(4

⇤

⇤
2

4

⇤

⇤
4

⇤

3 + 7

⇤

3) + (4

7 + (4
2)

(4

2)
⇤
⇤
7 + (4
⇤

⇤

⇤

2)+

⇤
3) + 7 = 424 (3.5)

Les poids des biais des portes d’entrées et de sorties sont initialisés à -0.5
pour le premier bloc, puis sont décrémentés de -0.5 pour les suivants (-1, -1.5,
-2). Pour les portes d’oubli des 4 blocs, c’est l’inverse qui s’opère : pour le
premier bloc, le biais de la porte est initialisé à 0.5, puis s’incrémente de 0.5
pour les autres portes des autres blocs (1, 1.5, 2). Tous les autres poids sont
initialisés, ainsi que les biais de la couche de sortie, de manière aléatoire entre
[-0.2, 0.2]. Pour cette implémentation, pas de momentum, mais un taux d’ap-
prentissage initialisé à 0.5 puis multiplié par 0.99 tous les 100 pas de temps.
Lors de l’apprentissage, mais aussi lors de la prédiction, au début de chaque
séquence, les états précédents st−1, ainsi que les entrées aux pas de temps pré-
cédents sont remis à zéro. Le contexte est ainsi réinitialisé au début de chaque
séquence.

116

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Reproduction des expériences de Servan-Schreiber : Apprentissage
de séquences et généralisation

Nous avons reproduit les expériences menées sur SRN avec les mêmes tailles
de corpus. Nous présentons ci-dessous les résultats de ces tests menés sur des
séquences RG et menés sur les séquences ERG. Pour chaque grammaire, nous
avons réalisé un apprentissage de 200 000 séquences avec un RNN-LSTM. Nous
avons ensuite réalisé 10 simulations, pour lesquelles 2 corpus de séquences ont
été générés à chaque fois : le premier de 20 000 séquences issues de la gram-
maire considérée et 130 000 séquences aléatoires utilisant les symboles de cette
grammaire.

RG

ERG

200 000
1 397 109
7.98
3.35

Corpus d’apprentissage
200 000
Taille
2 198 671
Nombre d’échantillons
11.99
Longueur moyenne
3.37
Déviation standard
Corpus de séquences grammaticales de test
Taille
Nombre d’échantillons moyen
Longueur moyenne
Déviation standard
Corpus de séquences aléatoires de test
Taille
Nombre d’échantillons moyen
Longueur moyenne
Déviation standard

130 000
219 533
7.01
5.50

20 000
219 411
11.97
3.35

20 000
140 193
8.02
3.47

130 000
140 538
8.00
6.51

Table 3.11 – Caractéristiques des corpus utilisés. Les chiﬀres pour les corpus
de tests sont des moyennes calculées sur 10 corpus.

Dans ce contexte de RNN-LSTM, une prédiction est considérée correcte
si le carré de l’erreur quadratique moyenne de chacune des 7 unités de sor-
tie est inférieur à 0.49. Nous avons réalisé cette expérience d’une part avec
des séquences issues de la grammaire de Reber et des séquences issues de la
grammaire de Reber embarquée. Dans ce cas aussi, nous avons mesuré le pour-
centage de séquences grammaticales acceptées et le pourcentage de séquences
aléatoires acceptées. Un RNN-LSTM ayant de bonnes performances présentera
un taux élevé (proche de 100%) de séquences grammaticales et un taux faible
(0%) de séquences aléatoires. Les ﬁgures 3.11 et 3.12 présentent les courbes des
erreurs moyennes dans le cas de RNN-LSTM appliqués à des séquences RG
et des séquences ERG. Tout comme les SRN, il s’agit de l’erreur quadratique

Extraction de règles de réseaux récurrents

117

3.3. Comparaison du SRN et RNN-LSTM

moyenne commise à chaque pas de temps, puis lissée en calculant une moyenne
à chaque pas de temps sur les 5000 dernières erreurs enregistrées. Dans les deux
cas, les erreurs décroissent ce qui témoigne de bonnes performances d’appren-
tissage.

Figure 3.11 – Evolution de la moyenne de l’erreur lors de l’apprentissage du
RNN-LSTM de 200 000 séquences issues de la grammaire de Reber

La ﬁgure 3.13 présente les résultats moyens sur 10 simulations pour la
grammaire de Reber et la grammaire ERG. Le RNN-LSTM présente de bonnes
performances avec un taux de séquences grammaticales acceptées à 99% pour
les deux grammaires et un taux de séquences aléatoires acceptées faibles. Ces
résultats alliés à ceux des ﬁgures 3.11 et 3.12, témoignent de la bonne capacité
du RNN-LSTM non seulement pour apprendre, mais aussi pour généraliser
son apprentissage. Une fois les règles encodées (suite à son apprentissage), le
réseau arrive avec succès à reconnaitre les séquences grammaticales nouvelles
qui lui sont soumises et à rejeter celles aléatoires, qui ne respectent pas les
règles.

Reproduction des résultats de Gers : apprentissage de ﬂux

Nous montrons dans cette partie les performances du modèle concernant
l’apprentissage de ﬂux continu. Nous déﬁnissons un ﬂux, comme un ensemble
de séquences ERG mises bout à bout. Gers déﬁnit une solution parfaite comme
106 prédictions successives sans erreur. Pour réaliser ce test, nous alternons ap-
prentissage et prédiction : Nous faisons apprendre au réseau un ﬂux de 100 000
symboles successifs au bout duquel nous gelons les poids. Ces poids sont utilisés
pour une phase de tests portant sur 10 ﬂux de 100 000 symboles. Le nombre

118

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Figure 3.12 – Evolution de la moyenne de l’erreur lors de l’apprentissage du
RNN-LSTM de 200 000 séquences issues de la grammaire de Reber Embarquée

Figure 3.13 – Performances moyennes sur 10 simulations pour RNN-LSTM
appliquées à la grammaire de Reber (RG) et la grammaire de Reber embarquée
(ERG).

Extraction de règles de réseaux récurrents

119

3.3. Comparaison du SRN et RNN-LSTM

d’erreurs moyen de prédiction a été mesuré à chaque phase de tests. Nous
avons réussi à tester 30 000 ﬂux d’entrainement sans que le réseau ne présente
d’erreur. Autrement dit, le réseau a présenté 100% de prédictions correctes sur
l’ensemble des phases de tests selon le critère de Gers.

Ce test a permis de tester la résistance du réseau face aux ﬂux de symboles,
où il n’y a pas de début ou de ﬁn explicite. Le réseau a su gérer l’ambiguïté
des séquences ERG et lorsque celles-ci s’enchainent, le réseau a continué à
les prendre en charge. Le réseau RNN-LSTM est par conséquent capable de
réaliser un bon apprentissage de séquences simples, mais aussi plus ambiguës
ou encore de ﬂux de symboles.

Dépendances séquentielles : SRN vs LSTM

Nous avons montré la faiblesse de SRN lors de l’apprentissage de la gram-
maire manuelle. Lorsque l’information pertinente à conserver est trop loin dans
le passé (i.e lorsque le nombre de C est important) le réseau n’arrive pas à réa-
liser la bonne prédiction. L’information sur le court terme vient écraser celle
qui devait être conservée. Ce qui engendre une faible capacité à retenir des
informations (un contexte) sur le long terme. L’utilisation de la grammaire
RG a permis de tester les capacités des SRN et RNN-LSTM à apprendre et
prédire selon un contexte où le début et la ﬁn de séquences sont facilement
identiﬁables. Les deux RNN ont montré de bonnes performances d’appren-
tissage et de généralisation, avec un bémol cependant pour le SRN dont les
performances sont très variables selon les paramètres. La grammaire ERG,
teste la capacité des réseaux à gérer l’ambiguïté au niveau des symboles de
début et de ﬁn de séquences. Ces dernières ne sont plus aux extrémités seule-
ment, mais aussi à l’intérieur des séquences. Le modèle LSTM a montré de
bonnes performances pour cela : sur 10 simulations, il a reconnu l’ensemble
des séquences ERGs comme grammaticale (100% de séquences acceptées) et
rejeté la majorité des séquences aléatoires (pourcentage d’acceptation faible de
3%). Enﬁn la grammaire CERG, supprime la notion de début et de ﬁn. Les
réseaux doivent apprendre un ﬂux continu et être capables de faire les pré-
dictions adéquates en fonction du contexte local et de celui plus loin dans le
passé. Le RNN-LSTM a montré de très bonnes performances puisque aucune
erreur de prédiction n’a été enregistrée sur l’ensemble de 30 000 ﬂux de 100
000 symboles. En résumé, le RNN-LSTM a montré de meilleures performances
que le SRN au niveau de l’apprentissage mais aussi de la généralisation de cet
apprentissage. Par ailleurs, il a aussi montré de bonnes performances dans la
prédiction des symboles successifs d’un ﬂux, alors que le SRN a des perfor-
mances ﬂuctuantes et souvent faibles avec des séquences de Reber simples. La
répétition de l’apprentissage et de la phase de test, 30 000 fois sur 100 000 ﬂux
diﬀérents composés de séquences ERG, a montré la solidité du RNN-LSTM à

120

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

maintenir l’information sur le long et court terme.

Discussion

Dans cette partie nous souhaitions comparer les performances des 2 RNN
en termes d’apprentissage séquentiel et de généralisation. Nous avons montré
que le SRN, malgré son apprentissage et ses performances pour la conﬁgura-
tion C11, possède de nombreuses contraintes liées au choix des paramètres. Le
nombre de neurones dans la couche cachée, le taux d’apprentissage mais aussi
le momentum, peuvent induire une augmentation des erreurs et/ou une mau-
vaise généralisation. Et cela bien que nous nous soyons appuyé sur des données
issues de la littérature. A contrario, les LSTM ont montré de bonnes perfor-
mances pour l’apprentissage de séquences simples RG, de séquences plus ambi-
guës ERG, mais aussi de ﬂux CERG. Nous observons aussi que l’apprentissage
induit de bonnes performances de généralisation. Le modèle RNN-LSTM est
donc plus performant pour l’apprentissage de séquences complexes porteuses
de contexte local et plus étendu dans le temps que le modèle SRN. Le RNN-
LSTM présentant des meilleures performances indique que sa représentation
interne est de meilleure qualité.

Néanmoins, il nous faut soulever ici un point sur le RNN-LSTM : Pour nos
simulations, notre protocole de test a nécessité l’apprentissage par le réseau de
200 000 séquences durant lesquelles le taux d’apprentissage a été mis à jour
tous les 100 pas de temps (soit une fois tous les 100 symboles présentés). Pour
la grammaire RG, nous avons mesuré que le taux d’apprentissage a été mis à
jour 14 000 fois. Or d’après la ﬁgure 3.14 qui présente l’évolution du taux d’ap-
prentissage durant les 1000 premières mises à jour, (10 000 premiers symboles
présentés), la valeur du taux d’apprentissage décroit rapidement et avoisine la
valeur nulle dès la 800ème mise à jour. Cela laisse entendre que le réseau au
ﬁnal n’apprend plus au delà ce seuil. Ce point sera discuté ultérieurement dans
la section 3.5.

3.4 Extraction de règles à partir du RNN-LSTM

Dans cette section, nous allons présenter un nouveau modèle réalisé dans le
but d’extraire des règles encodées dans un RNN-LSTM après l’apprentissage
de plusieurs séquences régies par ces règles. Notre but est de trouver un moyen
d’expliciter ce qui se passe dans un réseau de neurones qui apprend une gram-
maire. Nous allons dans un premier temps décrire les données que nous avons
exploitées. Dans un second temps, nous expliciterons les résultats préliminaires
obtenus grâce au clustering hiérarchique. Enﬁn, nous expliciterons notre pro-
cédé d’extraction des règles et les résultats obtenus avec des séquences issues
de la grammaire de Reber, puis de la grammaire de Reber embarquée. Nous

Extraction de règles de réseaux récurrents

121

3.4. Extraction de règles à partir du RNN-LSTM

Figure 3.14 – Evolution du taux d’apprentissage lors de l’apprentissage de
10 000 symboles (soit 1000 mises à jour du taux d’apprentissage) issues de
séquences RG avec le RNN-LSTM décrit section 3.3.2.

appliquerons, pour ﬁnir, nos travaux sur une grammaire électrique, extraite
manuellement par nos soins à partir de l’étude de schémas électriques repré-
sentant des installations réelles. L’algorithme 3 décrit le processus d’extraction
des règles sous la forme d’un graphe-automate (GA) doté d’étiquettes longues
(ﬁgure 3.18) à partir des patterns d’activité d’un RNN-LSTM.

3.4.1 Patterns d’activités de la couche cachée

Lors d’un apprentissage de séquences dans un RNN, c’est au niveau de
l’espace d’états des unités de la couche cachée, que se trouvent encodées les
règles abstraites des séquences. Pour extraire celles-ci, nous avons utilisé le
RNN-LSTM décrit précédemment dans ce chapitre. Nous avons d’abord réalisé
un apprentissage sur les séquences issues des grammaires RG et ERG. Une fois
cela fait, nous avons généré un corpus de test issu de la même grammaire, que
nous avons fourni au réseau. A chaque pas de temps (autrement dit à chaque
présentation de symbole), nous avons choisi de récupérer et stocker, en plus du
symbole traité à l’instant t, les patterns d’activités de la couche cachée, soit les
sorties des cellules LSTM. En nous focalisant sur cette information, parmi les
multitudes que les unités LSTM contiennent, nous souhaitions transposer les
approches connues d’extraction des règles à partir des patterns d’activités de
la couche cachée des SRN sur des LSTM. Nous nous sommes ainsi constitué
une liste de patterns d’activité, chacun doté d’un identiﬁant unique constitué
du symbole courant et du pas de temps.

122

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Algorithme 3 Algorithme d’extraction des règles sous la forme d’un graphe-
automate (GA) doté d’étiquettes longues à partir des patterns d’activité d’un
RNN-LSTM
Pré-conditions : # Apprentissage et test du réseau de neurones —–

RNN\_LSTM.apprentissage(corpus\_apprentissage)
# liste\_etiquettes : liste des symboles présentés au réseau durant les tests
liste\_patterns\_activité, liste\_etiquettes = RNN\_LSTM.test(corpus\_test)

extraction\_règles

Fonction
liste\_etiquettes, k) :
# Clustering —————————————————-
liste\_clusters = k\_means(k, liste\_patterns\_activité)

(liste\_patterns\_activité,

# Creation automate —————————————————-
GA = {} # Dictionnaire
noeud\_courant= -1
GA[’noeuds’].ajouter(noeud\_courant)
GA[’arcs’] = [ ] # Liste de dictionnaires
Pour tout pattern h d’indice i de la liste\_patterns\_activité faire

cluster\_associé = liste\_clusters[i]
Si cluster\_associé

GA[’noeuds’] alors
GA[’noeuds’].ajouter(cluster\_associé)

62

Fin Si
arc= {} # Dictionnaire
arc[’id’] = (noeud\_courant, cluster\_associé)
Si arc

GA[’arcs’] alors

62

nouvel\_arc = arc
nouvel\_arc[’poids’] = 1
nouvel\_arc[’etiquette’] = liste\_etiquettes[i]
GA[’arcs’].ajouter(nouvel\_arc)

Sinon

arc[’poids’] = arc[’poids’] +1
arc[’etiquette’] = arc[’etiquette’]+ liste\_etiquettes[i]
GA[’arcs’].mise\_à\_jour(arc)

Fin Si
# Mise à jour de la valeur du noeud courant
noeud\_courant= cluster\_associé

Fin Pour

return GA

Fin Fonction

Extraction de règles de réseaux récurrents

123

3.4. Extraction de règles à partir du RNN-LSTM

3.4.2 Algorithme d’extraction des règles

Le clustering

Pour l’extraction des règles encodées, nous avons utilisé l’algorithme dé-
crit section 2.4.1. Nous avons choisi d’exploiter l’algorithme k-means pour le
clustering en faisant varier le nombre de clusters demandés. Cet algorithme
consiste à partitionner les données en k groupes en minimisant la distance
entre les données à l’intérieur de chaque partition. Le nombre de groupes ou
clusters en lequel il faut partitionner les données est à la charge de l’opéra-
teur humain. Comme il s’agit d’une étape critique du processus d’extraction
des règles, nous avons choisi de faire varier k dans un intervalle de valeur [2,
20] (soit 18 valeurs diﬀérentes) et de générer un automate pour chaque valeur
de k. Nous avons obtenu pour chaque grammaire 18 graphes-automates dif-
férents, un pour chaque valeur de k choisie. Le choix de la plage de valeur a
été ﬁxé en fonction du nombre de nœuds dans les grammaires étudiées. Au
niveau technique, nous avons choisi d’utiliser la librairie Python scikit-learn
dédiée au machine learning [Pedregosa et al., 2011]. Pour chaque valeur de k
et grâce à cette librairie, nous obtenons une liste d’entiers, de la taille de la
liste de nos patterns d’activité cachée que nous étudions. Nous désignerons à
partir d’ici la liste d’entier par "liste-cluster" et la liste des patterns d’activités
par "liste-patterns". Ces deux listes sont de même longueur et puisque la se-
conde a été générée en fonction de la première, il existe une adéquation entre
les deux indices. En eﬀet, chaque entier de la liste-cluster, indique le numéro
de cluster auquel est rattaché le pattern d’activité à la même position sur la
liste-patterns.

Considérons deux exemples de listes : Une liste-patterns de 5 patterns : [h0,
h1, h2, h3, h4] et une liste-cluster [0 3 2 2 0 ] issus d’un clustering utilisant
k-means et pour lequel k=4. La lecture de ces deux listes consiste à dire que :
- les patterns h0 et h4 appartiennent au cluster 0
- le pattern h1 appartient au cluster 3
- les patterns h2 et h3 appartiennent au cluster 2
Grâce à cette lecture, il est possible d’associer chaque pattern à un cluster et
donc un nœud dans le graphe-automate que nous souhaitons extraire. La ﬁgure
3.15 présente un exemple de génération d’automate utilisant les 5 patterns cités
ci dessus.

La génération de l’automate

Nous avons commencé la génération de l’automate avec les éléments sui-
vants à notre disposition : La liste-patterns et la liste-clusters. La liste-cluster,
ne comporte que des entiers positifs. Nous avons donc choisi de commencer la
génération de notre graphe-automate par l’ajout d’un nœud avec l’identiﬁant
-1 comme point de départ (ﬁgure 3.15, label A). Pour faciliter l’analyse des

124

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Figure 3.15 – Exemple de génération d’automate avec une liste-patterns [h0,
h1, h2, h3, h4] et une liste-cluster [0 3 2 2 0].
A - Automate après le pas de temps t0, création d’un nouveau nœud (état).
B - Automate après le pas de temps t1, création d’un second nœud (état).
C - Automate après le pas de temps t3 et t4 qui appartiennent au même cluster,
ce qui génère une boucle.
D - Automate après le pas de temps t4 qui appartient à un cluster déjà visité,
ce qui crée un arc entre le nœud courant (C2) et le nœud connu (C0).

Extraction de règles de réseaux récurrents

125

3.4. Extraction de règles à partir du RNN-LSTM

graphes volumineux, mais aussi donner un point de repère pour la comparai-
son de diﬀérents graphe-automates, nous avons introduit cette idée, nouvelle
par rapport aux approches connues. Les clusters ne pouvant avoir un identi-
ﬁant négatif, nous avons choisit une valeur négative pour être sûr du caractère
unique de cet identiﬁant.

Nous avons ensuite parcouru la liste-patterns et considéré chaque cluster
associé : pour chaque premier pattern d’un nouveau cluster, nous ajoutons un
nœud à notre graphe-automate (avec pour identiﬁant le numéro du cluster) et
un arc orienté du nœud précédent vers le nouveau nœud (ﬁgure 3.15, label A,
B, C). Si par contre, le pattern courant appartient à un cluster dont le nœud
est déjà existant dans notre graphe-automate, alors nous ajoutons un arc dirigé
entre le nœud précédent et le nœud courant (ﬁgure 3.15, label D, pas de temps
t4). Si deux patterns se suivant appartiennent au même cluster, une connexion
récurrente est ajoutée sur le nœud représentant le cluster (ﬁgure 3.15, label C,
pas de temps t3).

Pour compléter cette génération, nous avons souhaité attribuer des éti-
quettes aux arcs, à l’image des étiquettes des arcs des grammaires de Reber
et de ses variations, aﬁn de mettre en lumière comment un RNN encode au
niveau temporel les diﬀérents symboles qui lui sont présentés successivement.
Pour cela, nous avons apporté deux ajouts à l’algorithme précédent : le premier
consiste à modiﬁer de manière incrémentale les étiquettes des arcs ajoutés. A
chaque analyse d’un pattern de la liste-pattern, nous récupérons son identiﬁant
alphanumérique à partir des données que nous avons au préalable préparées.
La partie alphabétique désigne le symbole que le RNN traite à l’instant t et
la partie numérique désigne le pas de temps. Une fois le cluster identiﬁé et
le nœud du graphe-automate créé (si besoin), il nous faut ajouter l’arc liant
l’ancien nœud au nouveau, en lui associant cette fois ci son identiﬁant alpha-
numérique. La particularité du graphe-automate est que si un arc existe déjà
entre deux nœud, il ne le créera pas une seconde fois. Ainsi dans une approche
traditionnelle, il n’est pas possible de voir combien de fois une transition entre
deux nœuds (états) a lieu, à moins de lui associer un poids ou une étiquette
qui explicite cela. Nous avons donc choisi d’implémenter ces deux options :
à chaque fois que l’algorithme de génération pointe sur un arc existant, l’éti-
quette de cet arc sera actualisée pour comporter le nouveau identiﬁant du
pattern en cours et son poids sera incrémenter de 1. Nous obtenons à la ﬁn de
cette première évolution un graphe-automate, avec des étiquettes très longues,
mais néanmoins qui montre l’organisation temporelle des patterns du RNN.
La ﬁgure 3.17 présente l’exemple d’un automate généré sans étiquette et la
ﬁgure 3.18 présente le même automate avec les étiquettes alphanumériques.

Le second ajout apporté à l’algorithme découle du premier : pour chaque

126

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

arc, nous avons réduit l’occurence de chaque symbole dans l’étiquette à une
seule occurence, et nous avons enlevé la partie numérique. Nous souhaitions
réduire la complexité d’analyse des graphes-automates et surtout pouvoir com-
parer les grammaires d’origines que nous avions utilisées (RG, ERG et CERG)
avec les automates obtenus. Nous présentons et détaillons nos résultats dans
la section suivante.

3.4.3 Application à la grammaire de REBER et à ses

variations

Nous étudions dans cette section les patterns d’activité de la couche cachée
de notre RNN-LSTM décrit précédemment.La ﬁgure 3.16 rappelle les gram-
maires RG et ERG utilisées dans cette section.

Figure 3.16 – La grammaire de Reber à gauche et la grammaire de Reber
embarquée à droite

La grammaire de Reber (RG)

Nous avons commencé par réaliser un apprentissage sur 200 000 séquences.
Ensuite, lors de la phase de test, sur les 10 simulations réalisées (à chaque fois
sur 20 000 nouvelles séquences RG) nous avons récupéré les patterns d’activité
de la couche cachée pour une seule simulation. Les résultats présentés dans
les sections 3.17 à 3.19 représentent les graphes-automates extraits à partir
de l’analyse des 50 premiers pas de temps lors de la prédiction qui portaient
sur plusieurs occurrences de 2 séquences : BPVVE et BTXSE. En suivant
les algorithmes décrits précédemment, nous obtenons le graphe-automate sans
étiquette, représenté en ﬁgure 3.17, le graphe-automate avec les étiquettes
longues (pour 15 clusters) représentés en ﬁgure 3.18 et le graphe-automate
ﬁnal en ﬁgure 3.19.

La comparaison du graphe-automate ﬁnal avec la sous-partie de la gram-
maire de Reber responsable de l’encodage des 2 séquences montre de fortes si-
militudes entre les deux représentations. Soulignons ici que l’absence de boucles

Extraction de règles de réseaux récurrents

127

3.4. Extraction de règles à partir du RNN-LSTM

Figure 3.17 – Un graphe-automate sans étiquette obtenu avec un algorithme
k-means pour le clustering, où k=15, et issu d’un RNN-LSTM ayant appris
des séquences RG. Extraction réalisée sur les 50 premiers pas de temps.

Figure 3.18 – Un graphe-automate avec étiquettes longues obtenu avec un
algorithme k-means pour le clustering, où k=15 et issu d’un RNN-LSTM ayant
appris des séquences RG. Extraction réalisée sur les 50 premiers pas de temps.

128

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Figure 3.19 – Un graphe-automate ﬁnal obtenu avec un algorithme k-means
pour le clustering, où k=15 et issu d’un RNN-LSTM ayant appris des séquences
RG. Extraction réalisée sur les 50 premiers pas de temps.

dans les graphes-automates résulte de l’absence de symboles répétés dans les
séquences utilisées pour l’analyse. Nous avons ensuite voulu appliquer cela à
des séquences plus longues et complexes : nous avons choisi d’appliquer cela
aux séquences de la grammaire de Reber embarquée.

La grammaire de Reber Embarquée (ERG)

Un apprentissage sur 200 000 séquences ERG a été réalisé sur le même
RNN-LSTM décrit ci dessus. Pour un rendu visuellement accessible, nous pré-
sentons l’automate extrait pour les 80 premiers pas de temps qui portaient
sur plusieurs occurences des 4 séquences suivantes : BPBPVVEPE, BPBBTX-
SEPE, BPBPVVETE, BPBBTXSETE. Nous obtenons ainsi les 3 graphes-
automates : le graphe-automate sans étiquette, représenté en ﬁgure 3.20, le
graphe-automate avec les étiquettes longues (pour 19 clusters) représenté en
ﬁgure 3.21 et le graphe-automate ﬁnal en ﬁgure 3.22. L’étude du graphe-
automate ﬁnal obtenu pour la grammaire ERG, permet de retrouver les sé-
quences utilisées lors de la phase de test de RNN-LSTM. On retrouve aussi
par ailleurs les mêmes transitions que celle de la grammaire d’origine. Pour
conﬁrmer la validité de l’automate obtenu, nous avons procédé manuellement :
nous avons généré des séquences que nous avons soumises à la grammaire
d’origine ERG. La grammaire a validé les séquences dans 80% des cas.

Extraction de règles de réseaux récurrents

129

3.4. Extraction de règles à partir du RNN-LSTM

Figure 3.20 – Un graphe-automate sans étiquette obtenu avec un algorithme
k-means pour le clustering, où k=19 et issu d’un RNN-LSTM ayant appris des
séquences ERG. Extraction réalisée sur les 80 premiers pas de temps.

130

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Figure 3.21 – Un graphe-automate avec étiquettes longues obtenu avec un
algorithme k-means pour le clustering, où k=19 et issu d’un RNN-LSTM ayant
appris des séquences ERG. Extraction réalisée sur les 80 premiers pas de temps.

Extraction de règles de réseaux récurrents

131

3.4. Extraction de règles à partir du RNN-LSTM

Figure 3.22 – Un graphe-automate ﬁnal non-compressé obtenu avec un al-
gorithme k-means pour le clustering, où k=19 et issu d’un RNN-LSTM ayant
appris des séquences ERG. Extraction réalisée sur les 80 premiers pas de temps.

132

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

3.4.4 Application à une grammaire électrique (EL)

Dans cette section, nous allons aborder une des originalités de notre travail :
les séquences de composants électriques. Jusqu’à présent les études expérimen-
tales ont portée sur des grammaires connues mais artiﬁcielles. Nous allons, ici,
appliquer notre nouvelle approche dans un nouveau contexte, industrielle et
issue de données réelles. Nous avons souhaité montrer comment notre approche
permet, en exploitant les séquences de composants électriques, de reconstruire
en partie la base de données. Nous insistons surtout sur les règles métiers et les
habitudes de représentations qui avaient été extraites via des échanges verbaux
avec le client de l’entreprise Algo’Tech.

Figure 3.23 – Grammaire électrique issue de l’étude de 3 schémas électriques
d’un même client

Figure 3.24 – Evolution de la moyenne de l’erreur lors de l’apprentissage du
RNN-LSTM de 200 000 séquences issues de la grammaire électrique.

Extraction de règles de réseaux récurrents

133

3.4. Extraction de règles à partir du RNN-LSTM

Nous présentons en ﬁgure 3.23 la grammaire d’un client que nous avons
extrait manuellement par l’étude de schémas électriques. Nous avons, comme
décrit précédemment dans le chapitre 1, mimer le comportement d’un expert
Algotech en étudiant 3 schémas électriques : nous avons parcourus les ﬁls de
chaque schéma aﬁn d’extraire les relations entre les composants électriques
d’un folio à un autre. Ce travail chronophage, nous a ainsi permis d’extraire
une grammaire pour chacun des schémas, que nous avons fusionné ensemble
pour obtenir une grammaire électrique du client considéré. La validation de ce
travail a nécessité la collaboration d’un expert Algo’Tech et entrainé plusieurs
échanges concernant les symboles électriques, leur signiﬁcation et les relations
entre eux. La complexité et la charge de travail de ces travaux justiﬁent notre
choix premier d’utiliser des grammaires artiﬁcielles pour les étapes de notre
approche décrites précédemment. Celle-ci est plus complexe que la grammaire
de REBER. Elle comporte 15 nœuds et 25 transitions. A partir de cette gram-
maire, nous avons généré 200 000 séquences de composants électriques pour
l’apprentissage et 20 000 pour le corpus de tests.

Nous avons fait apprendre au RNN-LSTM, et lors de la phase de test,
nous avons enregistré l’ensemble des patterns d’activation de la couche cachée
à chaque pas de temps. Pour 20 000 séquences, nous avions en moyenne 165
273 échantillons, soit autant de patterns d’activités. La ﬁgure 3.24 présente la
courbe de moyenne d’erreur (les 2000 premières moyennes). Cette grammaire
possédant 15 nœuds, nous avons réalisé un clustering k-mean pour des valeurs
de k allant de [2 à 50]. Nous avons ensuite passé en revue les graphes-automates
obtenus. Les ﬁgures 3.25, 3.26, 3.27 présente le graphe automate pour k= 41
car c’est celui qui visuellement se rapprochait le plus de la grammaire d’ori-
gine. Ces premiers résultats préliminaires montrent que le RNN-LSTM arrive à
apprendre des séquences électriques et que l’extraction des règles est possible.

3.4.5 Discussion sur l’extraction de règles à partir de

RNN-LSTM

Le travail d’extraction des règles présenté dans cette section fait partie des
contributions majeures que nous avons réalisées. Dans le but de mettre en
évidence les règles implicites dans des séquences apprises par les RNN, nous
avons réussi à transposer une technique d’extraction jusque là réservée au SRN
dans un nouveau contexte, celui des LSTM, proposant de ce fait une nouvelle
méthode. Testée en premier lieu sur des grammaires artiﬁcielles que sont les
grammaires de Reber et de Reber embarquée, nous avons pu transposer cette
technique sur une problématique industrielle concrète, à savoir l’extraction des
règles de connectivité électrique implicites à partir d’un RNN-LSTM ayant ap-
pris des séquences issues d’une grammaire électrique réelle élaborée à partir
de l’étude de schémas électriques. Bien que notre approche ne soit qu’à ses
débuts, le fait qu’elle puisse être appliquée à une problématique concrète laisse

134

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Figure 3.25 – Un graphe-automate sans étiquette obtenu avec un algorithme
k-means pour le clustering, où k=41 et issu d’un RNN-LSTM ayant appris des
séquences EL. Extraction réalisée sur les 79 premiers pas de temps.

Extraction de règles de réseaux récurrents

135

3.4. Extraction de règles à partir du RNN-LSTM

Figure 3.26 – Un graphe-automate avec étiquettes longues obtenu avec un
algorithme k-means pour le clustering, où k=41 et issu d’un RNN-LSTM ayant
appris des séquences EL. Extraction réalisée sur les 79 premiers pas de temps.

136

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Figure 3.27 – Un graphe-automate ﬁnal non-compressé obtenu avec un al-
gorithme k-means pour le clustering, où k=41 et issu d’un RNN-LSTM ayant
appris des séquences EL. Extraction réalisée sur les 79 premiers pas de temps.

Extraction de règles de réseaux récurrents

137

3.4. Extraction de règles à partir du RNN-LSTM

Figure 3.28 – Règles extraites à partir d’un RNN-LSTM ayant appris des
séquences RG. Exemples issus, de gauche à droite, du graphe-automate sans
étiquettes (ﬁgure 3.17), du graphe-automate doté d’étiquettes longues (ﬁgure
3.18) et du graphe-automate avec une étiquette simple (ﬁgure 3.19)

penser qu’elle peut donc être transposée dans les domaines techniques où l’on
retrouve des séquences, ce qui oﬀre des axes de développement intéressants et
prometteurs.

L’algorithme que nous avons conçu au cours de ce travail de thèse corres-
pond ainsi bien à nos attentes puisqu’il permet d’extraire des règles implicites
en exploitant les patterns d’activité de la couche cachée d’un RNN-LSTM
après la phase d’apprentissage de celui-ci. Grâce à cela, nous avons proposé
3 variantes de graphes-automates, proches des grammaires d’origine. La pre-
mière, sans-étiquette, explicite l’agencement des clusters (états) de l’automate
extrait. La seconde dotée d’étiquettes longues permet de suivre l’achemine-
ment temporel des patterns et ainsi de lire l’automate. Les arcs de ce second
graphe-automate comportant des poids, cela permet potentiellement d’analy-
ser quelles sont les transitions (les règles de succession) les plus fortes. Dans le
contexte de la question des RNN "boites noires", il semble important de sou-
ligner que cette représentation permet de mettre en lumière le fonctionnement
interne d’un RNN-LSTM en explicitant la succession temporelle des patterns
et leur prise en charge interne par le réseau de neurones récurrent. Enﬁn, le
dernier graphe-automate ne comportant qu’une étiquette simple (un seul sym-
bole) permet d’attribuer une étiquette unique à chaque arc. Il résume à lui
seul notre approche puisqu’il propose une réprésentation explicite des connais-
sances implicites encodées par le RNN-LSTM. C’est la représentation qui se
rapproche le plus des grammaires d’origines. La ﬁgure 3.29 présente un com-
paratif entre la grammaire de Reber et le graphe-automate extrait par notre

138

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

algorithme. Ce dernier ayant été réalisé pour 50 pas de temps, ne permet pas
de représenter toute la grammaire mais présente de grandes similarités avec
les règles de la grammaire d’origine.

Autre point qu’il nous faut souligner, l’insertion d’un nœud initial dans cha-
cun de nos graphes-automates. Lorsqu’un graphe automate contient un faible
nombre de cluster, il est simple de l’analyser. A contrario, lorsque le nombre
est important, l’analyse peut s’avérer plus longue et énergivore (spécialement
si l’on étudie le second type de graphe-automates avec des étiquettes longues).
Pour faciliter cette étape, nous avons introduit un nœud doté de l’identiﬁant
1 (les clusters ne pouvant pas avoir un identiﬁant négatif dans notre ap-
−
proche) qui permet d’interpréter plus facilement les résultats en fournissant
un point de départ à la génération des séquences et en oﬀrant un même point
de comparaison entre les diﬀérentes représentations. La ﬁgure 3.28 présente
un exemple de règles extraites dans le cadre de la grammaire de Reber selon
ces trois représentations avec le même point de départ, le nœud
1. A notre
connaissance, de tels travaux d’extraction de règles à partir de LSTM dans le
cas de langages réguliers n’avaient pas encore été réalisés.

−

Dans les cas présentés, lors de l’analyse des graphes-automates pour déter-
miner le nombre de cluster adéquat pour partionner l’espace d’état du RNN-
LSTM, le nombre de graphes à tester (une soixantaine) bien qu’important est
resté accessible à un opérateur humain. Cependant, nous sommes conscient que
pour pouvoir garantir la transposition de ces travaux à d’autres domaines, des
techniques plus automatiques de validation des graphes devraient être mise en
place. La taille des graphes ne serait alors plus un problème potentiel. La taille
peut aussi être réduite via des algorithmes de minimisation des graphes tel
que l’algorithme de Hopcroft qui permet de calculer l’automate déterministe
ﬁni minimal, à partir d’un automate ﬁni donné [Hopcroft, 1971]. Ce procédé,
décrit comme un ingrédient à part entière du procédé d’extraction des règles
par Jacobsson [2005], permettrait de comparer les graphes-automates obtenus
plus facilement. Cela permettrait (éventuellement) aussi d’explorer la stabilité
de la représentation interne des LSTM par exemple, si plusieurs paramètres
de clustering permettent d’obtenir un même automate minimal.

L’un des points importants de notre approche actuelle est le choix du
nombre de clusters k, que nous utilisons pour l’algorithme de clustering k-
means. Dans les cas des grammaires RG, ERG et EL, au début de nos tests,
le fait de connaitre la grammaire nous a aidé à ﬁxer un intervalle de valeurs
restreint pour faire varier le paramètre k (nombre de clusters). Mais cela n’a
souvent pas été suﬃsant et nous avons dû réaliser des tests sur des intervalles
plus larges ([2, 20] pour les grammaires RG et ERG et [2,50] pour la grammaire
électrique). Pour trouver la valeur de k qui approxime le mieux la grammaire

Extraction de règles de réseaux récurrents

139

3.4. Extraction de règles à partir du RNN-LSTM

d’origine nous avons dû manuellement passer en revue l’ensemble des graphes
obtenus. En générant des séquences manuellement et en les soumettant aux
grammaires d’origines, c’est ainsi que nous avons choisi les valeurs k=15 pour
les ﬁgures 3.17, 3.18 et 3.19, et k=19 pour les ﬁgures 3.20, 3.21 et 3.22.

Nous sommes conscients que ce point représente l’une des faiblesses de notre
approche. Le travail manuel de comparaison entre les graphes-automates ob-
tenus et les grammaires d’origines aﬁn de déterminer le paramètre k qui oﬀre
la meilleure représentation tend à devenir chronophage lorsque la plage de va-
riation de k augmente. Sans compter qu’étant justement manuel, il ne peut
être reproduit ou déclaré ﬁable. Le procédé actuel de sélection du paramètre k
est donc en soit insatisfaisant. L’adoption d’autres techniques de clustering ou
de méthodes plus automatisées pour le choix de ce paramètre k est donc un
point incontournable dans la poursuite de nos travaux. L’exploration d’autres
techniques de clustering permettrait ainsi, d’une part, de lever la contrainte
de choix explicite (ce qui nous permettrait d’étendre notre approche à des do-
maines où la grammaire est inconnue, puisque ce paramètre serait alors ﬁxé
en fonction des données) et d’autre part, d’explorer l’adéquation entre notre
approche et d’autre techniques de réseaux de neurones.

L’exploration pourrait aussi s’étendre à l’exploitation d’autres données du
RNN-LSTM. Rappelons que le LSTM est un modèle connexionnistes qui pos-
sède une représentation explicite et interne du temps. L’une de ses caractéris-
tiques principales est qu’il permet donc d’expliciter le temps. Par extension, il
permet d’accéder explicitement à un ensemble de données tel que l’état interne
de chaque cellule, le CEC, les valeurs des portes, etc. Cet ensemble de données
représente ainsi un ensemble de travaux potentiels de recherches supplémen-
taires qui pourraient découler de ces travaux préliminaires.

140

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

Figure 3.29 – Grammaire de Reber à droite et Graphe-automate extrait à
gauche (pour 50 premiers pas de temps).

3.5 Discussion

A ce stade, nous considérons avoir répondu à la question initiale : à savoir
comment extraire les règles implicites encodées dans des séquences porteuses
de dépendances (sur le long et court terme) à partir d’un réseau de neurones
récurrents ayant appris ces séquences.

La première partie de ce chapitre se concentre surtout sur l’explication et
la mise en évidence de l’importance du choix du RNN pour l’apprentissage
séquentiel. Nous avons exploité un premier RNN, le SRN en évaluant à la fois
son apprentissage et sa capacité à généraliser sur des séquences issues de la
grammaire de REBER. Nous avons ainsi montré qu’il s’agissait d’un RNN qui
présente des faiblesses pour la rétention d’informations sur le long terme. En
eﬀet, l’information récente venant écraser celle plus ancienne, empêche de sto-
cker un contexte sur de longues périodes de temps. Nous avons ensuite étudié
les performances d’un second RNN, le RNN-LSTM, en terme d’apprentissage
mais aussi de généralisation. Nous avons mis en évidence les bonnes perfor-
mances de ce modèle, puisque après un apprentissage sur 200 000 séquences
issues de la grammaire de Reber, le réseau a su reconnaitre 100% des séquences
présentées lors de la phase de test, et cela sur 10 simulations de suite. Ces ré-
sultats encourageants ont aussi été reproduits avec des séquences issues de la
grammaire de Reber embarquée, qui présentent plus d’ambiguïtés. Pour ﬁnir,
nous avons testé ce modèle avec des ﬂux continus de séquences ERG mises
bout à bout. Sur 30 000 ﬂux composés de 100 000 symboles successifs, le ré-
seau a présenté 100% de prédictions correctes. Ces bonnes performances sont
contre-balancées par l’apprentissage qui s’eﬀectue durant les premiers 1000 pas
de temps (ﬁgure 3.14) : la mise à jour du taux d’apprentissage tous les 100 pas

Extraction de règles de réseaux récurrents

141

3.5. Discussion

de temps, induit une diminution de la valeur de ce dernier jusqu’a atteindre
une valeur quasi-nulle à partir de la 800ème mise à jour. Ce paramètre est donc
à garder à l’esprit pour des implémentations futures aﬁn d’arrêter l’apprentis-
sage lorsque le réseau n’apprend plus.

En dehors des diﬀérences de performances, rappelons aussi la diﬀérence
dans la représentation du temps dans le fonctionnement de ces deux RNN du
fait de leur architecture. Le SRN possède une représentation interne et im-
plicite du temps, alors que le LSTM possède une représentation explicite et
interne du temps. Dans le premier cas, la représentation du passé est implicite
mais fragile (puisque écrasé par un passé plus récent). A contrario, dans le
second cas la représentation du passé est explicite et maintenue, lorsque néces-
saire, sur plusieurs pas de temps sans altération. Les données sont donc plus
accessible et préservée dans le cas des RNN-LSTM que dans le cas du SRN.
L’ensemble de ces résultats nous ont ainsi amené à considérer le RNN-LSTM
au lieu du SRN pour l’étude et l’extraction des règles implicites encodées du-
rant l’apprentissage.

L’approche d’extraction des règles proposée dans la littérature était majo-
ritairement réalisée sur les SRN. En la transposant sur des LSTM, nous avons
proposé une nouvelle approche permettant d’exploiter un modèle connexion-
niste représentant le temps de manière explicite et en interne. Nous avons
ainsi, dans la seconde partie de ce chapitre, mis en évidence la représentation
implicite de la grammaire encodée par le RNN-LSTM. Nous avons utilisé l’al-
gorithme du k-means utilisant un apprentissage non-supervisé par dessus le
RNN-LSTM. En exploitant les patterns des unités de la couche cachée, cet
algorithme réalise le partitionnement de l’espace d’état des unités selon un
paramètre k choisit par l’opérateur humain. En d’autre terme, si l’on assimile
le RNN-LSTM à un expert en apprentissage séquentiel, l’algorithme k-mean
peut être assimilé à une entité observatrice extérieure qui analyse et interprète
le comportement de l’expert aﬁn d’expliciter les connaissances implicites de
cet expert. Les études expérimentales portant sur cette nouvelle approche ont
porté sur des séquences diﬀérentes de trois grammaires diﬀérentes : la gram-
maire de Reber, sa variante la grammaire de Reber embarquée, ainsi qu’une
grammaire électrique représentant un cas industriel concret. A chacune de ses
études, nous avons réussi à extraire trois graphes-automates proches des gram-
maires d’origine, représentant le fonctionnement interne du RNN. Chacune
de ces représentations apporte un élément de réponse à la question des RNN
”boites noires". Le premier graphe-automate informe sur l’agencement des états
et les transitions existantes entre eux. Le second graphe-automate met en évi-
dence l’acheminement temporel des patterns entre les diﬀérents états détectés.
A partir du nœud initial -1, il est possible de suivre l’encodage interne du
RNN-LSTM des patterns qu’il génère. Enﬁn, le troisième graphe-automate,

142

Ikram Chraibi Kaadoud

CHAPITRE 3 : IMPLÉMENTATION

résumé des deux précédents, est la représentation la plus proche des gram-
maires d’origine.

Nos travaux ont donc permis de réaliser le passage de séquences dotées de
règles implicites à des grammaires (graphes-automates) présentant explicite-
ment ces mêmes règles. Bien que préliminaires, ces travaux ont débouché sur
des résultats riches en terme d’informations, de potentiel d’exploration au ni-
veau technique (comme nous l’avons discuté précédemment section 3.4.5), et
de potentiel d’exploration dans d’autres domaines.

Extraction de règles de réseaux récurrents

143

3.5. Discussion

144

Ikram Chraibi Kaadoud

Conclusion générale

Table des matières

Résumé des travaux . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
Résultats
3.5.1 Au niveau industriel
. . . . . . . . . . . . . . . . . . . . 147
3.5.2 Au niveau machine learning . . . . . . . . . . . . . . . . 148
. . . . . 149
3.5.3 Au niveau gestion des connaissances implicites
Perspectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
3.5.4 Amélioration de l’approche actuelle . . . . . . . . . . . . 150
3.5.5 Application à d’autres domaines . . . . . . . . . . . . . . 153

Résumé des travaux

Dans ces travaux, nous nous sommes penché sur la problématique d’extrac-
tion de l’expertise et surtout des connaissances implicites. Ces notions étant
transversales à de nombreux domaines, nous avons puisé nos connaissances
dans diﬀérents axes d’études : la cognition, le knowledge management et l’IA
avec les réseaux de neurones récurrents. Nous avons ciblé le domaine électrique
et notamment les connaissances encodées dans les séquences de composants
électriques car il s’agit d’une part du domaine d’application de notre parte-
naire, l’entreprise Algo’tech, et d’autre part, c’est un domaine où l’organisation
des connaissances rejoint celle chez l’humain, à savoir explicite et implicite.
Plus précisément, nous nous sommes intéressé à la question de l’apprentissage
séquentiel des RNN. Nous avons souhaité mettre en évidence la possibilité
d’extraire les règles encodées implicitement par un RNN-LSTM après son ap-
prentissage de séquences aﬁn d’amener dans le domaine de l’explicite ce qui a
été appris implicitement.

C’est dans cette optique, et suite aux interrogations soulevées en introduc-

tion, que nous avons produit les contributions suivantes :

Notre première contribution applicative, au niveau du premier chapitre, a
été de proposer un procédé d’extraction et de représentation des connaissances

145

Résumé des travaux

techniques dans les schémas électriques. Nous avons développé une approche
qui vise à construire une représentation selon diﬀérents niveaux d’abstractions
à partir de ces documents techniques. Pour cela nous avons mené tout un tra-
vail de fouille et de préparation des données en vue de mettre en évidence la
structure hiérarchique (des concepts métiers aux composants électriques), la
structure modulaire (organisation en concepts métiers) et la structure séquen-
tielle (séquences de concepts, séquences de folios, séquences de composants
électriques) des connaissances contenues dans les schémas électriques. Nous
avons ainsi mis en évidence l’existence des règles métiers implicites contenues
dans les séquences de composants électriques, que nous avons ciblées pour la
suite de notre étude.

Pour notre seconde contribution nous avons étudié et comparé deux mo-
dèles de réseaux de neurones pour l’apprentissage séquentiel : un SRN et un
RNN-LSTM. Nous nous sommes essentiellement intéressé à leurs performances
en termes d’apprentissage séquentiel et de généralisation. Nous avons montré
que le SRN, malgré son apprentissage et ses performances, est très sensible
aux variations de paramètres. Nous avons pour ce modèle proposé 36 conﬁgu-
rations diﬀérentes dans lesquelles nous faisions varier le nombre de neurones
de la couche cachée, le taux d’apprentissage et le momentum. Nous avons ainsi
mis en exergue qu’un bon apprentissage du SRN ne s’accompagne pas sys-
tématiquement de bonnes capacités de généralisation et inversement. Et cela
bien que nous nous soyons appuyé sur des données issues de la littérature.
Par ailleurs, nous avons aussi mis en évidence le phénomène d’écrasement du
passé lointain à la faveur du passé récent qui a lieu dans le SRN, ce qui re-
présente une grande limite à son utilisation pour l’étude de séquences longues.
A contrario, les LSTM ont montré de bonnes performances pour l’apprentis-
sage et la généralisation des séquences RG (dotées d’un début et d’une ﬁn
explicites), des séquences ERG (où le réseau doit apprendre à détecter les am-
biguïtés entre les symboles de début et de ﬁn situés à plusieurs endroits dans
une même séquence), mais aussi de ﬂux CERG (succession de séquences ERG
mises bout à bout). Au niveau des dépendances séquentielles, le fait que le
RNN-LSTM ait réussi à gérer les ﬂux de symboles continus prouve sa solidité
et son adéquation pour la gestion de séquences longues : le passé est conservé
sur de long laps de temps sans être altéré ou oublié. Le modèle RNN-LSTM est
par conséquent plus performant pour l’apprentissage de séquences complexes
porteuses de contexte, local et étendu dans le temps, que le modèle SRN.

Notre troisième contribution, que l’on qualiﬁera de contribution algorith-
mique, a été de proposer un procédé d’extraction des règles encodées par un
RNN-LSTM après apprentissage, procédé qui n’était jusque là appliqué que sur
des SRN. Nous avons ainsi décrit les diﬀérentes étapes d’une nouvelle méthode
qui visent à l’extraction et la représentation de manière explicite des règles im-

146

Ikram Chraibi Kaadoud

Conclusion générale

plicites présentes dans des séquences à partir d’unités LSTM. Cette approche a
été réalisée sur la grammaire RG et la grammaire ERG en présentant des pre-
miers résultats encourageants. Enﬁn nous avons testé notre approche sur une
grammaire électrique, obtenue à partir de l’analyse de schémas électriques d’un
client, ce qui a permis de montrer que nos travaux sont aussi transposables à
des cas concrets. A chaque fois nous avons utilisé un algorithme de clustering k-
mean, en complément du RNN-LSTM, aﬁn de partitionner l’espace d’état des
unités LSTM de la couche cachée. Ce partionnement, étape essentielle du pro-
cessus d’extraction, a nécessité plusieurs itérations, selon diﬀérents paramètres
k, aﬁn d’obtenir 3 graphes-automates proches des grammaires d’origine : Un
premier sans-étiquettes explicitant l’agencement des clusters et les transitions
entre eux, un second doté d’étiquettes longues permettant de suivre l’ache-
minement temporel des patterns entre les clusters dans l’espace d’états et un
troisième, qui attribue une étiquette simple (un symbole) à chaque transition
et qui se rapproche le plus des grammaires d’origines.

En résumé, à travers nos trois approches, nous avons proposé une solution
générique d’extraction des connaissances implicites à partir de schémas élec-
triques.

Résultats

3.5.1 Au niveau industriel

Au niveau industriel, pour l’entreprise Algo’Tech, deux résultats ont été
obtenus et centralisés dans le prototype réalisé et présenté dans le chapitre
1. Le premier résultat concerne l’extraction des concepts métiers. Nous avons
élaboré une méthode qui permet d’analyser les schémas électriques d’un client
et d’en proposer trois représentations synthétiques qui tiennent sur une page
(plutôt que des dizaines ou centaines de folios) : un premier graphe représen-
tant les folios électriques et les liaisons entre eux, un second graphe qui explicite
l’organisation d’un schéma folio par folio en fonction des concepts métiers (pré-
sents dans ce schéma) et un troisième graphe qui représente l’agencement des
concepts métier dans un schéma. Un quatrième graphe permet de représenter
les concepts métiers présents dans l’ensemble des schémas d’un client et oﬀre
ainsi une vue synthétique du métier d’un client.

Le second résultat est l’extraction de l’empreinte implicite par des tech-
niques de data mining, ou autrement dit, la mise en évidence d’habitudes de
travail au niveau des séquences de composants électriques. Nous avons, lors de
cette analyse, comparé des folios d’un même client mais réalisés à des années
diﬀérentes. Nous avons ainsi observé dans plusieurs cas que des folios nommés

Extraction de règles de réseaux récurrents

147

Résultats

diﬀéremment comportaient la même séquence de composants électriques. Rap-
pelons que dans le contexte du premier chapitre, les séquences sont constituées
de composants électriques présents dans les folios et ordonnés selon leurs co-
ordonnées (x, y) croissantes. En se basant sur ces travaux, nous avons mis en
évidence les habitudes implicites de travail (en rapport avec les séquences de
composants électriques) sous la forme d’arbres de clusters et de dendogrammes.

C’est ainsi l’alliance de ces deux approches, extraction de concepts métiers
et extraction d’empreinte implicite présente dans les séquences, que nous avons
réussi à extraire les corrélations en termes de concepts et de séquences de com-
posants électriques d’un client donné.

Le prototype réalisé en Python est actuellement en test chez Algo’Tech. Au
niveau humain, nous avons souhaité évaluer l’impact chez les collaborateurs
d’Algo’Tech face à un outil d’extraction des connaissances et des règles de câ-
blage électrique. Malgré l’accueil positif du prototype au sein de l’entreprise,
il est trop tôt actuellement pour évaluer s’il y aura un impact signiﬁcatif dans
la méthode de travail des collaborateurs Algo’Tech.

3.5.2 Au niveau machine learning

Au niveau machine learning, nos travaux ont permis de montrer qu’il était
possible d’extraire les règles apprises par les LSTM. L’extraction des règles
n’était jusque là réalisée, à notre connaissance, que sur des SRN. En adaptant
cette méthode sur les patterns d’activité de la couche cachée du RNN-LSTM,
nous avons établi que les sorties des cellules des unités LSTM, étaient aussi
porteuses d’une information relative à la représentation implicite des régulari-
tés encodées par le réseau. Nous avons testé cette nouvelle approche dans trois
contextes diﬀérents et montré que l’on obtenait toujours des résultats intéres-
sants. Dans les 3 contextes (RG, ERG et EL), nous avons réussi à expliciter les
règles apprises implicitement par les LSTM et cela sous trois représentations
distinctes. Parmi elles, le second graphe-automate, représentation dotée d’éti-
quettes longues sur ses transitions, permet d’observer le comportement interne
d’un RNN en suivant la prise en charge temporelle des patterns dans l’espace
d’états.

Ce dernier point permet de rebondir sur la question de l’interprétabilité
des réseaux de neurones récurrents en général et de leur fonctionnement, à
savoir la question des RNN ”boites noires”. Si l’on reprend une métaphore faite
précédemment, l’utilisation d’un algorithme de clustering par dessus le RNN-
LSTM peut être assimilé à observer un expert en cours de ”raisonnement” et
interpréter son comportement pour en extraire des règles. Ces dernières étant

148

Ikram Chraibi Kaadoud

Conclusion générale

les règles implicites qui ont une incidence sur le ”raisonnement” de l’expert.
L’agencement temporel des patterns à partir du graphe-automate doté d’éti-
quettes longues, permet d’observer ce ”raisonnement” en tant que succession
d’éléments répartis dans l’espace d’état. Sans avoir la prétention d’avoir ré-
solu la question, nous pensons que ce point mérite néanmoins d’être souligné
pour la richesse d’information qu’il permet et le potentiel de perspectives de
recherche qu’il engendre. Nous y reviendrons par la suite.

Une autre originalité de notre travail réside dans l’application de celui-ci
dans un nouveau domaine, celui des séquences de composants électriques. Nous
avons extrait et créé manuellement une grammaire de connectivité de compo-
sants électriques à partir de schémas électriques représentant une installation
réelle. Cette grammaire a ensuite été utilisée pour générer des séquences de
composants électriques. Soumises à notre approche d’extraction de règles, ces
séquences ont permis l’extraction d’un automate, certes complexe mais présen-
tant des similarités avec la grammaire d’origine. Ce travail préliminaire montre
qu’il existe tout un potentiel qui découle de cette approche dans le domaine de
l’extraction de l’expertise à partir des schémas électriques, et plus globalement
à partir de schémas techniques (contenant des séquences).

Enﬁn il semble important de souligner que le fait de se focaliser sur l’étude
des séquences dans ces documents plutôt que l’ensemble des documents, permet
de se libérer de la nécessité d’avoir de gros volumes (en terme de documents)
pour pouvoir appliquer des techniques populaires de deep learning ou de data
mining. En se focalisant sur les séquences, il est possible d’avoir des résultats
exploitables pour un industriel et cela juste avec une centaine de schémas
techniques.

3.5.3 Au niveau gestion des connaissances implicites

L’un des apports majeurs de nos travaux est la mise en évidence de la
pertinence d’une approche centrée autour de l’humain pour l’extraction des
connaissances implicites (i.e l’expertise) contenues dans les schémas techniques.
En se basant sur l’organisation cognitive des connaissances d’un individu (ex-
plicite et implicite ; séquentielle et hiérarchique), deux approches d’extraction
des connaissances ont été proposées. Dans le chapitre 1, nous avons mis en évi-
dence l’existence d’une structure hiérarchique et séquentielle dans les schémas
électriques en nous inspirant du processus de planiﬁcation chez l’humain et de
la mémoire sémantique. Dans la seconde partie du chapitre 2, nous avons aussi
montré, grâce à notre connaissance de la mémoire implicite, qu’il était possible
de modéliser l’apprentissage implicite en utilisant un RNN-LSTM. L’exploita-
tion des séquences par les réseaux de neurones récurrents alliée à une approche
d’extraction des règles nous a permis d’expliciter les règles implicites présentes

Extraction de règles de réseaux récurrents

149

Perspectives

dans les séquences. Dans une dimension plus globale, nous avons montré qu’une
approche orientée autour du processus humain de génération des connaissances
permet de mieux comprendre et d’exploiter les données et cela même dans un
domaine très technique.

Perspectives

3.5.4 Amélioration de l’approche actuelle

Concernant le prototype du logiciel que nous avons réalisé, il s’agit d’une
preuve de concept pour laquelle nous nous sommes essentiellement focalisé sur
le coté fonctionnel. Ayant montré l’apport de nos travaux dans le domaine du
schéma électrique, nous pensons que la réalisation d’une version plus aboutie
dotée d’une interface ergonomique permettrait une meilleure prise en main en
interne à Algo’Tech. Par ailleurs, cela permettrait aussi de favoriser la réalisa-
tion de tests sur d’autres clients travaillant dans le même domaine que ceux
déjà analysés et d’autres clients travaillant dans des domaines diﬀérents. Cela
permettrait de valider l’approche d’extraction des concepts métiers dans diﬀé-
rents domaines techniques.

Au niveau des réseaux de neurones et de la comparaison entre le SRN
et le RNN-LSTM, nous souhaiterions interroger la notion de performance en
ajoutant d’autres mesures de performances. Dans chacune de nos implémen-
tations, nous nous sommes basés sur des paramètres issus de la littérature
pour l’architecture des réseaux et pour déterminer les critères d’évaluation des
performances qui leur sont applicables. Si nous modiﬁons ces critères, aurions-
nous des grammaires diﬀérentes ? Nous pourrions déterminer s’il existe une
dépendance entre les critères des performances des réseaux et les grammaires
extraites et si oui ses modalités. Le pourcentage de prédiction correcte selon la
grammaire d’origine est un critère que nous mesurons actuellement. En testant
cela sur des séquences grammaticales non bruitées, il serait possible de compa-
rer la justesse de la prédiction de chaque réseau. Nous tenons particulièrement
à souligner les limites des deux approches implémentées pour les SRN et le
RNN-LSTM qui représentent autant de perspectives de travail dans un axe
purement apprentissage. La compréhension des critères de succès d’une bonne
prédiction (0.3 pour le SRN et 0.49 pour le RNN-LSTM) ainsi que l’étude plus
approfondie des performances et de l’erreur des réseaux lors de l’apprentissage
nous semblent des axes essentiels de poursuite de nos travaux.

Cette question des paramètres desquels dépend la grammaire, se pose aussi
dans le procédé d’extraction des règles que nous avons établi. Dans notre mé-
thode, nous avons choisi de débuter notre exploration de l’espace d’état d’un
RNN-LSTM en exploitant et analysant les sorties des cellules des unités LSTM

150

Ikram Chraibi Kaadoud

Conclusion générale

car nous souhaitions transposer l’approche appliquée au SRN sur les LSTM.
Nous avons ainsi mis en évidence la représentation implicite du réseau en ac-
cédant à sa mémoire implicite. Le LSTM étant un modèle connexionniste qui
représente le temps de manière interne mais surtout explicite, il permet, par son
architecture, d’accéder à un ensemble d’information explicitement : les valeurs
du CEC à chaque pas de temps, les valeurs d’activité des portes, les valeurs des
entrées en cellules, ainsi que les informations relatives aux pas de temps précé-
dents (CEC et sortie des cellules). Appliquer notre approche à chacune de ses
données, mais aussi à l’alliance des données entre elles, permettrait potentiel-
lement, d’obtenir des grammaires diﬀérentes. Ces représentations diﬀérentes
nous permettrait, par exemple, de mettre en évidence les connaissances locali-
sées au niveau du CEC. Soulignons que cette question des paramètres desquels
dépend la grammaire se pose aussi si nous appliquons notre approche sur des
variations des LSTM standards, tels les GRU ou les LSTM dotés de connexions
peepholes.

Dans le cadre de notre approche d’extraction des règles à partir du RNN-
LSTM, une prochaine étape de validation serait d’appliquer cet algorithme sur
des séquences de composants électriques de diﬀérents clients et de soumettre
la grammaire extraite non seulement aux experts Algo’Tech, mais aussi aux
experts clients. Nous pourrions comparer nos résultats obtenus aux bases de
données contenant les métiers des clients et valider ou ajuster ainsi nos travaux.
Elargir à d’autres clients d’autres domaines, permettrait aussi de conﬁrmer la
transposabilité de notre approche face à d’autres domaines et surtout sa soli-
dité.

Dans une dimension plus technique, nous allons rappeler ici les axes d’amé-
lioration discutés précédemment pour l’approche d’extraction des règles (sec-
tion 3.4.5). Nous pensons que l’ajout d’un algorithme de minimisation des
automates permettrait d’explorer de nouvelles possibilités pour l’étude des
règles extraites. Comparer les représentations compressées des automates per-
mettrait d’évaluer quelles sont les règles qui ont tendance à rester présentes,
et surtout de réaliser un comparatif entre les diﬀérentes techniques de cluste-
ring de l’espace d’états. Cela nous permettrait aussi de réduire les automates
en terme de complexité : nous obtiendrons un automate réduit pour chaque
automate que nous avons. Il serait possible de comparer les diﬀérentes ver-
sions réduites et voir s’il existe une stabilité dans la représentation interne
du RNN-LSTM. L’ajout d’un algorithme d’analyse qui soumettrait des sé-
quences générés à partir d’un automate à la grammaire d’origine, permettrait
de discerner plus facilement les automates (et par extension les paramètres k
du k-mean par exemple) qui sont adéquats ou non pour le clustering. Enﬁn,
concernant ce dernier, nous souhaiterions tester des approches diﬀérentes plus
proches des données. Nous pensons notamment à l’algorithme Growing When

Extraction de règles de réseaux récurrents

151

Perspectives

Required (GWR) [Marsland et al., 2002] et l’algorithme Growing Neural Gas
[Fritzke, 1995] qui consistent à utiliser des approches incrémentales où les ré-
seaux s’adaptent aux données. En théorie, ces approches permettent de réaliser
un clustering de manière automatique sans qu’un opérateur humain fasse de
choix explicite. L’automate extrait serait plus proche et ﬁdèle aux données.
Nous pourrions alors comparer nos automates actuels à ceux résultant de ces
approches, qui correspondent plus à un fonctionnement cognitif (i.e sans l’in-
tervention d’un intervenant extérieur).

Dans nos travaux nous avons mis en place l’attribution de poids aux transi-
tions dans le second graphe-automate. Nous pensons que de telles données per-
mettraient d’étudier les variations de comportement dans les habitudes d’une
entité (société ou personne) en comparant diﬀérents graphes-automates réali-
sés à diﬀérentes périodes par exemple. Dans les contextes où la grammaire est
inconnue, cette information serait probablement pertinente pour l’analyse des
transitions entre les états.

Dans les contextes de séquences très longues, il arrive souvent qu’un évè-
nement ne se répète que sur des pas de temps tellement longs qu’une analyse
humaine soit compliquée. Nous pensons qu’appliquer notre approche sur de
telles séquences, et notamment exploiter le graphe-automate doté d’étiquettes
longues permettrait de mettre en évidence les transitions (les règles implicites)
qui n’ont lieu que très rarement. Cette mesure de l’occurence de certaines
règles, rejoint l’idée abordée précédemment ci-dessus (i.e concernant les gram-
maires inconnues) car cela permettrait d’apporter des éléments d’analyses sup-
plémentaires.

Enﬁn, une dernière perspective au niveau technique qui nous intéresse est
l’ajout de couches cachées supplémentaires et l’extraction des automates pour
chaque couche. En eﬀet, dans nos travaux nous avons réalisé un RNN-LSTM
d’une seule couche cachée, majoritairement en raison de notre souhait de com-
parer les LSTM et des neurones artiﬁciels standards. Chung et al. [2016] a
montré, lors de traitement de phrases en anglais par un RNN-LSTM doté de
3 couches cachées, que chaque couche encodait un niveau d’informations dif-
férent : la première couche apprenant chaque lettre (et espace), la seconde
couche apprenant les mots et la troisième couche apprenant des portions de
phrases. Appliqué à notre contexte de séquences électriques, serait-il possible
d’extraire une grammaire de connectivité de composants électriques pour la
première couche, une grammaire de connectivité des folios pour la seconde
couche et une autre liée à des portions d’installations réelles pour la dernière
couche ? Par ailleurs, nous avons choisi d’appliquer un RNN pour le clustering
par dessus le RNN-LSTM. Mais il aurait été aussi envisageable d’en ajouter
un troisième et d’observer un processus d’extraction pour ce réseau aussi. Le

152

Ikram Chraibi Kaadoud

Conclusion générale

nombre de couche et le nombre d’algorithme que nous pouvons ajouter pour
l’analyse sont deux questions potentielles à explorer.

3.5.5 Application à d’autres domaines

Nos travaux actuels peuvent se résumer à l’équation suivante :

En faisant varier les séquences nous pourrions appliquer théoriquement
cette équation dans diﬀérents domaines. Dans notre domaine d’application,
bien que nous ayons travaillé sur des séquences issues de grammaires artiﬁcielles
ou issues de schémas électriques de clients, nous étions toujours contraint par
un cadre déterminé avec des contraintes et une combinatoire limitée. Si nous
transposions nos travaux, dans des domaines où les cadres sont moins ﬁgés et
dans lesquelles les codes implicites ne sont pas connus, comme chez l’humain
par exemple, quelles grammaires obtiendrons-nous ? Ces dernières seraient-
elles spéciﬁque à un individu, ou à un groupe d’individu ? Dans le domaine de
suivi d’activité notamment, si les séquences sont celles des déplacements d’une
personne chez elle sur toute une journée (plusieurs laps de temps), pourrions-
nous en suivant notre approche, extraire une grammaire des déplacements de
cet individu (par mois, par jour, par semaine par exemple) ? et pourrions-nous
observer l’évolution de celle-ci (apparition de nouveau nœuds, de nouvelles
transitions, renforcement de certaines par l’analyse des poids des transitions
et fragilisation d’autres, etc.) ? Dans le cas des personnes âgées, si cela était
possible, cela permettrait potentiellement, non seulement de suivre l’activité de
ces personnes et de s’assurer régulièrement qu’elles ont un bon rythme de vie
(sorties en extérieur, déplacement en cuisine, courses, etc.), mais cela permet-
trait théoriquement aussi d’étudier les évolutions des habitudes des individus
lors de l’apparition de certaines pathologies, notamment celles neurodégénéra-
tives par exemple.

Dans le domaine de l’interprétabilité des RNN, nous avions en introduction
cité le cas de la DARPA. Nous pensons qu’en appliquant notre approche dans
un domaine dont la grammaire est connue, nous pourrions grâce à l’automate
extrait (et surtout à la représentation avec des étiquettes longues) détermi-
ner les paramètres qui fournissent une grammaire qui approxime le mieux
la grammaire d’origine, ce qui permet dans un premier temps d’observer le

Extraction de règles de réseaux récurrents

153

Perspectives

comportement du RNN dans ce cas précis. Mais ensuite en faisant varier les
paramètres (architecture du réseau, nombre d’unités, nombre de couches, etc.),
nous pourrions observer les variations de comportement et surtout l’impact au
niveau de la représentation implicite des règles. Cela pourrait s’avérer utile
dans le cas des algorithmes de Deep Learning, où certes, plus il y a de couches
plus l’algorithme est puissant, mais il devient aussi de plus en plus obscur
et inaccessible. Une telle approche permettrait potentiellement de visualiser
les successions d’éléments qui induisent les résultats de ces algorithmes. Dé-
cortiquer le "raisonnement" des algorithmes et apporter plus d’explications
aux décisions prises par les algorithmes de Deep Learning permettraient, entre
autre, d’améliorer la conﬁance des utilisateurs dans ces outils et peut-être li-
miter les dérives liées à leurs utilisations.

154

Ikram Chraibi Kaadoud

Annexe A

Manuel utilisateur prototype

155

Extraction des 
connaissances 
Manuel d’utilisation 

Algo’Tech Informatique - Technopole Izarbel - Hôtel d’entreprises 
64210 Bidart 
Tel : 05.59.01.59.60 - Fax : 05.59.01.59.69 
Email : infos@algotech.fr - Web : www.algotech-informatique.com 

 
 
 
 
 
Table des matières 

I- 

II- 

Application à installer en amont du projet ................................................................................. 3 

Presentation du dossier de installation ...................................................................................... 4 

III- Présentation générale de l’interface .......................................................................................... 6 

A. 

B. 

Partie Gauche ......................................................................................................................... 7 

Partie Droite ........................................................................................................................... 9 

IV- Etape 1 : Paramétrage de l’application .................................................................................... 10 

A. 

B. 

Etape préalable à l’utilisation de l’application à chaque nouveau client : ........................... 10 

Fichier de configuration : configFile.txt ............................................................................... 11 

V- 

Etape 2 : Analyse et extraction des données dans un fichier Excel .......................................... 19 

A. 

B. 

C. 

D. 

Lancement de l’analyse ........................................................................................................ 19 

Fonctionnement de l’analyse ............................................................................................... 20 

Fichier DataExtraction.xls ..................................................................................................... 20 

Fichier dictionnary.json ........................................................................................................ 21 

VI- Etape 3 : La génération de graphes .......................................................................................... 22 

A. 

B. 

VII- 

VIII- 

A. 

B. 

Fonctionnement de l’algorithme ......................................................................................... 22 

Images produites ................................................................................................................. 23 

Etape 4 : Le clustering hiérarchique ..................................................................................... 23 

Annexe et Liens supplémentaires ........................................................................................ 25 

Expressions régulières .......................................................................................................... 25 

Annexe : dictionnary.json de \_\_\_\_\_ ..................................................................................... 25 

 
 
 
 
 
 
I- 
Application à installer en amont du projet 
Graphviz est un outil de visualisation et de génération des graphes. 

Son absence bloque la génération des graphes lors de l’étape 3, mais n’est pas préjudiciable à l’étape 
1 du projet, soit l’analyse. 

Il est cependant conseillé de réaliser les étapes ci-dessous pour une bonne exécution du projet. 

1. Graphviz 

• Décompresser le fichier« KnoweldgeExtraction\_Install.zip » 
• Aller dans le répertoire « outils » 
• Double cliquer sur « graphviz-2.38.msi » 
• Procéder à l’installation en cliquant sur « Suivant » jusqu'à là fin. 
• 

Si l’application c’est bien installé, un nouveau répertoire est crée au lien suivant : 

C:\Program Files\Graphviz2.38

2. Modifier la variable d’environnement PATH 

Notre application « KnowledgeExtraction.exe » lance un appel à l’exécutable dot.exe du répertoire 
C:\Program Files\Graphviz2.38\bin. 

Il faut donc modifier la variable d’environnement PATH afin d’inclure ce nouveau chemin. 

• Cliquer sur le bouton rond « Démarrer » 
• Positionner la souris sur Ordinateur et cliquer droit 
• Cliquer sur « Propriétés » 
• Dans la fenêtre « Système » qui s’ouvre (aussi localiser à Panneau de configuration\Système 
et sécurité\Système), sur la colonne gauche, cliquer sur « Paramètres systèmes avancés » 

• Dans la fenêtre « Propriétés système », cliquer sur « Variables d’environnement » afin 

d’ouvrir une nouvelle fenêtre « Variables d’environnement » 

 
 
 
• Dans la section du bas nommé « Variables systèmes », rechercher la variable « Path », se 

positionner dessus puis cliquer sur le bouton « Modifier » 

• Dans la popup « Modifier la variable système » qui s’ouvre, au niveau du champ « Valeur de 
la variable », se positionner à la fin et ajouter : « ;C:\Program Files\Graphviz2.38\bin » 

• Attention à ne pas oublier le point virgule avant le nouveau chemin. 

• Cliquer sur OK, fermez tout et redémarrer votre ordinateur pour qu’il prenne en compte la 

modification. 

II- 

Presentation du dossier de installation 

Une fois le fichier« KnoweldgeExtraction\_Install.zip » décompressé, on obtient le répertoire suivant : 

 
 
 
 
 
 
Dans ce dossier, il y a 4 types de fichiers qui sont important : 

1) Le répertoire « Démonstration\_\_\_\_\_ » contient un exemple d’analyse sur 3 fichiers 

\_\_\_\_\_ 

2) Le répertoire « Outil » contient le programme Graphviz à installer (voir section II-

Application à installer en amont du projet) 

3) Le fichier de paramétrage « configFile.txt » : l’utilisateur peut le consulter mais il est 
déconseillé de le modifier directement. Il est nécessaire pour le fonctionnement de 
l’application. IL est possible de faire des sauvegardes en ajoutant un préfixe ou suffixe 
pour un client, mais seul le fichier nommé exactement « configFile.txt » qui sera pris en 
compte. 

4) Le fichier de paramétrage « confiFilePreParametrer.txt » : est un fichier générique pour 

démarrer un nouveau client. 

5) Le fichier de paramétrage « configFile\_\_\_\_\_.txt » : est un fichier de paramétrage \_\_\_\_\_ 

pour l’exemple. 

6) Le fichier « dictionnary.json » : contient les concepts qui sont extrait pour un nouveau 

client, ce fichier est pré-rempli pour l’exemple \_\_\_\_\_ . 

7) Le fichier « dictionnaryPreParameter.json » : est le fichier générique qui contient les 
concepts qui seront extrait pour un nouveau client. Ce fichier sera modifié à chaque 
client. Il ne contient par défaut qu’une paire d’accolade. 

8) L’application « main.exe » est le programme central à paramétrer et lancer. 
9) ManuelUtilisateur.docx est le manuel utilisateur ici présent. 

 
 
 
 
 
III- 

Présentation générale de l’interface 

 
 
 
 
 
A. Partie Gauche 

La partie gauche de l’application contient l’ensemble des champs à renseigner pour permettre à 
l’analyse d’avoir lieu. 

3. Les champs 

• « Fichier de configuration » rappel le nom du fichier de configuration utilisé pour charger et 
sauvegarder les informations. Son nom est fixé dans le code. Il est donc conseillé d’avoir un 
fichier par client (configFileNom du Client.txt) qu’il faudra renommé «configFile.txt » à 
chaque fois que l’on souhaitera basculer sur ce client là. 

• « Répertoire des PDFs à analyser » contient le chemin du PDF (ou du répertoire contenant 
les pdfs à traiter) qui sera traité. Ces fichiers peuvent être n’importe où sur le disque, il faut 
juste veiller à ce que le chemin soit toujours d’actualité et sans espace. 

• « Image représentative d’un folio du client » contient le chemin vers une image d’un folio 

du client. Cette image servira pour le paramétrage et est essentielle. 

 
 
 
 
• « Coordonnées du titre des folios » contient le rectangle de coordonnées du titre sur les 

folios: 

o Les coordonnées sont de la forme « a, b, c, d » 

§ a : coordonnée horizontale du point en haut à gauche du rectangle 
§ b : coordonnée verticale du point en haut à gauche du rectangle 
§ 
c : coordonnée horizontale du point en bas à droite du rectangle 
§ d : coordonnée verticale du point en bas à droite du rectangle 

Ces coordonnées sont déduites par le processus de sélection des données sur l’image de référence 
lors de la phase de paramétrage. Cela sera expliqué ultérieurement (section V.A-Etape Préalable à 
l’utilisation à chaque nouveau client) 

• « Coordonnées des numéros des folios » contient le rectangle de coordonnées du numéro 

de folio sur les folios 

• « Coordonnées du contenu des folios » contient le rectangle de coordonnées de la partie 

• 

schéma sur les folios 
 « Structure fixe des numéros de folios » permet d’expliciter quelle est la partie fixe d’un 
numéro de folio 

• « Expression régulières pour les numéros de folio » désignent l’expression générique qui va 

• 

• 

permettre de rechercher les numéros de folio dans chaque folio 
 « Expression régulières pour les renvois folio » désignent l’expression générique qui va 
permettre de rechercher les renvois folio dans chaque folio 
 « Analyser aussi les fichiers DXF » : Case à cocher qui permet d’inclure ou non les fichiers 
DXF dans l’analyse 

Remarque : 
Les expressions régulières constituent un système très puissant et très rapide pour faire des 
recherches dans des chaînes de caractères (des phrases, par exemple). Dans notre cas, l’application 
extrait le texte des PDFs afin de pouvoir leur appliquer les expressions régulières 
Les expressions régulières seront détaillé ultérieurement (section V.A.2.Identifier les repères, les 
lister et établir leurs expressions régulières) 

• « Chemin du fichier généré » contiendra le chemin et le nom du fichier généré à la fin de 

l’analyse. 

 
 
 
 
4. Les boutons 

Ce bouton situé au niveau du champs « Répertoire des 
PDFs à analyser » permet de localiser le fichier PDF à 
traiter 
Ce bouton permet de générer un fichier texte contenant 
le texte extrait du PDF. Le fichier généré porte le même 
nom que le PDF d’origine et sera généré dans le même 
répertoire que lui 

Ce bouton permet de générer un fichier XML contenant 
le texte extrait du PDF 
Le fichier généré porte le même nom que le PDF d’origine 
et sera généré dans le même répertoire que lui 

Ce bouton situé au niveau du champs « Image 
représentative d’un folio d’un client» permet de localiser 
l’image faite à partir d’un folio d’un client et qui 
représente l’organisation classique d’un folio du client 
Ce bouton permet d’ouvrir une fenêtre supplémentaire 
qui affiche l’image représentative, sur laquelle il est 
possible de localiser l’information à extraire. Chaque 
bouton permet de récupérer l’information relative au 
champ à coté duquel il est placé. 
Ce bouton permet la sauvegarde des paramètres dans le 
fichier configFile.txt si l’utilisateur souhaite les conserver. 

Ce bouton permet de lancer l’analyse du fichier et le 
processus d’extraction des connaissances. Un fichier 
Excel est créé à l’issue de cette étape. 
A chaque lancement d’une nouvelle analyse, il vaut 
mieux s’assurer que le fichier excel a été fermé ou 
renommer, car l’application écrase le fichier du jour. 

Une fois l’analyse réalisée, ce bouton permet l’ouverture 
du fichier excel créé 
Ce bouton permet de lancer la génération des graphes, 
spécifiques à chaque schéma, mais aussi globaux, offrant 
ainsi une autre représentation des schémas. 
Ce bouton permet de générer un histogramme des 
renvois folios par concepts. 
Ce bouton permet de lancer la fouille des données sur 
l’ensemble des données extraites des PDFs 

B. Partie Droite 

Ce bouton permet de quitter l’application 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Le champ textuel ci-dessus, affiche les sorties standards et erreur de l’application. Cette partie 
récapitule les paramètres à chaque nouvelle analyse lancée et ainsi permet de garder une trace des 
actions réalisés lors d’une session. 

Les erreurs affichées contiennent souvent des données techniques permettant une correction plus 
rapide, de ce fait, elles peuvent sembler peu intelligibles. 

IV- 

Etape 1 : Paramétrage de l’application 

A. Etape préalable à l’utilisation de l’application à chaque nouveau client : 
1. Création d’une image représentative 

Pour permettre à l’application de savoir où rechercher les informations à extraire, il faut lui donner 
les coordonnées des données à extraire. 

Ce paramétrage nécessite d’ouvrir les schémas des clients, et de trouver un folio « représentatif » du 
client. Ce folio doit contenir au niveau du cartouche un titre, un numéro de folio, et une partie 
circuit. 

L’utilisateur peut alors faire une capture d’écran du folio et enregistrer l’image au format png avec 
les PDFs du client. 

 
2. 

Identifier les repères, les lister et établir leurs expressions régulières 

Lors de la phase de fouille des données, l’application extrait les repères des composants électriques 
sur l’ensemble des PDF. 

Pour cela, il faut lui passer en paramètre les expressions régulières qu’elle doit rechercher : les 
différents masques des différents composants électriques qu’elle doit extraire. 

L’utilisateur doit donc au préalable, sélectionner un PDF ou plusieurs (cela est laissé à l’appréciation 
de l’utilisateur) parmi ceux du client représentatif et lister les repères qu’il rencontre. 

Dans le fichier ElectricalDictionnary.json, sont recensé les 25 catégories de composants électriques. 

Les repères doivent être regroupés selon ces 25 catégories lorsqu’elles sont présentes afin de 
faciliter l’analyse des similitudes. 

A partir de ces suites de repères, l’utilisateur peut alors en déduire les expressions régulières 
adéquates et compléter le fichier de configuration. 

Voici un exemple de paramétrage des repères dans le cas \_\_\_\_\_ pour les moteurs : 

Etape 1 : recensé tous les repères sur un ou plusieurs Pdfs 

"M" : ["MOTEURS"], 

M1V M1L M51 M52 M53 M54 M70D M70R M71 M72 M73 M77 

Etape 2 : recensé les structures 

Lettre+chiffre+lettre 

Lettre+Chiffre 

Etape 3 : construire une expression régulière adéquate : "^(M[A-Z]?[0-9]{1,3}[A-Z]?)" 

Indication sur l’expression régulière de \_\_\_\_\_ : 

^M 

[A-Z]? 

[0-9] {1,3} 

[A-Z]? 

^ : désigne que la chaine débute par un M 
M : partie fixe de la chaine recherchée 

[A-Z] : Il peut y avoir une lettre entre A et Z 
?: désigne que la lettre peut être présente ou 
absente (il s’agit d’une option) 
ex : MR ou M 
Un chiffre entre 0 et 9 répété 1 à 3 fois 
Ex : 1, 12, 123 

[A-Z] : Il peut y avoir une lettre entre A et Z 
?: désigne que la lettre peut être présente ou 
absente (il s’agit d’une option) 

B. Fichier de configuration : configFile.txt 

Voici un fichier de configuration vide à l’initialisation d’un projet : 

 
 
 
 
 
 
 
A l’exception des expressions régulières pour recherche les repères des composants électriques dans 
les textes des folios, l’ensemble des champs, peuvent être paramétré à partir de l’interface du 
prototype. 

Des indications importantes sont données en haut du document concernant sa forme et son 
contenu. 

Voici l’interface au démarrage d’un nouveau client : 

 
 
L’application : 

- 
- 
- 

Charge les informations du fichier configFile.txt 
Localise le fichier dictionnary.json qui contiendra les concepts globaux du client 
Charge les données du dictionnaire électrique ElectricalDictionnary.json (document non 
accessible depuis l’interface) et qui liste pour chacune des 25 familles de composants 
électriques, un vecteur binaire l’identifiant. 

1. Localiser le répertoire de PDFs ou le fichier à analyser 

Quel que soit l’élément que l’on souhaite analyser, un PDF ou un ensemble de PDF, il faut toujours 
sélectionner un PDF dans le répertoire souhaité. 

Ensuite si l’on souhaite analyser en une seule fois, tous les PDFs qui sont dans ce répertoire, il suffit 
de modifier directement à la main le chemin comme ci-dessous : 

En effet, l’application ne permet pas de sélectionner un répertoire directement, mais que des PDFs. 

2. Génération des fichiers textes et XMLs 

Une fois la localisation réalisée, il est recommandé de lancer la génération des fichiers textes associés 
puis celles des fichiers XML. 

Cette partie permettra de générer pour chaque PDF du client, un document texte et xml nommé 
comme le pdf. 

 
 
 
 
 
 
Le document txt, permettra de vérifier que l’application est capable d’exploiter les PDFs en question. 

Le document xml permettra de gagner du temps, sur la phase d’analyse, qui les nécessitera dans tous 
les cas. 

3. 

Image de référence et coordonnées à extraire 

Préalablement, l’utilisateur a préparé une image png de référence représentant un folio du client 
courant. 

a) Localisation de l’image de référence 

Il s’agit de la première étape pour permettre l’extraction des coordonnées des différentes 
informations nécessaires à l’analyse 

b) Extraction des coordonnées des titres des folios 

Remarque : 

Pour l’extraction de l’image, il vaut mieux avoir la même taille d’image de référence. Il faut donc veiller à 
ne pas redimensionner l’image entre deux phases d’extraction des coordonnées, car sinon les dimensions 
de l’image de référence ne seront pas les mêmes. 

L’utilisateur doit cliquer sur le bouton « Paramétrer », il verra ensuite l’image de référence se charger 
devant lui. 

Il lui faut tracer un rectangle avec la souris autour de l’information qu’il souhaite extraire. Lors de 
cette étape, l’utilisateur doit veiller à bien englober la donnée. 

A chaque rectangle tracé, un message apparait explicitant les coordonnées extraites. 

Ces coordonnées sont aussi affichées dans le champ associé au niveau de l’interface principale de 
l’application. 

 
L’utilisateur peut cliquer et créer autant de rectangle qu’il le souhaite sur la figure. Seul le dernier 
rectangle dessiné sera pris en compte. 

Il est a noté que les dimensions de l’image sont aussi chargées à chaque extraction. Ces coordonnées 
sont essentielles pour le calcul des nouvelles coordonnées à chaque fois que la page est de taille 
différente. 

c) Extraction des coordonnées des numéros de folios 

L’extraction de ces coordonnées suit le même procédé que l’extraction des coordonnées des titres 
des folios 

d) Extraction des coordonnées de la partie schéma des numéros de folios 

L’extraction de ces coordonnées suit le même procédé que l’extraction des coordonnées des titres 
des folios 

 
 
 
 
4. Partie fixe d’un numéro de folio 

Remarque : 
Cette étape nécessite d’ouvrir deux fichiers textes relatif a deux pdfs distincts d’un même 
client, afin de chercher deux ou trois numéros de folios. 
Cela permet d’une part de vérifier que cette information est bien extraite par l’application, et 
d’autre part de vérifier qu’elle est toujours extraite de la même manière. 

Si les folios sont Folio+numéro, alors la partie fixe est ‘Folio‘. Dans le cas du client \_\_\_\_\_ , la partie 
fixe est : 

5. Expressions régulières pour les numéros de folios 

Remarque : 
Nous avons choisi des expressions régulières pour le langage de programmation Python. Il faut 
donc faire attention à ce paramètre lors de recherche Google éventuelles car les regex 
s’adaptent selon le langage utilisé. 

Les expressions régulières sont des expressions génériques qui permettent de rechercher dans un 
ensemble de texte des éléments bien particulier. 

Plus une expression régulière se veut précise, plus elle est complexe. Il est recommandé d’utiliser les 
deux liens donnés en annexe (section IX.A.Expressions régulières) pour tester et en apprendre plus 
sur les expressions régulières, aussi appelés regex. 

Remarque : 
Pour les expressions régulières, en général, il vaut mieux aussi contrôler que l’extraction se fait 
bien en se basant sur les fichiers textes. 
Il est alors possible d’utiliser le texte extrait sur un site de test de regex en ligne afin de vérifier 
que son expression retourne le résultat escompté. 

Voici quelques indications pour rechercher des numéros de folio en se basant sur le cas \_\_\_\_\_: 

Indication sur l’expression régulière de \_\_\_\_\_: 

F\\_ 

La partie fixe permet de préciser à la regex de 
ne chercher que les textes qui commencent par 
cette partie. 

(?P XXX ) 

Cette expression crée une balise « NumFolio », 

 
 
 
 
 
 
 
[0-9] 

[0-9] {3} 

indiqué par « ?P » et entouré de parenthèses, 
qui sera ensuite exploiter dans le code. 
Il faut donc inclure dans chaque regex créé : 
( ?P) 
XXX correspond à la regex, soit la partie variable 
que l’on cherche à extraire 
Désigne que l’on cherche un chiffre entre 0 et 9 

Désigne que l’on cherche un chiffre entre 0 et 9 
qui est répété 3 fois 

Cette expression va permettre d’extraire les textes suivants : F\_200, F\_201, etc. 

En fournissant à l’application l’indication que « NumFolio » est 200, 201, … 

6. Expressions régulières pour les renvois folios 

Indication sur l’expression régulière de \_\_\_\_\_: 

(?:\n\s|\n|:|\s) 

(?P XXX ) 

[0-9] {3} 

-(?:[0-9]{1,2}){1}$ 

? : désigne que ce qui se trouve entre parenthèse 
sert juste pour la recherche, mais qu’on en souhaite 
pas le retrouver dans le résultat final 
\n\s|\n|:|\s implique que l’on cherche tout ce qui 
commence par 

o Soit un retour chariot suivi d’un 

espace(\n\s) 

o Soit un retour chariot seul(\n) 
o Soit du caractère : 
o Soit d’un espace blanc (\s) 
NB : la barre verticale « | » implique un OU 
Cette expression crée une balise « NumPage », 
indiqué par « ?P » et entouré de parenthèses, qui 
sera ensuite exploiter dans le code. 
Il faut donc l’inclure dans chaque regex créér par la 
suite. 
XXX correspond à la regex, soit la partie variable que 
l’on cherche à extraire 
C’est tout se qui se trouve entre les parenthèses de 
la balise, qui sera exploité par l’application. Ce qu’il y 
a autour sert à préciser la regex. 
La partie fixe permet de préciser à la regex de ne 
chercher que les textes qui commencent par cette 
partie. 

? : désigne que ce qui se trouve entre parenthèse 
sert juste pour la recherche, mais qu’on en souhaite 
pas le retrouver dans le résultat final 
[0-9]{1,2} : désigne un chiffre entre 0 et 9 qui peut 

 
 
 
 
 
 
être présent 1 à 2 fois. 
{1} : implique que l’on cherche des textes où ([0-
9]{1,2}) est présent une fois 
$ désigne que le texte recherché se situe en fin de 
ligne 

Cette expression complexe va permettre d’extraire les textes suivants : 200-12, 201-40, etc. 

En fournissant à l’application l’indication que « NumPage » est 200, 201, … 

7. Analyse des fichiers DXF 

Si pour chaque PDF, le client fournit un fichier DXF unique, il est alors possible de cocher la case à 
cocher «Analyser aussi les fichiers DXF ». 

Le programme créera alors des onglets en plus relatif à l’analyse supplémentaire des fichiers DXF. 

8. Sauvegarde des paramètres 

A chaque sauvegarde des paramètres, un message s’affiche pour confirmer la bonne réalisation de 
l’opération. 

De plus, dans la partie droite de l’écran, il est possible de trouver un récapitulatif des nouveaux 
paramètres. 

Cette affichage permet, s’il l’on a changé plusieurs fois de paramètres, d’avoir un rappel des 
différents paramètres testés. 

 
 
 
 
 
 
 
 
V- 

Etape 2 : Analyse et extraction des données dans un fichier Excel 

A. Lancement de l’analyse 
1. Analyse des titres issus des PDFs 

Pour lancer l’analyse, il faut cliquer le bouton 

. 

Le prototype va alors vérifier que pour chaque PDF, un fichier xml existe. Si c’est le cas, l’application 
demande si l’on veut regénérer le fichier xml. 

Si le fichier xml correspondant n’existe pas, l’application se chargera de le générer puis de l’exploiter. 

Un message avertit de la fin de l’analyse : 

Et dans la partie droite de l’écran, l’utilisateur 
retrouve des logs qui indiquent l’avancé de 
l’analyse, ainsi que les fichiers créés. 

Au niveau de l’interface, le champ « Chemin du fichier de l’analyse » est désormais renseigné : 

 
 
 
 
 
 
2. Analyse des fichiers DXF 

L’analyse des fichiers DXF nécessitent d’avoir pour chaque PDF, un seul fichier DXF associé complet. 

L’analyse prend alors plus de temps, mais génère en sortie un fichier avec plus d’informations. 

Voir section VI.C.2 Analyse avec les fichiers DXFs 

B. Fonctionnement de l’analyse 
1. Génération de sac de mots à partir des titres 

Chaque titre de chaque pdf est décomposé en sac de mots. 

Les sacs de mots de chaque schéma électrique sont utilisés pour calculer l’intersection entre eux. 

Les mots communs résultants les concepts globaux détectés d’un client donné. Ces mots sont listé 
dans le fichier dictionnary.json qui est mis à jour à chaque analyse. 

Les titres par folios et les sacs de mots sont aussi listés dans les premiers onglets du fichier excel 
d’analyse « DataExtraction.xls » 

C. Fichier DataExtraction.xls 
1. Analyse de base 

Lorsque l’application réalise une analyse, elle produit en sortie un fichier excel en date du jour qui 
comporte plusieurs onglets : 

Onglet 

Configuration 
List\_of\_Titles 
Bags\_of\_words 
Cleaned\_Bags\_Of\_Words 

BOW\_Analysis 

FoliosIndication\_Analysis 

Information 
Reprend et liste les paramètres utilisés pour générer le fichier 
Liste des titres par pdf 
Sac de mots issus des titres de chaque PDF 
Sac de mots filtré : 

- 
- 

Pas de mots <3 lettres 
Pas de caractères numériques ou non alphabétique 

Anayse des sac de mots : calcul d’union et d’intersection 
entre les différents sac de mots 
Comptabilisation des renvois folios distincts par PDF et par 
page 

NF\_FoliosIndication\_Analysis Comptabilisation des renvois folios par PDF et par page quel 

CommonWords\_Analysis 

que soit le nombre de répétition 
Comptabilisation du nombre d’occurrence des mots 
communs sur le panel de pdf étudié 

 
 
 
 
2. Analyse avec les fichiers DXFs 

Lorsque les fichiers DXF sont fournit, l’analyse est alors plus longue, mais elle permet de générer des 
onglets supplémentaires. 

Onglet 
EleCompLabel\_Analysis 

ElecCompFamily\_Analysis 

NomFichierPDF 

Seq\_NomFichierPDF 

Information 
Liste des repères des composants électriques par PDF et trié par 
ordre alphabétique 
Liste des repères des composants électriques par famille de 
composants électriques 
Liste l’ensemble des blocs INSERT (symboles de composants 
électriques) du fichier DXF ainsi que l’ensemble des attributs 
Liste par folio du PDF la séquence de repères ordonnée selon les 
cordonnées électriques et aussi la séquence de famille de 
composants qui en découle 

Remarque : Les deux derniers onglets décrits sont spécifiques à un PDF. Il y en aura autant de créer 
que de PDF traités. 

D. Fichier dictionnary.json 

A la fin de l’analyse, le programme a réussi à extraire l’ensemble des concepts globaux en se basant 
sur les mots composant les titres des folios. 

Ces concepts globaux, sont listés dans un fichier « dictionnary.json » que l’utilisateur peut alors 
visualiser, mais aussi compléter. 

Le fichier se présente sous la forme : 

{ 

} 

conceptGlobal1 : [« mot associé 1 », « mot associé 2 », « mot associé 3 »] 

conceptGlobal2 : [ « mot associé 4 », « mot associé 5 », « mot associé 6 »] 

Un exemple du dictionnaire pour \_\_\_\_\_ (généré automatiquement et complété manuellement) est 
disponible en annexe sous le titre « dictionnary\_\_\_\_\_.json » 

Une fois le dictionnaire modifié et le fichier d’analyse excel sauvegardé sous un autre nom, une 
phase d’analyse peut être généré à nouveau pour voir les résultats de l’extraction. 

Il est fortement conseillé de consulté et compléter ce fichier dictionnary.json en parallèle du fichier 
excel DataExtraction avant de lancer la génération des graphes. Car cette étape suivante se base sur 
ce dictionnaire de concepts. 

 
 
 
 
 
VI- 

Etape 3 : La génération de graphes 

A. Fonctionnement de l’algorithme 
1. Obtention de la logique métier de chaque pdf 

La première étape du traitement a été de générer un graphe à partir de chaque PDF en se basant sur 
les numéros de folios, et les renvois folios, les labels des nœuds étant les titres des folios. 

Ensuite chaque titre (label de nœud) est remplacé par le mot commun associé en se basant sur le 
fichier dictionnary.json. 

Les nœuds portant un même label sont alors fusionnés afin d’obtenir une représentation simplifiée 
de la connaissance contenu dans le PDF d’origine. 

2. Fusion des graphes de logique métier de chaque pdfs 

La seconde étape du traitement a été de récupéré pour chaque PDF, le graphe de la logique métier et 
de les concaténer ensemble. 

En partant de l’hypothèse que chaque graphe est une instanciation de la connaissance globale (une 
partie) alors, leur concaténation, permettra de reconstruire la connaissance globale. 

L’utilisateur obtiendra ainsi le graphe des connaissances globale du client construit directement de 
ses pdfs. 

 
 
 
 
 
B. Images produites 

Pour un schéma PDF, 2 graphes sont créée : 

NomPDF\_original.png 

NomPDF\_Filtered.json 

NomPDF\_JLG.png 

NomPDF\_JLG\_Filtered.json 

Un graph au format png avec tout les folios, même ceux sans 
lien 
Un fichier json qui décrit le graph original sans les nœuds sans 
lien 
Un graph graph de la logique métier avec des nœuds 
standardisés 
JLG = Job Logic Graph (graph de la logique métier 
Un fichier json qui décrit le graph standardisé sans les nœuds 
sans lien 

Si l’analyse porte sur plus de 2 PDFs, alors le graphe des concepts métier est aussi généré sous le 
nom DateDuJour\_SemanticGraph\_.png. 

Ce dernier regroupe tous les concepts métiers utilisés par le client sur l’ensemble des PDFs analysés 
ainsi que les relations entre eux. 

VII- 

Etape 4 : Le clustering hiérarchique 

1. Processus et fichiers générés 

Le clustering hiérarchique est une méthode d’analyse qui vise à construire une hiérarchie de cluster 
en utilisant une distance et une méthode de mesure pour créer ces clusters. 

Un cluster étant un ensemble dont les composants partagent des caractéristiques communes. 

Grace à la phase d’analyse, le programme a extrait les composants électriques sur chaque folio ainsi 
que leur coordonnées x, y. 

C’est selon un x et un y croissant que les composants électriques ont été ordonnées au sein de 
chaque folio. 

Pour cette partie, deux jeux de données ont été constitués : 

- 
- 

Le premier jeu de données : regroupe tous les folios ensemble tout PDF confondus 
Le second jeu de données regroupe les folios tout PDF confondus, mais par concept métier 
d’appartenance. 

Chaque folio, qui correspond à une séquence de composant électrique est ensuite traduit en un 
vecteur binaire de longueur 25 (car nous avons recensé 25 familles de composants électriques) : 
chaque unité désignant la présence (1) ou l’absence (0) d’une famille de composant au sein d’un 
folio. 

Ces deux ensemble de données sont ensuite analysée. Dans le répertoire des fichiers analysés, 
plusieurs éléments sont créés : 

- Un répertoire GLOBAL\_HC qui correspond au résultat du hierarchical clustering sur le 

premier jeux de données 

 
 
Ce répertoire comporte alors plusieurs fichiers : 

Le fichier d’analyse excel en date du jour 

- 
- DictData.json : qui contient le détail des données du premier set. Pour chaque folio, on y 

trouve : 

o Le vecteur booléen associé 
o La séquence de catégorie de composant ex : [Q, Q] 
o Le titre du folio 
o Son PDF d’origine 
o La séquence de repère tel qu’elle apparait dans le folio ex : [Q100, Q102] 
o Le numéro de page dans le PDF 
o Le vecteur numérique : celui qui comptabilise le nombre de composants de chaque 

famille dans le folio 

- DictVectord.json : qui contient des détails des données du premier set (mais moins de 

données que DictData.json). Pour chaque folio, on y trouve : 
o La séquence de catégorie de composant ex : [Q, Q] 
o Le titre du folio 
o Le vecteur booléen associé 
o Le vecteur numérique associé 
o La séquence de repère tel qu’elle apparait dans le folio ex : [Q100, Q102] 

- HC\_DictVectorsClusters.png : Dendogram (représentation graphique des résultats du 
clustering hiérarchique) avec les numéros de clusters explicitement marqué dessus 

- HC\_average\_jaccard.png : Dendogram avec les distances entre cluster calculer explicitement 

marqué dessus. 

- HC\_simpleDendogram.png : Dendogram simple sans annotation sur lequel on trouve les 

noms des folios sur l’axe des abscisses (l’axe des x) 

- HC\_Tree.png : est un arbre hiérarchique de cluster dans lequel les nœuds sont des folios. 

Chaque nœud comporte : 

o Le numéro de cluster 
o Le nom du folio 
o La séquence de catégorie de composant ex : [Q, Q] 
o La séquence de repères ex : [Q100, Q100] 

- 

ListOfPageWithComponent.json : Fichier json qui comporte pour chaque cluster : 

o La séquence de catégorie de composant 

 
o Le titre du folio 
o La séquence de repère 

- 

ListOfPageWithSortedComponent.json : comporte pour chaque folio traité 

o Son titre 
o La séquence ordonnée de catégorie de famille selon leur coordonnées x et y 

croissantes 

2. Fichier Excel HC\_ClusterAnalysis.xlsx 

Ce fichier comporte un onglet important : « CLusterAnalysis » 

Cet onglet regroupe l’analyse des clusters en explicitant à chaque ligne : 

- 
- 
- 
- 
- 

- 

les clusters calculés 2 à 2 
leurs vecteurs binaires associés, 
leur titre du folio 
Leurs séquences de repères mais aussi de famille de composants 
Et surtout les distances calculées selon différentes mesures : Jacquard (celle utilisé par 
défaut pour l’instant) Hamming, Euclidien, 
Et enfin pour chaque ligne, le nombre total de clusters qui ont été fusionné ensemble. 

VIII- Annexe et Liens supplémentaires 

A. Expressions régulières 
- 

Pour en savoir plus sur les expressions régulières : http://apprendre-python.com/page-
expressions-regulieres-regular-python 
Pour tester ses propres expressions régulières sans utiliser le prototype : 
https://regex101.com/ 

- 

B. Annexe : dictionnary.json de \_\_\_\_\_ 

{ 
 "ECHANGES": ["ECHANGES"], 
 "INDICE": ["INDICE"], 
 "CARNET": ["CARNET"], 
 "PROTECTIONS": ["PROTECTIONS"], 
 "ELECTRIQUE": ["ELECTRIQUE", "ELECTRIQUES"], 
 "DATE": ["DATE"], 
 "VARIATEUR": ["VARIATEUR", "VARIATEURS", "VAR.", "RESEAU", "RESEAUX"], 
 "RACCORDEMENT": ["RACCORDEMENT"], 
 "INSTALLATION": ["INSTALLATION", "INSTALLATIONS"], 
 "SECURITE": ["SECURITE", "LIGNE", "LIGNES", "SIGNALISATION", 
"SIGNALISATIONS", "RADAR", "RADARS"], 
 "LOCAL": ["LOCAL"], 
 "NORME": ["NORME"], 
 "ETUDIE": ["ETUDIE"], 
 "GENERALE": ["GENERALE"], 
 "AUTOMATE": ["AUTOMATE", "AUTOMATES", "STATION", "STATIONS", "SORTIE", 

 
 
 
 
 
"SORTIES", "Q0.0", "Q0.3", "Q1.0", 
 "Q1.3", "Q2.0", "Q2.3", "Q3.0", "Q3.3", "Q4.0", "Q4.3", "Q5.0", "Q5.3", 
"Q6.0", "Q6.3", "Q7.0", "Q7.3", 
 "Q8.0", "Q8.3", "Q9.0", "Q9.3", "Q10.0", "Q10.3", "Q15.0", "Q15.3", 
"Q16.0", "Q16.3", "Q17.0", "Q17.3", 
 "Q18.0", "Q18.3", "Q19.0", "Q19.3", "Q20.0", "Q20.3", "Q21.0", "Q21.3", 
"Q22.0", "Q22.3", "Q23.0", "Q23.3", 
 "Q24.0", "Q24.3", "Q25.0", "Q25.3", "ENTREE", "ENTREES", "I0.0", 
"I0.3", "I1.0", "I1.3", "I2.0", "I2.3", "I3.0", 
 "I3.3", "I4.0", "I4.3", "I5.0", "I5.3", "I6.0", "I6.3", "I7.0", "I7.3", 
"I8.0", "I8.3", "I9.0", "I9.3", "I10.0", 
 "I10.3", "I15.0", "I15.3", "I16.0", "I16.3", "I17.0", "I17.3", "I18.0", 
"I18.3", "I19.0", "I19.3", "I20.0", "I20.3", 
 "I21.0", "I21.3", "I22.0", "I22.3", "I23.0", "I23.3", "I24.0", "I24.3", 
"I25.0", "I25.3"], 
 "BORNE": ["BORNE", "BORNES", "BORNIERS", "BORNIER", "LISTE", "LISTES"], 
 "OBSERVATION": ["OBSERVATION"], 
 "CABLE": ["CABLE", "CABLES"], 
 "ELEMENT": ["ELEMENT", "ELEMENTS"], 
 "FILTRE": ["FILTRE"], 
 "PUPITRE": ["IMPLANTATION", "IMPLANTATIONS", "PORTE", "PORTES", 
"ARMOIRE", "ARMOIRES"], 
 "CARACTERISTIQUES": ["CARACTERISTIQUE", "CARACTERISTIQUES"], 
 "STOCK": ["STOCK"], 
 "ALIMENTATION": ["ALIMENTATION", "ALIMENTATIONS", "DISTRIBUTION", 
"PUISSANCE", "PUISSANCES"], 
 "BARRES": ["BARRES"], 
 "EQUIPEMENT": ["EQUIPEMENT", "EQUIPEMENTS", "MOTEUR", "MOTEURS", 
"MOVIMOTS", "MOVIMOT", "MOVI", 
 "MOVIDRIVES", "MOVIDRIVE", "CONVOYEUR", "CONVOYEURS", "CODEUR", 
"CODEURS", "VENTILATEUR", "VENTILATEURS"], 
 "MASSES": ["MASSES"], 
 "BARRAGE": ["BARRAGE", "BARRAGES"], 
 "COMPOSANT": ["COMPOSANT", "COMPOSANTS"], 
 "NAVETTE": ["NAVETTES", "NAVETTE"] 
} 

 
 
182

Ikram Chraibi Kaadoud

Annexe B

Dictionnaire des mots communs

183

{

"ECHANGES": ["ECHANGES"],
"ELECTRIQUE": ["ELECTRIQUE", "ELECTRIQUES"],
"CARNET": ["CARNET"],
"PUPITRE": ["IMPLANTATION", "IMPLANTATIONS", "PORTE", 

"PORTES", "ARMOIRE", "ARMOIRES"],

"INSTALLATION": ["INSTALLATION", "INSTALLATIONS"],
"SECURITE": ["SECURITE", "LIGNE", "LIGNES", "SIGNALISATION", 

"SIGNALISATIONS", "RADAR", "RADARS"],

"VARIATEUR": ["VARIATEUR", "VARIATEURS", "VAR.", "RESEAU", 

"RESEAUX"],

"NORME": ["NORME"],
"GENERALE": ["GENERALE"],
"AUTOMATE": ["AUTOMATE", "AUTOMATES", "STATION", "STATIONS", 
"SORTIE", "SORTIES", "Q0.0", "Q0.3", "Q1.0", "Q1.3", "Q2.0", "Q2.3", 
"Q3.0", "Q3.3", "Q4.0", "Q4.3", "Q5.0", "Q5.3", "Q6.0", "Q6.3", 
"Q7.0", "Q7.3", "Q8.0", "Q8.3", "Q9.0", "Q9.3", "Q10.0", "Q10.3", 
"Q15.0", "Q15.3", "Q16.0", "Q16.3", "Q17.0", "Q17.3", "Q18.0", 
"Q18.3", "Q19.0", "Q19.3", "Q20.0", "Q20.3", "Q21.0", "Q21.3", 
"Q22.0", "Q22.3", "Q23.0", "Q23.3", "Q24.0", "Q24.3", "Q25.0", 
"Q25.3", "ENTREE", "ENTREES", "I0.0", "I0.3", "I1.0", "I1.3", 
"I2.0", "I2.3", "I3.0", "I3.3", "I4.0", "I4.3", "I5.0", "I5.3", 
"I6.0", "I6.3", "I7.0", "I7.3", "I8.0", "I8.3", "I9.0", "I9.3", 
"I10.0", "I10.3", "I15.0", "I15.3", "I16.0", "I16.3", "I17.0", 
"I17.3", "I18.0", "I18.3", "I19.0", "I19.3", "I20.0", "I20.3", 
"I21.0", "I21.3", "I22.0", "I22.3", "I23.0", "I23.3", "I24.0", 
"I24.3", "I25.0", "I25.3"],

"BORNE": ["BORNE", "BORNES", "BORNIERS", "BORNIER", "LISTE", 

"LISTES"],

"CABLE": ["CABLE", "CABLES"],
"ELEMENT": ["ELEMENT", "ELEMENTS"],
"FILTRE": ["FILTRE"],
"RACCORDEMENT": ["RACCORDEMENT"],
"CARACTERISTIQUES": ["CARACTERISTIQUE", "CARACTERISTIQUES"],
"STOCK": ["STOCK"],
"ALIMENTATION": ["ALIMENTATION", "ALIMENTATIONS", 

"DISTRIBUTION", "PUISSANCE", "PUISSANCES"],

"EQUIPEMENT": ["EQUIPEMENT", "EQUIPEMENTS", "MOTEUR", 
"MOTEURS", "MOVIMOTS", "MOVIMOT", "MOVI", "MOVIDRIVES", "MOVIDRIVE", 
"CONVOYEUR", "CONVOYEURS", "CODEUR", "CODEURS", "VENTILATEUR", 
"VENTILATEURS"],

"MASSES": ["MASSES"],
"BARRAGE": ["BARRAGE", "BARRAGES"],
"COMPOSANT": ["COMPOSANT", "COMPOSANTS"],
"NAVETTE": ["NAVETTES", "NAVETTE"]

}

Annexe C

Dictionnaire des catégories de
composants électriques

185

{
"A" : ["ENSEMBLE FONCTIONNEL"],
"B" : ["TRANSDUCTEURS"],
"C" :["CONDENSATEURS"],
"D" : ["OPERATEURS BINAIRES-DISPOSITIFS DE TEMPORISATION DE MISE EN 
MEMOIRE"],
"E" : ["MATERIELS DIVERS"],
"F" : ["DISPOSITIFS DE PROTECTION"],
"G" : ["GENERATEURS DISPOSITIFS ALIMENTATION"],
"H" : ["DISPOSITIFS DE SIGNALISATION"],
"K" : ["RELAIS ET CONTACTEURS"],
"KA" : ["CONTACTEURS AUXILIAIRES RELAIS D AUTOMATISME"],
"KM" : ["CONTACTEURS PRINCIPAUX"],
"L" : ["INDUCTANCES"],
"M" : ["MOTEURS"],
"N" : ["SOUS-ENSEMBLES HORS SERIE"],
"P" : ["INSTRUMENT DE MESURE ET D ESSAI"],
"Q" : ["APPAREILS MECANIQUE DE CONNEXION POUR CIRCUITS DE 
PUISSANCE"],
"R" : ["RESISTANCES"],
"S" : ["APPAREILS MECANIQUES DE CONNEXION POUR CIRCUIT DE 
COMMANDE"],
"T" : ["TRANSFORMATEURS"], 
"U" : ["MODULATEURS-CONVERTISSEURS"],
"V" : ["TUBES ELECTRONIQUES- SEMI CONDUCTEURS"],
"W" : ["VOIES DE TRANSMISSION-GUIDES D ONDES-ANTENNES"],
"X" : ["BORNES-FICHES-SOCLES"],
"Y" : ["APPAREILS MECANIQUES ACTIONNES ELECTRIQUEMENT"],
"Z" : ["CHARGES CORRECTIVES-TRANSFORMATEURS DIFFRENTIELS-FILTES 
CORRECTEURS-LIMITEURS"]
}

Annexe D

Algorithme d’apprentissage de
SRN

187

Algorithme 4 Algorithme d’apprentissage du SRN

Pour tout séquence de taille n faire

Pour t = 0 à (n

1) faire

−
# Etape 1 : propagation avant
entrée\_réseau = séquence[t]
sortie\_attendue = séquence[t + 1]
sortie\_obtenue = SRN.propagation\_avant(entrée\_réseau)
Erreur\_SRN = sortie\_attendue - sortie\_obtenue
# Etape 2 : propagation arrière
# Etape 2a : calcul de l’erreur pour chaque unité de la couche de sortie
# σ0 : dérivée de la fonction d’activation σ des unités du SRN
Pour tout unité k de la couche de sortie faire
δk = Erreur\_SRN[k] \* σ ’(sortie\_obtenue[k])

Fin Pour
# Etape 2b :Calcul de l’erreur au niveau de la couche cachée pour
chaque unité j et au niveau de la couche d’entrée
Pour tout couche du réseau, de la couche cachée à la couche d’entrée
faire

Pour tout unité j de la couche courante c faire

# σ0 : dérivée de la fonction d’activation σ des unités du SRN
# w(k,j) : poids provenant de j et allant vers k
# yj : somme des entrées de l’unité j
δj = σ ’(yj) \* Pk(w(k,j) \* (δk))

Fin Pour

Fin Pour
# Etape 2c : Mise à jour des poids
Pour tout poids w(j,i) faire

# xji : entrée associée au lien entre l’unité i vers l’unité j
Variation du poids = Taux d’apprentissage \* δj \* xji + momentum
\* (Variation du poids(t-1))
Nouveau poids = Ancien poids + Variation du poids

Fin Pour

Fin Pour

Fin Pour

188

Ikram Chraibi Kaadoud

Annexe E

Algorithme d’apprentissage de
RNN-LSTM d’après Gers et
Schmidhuber [2001]

Voici ci dessous notre pseudo-code présentant l’apprentissage dans un RNN-

LSTM largement inspiré des travaux de Gers et Schmidhuber [2001]

Algorithme 5 Algorithme d’apprentissage d’un RNN-LSTM
Pré-conditions : Soit les fonctions d’activations suivantes :

1+e−x

f (x) = 1
h(x) = 2
1
g(x) = 4
2
et leur dérivées respectives f 0, h0 et g0
Pour tout sequence de taille n faire

f (x)
f (x)

−
−

⇤
⇤

Pour i = 0 à (n

1) faire

−
# Etape 1 : propagation avant
entrée\_réseau = sequence[i]
sortie\_attendue = sequence[i + 1]
# Etape 1 : propagation avant
sortie\_obtenue = RNN\_LSTM.propagation\_avant(entrée\_réseau)
Erreur\_RNN\_LSTM = sortie\_attendue - sortie\_obtenue
# Etape 2 : propagation arrière
RNN\_LSTM.propagation\_arrière(Erreur\_RNN\_LSTM)

Fin Pour

Fin Pour

189

Algorithme 6 Apprentissage d’un RNN-LSTM : propagation avant

Fonction propagation\_avant (entrée\_réseau)
# propagation avant de la couche d’entrée à la couche cachée
Pour tout bloc j faire

Pour tout cellule cv

j faire

# Calcul des activations reçues (parmi les neurones m, se trouvent les neu-
rones de la couche d’entrée qui reçoivent l’entrée\_réseau)

netl(t) = X
m

(w(l,m) ·

ym(t)) + X
p

1))

yp(t

(w(l,p) ·
cv
j , inj, φj, outj

−

}

avecl

2 {

# Calcul des activations selon les fonctions d’activations

yinj (t) = finj (netinj (t))
yφj (t) = fφj (netφj (t))
youtj (t) = foutj (netoutj (t))
yφj (t)
yinj (t) + scv
j (t
·
ycv
youtj (t)
j (t) = h(scv

−
j (t))

1)

·

·

scv

j (t) = g(netcv

j (t))

# Calcul des dérivées partielles

dSjv

c,m(t) = dSjv

cm(t

1)

⇤

−

yφj (t) + g0

(netcv

j )(t)

⇤

yinj (t)

dSjv

in,m(t) = dSjv

in,m(t

1)

⇤

−

yφj (t) + g(netcv

j (t))

f 0
inj (netinj (t))

⇤

dSjv

φ,m(t) = dSjv

φ,m(t

1)

⇤

−

yφj (t) + h(scv

j (t))

f 0
φj (netφj (t))

⇤

Fin Pour

Fin Pour
# propagation avant de la couche cachée à la couche de sortie
Erreur\_RNN\_LSTM = vecteur de taille k
Pour tout unité k de la couche de sortie faire

# Calcul de la sortie du réseau : activation de la couche de sortie

netk(t) = X
m

(w(k,m) ·

yp(t

ym(t)) + X
p

(w(k,p) ·
yk(t) = fk(netk(t))
Erreur\_RN N \_LST M [k] = yk(t)

1))

−

(E.1)

(E.2)

(E.3)

(E.4)

(E.5)

(E.6)

(E.7)

⇤

⇤

⇤

ym(t

ym(t

1)
−
(E.8)

1)
−
(E.9)

ym(t

1)
−
(E.10)

(E.11)

(E.12)

(E.13)

Fin Pour
return Erreur\_RNN\_LSTM
Fin Fonction

190

Ikram Chraibi Kaadoud

E. Algorithme d’apprentissage de RNN-LSTM d’après Gers et Schmidhuber
[2001]

Algorithme 7 Algorithme d’apprentissage d’un RNN-LSTM : propagation
arrière

Fonction propagation\_arrière (Erreur\_RNN\_LSTM) :
# Etape 2 : propagation arrière —————————————————-
Pour tout k de la couche de sortie faire

ek(t) = Erreur\_RN N \_LST M [k]
(ek(t))

δk(t) = f 0

k(netk(t))

⇤

(E.14)

(E.15)

Fin Pour
Pour tout bloc j faire

Pour tout cellule cv

j faire

# entre les unités k de la couche de sortie et les portes de sorties des
blocs j

δoutj (t) = f 0

outj (netoutj (t))

Sj

(

X
v=1

⇤

h(scv

j (t))

(X
m

⇤

wkcv

j ⇤

δk(t)))

(E.16)

# entre les unités k de la couche de sortie et les portes d’entrées, les
portes d’oublies, ainsi que les cellules des blocs j

ecv

j (t) = youtj

h0(scv

j (t))

⇤

(X
m

⇤

wkcv

j ⇤

δk(t)))

(E.17)

Fin Pour

Fin Pour
# Mise à jour des poids d’un réseau RNN-LSTM

∆lm(t) = α
(δl(t))
⇤
⇤
j ,m(t) = α
∆cv
Sj

∆in,m(t) = α

(

⇤

X
v=1
Sj

ym(t), l
ecv
j (t)

⇤

ecv

j (t)

⇤

2 {

k, i, out
}
dSjvc,m(t)

⇤

dSjvin,m(t))

∆φ,m(t) = α

(

X
v=1

⇤

ecv

j (t)

dSjvφ,m(t))

⇤
k, i, out, cv

j , in, φ

}

Wzm(t) = Wzm(t) + ∆zm(t), z

Fin Fonction

2 {

(E.18)

(E.19)

(E.20)

(E.21)

(E.22)

Extraction de règles de réseaux récurrents

191

192

Ikram Chraibi Kaadoud

Bibliographie

Aimé, Xavier, Furst, Frédéric, Kuntz, Pascale et Trichet, Francky, 2010.
Enrichissement sémantique de requêtes au moyen d’ontologies de domaine
personnalisées. Actes de l’atelier Personnalisation du Web, 10ième Jour-
nées francophones d’Extraction et de Gestion de Connaissances (EGC’2010),
Hammamet, 189.

Aimé, Xavier et Charlet, Jean, 2013. IC : Ingénierie des connaissances ou
ingénierie du conformisme ? Dans IC-24èmes Journées francophones d’In-
génierie des Connaissances.
URL https://hal.inria.fr/hal-01103767/

Alonso, Fernando, Martínez, Loïc, Pérez, Aurora et Valente, Juan P,
2012. Cooperation between expert knowledge and data mining discovered
knowledge : Lessons learned. Expert Systems with Applications, 39(8) :7524–
7535.

Alvarez-Macias, JL, Mata-Vazquez, J et Riquelme-Santos, JC, 2004.
Data mining for the management of software development process.
In-
ternational Journal of Software Engineering and Knowledge Engineering,
14(06) :665–695.

Amini, Massih-Reza et Gaussier, Eric, 2013. Recherche d’information : ap-

plications, modèles et algorithmes. Editions Eyrolles.

Anderson, John R, 1987. Skill acquisition : Compilation of weak-method

problem situations. Psychological review, 94(2) :192.

Anderson, John R, 2005. Cognitive psychology and its implications. Macmil-

lan.

Anderson, JR, 1983. Cognitive science series. the architecture of cognition.

Angeline, Peter J, Saunders, Gregory M et Pollack, Jordan B, 1994.
An evolutionary algorithm that constructs recurrent neural networks. IEEE
transactions on Neural Networks, 5(1) :54–65.

193

BIBLIOGRAPHIE

Barney, Jay, 1991. Firm resources and sustained competitive advantage.

Journal of management, 17(1) :99–120.

Belaïd, Abdel et Cecotti, Hubert, 2006. Reconnaissance de caractères :

évaluation des performances.

Bengio, Yoshua, Simard, Patrice et Frasconi, Paolo, 1994. Learning long-
term dependencies with gradient descent is diﬃcult. IEEE transactions on
neural networks, 5(2) :157–166.

Bengio, Yoshua et al., 2009. Learning deep architectures for ai. Foundations

and trends R
(

in Machine Learning, 2(1) :1–127.

Berry, Dianne C et Dienes, Zoltan, 1991. The relationship between implicit
memory and implicit learning. British Journal of psychology, 82(3) :359–373.

Bhargava, Sanjay et Purohit, GN, 2011. Parsing with neural and ﬁnite
automata networks : A graph grammar approach. International Journal of
Computer Applications, 23(4).

Black, Paul E., 2004. "ﬁnite state machine".

in Dictionary of Algorithms
and Data Structures [online], Vreda Pieterse and Paul E. Black eds. Avai-
lable from : https ://xlinux.nist.gov/dads/HTML/ﬁniteStateMachine.html
(accessed 15 November 2017).

Blanco, Armando, Delgado, Miguel et Pegalajar, MC, 2000. Extracting
rules from a (fuzzy/crisp) recurrent neural network using a self-organizing
map. International Journal of Intelligent Systems, 15(7) :595–621.

Bootz, Jean-Philippe, 2015. Comment concilier auto-organisation et contrôle
au sein des communautés de pratique pilotées ? : une scoping review/how to
reconcile self-organization and control in driven communities of practice ? : a
scoping review/como se pueden conciliar la auto organización y el control en
las comunidades de práctica conducidas ? una scoping review. Management
International, 19(3) :15.

Jean-Philippe,
Available

Bootz,
Comment
pert ?
https
content/uploads/2016/01/BourbaKeM-5.pdf
2017).

2016.
from :

et

gérer

déﬁnir

l’ex-
://www.agecso.com/wp/wp-
20 December
(accessed

Bootz, Jean-Philippe et Schenk, Eric, 2014. L’expert en entreprise : proposi-
tion d’un modèle déﬁnitionnel et enjeux de gestion. Management & Avenir,
(1) :78–100.

194

Ikram Chraibi Kaadoud

BIBLIOGRAPHIE

Bouzidi, Khalil Riad, Faron-Zucker, Catherine, Fies, Bruno, Corby, Oli-
vier et Le Thanh, Nhan, 2010. Modélisation de documents réglementaires
dans le domaine du bâtiment. Dans Proc. 12e Conférence Internationale
Francophone sur l’Extraction et la Gestion de Connaissance, EGC 2011.

Broadbent, Donald E, 1977. Levels, hierarchies, and the locus of control.

The Quarterly Journal of Experimental Psychology, 29(2) :181–201.

Carr, Thomas H, McCauley, Charley, Sperber, Richard D et Parmelee,
CM, 1982. Words, pictures, and priming : on semantic activation, conscious
identiﬁcation, and the automaticity of information processing. Journal of
Experimental Psychology : Human Perception and Performance, 8(6) :757.

Casey, Mike, 1998. Correction to proof that recurrent neural networks can
robustly recognize only regular languages. Neural Computation, 10(5) :1067–
1069.

Cernansky, Michal et Benuskova, Lubica, 2003. Simple recurrent net-
work trained by rtrl and extended kalman ﬁlter algorithms. Neural Network
World, 13(3) :223–234.

Chambaron-Ginhac, Stéphanie, 2005. Apprentissage implicite de régulari-
tés : mise en évidence d’une diﬀérence d’apprentissage entre tâches motrices
continues et discrètes. Thèse de doctorat, Dijon.

Charlet, Jean, Declerck, Gunnar, Dhombres, Ferdinand, Gayet, Pierre,
Miroux, Patrick et Vandenbussche, Pierre-Yves, 2012. Construire une
ontologie médicale pour la recherche d’information : problématiques termi-
nologiques et de modélisation. Dans 23es journées francophones d’Ingénierie
des connaissances, pages 33–48.

Charness, Neil et Campbell, Jamie ID, 1988. Acquiring skill at mental
calculation in adulthood : A task decomposition. Journal of Experimental
Psychology : General, 117(2) :115.

Chaudet, Bruno, 2009. Donnée, information, connaissance.

URL
donnee-information-connaissance/

https://brunochaudet.wordpress.com/2009/03/30/

Cherfi, Hacene, Napoli, Amedeo et Toussaint, Yannick, 2003. Vers
une méthodologie de fouille de textes s’ appuyant sur l’extraction de mo-
tifs fréquents et de règles d’association. Dans Conférence d’Apprentissage
(CAp’03), dans le cadre de la plate-forme (AFIA’03), pages 61–76. Presses
universitaires de Grenoble.

Extraction de règles de réseaux récurrents

195

BIBLIOGRAPHIE

Cho, Kyunghyun, Van Merriënboer, Bart, Gulcehre, Caglar, Bahda-
nau, Dzmitry, Bougares, Fethi, Schwenk, Holger et Bengio, Yoshua,
2014. Learning phrase representations using rnn encoder-decoder for statis-
tical machine translation. arXiv preprint arXiv :1406.1078.

Chomsky, Noam, 1956. Three models for the description of language. IRE

Transactions on information theory, 2(3) :113–124.

Chung, Junyoung, Ahn, Sungjin et Bengio, Yoshua, 2016. Hierarchical mul-

tiscale recurrent neural networks. arXiv preprint arXiv :1609.01704.

Chung, Junyoung, Gülçehre, Çaglar, Cho, KyungHyun et Bengio, Yo-
shua, 2014. Empirical evaluation of gated recurrent neural networks on
sequence modeling. CoRR, abs/1412.3555.
URL http://arxiv.org/abs/1412.3555

Cleeremans, A et Jiménez, L, 1996. Implicit cognition with the symbolic
metaphor of mind : Theoretical and methodological issues. Unpublished
manuscript.

Cleeremans, Axel, 1993. Mechanisms of implicit learning : Connectionist

models of sequence processing. MIT press.

Cleeremans, Axel et McClelland, James L, 1991. Learning the struc-
ture of event sequences. Journal of Experimental Psychology : General,
120(3) :235.

Cleeremans, Axel, Servan-Schreiber, David et McClelland, James L,
1989. Finite state automata and simple recurrent networks. Neural compu-
tation, 1(3) :372–381.

Cordier, Amélie, Lefevre, Marie, Champin, Pierre-Antoine, Mille, Alain,
Georgeon, Olivier L et Mathern, Benoît, 2014. Connaissances et raison-
nement sur les traces d’interaction. Revue d’Intelligence Artiﬁcielle, 28(2-
3) :375–396.

Coulet, Jean-Claude, 2014. Des caractéristiques de l’expertise au manage-
ment des compétences individuelles et collectives. Management & Avenir,
(1) :122–135.

Cowan, Robin, David, Paul Allan et Foray, Dominique, 2000. The explicit
economics of knowledge codiﬁcation and tacitness. Industrial and corporate
change, 9(2) :211–253.

Crié, Dominique, 2003. De l’extraction des connaissances au knowledge ma-

nagement. Revue française de gestion, (5) :59–79.

196

Ikram Chraibi Kaadoud

BIBLIOGRAPHIE

Dauce, Emmanuel, 2002. Traité de Sciences Cognitives : Approche dynamique

de la cognition artiﬁcielle. Hermès, Paris, France.
URL http://emmanuel.dauce.free.fr/Hermes/chap3.pdf

De Mulder, Wim, Bethard, Steven et Moens, Marie-Francine, 2015. A
survey on the application of recurrent neural networks to statistical language
modeling. Computer Speech & Language, 30(1) :61–98.

De Saussure, Ferdinand, 1989. Cours de linguistique générale : Édition cri-

tique, tome 1. Otto Harrassowitz Verlag.

Deacon, S Hélène, Conrad, Nicole et Pacton, Sébastien, 2008. A statistical
learning perspective on children’s learning about graphotactic and morpholo-
gical regularities in spelling. Canadian Psychology/Psychologie Canadienne,
49(2) :118.

Deloche, François, 2017. Schéma d’un réseau de neurones récurrents à une
unité reliant l’entrée et la sortie du réseau. a droite la version dépliée de la
structure.
URL https://goo.gl/kc6vQs

Dienes, Zoltan et Berry, Dianne, 1997. Implicit learning : Below the sub-

jective threshold. Psychonomic bulletin & review, 4(1) :3–23.

Dienes, Zoltan, Broadbent, Donald et Berry, Dianne C, 1991. Implicit
and explicit knowledge bases in artiﬁcial grammar learning. Journal of Ex-
perimental Psychology : Learning, Memory, and Cognition, 17(5) :875.

Dienes, Zoltan et Perner, Josef, 1999. A theory of implicit and explicit

knowledge. Behavioral and Brain Sciences, 22(5) :735–808.

Ding, Ying et Foo, Schubert, 2002. Ontology research and development.
part 1-a review of ontology generation. Journal of information science,
28(2) :123–136.

Dubuc, Bruno, 2002. MÉmoire et apprentissage.

URL http://lecerveau.mcgill.ca/flash/i/i\_07/i\_07\_p/i\_07\_p\_tra/
i\_07\_p\_tra.html

Dumais, Susan T, 1981. The development of automatism. Cognitive Skills

and Their Acquisition, Hillsdale, NJ : Erlbaum, pages 111–140.

Durand, Stéphane, 1995. TOM, une architecture connexionniste de traite-
ment de séquences : application à la reconnaissance de la parole. Thèse de
doctorat, Nancy 1.

Extraction de règles de réseaux récurrents

197

BIBLIOGRAPHIE

El Hihi, Salah et Bengio, Yoshua, 1996. Hierarchical recurrent neural net-
works for long-term dependencies. Dans Advances in neural information
processing systems, pages 493–499.

Elman, Jeﬀrey L, 1990. Finding structure in time. Cognitive science,

14(2) :179–211.

Ermine, Jean-Louis, 2001. Les processus de la gestion des connaissances. Dans

EGC, pages 17–29.

Ettlinger, Marc, Margulis, Elizabeth H et Wong, Patrick CM, 2011.

Implicit memory in music and language. Frontiers in psychology, 2.

Evans, Jonathan St BT, 2011. Dual-process theories of reasoning : Contem-
porary issues and developmental applications. Developmental Review,
31(2) :86–102.

Fang, Yu-Ching, Huang, Hsuan-Cheng, Chen, Hsin-Hsi et Juan, Hsueh-Fen,
2008. Tcmgenedit : a database for associated traditional chinese medicine,
gene and disease information using text mining. BMC complementary and
alternative medicine, 8(1) :58.

Fataicha, Youssef, 2005. Recherche d’information dans les images de docu-

ments. Thèse de doctorat, École de technologie supérieure.

Fayyad, Usama, Piatetsky-Shapiro, Gregory et Smyth, Padhraic, 1996.
From data mining to knowledge discovery in databases. AI magazine,
17(3) :37.

Fischer, Flora, GIBON, Marie-Noel, Raffaelli AFFAELLI, Jean-Luc et
Boutonnet, Christophe, 2015. Economie des données personnelles : les
enjeux d’un business éthique.
http://www.cigref.fr/wp/wp-content/uploads/2015/11/
URL
CIGREF-Economie-donnees-perso-Enjeux-business-ethique-2015.pdf

Fitts, Paul M et Posner, Michael I, 1967. Human performance. brooks.

Cole, Belmont, CA, 5 :7–16.

Frensch, Peter A et Stadler, Michael A, 1998. Handbook of implicit lear-

ning. Sage Publications.

Fritzke, Bernd, 1995. A growing neural gas network learns topologies. Dans

Advances in neural information processing systems, pages 625–632.

Frosst, Nicholas et Hinton, Geoﬀrey, 2017. Distilling a neural network into

a soft decision tree. arXiv preprint arXiv :1711.09784.

198

Ikram Chraibi Kaadoud

BIBLIOGRAPHIE

Gasparini, Silvia, 2004. Implicit versus explicit learning : Some implications
for l2 teaching. European Journal of Psychology of Education, 19(2) :203–
219.

Gegick, Michael, Rotella, Pete et Xie, Tao, 2010. Identifying security bug
reports via text mining : An industrial case study. Dans Mining software
repositories (MSR), 2010 7th IEEE working conference on, pages 11–20.
IEEE.

Gelly, Grégory, 2017. Réseaux de neurones récurrents pour le traitement

automatique de la parole. Thèse de doctorat, Université Paris-Saclay.

Gers, Felix, 2001. Long short-term memory in recurrent neural networks.
Unpublished PhD dissertation, Ecole Polytechnique Fédérale de Lausanne,
Lausanne, Switzerland.

Gers, Felix A et Schmidhuber, E, 2001. Lstm recurrent networks learn
simple context-free and context-sensitive languages. IEEE Transactions on
Neural Networks, 12(6) :1333–1340.

Gers, Felix A et Schmidhuber, Jürgen, 2000. Recurrent nets that time
and count. Dans Neural Networks, 2000. IJCNN 2000, Proceedings of the
IEEE-INNS-ENNS International Joint Conference on, tome 3, pages 189–
194. IEEE.

Gers, Felix A, Schmidhuber, Jürgen et Cummins, Fred, 1999. Learning to

forget : Continual prediction with lstm.

Giles, C. L., Miller, C. B., Chen, D., Chen, H. H., Sun, G. Z. et Lee,
Y. C., 1992a. Learning and extracting ﬁnite state automata with second-
order recurrent neural networks. Neural Computation, 4(3) :393–405. doi :
10.1162/neco.1992.4.3.393.
URL https://doi.org/10.1162/neco.1992.4.3.393

Giles, C Lee, Chen, D, Miller, CB, Chen, HH, Sun, GZ et Lee, YC, 1991.
Second-order recurrent neural networks for grammatical inference. Dans
Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference
on, tome 2, pages 273–281. IEEE.

Giles, C Lee, Miller, Cliﬀord B, Chen, Dong, Chen, Hsing-Hen, Sun,
Guo-Zheng et Lee, Yee-Chun, 2008. Learning and extracting ﬁnite state
automata with second-order recurrent neural networks. Learning, 4(3).

Giles, C Lee, Miller, Cliﬀord B, Chen, Dong, Sun, Guo-Zheng, Chen,
Hsing-Hen et Lee, Yee-Chun, 1992b. Extracting and learning an unknown
grammar with recurrent neural networks. Dans Advances in neural infor-
mation processing systems, pages 317–324.

Extraction de règles de réseaux récurrents

199

BIBLIOGRAPHIE

Golab, Lukasz, Karloff, Howard, Korn, Flip, Saha, Avishek et Srivas-
tava, Divesh, 2009. Sequential dependencies. Proceedings of the VLDB
Endowment, 2(1) :574–585.

Gombert, Jean Emile, 2006. Epi/méta vs implicite/explicite : niveau de
contrôle cognitif sur les traitements et apprentissage de la lecture. Langage
& pratiques, 38 :68–76.

Goyal, Shubham, 2017. Review paper on sentiment analysis of twitter data

using text mining and hybrid classiﬁcation approach.

Graves, Alex et Schmidhuber, Jürgen, 2005. Framewise phoneme classiﬁca-
tion with bidirectional lstm and other neural network architectures. Neural
Networks, 18(5) :602–610.

Greff, Klaus, Srivastava, Rupesh K, Koutník, Jan, Steunebrink, Bas R
IEEE

et Schmidhuber, Jürgen, 2017. Lstm : A search space odyssey.
transactions on neural networks and learning systems.

Grundstein, Michel, 1995. La capitalisation des connaissances de l’entre-
prise, système de production des connaissances. Dans Actes du Colloque
L’Entreprise Apprenante et les Sciences de la Complexité, Aix en Provence,
France, pages 22–24.

Haddad, Hatem et Mulhem, Philippe, 2001. Utilisation de la fouille de don-
nées images pour l’indexation automatique des images. Dans INFORSID,
pages 405–418.

Hamming, Richard W, 1950. Error detecting and error correcting codes. Bell

Labs Technical Journal, 29(2) :147–160.

He, Wu, Zha, Shenghua et Li, Ling, 2013. Social media competitive analysis
and text mining : A case study in the pizza industry. International Journal
of Information Management, 33(3) :464–472.

Hees, Jörn, 2015. SciPy hierarchical clustering and dendrogram tutorial.

URL
scipy-hierarchical-clustering-and-dendrogram-tutorial/

https://joernhees.de/blog/2015/08/26/

Hinaut, Xavier, 2013. Réseau de neurones récurrent pour le traitement de
séquences abstraites et de structures grammaticales, avec une application aux
interactions homme-robot. Thèse de doctorat, Université Claude Bernard
Lyon 1.

Hochreiter, Sepp, Bengio, Yoshua, Frasconi, Paolo, Schmidhuber, Jür-
gen et al., 2001. Gradient ﬂow in recurrent nets : the diﬃculty of learning
long-term dependencies.

200

Ikram Chraibi Kaadoud

BIBLIOGRAPHIE

Hochreiter, Sepp et Schmidhuber, Jürgen, 1997. Long short-term me-

mory. Neural computation, 9(8) :1735–1780.

Hoffman, Robert R, 2014. The psychology of expertise : Cognitive research

and empirical AI. Psychology Press.

Hopcroft, JE et al., 1979. Ullman. jd (1979) introduction to automata

theory, languages, and computation.

Hopcroft, John, 1971. An/n log n algorithm for minimizing states in kf in

ite automaton.

Hopfield, J J, 1982. Neural networks and physical systems with emergent
collective computational abilities. Proceedings of the National Academy of
Sciences, 79(8) :2554–2558.
URL http://www.pnas.org/content/79/8/2554.abstract

Jaccard, Paul, 1901. Étude comparative de la distribution ﬂorale dans une
portion des alpes et des jura. Bulletin del la Société Vaudoise des Sciences
Naturelles, 37 :547–579.

Jacobsson, Henrik, 2005. Rule extraction from recurrent neural networks :

Ataxonomy and review. Neural Computation, 17(6) :1223–1263.

Jaeger, Herbert, 2002. Tutorial on training recurrent neural networks, cove-
ring BPPT, RTRL, EKF and the" echo state network" approach, tome 5.
GMD-Forschungszentrum Informationstechnik.

Jin, Xiaolong, Wah, Benjamin W, Cheng, Xueqi et Wang, Yuanzhuo, 2015.
Signiﬁcance and challenges of big data research. Big Data Research, 2(2) :59–
64.

Jordan, MI, 1986. Serial order : a parallel distributed processing approach.
technical report, june 1985-march 1986. Rapport technique, California Univ.,
San Diego, La Jolla (USA). Inst. for Cognitive Science.

Karpathy, Andrej, Johnson, Justin et Fei-Fei, Li, 2015. Visualizing and

understanding recurrent networks. arXiv preprint arXiv :1506.02078.

Kembhavi, Aniruddha, Salvato, Mike, Kolve, Eric, Seo, Minjoon, Ha-
jishirzi, Hannaneh et Farhadi, Ali, 2016. A diagram is worth a dozen
images.
URL http://arxiv.org/abs/1603.07396

Khouzam, Bassem, 2014. Neural networks as cellular computing models for

temporal sequence processing. Thèse de doctorat, Supélec.

Extraction de règles de réseaux récurrents

201

BIBLIOGRAPHIE

Kim, Jong W, Ritter, Frank E et Koubek, Richard J, 2013. An integrated
theory for improved skill acquisition and retention in the three stages of
learning. Theoretical Issues in Ergonomics Science, 14(1) :22–37.

Kohonen, Teuvo, 1982. Self-organized formation of topologically correct fea-

ture maps. Biological cybernetics, 43(1) :59–69.

Korczak, J, Scheiber, C, Hommet, J et Lachiche, N, 2005. Fouille in-

teractive en temps réel de séquences d’images irmf.

Kruger, Norbert, Janssen, Peter, Kalkan, Sinan, Lappe, Markus, Leo-
nardis, Ales, Piater, Justus, Rodriguez-Sanchez, Antonio J et Wis-
kott, Laurenz, 2013. Deep hierarchies in the primate visual cortex : What
can we learn for computer vision ? IEEE transactions on pattern analysis
and machine intelligence, 35(8) :1847–1871.

Lapalme, Jasmin, 2006. Composition automatique de musique à l’aide de

réseaux de neurones récurrents et de la structure métrique.

Lawrence, Steve, Giles, C Lee et Fong, Sandiway, 2000. Natural language
grammatical inference with recurrent neural networks. IEEE Transactions
on Knowledge and Data Engineering, 12(1) :126–140.

Lee, Honglak, Ekanadham, Chaitanya et Ng, Andrew Y, 2008. Sparse deep
belief net model for visual area v2. Dans Advances in neural information
processing systems, pages 873–880.

Lelebina, Olga, 2014. Management of experts in industrial companies : dy-
namics of internal professional communities and career trajectories. Theses,
Ecole Nationale Supérieure des Mines de Paris.
URL https://pastel.archives-ouvertes.fr/tel-01123778

Lété, Bernard, 2006. L’apprentissage implicite des régularités statistiques de
la langue et l’acquisition des unités morphosyntaxiques. Langue française,
(3) :41–58.

Lipton, Zachary C, Berkowitz, John et Elkan, Charles, 2015. A critical
review of recurrent neural networks for sequence learning. arXiv preprint
arXiv :1506.00019.

Logan, Gordon D, 1988. Automaticity, resources, and memory : Theoretical
controversies and practical implications. Human factors, 30(5) :583–598.

Lowe, Richard K., 1993. Constructing a mental representation from an

abstract technical diagram. Learning and Instruction, 3(3) :157–179.
URL
095947529390002H

http://www.sciencedirect.com/science/article/pii/

202

Ikram Chraibi Kaadoud

BIBLIOGRAPHIE

Malhotra, Yogesh, 2000. Knowledge management and new organization
forms : A framework for business model innovation. Knowledge management
and virtual organizations, 2(1) :13–27.

Marsland, Stephen, Shapiro, Jonathan et Nehmzow, Ulrich, 2002. A self-
organising network that grows when required. Neural networks, 15(8) :1041–
1058.

Martinez, Régis, 2011. Dynamique des systèmes cognitifs et des systèmes

complexes. Thèse de doctorat, Université Lumière Lyon 2.

Mathern, Benoît, 2012. Découverte interactive de connaissances à partir
de traces d’activité : Synthèse d’automates pour l’analyse et la modélisation
de l’activité de conduite automobile. Thèse de doctorat, Université Claude
Bernard-Lyon I.

Meulemans, Thierry, 1998. Apprentissage implicite, mémoire implicite et

développement. Psychologie Française, 43(1) :27–37.

Mooney, Raymond J et Bunescu, Razvan, 2005. Mining knowledge from
text using information extraction. ACM SIGKDD explorations newsletter,
7(1) :3–10.

Mostafa, Mohamed M, 2013. More than words : Social networks’ text mi-
ning for consumer brand sentiments. Expert Systems with Applications,
40(10) :4241–4251.

Mozer, Michael C, Kinoshita, Sachiko, Shettel, Michael et al., 2007. Se-
quential dependencies in human behavior oﬀer insights into cognitive control.
Integrated models of cognitive systems, pages 180–193.

Murdoch, W James et Szlam, Arthur, 2017. Automatic rule extraction from

long short term memory networks. arXiv preprint arXiv :1702.02540.

Nadeau, Marie et Fisher, Carole, 2011. Les connaissances implicites et expli-
cites en grammaire : quelle importance pour l’enseignement ? quelles consé-
quences ? Bellaterra journal of teaching and learning language and literature,
4(4) :0001–31.

Nassirtoussi, Arman Khadjeh, Aghabozorgi, Saeed, Wah, Teh Ying et
Ngo, David Chek Ling, 2014. Text mining for market prediction : A syste-
matic review. Expert Systems with Applications, 41(16) :7653–7670.

Nassirtoussi, Arman Khadjeh, Aghabozorgi, Saeed, Wah, Teh Ying et
Ngo, David Chek Ling, 2015. Text mining of news-headlines for forex mar-
ket prediction : A multi-layer dimension reduction algorithm with semantics
and sentiment. Expert Systems with Applications, 42(1) :306–324.

Extraction de règles de réseaux récurrents

203

BIBLIOGRAPHIE

Neto, Joel Larocca, Santos, Alexandre D., Kaestner, Celso A.A.,
Alexandre, Neto, Santos, D., A, Celso A., Alex, Kaestner, Freitas,
Alex A. et Parana, Catolica, 2000. Document clustering and text summa-
rization.

Neves, David M et Anderson, John R, 1981. Knowledge compilation :
Mechanisms for the automatization of cognitive skills. Cognitive skills and
their acquisition, pages 57–84.

Nissen, Mary Jo et Bullemer, Peter, 1987. Attentional requirements of lear-
ning : Evidence from performance measures. Cognitive psychology, 19(1) :1–
32.

Nonaka, Ikujiro et Takeuchi, Hirotaka, 1995. The knowledge-creating com-
pany : How Japanese companies create the dynamics of innovation. Oxford
university press.

Omlin, Christian W et Giles, C Lee, 1996a. Constructing deterministic ﬁnite-
state automata in recurrent neural networks. Journal of the ACM (JACM),
43(6) :937–972.

Omlin, Christian W et Giles, C Lee, 1996b. Extraction of rules from discrete-

time recurrent neural networks. Neural networks, 9(1) :41–52.

Omlin, Christian W et Giles, C Lee, 2000. Symbolic knowledge represen-
tation in recurrent neural networks : Insights from theoretical models of
computation. Knowledge based neurocomputing, pages 63–115.

O’Neill, Catherine, 2016. Weapons of math destruction. How Big Data

Increases Inequality and Threatens Democracy.

O’Reilly, Randall C et Frank, Michael J, 2006. Making working memory
work : a computational model of learning in the prefrontal cortex and basal
ganglia. Neural computation, 18(2) :283–328.

Oudeyer, Pierre-Yves, 2009. L’auto-organisation dans l’évolution de la pa-

role.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B.,
Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V.,
Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot,
M. et Duchesnay, E., 2011. Scikit-learn : Machine learning in Python.
Journal of Machine Learning Research, 12 :2825–2830.

Perruchet, Pierre et Nicolas, Serge, 1998. L’apprentissage implicite : un

débat théorique. Psychologie française, 43 :13–26.

204

Ikram Chraibi Kaadoud

BIBLIOGRAPHIE

Perruchet, Pierre et Pacton, Sebastien, 2004. Qu’apportent à la pédagogie
les travaux de laboratoire sur l’apprentissage implicite ? L’année Psycholo-
gique, 104(1) :121–146.

Perruchet, Pierre et Vinter, Annie, 1998. Learning and development : The

implicit knowledge assumption reconsidered.

Pham, Nguyen-Khang, Morin, Annie, Poulet, François et Gros, Patrick,
2011. Analyse factorielle des correspondances hiérarchique pour la fouille
d’images. Dans EGC, pages 161–172.

Polanyi, Michael, 2009. The tacit dimension. University of Chicago press.

Prahalad, Coimbatore K et Hamel, Gary, 1999. The core competence of

the corporation. Dans Knowledge and strategy, pages 41–59. Elsevier.

Ramos, Sérgio, Figueiredo, Vera, Rodrigues, Fátima, Pinheiro, Raul et
Vale, Zita, 2007. Knowledge extraction from medium voltage load diagrams
to support the deﬁnition of electrical tariﬀs. Engineering Intelligent Systems
for Electrical Engineering and Communications, 15(3) :143–149.

Rasmussen, Jens, 1986. Information processing and human-machine interac-

tion. an approach to cognitive engineering.

Reber, Arthur S, 1967. Implicit learning of artiﬁcial grammars. Journal of

verbal learning and verbal behavior, 6(6) :855–863.

Reber, Arthur S, 1993. Oxford psychology series, no. 19. implicit learning

and tacit knowledge : An essay on the cognitive unconscious.

Reber, Paul J, 2013. The neural basis of implicit learning and memory : a
review of neuropsychological and neuroimaging research. Neuropsychologia,
51(10) :2026–2042.

Remm, Jean-François, 1996. Extraction de connaissances par réseaux neuro-

naux : application au domaine du radar. Thèse de doctorat, Nancy 1.

Roche, Mathieu, 2011. Fouille de Textes : de l’extraction des descripteurs lin-
guistiques à leur induction. Habilitation à diriger des recherches, Université
Montpellier II - Sciences et Techniques du Languedoc.
URL https://tel.archives-ouvertes.fr/tel-00816263

Rosenblatt, Frank, 1958. The perceptron : A probabilistic model for in-
formation storage and organization in the brain. Psychological review,
65(6) :386.

Extraction de règles de réseaux récurrents

205

BIBLIOGRAPHIE

Rosenblum, Nathan, Zhu, Xiaojin et Miller, Barton, 2011. Who wrote
this code ? identifying the authors of program binaries. Computer Security–
ESORICS 2011, pages 172–189.

Rougier, Nicolas P, 2000. Modeles de mémoires pour la navigation autonome.

Thèse de doctorat, Université Henri Poincaré-Nancy I.

Rumelhart, David E, Hinton, Geoﬀrey E et Williams, Ronald J, 1985.
Learning internal representations by error propagation. Rapport technique,
California Univ San Diego La Jolla Inst for Cognitive Science.

Samuelides, M et Cessac, Bruno, 2007. Random recurrent neural networks
dynamics. The European Physical Journal-Special Topics, 142(1) :89–122.

Schacter, Daniel L, 1987.

Implicit memory : History and current sta-
tus. Journal of experimental psychology : learning, memory, and cognition,
13(3) :501.

Serrour, Belkacem, Djouadi, Slimane et Mille, Alain, 2006. Gestion des

connaissances.

Servan-Schreiber, D, Cleermans, A et McClelland, JL, 1988. En-
coding sequential structure in simple recurrent networks. School of Com-
puter Science, Carnegie-Mellon University, Pittsburgh, PA, CMU-CS-88-
183.(November, 1988).

Servan-Schreiber, David, Cleeremans, Axel et McClelland, James L,
1989. Learning sequential structure in simple recurrent networks. Dans
Advances in neural information processing systems, pages 643–652.

Servan-Schreiber, David, Cleeremans, Axel et McClelland, James L,
1991. Graded state machines : The representation of temporal contingencies
in simple recurrent networks. Machine Learning, 7(2) :161–193.

Singhal, Amit, 2001. Modern information retrieval : A brief overview. IEEE

Data Eng. Bull., 24(4) :35–43.

Smith, Andrew, 2003. Grammar inference using recurrent neural networks.
Department of Computer Science, University of San Diego, California, www.
cse. ucsd. edu/˜ atsmith.

Smith, Elizabeth A, 2001. The role of tacit and explicit knowledge in the

workplace. Journal of knowledge Management, 5(4) :311–321.

Sokal, Robert R et Rohlf, F James, 1962. The comparison of dendrograms

by objective methods. Taxon, 11(2) :33–40.

206

Ikram Chraibi Kaadoud

BIBLIOGRAPHIE

Spiekermann, Sarah, Acquisti, Alessandro, Böhme, Rainer et Hui, Kai-
Lung, 2015. The challenges of personal data markets and privacy. Electronic
Markets, 25(2) :161–167.

Squire, Larry R, 1992. Declarative and nondeclarative memory : Multiple
brain systems supporting learning and memory. Journal of cognitive neu-
roscience, 4(3) :232–243.

Squire, Larry R et Zola, Stuart M, 1996. Structure and function of de-
clarative and nondeclarative memory systems. Proceedings of the National
Academy of Sciences, 93(24) :13515–13522.

Strobelt, Hendrik, Gehrmann, Sebastian, Huber, Bernd, Pfister, Hans-
peter et Rush, Alexander M, 2016. Visual analysis of hidden state dynamics
in recurrent neural networks. arXiv preprint arXiv :1606.07461.

Sutton, Richard S, 1988. Learning to predict by the methods of temporal

diﬀerences. Machine learning, 3(1) :9–44.

Tan, Ah-Hwee et al., 1999. Text mining : The state of the art and the chal-
lenges. Dans Proceedings of the PAKDD 1999 Workshop on Knowledge Di-
socovery from Advanced Databases, tome 8, pages 65–70. sn.

Teulier, Régine, Charlet, Jean et Tchounikine, Pierre, 2005. L’ingénierie
des connaissances : acquis et nouvelles perspectives. Dans Ingénierie des
connaissances, pages 11–26. L’Harmattan.

Touzet, Claude, 1992.
connexionnisme. EC2.

les réseaux de neurones artiﬁciels, introduction au

VanLehn, Kurt, 1996. Cognitive skill acquisition. Annual review of psycho-

logy, 47(1) :513–539.

Viola, Jean-Michel, 2005. La gestion des transferts de connaissances entre

générations.

Wang, Qinglong, Zhang, Kaixuan, Ororbia, II, Alexander, G, Xing,
Xinyu, Liu, Xue et Giles, C Lee, 2017. An empirical evaluation of recurrent
neural network rule extraction. arXiv preprint arXiv :1709.10380.

Weiss, Gail, Goldberg, Yoav et Yahav, Eran, 2017. Extracting automata
from recurrent neural networks using queries and counterexamples. arXiv
preprint arXiv :1711.09576.

Williams, Ronald J. et Zipser, David, 1989. Experimental analysis of the
real-time recurrent learning algorithm. Connection Science, 1(1) :87–111.
doi :10.1080/09540098908915631.
URL https://doi.org/10.1080/09540098908915631

Extraction de règles de réseaux récurrents

207

BIBLIOGRAPHIE

Wirtz, Angelika, Kron, Wolfgang, Löw, Petra et Steuer, Markus, 2014.
The need for data : natural disasters and the challenges of database mana-
gement. Natural Hazards, 70(1) :135–157.

Witt, Arnaud, 2010a. Artiﬁcial grammar implicit learning in children with
and without mental retardation : role of the material’s properties and in-
ﬂuence of the instructions. Theses, Université de Bourgogne.
URL https://tel.archives-ouvertes.fr/tel-00651503

Witt, Arnaud, 2010b. L’apprentissage implicite d’une grammaire artiﬁcielle
chez l’enfant avec et sans retard mental : rôle des propriétés du matériel et
inﬂuence des instructions. Thèse de doctorat, Université de Bourgogne.

Witten, Ian H, Frank, Eibe, Hall, Mark A et Pal, Christopher J, 2016.
Data Mining : Practical machine learning tools and techniques. Morgan
Kaufmann.

Yin, Wenpeng, Kann, Katharina, Yu, Mo et Schütze, Hinrich, 2017. Com-
parative study of cnn and rnn for natural language processing. arXiv preprint
arXiv :1702.01923.

Zaoui, Brahim et Mebarki, Bouhafs, 2017. Les nouvelles technologies de
communication et de l’information, quel impact sur le travail et son organi-
sation ?-approche ergonomique.

Zeng, Zheng, Goodman, Rodney M et Smyth, Padhraic, 1993. Learning
ﬁnite state machines with self-clustering recurrent networks. Learning, 5(6).

Zhang, Ji, Hsu, Wynne et Lee, Mong Li, 2001. Image mining : Issues, frame-
works and techniques. Dans Proceedings of the Second International Confe-
rence on Multimedia Data Mining, pages 13–20. Springer-Verlag.

208

Ikram Chraibi Kaadoud

