ReservoirPy: an Eﬀicient and User-Friendly Library to
Design Echo State Networks
Nathan Trouvain, Luca Pedrelli, Thanh Trung Dinh, Xavier Hinaut

To cite this version:

Nathan Trouvain, Luca Pedrelli, Thanh Trung Dinh, Xavier Hinaut. ReservoirPy: an Eﬀicient and
User-Friendly Library to Design Echo State Networks. ICANN 2020 - 29th International Conference
on Artificial Neural Networks, Sep 2020, Bratislava, Slovakia. ￿hal-02595026v2￿

HAL Id: hal-02595026

https://inria.hal.science/hal-02595026v2

Submitted on 25 Aug 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

ReservoirPy : an Eﬃcient and User-Friendly
Library to Design Echo State Networks

Nathan Trouvain1,2,3[0000−0003−2121−7826], Luca
Pedrelli1,2,3[0000−0002−4752−7622], Thanh Trung Dinh1,2,3[0000−0003−0249−2080],
and Xavier Hinaut1,2,3,∗[0000−0002−1924−1184]

1 INRIA Bordeaux Sud-Ouest, France.
2 LaBRI, Bordeaux INP, CNRS, UMR 5800.
3 Institut des Maladies Neurod´eg´en´eratives,
Universit´e de Bordeaux, CNRS, UMR 5293.
\*Corresponding author: xavier.hinaut@inria.fr

Abstract. We present a simple user-friendly library called ReservoirPy
based on Python scientiﬁc modules. It provides a ﬂexible interface to
implement eﬃcient Reservoir Computing (RC) architectures with a par-
ticular focus on Echo State Networks (ESN). Advanced features of Reser-
voirPy allow to improve up to 87.9% of computation time eﬃciency on
a simple laptop compared to basic Python implementation. Overall, we
provide tutorials for hyperparameters tuning, oﬄine and online train-
ing, fast spectral initialization, parallel and sparse matrix computation
on various tasks (MackeyGlass and audio recognition tasks). In particu-
lar, we provide graphical tools to easily explore hyperparameters using
random search with the help of the hyperopt library.

Keywords: Reservoir Computing · Echo State Networks · Oﬄine Learn-
ing · Online Learning · Hyperparameter Optimization · Parallel Comput-
ing · Sparse Matrix Computation · Toolbox.

1

Introduction

Reservoir Computing (RC) [16,12] is a paradigm to train Recurrent Neural Net-
works (RNN), while not as popular as fully trained neural networks typically
used in Deep Learning. It is attractive given the good performance/computation
cost ratio, and it is even at the state-of-the-art for several timeseries tasks [12].
Echo State Networks (ESN) [8] is the most well known instance of Reservoir
Computing paradigm. While programming a basic ESN is relatively easy – re-
quiring a hundred lines of code for the MackeyGlass timeseries prediction task4
– having a complete customizable ESN framework error-prone and including
hyperparameter optimization requires more eﬀort. Therefore, we want to pro-
vide new users or regular ones an easy to handle and ﬂexible library for Echo

4 See for instance the minimal version of Mantas Lukoˇseviˇcius saved at https://
mantas.info/code/simple\_esn or reproduced in examples directory of ReservoirPy:
https://github.com/neuronalX/reservoirpy/tree/master/examples.

2

N. Trouvain et al.

State Networks, and more generally extensible to Random RNN-based methods.
While it is still in active development, it already includes several useful and ad-
vanced features, such as various methods for oﬄine and online learning, parallel
computation, and eﬃcient sparse matrix computation. Importantly, we provide
integrated graphical tools to easily perform what is usually time-consumming for
each new task: explore the inﬂuence of various hyperparameters (e.g. spectral
radius, input scaling, ...) on the performance of a given task. Moreover, we would
like to emphasize the educational aspects of ReservoirPy, simple to manage for
beginners, and for experts it is easy to build more complex architectures like
deep or hierarchical reservoirs [4,13].

A decade ago, some integrated libraries were available, like Oger 5 in Python
language, or aureservoir [7] in C++. Several projects on ESNs can be found
on Github6. However, there is currently no equivalent library to Oger. Exist-
ing Python libraries either use speciﬁc frameworks such as PyTorch, or custom
implementations. In order to have a general, ﬂexible and easily extendable pro-
gramming library for RC, which encourages collaboration and educational pur-
poses, we developed ReservoirPy. Indeed, reservoir computing is an intuitive way
to dive into the processing of timeseries with RNNs; compared to less intuitive
training methods used in Long Short Term Memory (LSTM) for instance.

Moreover, we provide visualisation methods for hyperparameter exploration
that ease this dive into reservoirs for newcommers, and which is insightful for
experts. Several members of our team and students already used it for diﬀer-
ent tasks and purposes (e.g. to build Computational Neuroscience models and
Human-Robot Interaction modules [6,11,14]), it is now time to share it more
extensively.

2 The ReservoirPy library

2.1 Features summary

ReservoirPy can be accessed here: https://github.com/neuronalX/reservoirpy
The library provides several features:

– general features: washout, input bias, readout feedback, regulariza-

tion coeﬃcient, ...;

– custom or general oﬄine or online training methods (e.g. other methods

available in scikit-learn)

– save and load of ESNs in a readable structure;
– parallel computation of reservoir states for independent timeseries (with

jolib library);

– sparse matrix computation (using scipy.sparse);
– fast spectral initialization [5];
– tools for easy hyperparameter exploration (with hyperopt [3]).

5 Oger is no longer maintained; archived at https://github.com/neuronalX/Oger
6 See for instance https://github.com/topics/echo-state-networks

Title Suppressed Due to Excessive Length

3

Several tutorials and demos are provided (see section 5.4), along with a doc-
umentation. Nota Bene: In the following when we say “train the reservoir ” we
mean “train the readout (i.e. output weights) of the reservoir ”; the internal re-
current connections of the reservoir are always kept ﬁxed throughout the paper.

2.2 Precisions on Online learning feature

Alongside with oﬄine learning, ReservoirPy also provides the ability to perform
online (incremental) learning. Given a sequence of inputs, online learning allows
to train the reservoir sequentially on each time step, avoiding storing all data in
memory and making matrix inversion on large matrices. Thus, online learning
proposes a lighter approach to train reservoir with less computational demand
while still achieving compatible level of accuracy. More importantly perhaps,
online incremental learning methods are crucial for computational neuroscience
models [14] and developmental experiments in cognitive science (developmental
psychology, robotics, ...) [6,11]. Current implementation of online learning in
ReservoirPy is based on FORCE learning method [15], and resides in a separate
class: ESNOnline. More details on FORCE can be found in Appendix 8.3.

3 Getting Started with ReservoirPy

In this section, we introduce how to use basic features of ReservoirPy.

3.1 Requirements

Basic ReservoirPy (requirements.txt): numpy, joblib, scipy, tqdm. Advanced fea-
tures to use notebooks and hyperperameters optimization (examples.txt, require-
ments.txt): hyperopt, pandas, matplotlib, seaborn, scikit-learn. Installation in-
structions are given in appendix 8.1.

3.2 Prepare your dataset

1 data = np.loadtxt('MackeyGlass\_t17.txt').reshape(-1, 1)
2 # inputs and teachers for training and testing
3 x\_train, y\_train = data[0:train].T, data[1:train+1]
4 x\_test, y\_test = data[train:train+test], data[train+1:train+test+1]

3.3 Generate random matrices

The mat\_gen module contains functions to create new input, feedback and in-
ternal weights matrices, control spectral radius, modify sparsity and add bias.

1

2

3

4

from reservoirpy import mat\_gen
W = mat\_gen.generate\_internal\_weights(...)
Win = mat\_gen.generate\_input\_weights(...)
# optionnaly, generate a feedback matrix Wfb

4

N. Trouvain et al.

3.4 Oﬄine training

Set a custom oﬄine reservoir ESN can be created using various parameters,
allowing to set the leaking rate leak\_rate, the regularization coeﬃcient value
regularization\_coef, feedback between outputs and the reservoir, an activa-
tion function for feedback, or a reference to a Scikit-Learn linear regression model
(respectively lr, ridge, Wfb, fbfunc and reg\_model arguments).

1 from reservoirpy import ESN
2 esn = ESN(leak\_rate, W, Win, input\_bias, regularization\_coef, ...)
3 # Additional parameters: Wfb, fbfunc, reg\_model, use\_raw\_input

Train and test the reservoir The train method can handle a sequence of
inputs to train a readout matrix Wout, using various linear regression meth-
ods. The run method can then output the readout values from any sequence
of inputs. Internal states generated by the reservoir during both processes are
returned by all functions. wash\_nr\_timesteps argument also allows to consider
only the states generated after a warmup phase for training, ensuring to use only
dynamics generated from the input itself and not the initial zero state.

Inputs should be lists of time series. Each time series will be used to compute
the corresponding internal states. Between each time series, the internal states
of the reservoir are reinitialized, or can be reset to particular values.

1 # training
2 states\_train = esn.train(inputs=[x\_train,], teachers=[y\_train,],
3
4 # testing
5 out\_pred, states\_pred = esn.run(inputs=[test\_in,], reset\_state=False)
6 print("Root Mean Squared error:")
7 print(np.sqrt(np.mean((out\_pred[0] - y\_test)\*\*2)) / test\_len)

wash\_nr\_timesteps=100)

3.5 Online learning

A custom reservoir needs to be instantiated for online learning. Then, the reser-
voir can be trained and tested in the same way as for oﬄine learning.

alpha\_coef is needed to initialize P(0), where P is used in equations (1)
and (2) (see 8.3), more information on alpha\_coef can be found in the FORCE
learning paper. Wout needs to be initialized in the online version, because the
modiﬁcation of the weights starts since the beginning of the training. Wout could
be initialized with null matrix.

1 from reservoirpy import ESNOnline
2 Wout = ... #initializaton of Wout
3 esn = ESNOnline(... alpha\_coef, Wout, ...) # other parameters are the same

Title Suppressed Due to Excessive Length

5

4 A tutorial to explore visually hyperparameters

4.1 Random-search vs. Grid-Search

Setting a reservoir is easy, but training it optimally (or with good enough per-
formance) requires some expertise. Novices and experts’ ﬁrst reaction is to tune
parameters by hand, in order to get “some insights” on the inﬂuence of param-
eters. Many users will try grid-search to ﬁnd which hyperparameters produce
a good performance. Indeed, grid-search can be useful to have a global under-
standing on the inﬂuence of hyperparameters. However, in the following we show
that this can be done with random exploration as well, especially if you get help
from some graphical tools, such as the one we provide in ReservoirPy.

More importantly, as Bergstra et al. show [2], grid-search is suboptimal com-
pared to random search. Indeed, as shown in Figure 1, grid-search undersam-
ples the hyperparameter space compared to random-search. This undersampling
comes from the fact that grid-search repeatedly tests the same values for one
given hyperparameter while changing other hyperparameters. Therefore, grid-
search “looses” time (i.e. useful samples) when changing values of unimportant
hyperparameters while keeping ﬁxed important hyperparameters. Consequently,
random-search obtains better results by sampling more values of important hy-
perparameters.

Fig. 1. Why random search is better than grid search? With random search one is
able to obtain more samples from the important parameters, because with grid search
one undersamples the space by repeatedly sampling the same values for important
parameters. Image from [2].

4.2 Integrated graphical toolbox

In order to make hyperparameters optimisation more eﬃcient and less time-
consuming, we integrate a simple toolbox in ReservoiPy. It relies on the widely
used hyperopt [3] and Matplotlib Python libraries. This toolbox provides users

6

N. Trouvain et al.

with research management and visual tools adapted to the exploration of hy-
perparameters spaces of ESN. We will present these tools through a minimalist
experiment with random search, over a regression task with Mackey-Glass time
series (see also subsection 5.4). For this tutorial we will use the famous Mackey-
Glass task in the RC community: the aim is to perform chaotic timeseries pre-
diction.

4.3 Set the experiment

The ﬁrst step consists of deﬁning an objective function, based on the parameters
we want to explore. This function describes the experiment the user wants to
perform. Within this function, the model is instantiated, trained and tested using
the parameters yielded by the optimization algorithm. The function should then
return a quantitative evaluation of the subsequent model performances when
receiving a combination of parameters and input data.

In this example, the objective function returns to hyperopt the mean-squared
error (MSE) over the testing set, which is the required loss metric for hyperopt
optimisation. In addition to the loss metric, any other metric can be added
in the returned dictionary, by inserting another named item storing its value.
Additional metrics can give signiﬁcant insights on how hyperparameters inﬂu-
ence the ﬁnal results of the model. For the sake of the example we added the
root mean-squared error (RMSE) as an additional metric. The codomain of loss
functions should preferentially be R+ and they should have a reachable local or
global minimum within the range of explored parameters. If these conditions are
not reached, the trials results may be hard to analyse, and show no interesting
properties. In this case, the range of parameters deﬁned should be considered as
sub-optimal. Additional metrics functions codomain should be [0; 1]. Otherwise,
the visualisation tool will normalize the results by default to ensure that the
ﬁgure is readable, which may cause substantial loss of information and add bias
when interpretation the ﬁgure.

We call one hyperparameter (hp) combination (or hp conﬁguration) the set
of hyperparameters passed to the objective function: the result returned will
be represented by one point of data in Figure 2. During a hp search, the hp
combinations should preferably be computed and averaged on several reservoir
instances: i.e. it is preferable that the objective function returns the average loss
obtained from diﬀerent reservoir instances for the same set of hyperparameters
instead of the loss for one single reservoir instance. As the performance varies
from one reservoir instance to another, averaging over 5 or 10 instances is a good
compromise between representativeness of results and overall computation time.
In the example depicted in Figure 2, we set instances\_per\_trial to 10. In
case only one instance is used, the resulting performance of the hp conﬁguration
could not be trusted. In any case, one should not trust blindly to the best hp
combination found by hyperopt, but rather take the 1 or 2% best conﬁgurations
and think of it as a range of values for which hyperparameters are optimal.
Additionally, this procedure provides more robustness to the parameters found.

Title Suppressed Due to Excessive Length

7

def objective(train\_d, test\_d, config, \*, iss, N, sr, leak, ridge):
# the empty starred expression is mandatory in objective function
# definition. It separates hyperopt keyword arguments (right)
# and required arguments (left).

# unpack train and test data, with target values.
x\_train, y\_train = train\_d # proprocessing could be done here
x\_test, y\_test = test\_d

# if parametric

# train and test an 'insts' number of ESN
# This value can be extrated from the config
insts = config["instances\_per\_trial"] # = 10

mses = []; rmses = [];
for i in range(insts):

W = ...; Win = ...; reservoir = ...;
reservoir.train(inputs=[x\_train], teachers=[y\_train])
outputs, \_ = reservoir.run(inputs=[x\_test])
mses.append(mse(outputs[0], y\_test))
rmses.append(sqrt(mse(outputs[0], y\_test)))

# return a dictionary of averaged metrics.
# The 'loss' key is mandatory when using hyperopt.
return {'loss': np.mean(mses)

'rmse': np.mean(rmses)}

4.4 Deﬁne the parameters spaces

The next step is to declare the exploration space of parameters inside a JSON
structured ﬁle, named conﬁguration ﬁle. This convention allows to keep track
of all the parameters used for each exploration, and to uniquely identify every
experiments. It is important when doing random search explorations that all
choices of parameter ranges are detailed and saved to make the experiments
fully reproducible (e.g. deﬁne parameters that are kept constant like the number
of neurons N in our example). The conﬁguration ﬁle has the following structure,
and should always begin with an unique experiment name:

{

"exp": "hyperopt-mackeyglass-1",
"hp\_max\_evals": 1000,
"hp\_method": "random",
"instances\_per\_trial": 10,
"hp\_space": {

"N": ["choice", 300],
"sr": ["loguniform", 1e-6, 10],
"leak": ["loguniform", 1e-3, 1],
"iss": ["choice", 1.0],
"ridge": ["loguniform", 1e-8, 1] }

}

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

1

2

3

4

5

6

7

8

9

10

Not all parameters are tested at the same time. To maximize the chance
to obtain interesting results, we advise to keep some parameters constant. This
will minimize the number of covariant interactions, which are diﬃcult to analyse

8

N. Trouvain et al.

(e.g. spectral radius sr, leak-rate leak and input scaling iss are often interde-
pendent). In this example, only spectral radius, leaking rate and regularization
coeﬃcient (respectively sr, leak and ridge) are set with an active exploration
space. Other ﬁelds are used to conﬁgure the hyperopt module, setting the opti-
mization algorithm – random search in this case – and the number of trials –
one thousand. For example, these parameters could also set the number of initial
random trials of hyperopt (n\_startup\_jobs) when using the TPE (Tree-Parzen
Estimator) Bayesian optimizer (see [3] for more details). All these parameters
are deﬁned accordingly to hyperopt conventions.

4.5 Launch the trials

Then, we call the research function to run the experiment. The function will
call hyperopt algorithm and automatically save the results of each trial as JSON
structured ﬁles in a report directory. Objective function is passed as argument,
with the dataset and the paths to conﬁguration ﬁles and report directory.

1

2

3

best = research(loss\_ESN, dataset,

config="examples/mackeyglass-config.json",
report="examples/report")

4.6 Display the results

After the end of the random search, all results can be retrieved from the report
directory and displayed on a scatter plot, using the plot\_opt\_results function
(ﬁg. 2). This function will load the results and extract the parameters and ad-
ditional metric the user wants to display, speciﬁed with the params and metric
arguments. Other parameters can be used to adjust the ﬁgure rendering, for
instance by removing outliers or switching scales from logarithmic to linear.

1

2

fig = plot\_opt\_results("examples/report/hpt-mg"),
params=["sr", "leak", "ridge"],

metric="rmse")

In this example, we use the MSE as loss metric and the RMSE as additional
metric for display. The default behaviour of the function is to use loss as metric.
Every plot in the ﬁgure show the interaction between each couple of parameters,
weighted by the normalized loss value (gradient of color) and the normalized
additional metric (size of dots). The plots displayed on the diagonal of the ﬁgure
display the relation between the loss function and the parameters, with the top
ﬁve percent of trials, regarding to the additional metric, displayed in shades of
green. The plot given as example display interesting results: the loss function
have a convex proﬁle, and variations in dots density in cross parameters scatter
plots indicate acceptable ranges of parameters, for both spectral radius and
leaking rate. The regularization coeﬃcient does not seem to play an important

Title Suppressed Due to Excessive Length

9

Fig. 2. An example of ﬁgure obtained after 1000 trials over Mackey-Glass time series.
The random search was performed on spectral radius (sr), leaking rate (leak) and
regularization parameter (ridge). MSE and RMSE are displayed as evaluation metrics.
Each trial point represent the averaged evaluation metrics over 10 sub-trials. Each sub-
trial was performed on the same parameters combination within each trial, but with
diﬀerent ESN instances (e.g. diﬀerent random weights initialization).

10

N. Trouvain et al.

role in model performance, and can therefore be ﬁxed to a constant value for
further explorations. Violin plots at the bottom of the ﬁgure can then help with
choosing a range of acceptable parameters, by displaying the distribution of the
top 5 percent of trials parameters regarding to the additional metric.

A more complex example of random search visualization can be found in

Appendix 8.4.

5 Demo experiments

In this section, we provide applications on three tasks to showcase a selection of
features. The ﬁrst task is the well-known Mackey-Glass task: chaotic timeseries
prediction (used in subsections 5.4). For the other tasks, we chose more compu-
tationally expensive tasks (bird and human audio recognition tasks) in order to
better demonstrate the gain in computation time (used in subsections 5.1, 5.2
and 5.3). We used a canary song annotation corpus (we call it Canary dataset
in the following) which contains about 3.5 hours of annotated canary songs (i.e.
1,043,042 MFCC frames), with 41 target classes. The speech recognition cor-
pus TIMIT [1] is composed by 5.4 hours of spoken audio characterized by 5040
multidimensional time series with a total of 1,944,000 time steps.

5.1 Parallel computations

If the ESN is provided with a sequence of independent inputs during training
or running (for example for an independent sequence classiﬁcation task), the
reservoir internal states can be computed in parallel. The parallel computation
can be enabled by setting the workers parameters to a value > 1 in the train
and run methods. The backend parameter also allows to seamlessly control the
module used for parallel computation by the joblib package. To ensure minimal
performance overhead across all hardware environments, we recommend users
to keep the default threading backend.

1 # setting workers at -1 will use all available threads/processes
2 # for computation. Backend can be switched to any value proposed
3 # by joblib ("threading","multiprocessing", "loki"...)
4 states\_train = esn.train(..., workers=-1, backend="threading")

5.2 Sparse matrix computation

In order to address applications characterized by medium/big datasets, the state
computation of the network is implemented considering sparse matrix opera-
tions. Here, we show the improvement in terms of eﬃciency obtained by the
sparse computation of the network’s state on two audio datasets, the Canary
and TIMIT datasets. Table 1 shows the time spent (in seconds) by the network
in the state computation on Canary and TIMIT datasets by using parallelization

Title Suppressed Due to Excessive Length

11

Task
Canary 621 sec. (-)
TIMIT 849 sec. (-)

Dense – Serial Dense – Parallel

Sparse – Serial

Sparse – Parallel

442 sec. (28.82 %) 503 sec. (19.00 %) 380 sec. (38.81 %)
627 sec. (26.15 %) 191 sec. (77.50 %) 103 sec. (87.87 %)

Table 1. Comparison in terms of eﬃciency considering parallelization and sparse recur-
rent matrices for the state computation of the network on Canary and TIMIT datasets
by using 1000 units and 10% of sparsity, with and without parallel computation en-
abled. Performance was measured with an Intel Core i7-8650U, 1.90GHz with 8 cores
using the Canary dataset, and with an Intel Core i5, 2,7 GHz with 2 cores using TIMIT
dataset. The percentage of improvement is indicated by taking the Dense – Serial case
as baseline.

and sparse recurrent matrices with 1000 units and 10% of sparsity. Interestingly,
the sparse computation allows the network to signiﬁcantly improve the eﬃciency.
In particular, it obtains an improvement of 19.00% and 77.50% in terms eﬃciency
w.r.t. the dense computation on Canary and TIMIT tasks, respectively. Overall,
by combining the parallel and the sparse approach, the network obtains a very
good improvement spending of 38.81% and 87.87% in terms of eﬃciency w.r.t.
the baseline case on Canary and TIMIT tasks, respectively.

FSI

Units
1000 0.042 sec.
2000 0.226 sec.
5000 1.754 sec.

Eigen – Sparse Eigen – Dense

0.319 sec.
1.475 sec.
21.238 sec.

1.341 sec.
7.584 sec.
128.419 sec.

Table 2. Comparison in terms of eﬃciency among FSI, eigen-sparse and eigen-dense
by using 1000, 2000 and 5000 recurrent units and 10% of sparsity. Performance was
measured with an Intel Core i5, 2,7 GHz with 2 cores.

5.3 Fast Spectral Initialization

In the RC context, the recurrent weights are typically initialized by perform-
ing the spectral radius through eigenvalues computation. This can be expensive
when the application needs large reservoirs or a wide model selection of hyperpa-
rameters. A very eﬃcient initialization approach to address these cases is called
Fast Spectral Initialization (FSI) [5]. Here, we compare the Python implemen-
tation of the FSI approach integrated in this library with the typical methods
based on eigenvalues computation in sparse (eigen – sparse) and dense (eigen
– dense) cases typically used to initialize recurrent weights. Table 2 shows the
time (in seconds) spent by FSI, eigen-sparse and eigen-dense considering 1000,
2000 and 5000 recurrent units and 10% of sparsity. As expected, FSI obtains an
extremely better eﬃciency w.r.t. the typical initialization approaches which is
progressively enhanced when the number of units increases.

12

N. Trouvain et al.

5.4 Online learning

To demonstrate that online training with FORCE learning method is compet-
itive, we trained a reservoir and evaluated it on the Mackey-Glass task with
FORCE learning (with and without feedback). In addition, results are com-
pared with the oﬄine learning case. Surprisingly, online learning method obtains
slightly better result than oﬄine learning.

Method
Online learning (with feedback)
Online learning (without feedback)
Oﬄine learning

NRMSE (10−3)
3.47 (±0.09)
4.39 (±0.26)
6.06 (±1.67)

Table 3. Comparison of online learning and oﬄine learning on Mackey-Glass task. For
each cell: mean (± standard deviation) averaged on 30 reservoir instances. Hyperpa-
rameters are the same as the best results for the experiment performed in section 4.6
with sr = 0.5, leak = 0.6 and ridge = 0.02. Normalized Root Mean Square Error
(NRMSE).

6 Ongoing and Future Work

In the future, there is several other features we want to include in ReservoirPy:
more use-case examples (e.g. generative mode, hyperparameter search for more
tasks, ...); more online learning methods (e.g. LMS); GPU computations (e.g.
CuPy, JAX, ...); oﬄine batch computation; batch direct approach for ridge
regression7; framework to build layers of reservoirs (e.g. deep reservoirs [4],
hierarchical-task reservoirs [13]); Conceptors [9]; scikit-learn API compatibility.

7 Conclusion

We presented the ReservoirPy: a simple and user-friendly library for training
Echo State Networks, and soon more models of Random Recurrent Neural Net-
works. It provides a balance between a ﬂexible tool, based on pure Python library
using only scientiﬁc libraries, and a computational eﬀective one (parallel imple-
mentation, sparse matrix computations, ...), without the burden of a complex
framework such as TensorFlow or PyTorch.

The library includes several features that enables to computations more ef-
ﬁcient. By using sparse and parallel computations we showed computation time

7 An approach to incrementally compute the normal equations matrices in ridge re-
gression. This allows the learning algorithm to compute the readout weights by
saving memory in the case of large datasets.

Title Suppressed Due to Excessive Length

13

improvement from 38.8% to 87.9% depending on the dataset and the CPU.
Moreover, we provided a tutorial to explore eﬃciently hyperparameters with a
graphical tools.

References

1. Garofolo et al., J.: Timit acoustic-phonetic continuous speech corpus. Linguistic

Data Consortium LDC93S1 (1993)

2. Bergstra, J., Bengio, Y.: Random search for hyper-parameter optimization. Journal

of machine learning research 13(Feb), 281–305 (2012)

3. Bergstra, J., Yamins, D., Cox, D.D.: Hyperopt: A python library for optimizing
the hyperparameters of machine learning algorithms. In: Proceedings of the 12th
Python in Science Conference. pp. 13–20 (2013)

4. Gallicchio, C., Micheli, A., Pedrelli, L.: Deep reservoir computing: a critical exper-

imental analysis. Neurocomputing 268, 87–99 (2017)

5. Gallicchio, C., Micheli, A., Pedrelli, L.: Fast spectral radius initialization for re-

current neural networks. In: INNS BDDL (2020)

6. Hinaut, X., Spranger, M.: Learning to parse grounded language using reservoir
computing. In: 2019 Joint IEEE 9th International Conference on Development
and Learning and Epigenetic Robotics (ICDL-EpiRob) (Aug 2019)

7. Holzmann, G.: Eﬃcient c++ library for analog reservoir computing neural net-

works (echo state networks). http://aureservoir.sourceforge.net (2007-2008)

8. Jaeger, H.: The ”echo state” approach to analysing and training recurrent neu-
ral networks. Tech. Rep. 148, German National Research Center for Information
Technology GMD, Bonn, Germany (2001)

9. Jaeger, H.: Controlling recurrent neural networks by conceptors. arXiv preprint

arXiv:1403.3369 (2014)

10. Jaeger, H., Lukoˇseviˇcius, M., Popovici, D., Siewert, U.: Optimization and applica-
tions of echo state networks with leaky-integrator neurons. Neural Networks 20(3),
335–352 (Apr 2007)

11. Juven, A., Hinaut, X.: Cross-situational learning with reservoircomputing for lan-

guage acquisition modelling. In: IJCNN (2020)

12. Lukoˇseviˇcius, M., Jaeger, H.: Reservoir computing approaches to recurrent neural

network training. Computer Science Review 3(3), 127–149 (2009)

13. Pedrelli, L., Hinaut, X.: Hierarchical-task reservoir for anytime POS tagging from

continuous speech. In: IJCNN (2020)

14. Strock, A., Hinaut, X., Rougier, N.P.: A robust model of gated working memory.

Neural Computation 32(1), 153–181 (Jan 2020)

15. Sussillo, D., Abbott, L.: Generating coherent patterns of activity from chaotic

neural networks. Neuron 63(4), 544–557 (Aug 2009)

16. Verstraeten, D., Schrauwen, B., d’Haene, M., Stroobandt, D.: An experimental
uniﬁcation of reservoir computing methods. Neural Networks 20(3), 391–403 (2007)

8 Appendices

8.1 ReservoirPy installation
ReservoirPy can be installed easily with pip8:

8 If you want to be able to modify the code of reservoirpy, debug it (hopefully it would
not be necessary), or extend it, use the ‘-e‘ option. You will be able to inspect the

14

N. Trouvain et al.

1 git clone https://github.com/neuronalX/reservoirpy
2 # Simple installation
3 pip install reservoirpy
4 # Developper installation
5 pip install -e 

8.2 Echo State Network architecture

Echo State Networks (ESNs) are a class of Recurrent Neural Networks (RNNs)
implemented according to Reservoir Computing (RC) paradigm. Figure 3 shows
an example of ESN architecture. It is composed by a recurrent layer called
reservoir and an output layer called readout. The reservoir is randomly initialized
and left untrained, while, the readout weights are trained through oﬄine learning
or online learning. Please, see [8,10] for more information about ESN model.

Fig. 3. Schematic representation of an ESN. For each time step t, an input vector
u(t) is fed to the model through the input matrix W in. The internal states vector
x(t) results from the inner dynamics of the reservoir and their reaction with the input
data. A matrix W out then compute a readout from the state vector, to produce the
ˆy(t) output vector. Optionally, this vector can then be fed back to the reservoir, as a
feedback vector for the next update of internal states with the vector u(t + 1).

code during execution with any debugger, and changes will be applied without any
need to reinstall the package.

Title Suppressed Due to Excessive Length

15

8.3 Online learning: Details on FORCE learning

With FORCE learning9, the output weights matrix (Wout) is updated for each
time step, so as to keep prediction error as small as possible. The update of Wout
is governed by equation (1), where e−(t) is the diﬀerence between the prediction
output and ground truth at time t (i.e. prediction error), r(t) is the state vector
(of the reservoir) and P (t) is computed via equation (2).

Wout(t) = Wout(t − ∆t) − e−(t)P (t)r(t)

P (t) = P (t − ∆t) −

P (t − ∆t)r(t)rT (t)P (t − ∆t)
1 + rT (t)P (t − ∆t)r(t)

(1)

(2)

8.4 Example of random search visualization on the Canary dataset

The following plot was made using the same tool presented in 4.4. A random
search is performed to ﬁnd optimal ranges of parameters for a classiﬁcation task
over acoustic data representing canary songs. This data is fed to the ESN as two
diﬀerent vectors of features: a ﬁrst vector represents the ﬁrst order derivatives
of an MFCC signal extracted from the acoustic data, and the second vector the
second order derivatives of this MFCC signal. Each of these feature vectors have
their own input scaling parameter, respectively isd and isd2. This visualization
allows to quickly distinguish the best parameters range to use. Importantly, it
gives insights on interactions between the two input scaling parameters used for
the task and the leaking rate.

9 FORCE is a 2nd order learning method similar to RLS (Recursive Least Squares),

contrary to LMS (Least Mean Squares) which is 1nd order.

16

N. Trouvain et al.

Fig. 4. An example of ﬁgure obtained after 1000 trials over the Canary dataset. The
random search was performed on leaking rate (leak) and input scaling coeﬃcients (isd
and isd2), used to adjust two diﬀerent sets of features. Cross-entropy and F1-score are
displayed as evaluation metrics. For this experiment, the number of units was constant
(N = 300), as the spectral radius (sr = 0.4) and the regularization coeﬃcient (ridge
= 1.10−7).

