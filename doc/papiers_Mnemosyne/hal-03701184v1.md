Developmental Modular Reinforcement Learning
Jianyong Xue, Frédéric Alexandre

To cite this version:

Jianyong Xue, Frédéric Alexandre. Developmental Modular Reinforcement Learning. ESANN2022 -
30th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine
Learning, Oct 2022, Bruges / Virtual, Belgium. ￿hal-03701184￿

HAL Id: hal-03701184

https://inria.hal.science/hal-03701184

Submitted on 21 Jun 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Developmental Modular Reinforcement Learning

Jianyong Xue1,2,3 and Fr´ed´eric Alexandre1,2,3
1- Inria Bordeaux Sud-Ouest - Talence - France

2- LaBRI - Universite de Bordeaux, Bordeaux INP, CNRS, UMR 5800 - Talence - France

3- Institut des Maladies Neurod´eg´en´eratives, CNRS, UMR 5293 - Bordeaux - France

Abstract.
In this article, we propose a modular reinforcement learning
(MRL) architecture that coordinates the competition and the cooperation
between modules, and inspire, in a developmental approach, the generation
of new modules in cases where new goals have been detected. We evaluate
the eﬀectiveness of our approach in a multiple-goal torus grid world. Re-
sults show that our approach has better performance than previous MRL
methods in learning separate strategies for sub-goals, and reusing them for
solving task-speciﬁc or unseen multi-goal problems, as well as maintaining
the independence of the learning in each module.

1

Introduction

Traditional reinforcement learning algorithms have been proven successful in
solving complex, single-goal tasks; yet they perform badly in multi-tasks and
non-stationary environments with hidden states. Modular reinforcement learn-
ing (MRL) provides one solution that decomposes a complex problem into a
collection of concurrently running RL modules, each of which learns a separate
policy to solve a portion of the original problem.

MRL has been studied not only for the decomposition of complex tasks into
modules, but also for the reusability of separately learned modules in new strate-
gies for other tasks that have never been solved before [1]. Focusing both on the
optimality of the composite strategy for the entire task and on the independence
of learning in separate modules, [2] introduced a speciﬁc concept of “modular
reward” to propagate local rewards toward the entire task achievement between
modules. Considering the performance degradation in the composability of mod-
ules that have incomparable reward scales, [1] proposed a model, Arbi-Q, that
recruits an additional module of command arbitrator to assign probabilities to
the selection of a module for each state; then the selected module’s preferred ac-
tion becomes the agent’s action in that state. However, one limitation of Arbi-Q
is that the agent’s action always comes from one of the modules.

In this article, we adopt the idea of multiple model-based reinforcement learn-
ing from [3] and present a new MRL architecture that addresses all the concerns
mentioned above by borrowing principles and mechanisms from the mentioned
models. Additionally, a particular component of “coordination module” is intro-
duced, to mediate the competition and the cooperation between diﬀerent mod-
ules, and inspire the learning system to create new modules in circumstances
where current modules are not capable of handling unseen tasks.

(cid:19492)(cid:20174)(cid:11450)(cid:1961)(cid:19300)(cid:3422)(cid:1220)(cid:4024)(cid:1419)(cid:16855)

(cid:12749)(cid:1307)(cid:2560)(cid:7466)(cid:7142)

(cid:16030)(cid:12144)(cid:10476)(cid:5687)(cid:11450)(cid:7078)(cid:18437)

(cid:262)

(cid:262)

2 The developmental MRL architecture

Figure 1 shows the overall organization of the proposed Developmental MRL
(DMRL) architecture, which is composed of a coordination module plus a list
of n model-based RL (MBRL) modules. Each MBRL module shares the same
inputs and has an identical structure that consists of three components: the
reward predictor, the RL controller and the state predictor [3]. The coordination
module will be activated to create new modules whenever the summation of
errors from each state predictor reaches a threshold. Furthermore, learning in
each component is gated by a “responsibility signal” [3], which comes from the
gaussian softmax function of prediction errors and determines how much the
modules are responsible for the current task.

n

1

Reward
predictor

RL 
controller

Coordination 
module

Reward rt

x

x

x

Σ 

action
at

ENV

State
predictor

-

Responsibility 
(cid:76)
signal (cid:176)
(cid:87)

action at

(cid:76) 
Predicted errors (cid:169) 
(cid:87)

next state
st’

Fig. 1: . Schematic diagram of the DMRL structure.

2.1 State predictor and the responsibility signal

The state prediction function Fi(st+1, st, at : θ) gives the probability distribution
of the newly observed state st+1 ∈ S based on the previous state st ∈ S and the
action at ∈ A performed by the agent at step t, where S and A are the set of
state space and action space, i ∈ {1, . . . , n} is the index of modules. Speciﬁcally,
the dynamic function is realized through a three-layers neural network, where
the vector of parameters θ represents the weights of the network. This network is
trained by a collection of pairs of inputs (st, at) and their corresponding output
labels s′
t+1 from trajectories τ = {s0, a0, . . . , sT −2, aT −2, sT −1} of length T [4].
Initial parameters in this neural network are set as a small random value with
uniform distribution between 0.0 and 1.0. With the newly observed state st+1 ,
the state prediction error δi

st of module i is calculated as follows:

δi
st = Fi(j, st, at : θi) − c(j, st+1), j ∈ S

(1)

where c(j, st+1) is 1 if j = st+1 and 0 otherwise. Along with state prediction
errors, the responsibility signal λi
t for each module is formulated as an output of

the gaussian softmax function:

λi
t =

2

st

2σ2 δi
te− 1
ˆλi
te− 1
ˆλi
n
j=1

2σ2 δj

st

2 ,

ˆλi
t =

ρ

λi
t−1
n
j=1 λj

t−1

ρ

(2)

P

where ˆλi
t represents the prior knowledge about module selection, which was
used to maintain the temporal continuity of the selected module [3]. Parameter
σ controls the sensitivity of responsibility signals to state prediction errors, and
ρ (0 < ρ < 1) indicates the eﬀects of past module selections on current decision-
making. The parameter vector θ is updated as follows:

P

θi = θi + αλi

tδi
st

∂Fi(st+1, st, at : θi)
∂θi

(3)

where 0 < α < 1 is the learning rate.

2.2 Reward predictor
The reward predictor Ri(r′, st, at) provides an expected reward r′
t based on the
previous state st and the action at selected at step t. The modular reward
ˆri
t is formed as the sum of immediately received reward plus an extra bonus
for rewarding the module that better ﬁts the current sub-goal. Speciﬁcally, this
bonus is calculated from the product of modular value function and the temporal
diﬀerence in the responsibility signals; then we have the reward prediction error:

rt = ˆri
δi

t − Ri(r′, st, at),

ˆri
t = rt + (λi

t+1 − λi

t)V i(st+1)

therefore, parameters in the reward predictor are updated by:

Ri(r′, st, at) = Ri(r′, st, at) + αλi

tδi
rt

2.3 RL controller

(4)

(5)

The RL controller in module i provides two functions: the state-value function
V i(st) and the state-action-value function Qi(st, at). Combined with the state
transition function Fi(st+1, st, at) and the reward function Ri(r′, st, at), we have
the full Bellman equation [5] of value function:

V i
π(st) =

t(at|st)[Ri(r′
πi

t, st, at) + γ

Fi(st+1, st, at : θi)V i

π(st+1)] (6)

at∈A
X

st+1∈S
X

where γ is the discount factor that controls the eﬀects of future rewards. Further-
more, we recorded the eligibility traces ei
ts for each state s, which are updated
as:

ei
t(s) =

ηγei
ηγei

e−1(s)
if s 6= st
e−1(s) + 1 if s = st

(

(7)

then the state value V i(st) will be updated by:
vt = ˆri
δi

t + γV i(st+1) − V i(st)
V i(st) = V i(st) + αλi
vt is the temporal-diﬀerence (TD) error with modular reward ˆri
t.

t(s),

vtei

tδi

where δi

(8)

2.4 Action selection

For each candidate action aj ∈ A, (j = 1, . . . , M ), their preferences are calculated
by:

Q(st, aj) =

n

i=1
X

t[Ri(r′, st, aj) + γ
λi

Fi(sj, st, aj : θi)V (sj)]

(9)

j∈X
X

where X is the set of possible states that the agent observed after taking action
aj in state st. We use a softmax function as a stochastic version of the greedy
policy, where the action at is selected by

Pi(aj|st) =

eβQ(st,aj )
M
k=1 eβQ(st,ak)

(10)

where β was set as trials/500, which controls the stochasticity of action selection.
P

2.5 The coordination module

The coordination module (CM) is designed to (a) prevent learned modules for
speciﬁc subs-tasks from being modiﬁed, and (b) inspire the learning system to
create new modules in circumstances where current modules are not capable of
handling tasks that they never experienced before. In order to keep modules
independent in learning their own policies, the CM modulates the sensibility of
responsibility signals to smaller predictions errors by adjusting the value of the
parameter σ, as:

n

σt = −

log(δi

st)

(11)

i=1
X

In situations where the system detects a new sub-task that has never been
solved before, or when current modules are not capable of solving them, the
watch list records the sum of prediction errors in κ steps and calculates the
average value set to compare with the threshold. When set reaches the thresh-
old, the coordination module will directly duplicate the module that has the
least prediction error from current list of modules, and then train it through
trajectories observed in experimental data.

3 Experiments

3.1 Settings

We investigate the performance of DMRL in a torus grid world derived from [3].
In this hunter-prey world, the hunter agent chooses one of ﬁve possible actions:

{north (N), east (E), south (S), west (W), stay}, and tries to catch multiple
preys in a 7x7 torus grid world (as shown in Figure 2). Meanwhile, preys move
in either one of four directions: {N E, N W, SE, SW }, which are represented as
four diﬀerent kinds of targets: G = {g1, g2, g3, g4}.

hunter

prey

Reward

Reward

Task1

Task2

Reward

Target1

Target2

Target3

Target4

Task3

Reward

Reward

Task4

Target1

Target2

Target3

Target4

Fig. 2: The pursuit experiment in the torus grid world.

The experiment is divided into two parts: (a) all of 4 targets appear cyclically
in a ﬁxed order, which is G1 = (g1, g2, g3, g4) (as shown in Fig. 2). At the
beginning of each trial, the hunter and the prey are placed at a random position
and the prey’s movement follows the ﬁrst target g1. When the hunter catches
the prey, another prey appears at an other position and moves in the direction
of the next target. A reward r = 10 is given only when the last target g4
is caught, which terminates this trial; a cost for each movement is set as -
0.01. (b) Four diﬀerent tasks are designed in the order of increasing complexity:
T = {T1 = (g1), T2 = (g1, g2), T3 = (g1, g2, g3), T4 = (g1, g2, g3, g4)}.

3

2

1

0

-1

-2

-3

3

2

1

0

-1

-2

-3

3

2

1

0

-1

-2

-3

3

2

1

0

-1

-2

-3

-3

-2

-1

0

1

2

3

-3

-2

-1

0

1

2

3

-3

-2

-1

0

1

2

3

-3

-2

-1

0

1

2

3

Fig. 3: Value functions and prediction models learned by the DMRL.

3.2 Results

In the ﬁrst experiment, we compare the performance of DMRL with other MRL
approaches. Left three ﬁgures in Fig. 4 shows the average number of steps,
the averaged reward, and the total value of states respectively during 2000 trial
epochs in 20 simulation runs. It can be seen that the DMRL works signiﬁcantly
better than Arbi-Q and Inverse Arbi-Q (an adaptation we made, where action
preferences are evaluated by a joint policy, instead of coming from a single mod-
ule); also it converges faster than the MBRL and MBRL with modular reward
(MR). Fig. 3 shows examples of the value functions and the prediction models

learned by the DMRL. It can be seen that modules 1, 2, 3, 4 are specialized for
the prey moving to the NW, NE, SW, and SE, respectively. The landscapes of
the value functions are coherent with preys’ movement directions.

Fig. 4: Performance of DMRL compared with previous MRL approaches.

In the second experiment about increasing complexity reported in the ﬁg-
ure on the far right of Fig. 4, the DMRL gradually learns modules as new
sub-problems appear in the tasks, and amplitude of the spikes representing the
maximum of error decreases as the complexity of the tasks increases, which indi-
cates that the architecture is capable of reusing previously learned modules for
solving new problems.

4 Conclusion and future work

In summary, we present the simple yet eﬀective architecture DMRL to enable
evolving multiple-goal tasks. Speciﬁcally, the responsibility signal was used to
gate the learning of components in each module, and a particular coordination
module is introduced to mediate the competition and the cooperation between
diﬀerent modules, and inspire the learning system to create new modules as
needed. Nevertheless, a number of questions stands out as important targets
for the next stage of research, such as (i) the planning strategy or the proac-
tive way in modular reward; (ii) the top-down and bottom-up approaches for
task decomposition. Inspirations from continual learning approaches will be a
promising direction for learning tremendous task eﬀectively without creating
discrete modules.

References

[1] Christopher Simpkins and Charles Isbell. Composable modular reinforcement learning. In
Proceedings of the AAAI conference on artiﬁcial intelligence, volume 33, pages 4975–4982,
2019.

[2] Kazuyuki Samejima, Kenji Doya, and Mitsuo Kawato. Inter-module credit assignment in

modular reinforcement learning. Neural Networks, 16(7):985–994, 2003.

[3] Kenji Doya, Kazuyuki Samejima, Ken-ichi Katagiri, and Mitsuo Kawato. Multiple model-

based reinforcement learning. Neural computation, 14(6):1347–1369, 2002.

[4] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network
dynamics for model-based deep reinforcement learning with model-free ﬁne-tuning. In 2018
IEEE International Conference on Robotics and Automation (ICRA), pages 7559–7566.
IEEE, 2018.

[5] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT

press, 2018.

