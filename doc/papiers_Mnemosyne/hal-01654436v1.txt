Modeling the Role of Striatum in Stochastic Multi Context Tasks
Sabyasachi Shivkumar, V Srinivasa Chakravarthy, Nicolas P. Rougier

To cite this version:

Sabyasachi Shivkumar, V Srinivasa Chakravarthy, Nicolas P. Rougier.
Modeling the Role of Striatum in Stochastic Multi Context Tasks. 2017.
￿hal-01654436￿

HAL Id: hal-01654436

https://inria.hal.science/hal-01654436

Preprint submitted on 4 Dec 2017

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

Modeling the Role of Striatum in Stochastic Multi Context Tasks

Sabyasachi Shivkumar1, V. Srinivasa Chakravarthy1 and Nicolas P.
Rougier2

1 Computational Neuroscience Lab, Bhupat and Jyoti Mehta School of
Biosciences, Department of Biotechnology, Indian Institute of Technology
Madras, Chennai, India

2 INRIA Bordeaux Sud-Ouest, Institut des Maladies Neurodégénératives,
Université de

Bordeaux, Bordeaux, France

Email: Nicolas.Rougier@inria.fr

Abstract

Decision making tasks in changing environments with probabilistic reward
schemes

present various challenges to the agents performing the task. These
agents must use the

experience gained in the past trials to characterize the environment
which guides their

actions. We present two models to predict an agent’s behavior in these
tasks - a

theoretical model which defines a Bayes optimal solution to the problem
under realistic

task conditions. The second is a computational model of the basal
ganglia which

presents a neural mechanism to solve the same. Both the models are shown
to

reproduce results in behavioral experiments and are compared to each
other. This

comparison allows us to characterize the theoretical model as a bound on
the neural

model and the neural model as a biologically plausible implementation of
the theoretical

model. Furthermore, we predict the performance of the agents in various
stochastic

regimes which could be tested in future studies.

Introduction

Uncertainty is a common problem faced by animals and humans alike in
their day to day

decision making. This uncertainty can be grouped into either expected or
unexpected

uncertainty based on the nature of the variability (Yu and Dayan). For
example, a predator

pouncing on the prey has a general estimate of the environmental
variables like the speed of

the prey, wind speed, air drag etc. This presents a known risk of
failure to catch the prey.

However, there are some factors like wind speed whose distributions
themselves change

based on other factors. In such a case, the speed of a predator when it
is in the direction of the

wind may not be safe in the case the wind is against it. Such parameters
represent the context

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

which the animal must infer to adapt its behaviour. The first case falls
under expected

uncertainty which characterizes the variability in the different
parameters of the

environmental model constructed by the agent (since we use agents to
model animals

performing reward based tasks, we use the terms animal and agent
interchangeably). Another

common example of this type is seen when there is a stochastic reward
while the agent is

performing a reward based learning task. Standard reinforcement learning
models have been

used to tackle problems with expected uncertainty (Kaelbling, Littman et
al. 1996, Sutton and

Barto 1998). The second case of variability in the predator-prey example
falls under

unexpected uncertainty where we observe a consistent difference in the
observations of the

environment as compared to the predictions based on the agent’s internal
model. This could

occur for example when there is a change in the environment
(non-stationary environment).

Specialised reinforcement learning models like modular reinforcement
learning (Doya,

Samejima et al. 2002) identify the context of the environment and are
successful in tackling

such tasks. In this work, we study reward based tasks which involve both
expected and

unexpected uncertainty arising due to change in the context.

Earlier experiments have studied animal behaviour in stochastic tasks
(Schultz 2004). T-

Maze experiments are a common paradigm for studying such tasks where the
animal has to

choose between one of the two arms of the maze and gets a reward upon
traversing the

chosen arm (Brunswik 1939, Graybiel 2005). Another interesting task to
study decision

making with stochastic rewards is the shape selection task where the
animal has to choose

amongst several shapes (each associated with a probability of reward)
displayed on a screen

(Pasquereau, Nadjar et al. 2007). Experiments involving non-stationary
environments often

have a cue indicating change in environment. However, some tasks like
the serial reversal

task have a reward distribution that varies with the environmental
context. In these tasks, the

animal has to figure out a change in context by a trial and error method
(Brunswik 1939).

While stochastic and non-stationary tasks have been well studied
separately, tasks involving

both stochasticity and changing environments are relatively unexplored
and are the focus of

this work.

A lot of results tend to identify Basal Ganglia (BG) as a key player in
reward based learning

tasks and model it as a Reinforcement Learning (RL) engine (Joel, Niv et
al. 2002,

Chakravarthy, Joseph et al. 2010). Furthermore, striatum, which is a
major component of the

BG, has a rich microcircuitry consisting of central structures called
striosomes, and

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

matrisomes surrounding the striosomes (Graybiel, Flaherty et al. 1991).
The striatum is

believed to form representations of state and action space used for
performing RL tasks

(Charpier and Deniau 1997). Specifically, the striosomes and matrisomes
are believed to

map the state space (Wilson, Takahashi et al. 2014), and the action
space (Flaherty and

Graybiel 1994) respectively, based on their differential cortical
projections. In addition, the

striatum has reciprocal projections to both the Ventral Tegmental Area
(VTA) and the

Substantia Nigra pars compacta (SNc). It receives reward prediction
error information from

these midbrain nuclei and uses it to map the developed representations
to state (Granger

2006) and action values (Seo, Lee et al. 2012) which are used for action
      selection. The

striatum has also been hypothesized to perform context dependent tasks
by mapping different

contexts to different striatal modules (Amemori, Gibb et al. 2011,
Shivkumar, Muralidharan

et al. 2017).

In this article, we focus on stochastic and multi-context tasks
(formally defined in Methods)

and develop both theoretical and biologically plausible models to solve
them. After

formalizing the task description, we derive a model performing full
Bayesian inference on the

same. To compare the model performance to the animals performing these
tasks (Brunswik

1939, Lloyd and Leslie 2013), we introduce some realistic task
constraints to develop the

theoretical model which does Bayesian inference in an iterative fashion
(see Methods).

Following this, we present a biologically plausible model of the
striatum which is a variant to

the one in the basal ganglia model developed to solve context dependent
tasks (Shivkumar,

Muralidharan et al. 2017). This model uses a layered Self Organizing Map
(Kohonen 1998)

architecture to model the striosomes and matrisomes as Strio-SOM and
Matri-SOM where a

single Strio-SOM neuron projects to a neighbourhood of the surrounding
Matri-SOM

neurons. The Strio-SOM and the Matri-SOM activity are mapped to compute
state and action

values respectively and used for action selection. This striatal model
is extended to a multi-

module based architecture to deal with multiple context paradigms. The
biological

plausibility imposes on the model limitations such as finite memory
which is also

incorporated into the theoretical model. Thus, the theoretical model
sets a bound on the

expected performance for a probabilistic context dependent task. We show
that the neural

model is very close to this bound for low values of stochasticity in the
reward distribution.

Methods

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

Stochastic Multi Context Task

A stochastic multi context tasks is an extension of the standard task
used in a RL setting. In

this section, we introduce the various task settings and parameters and
define the notation

used in the rest of the paper. In a standard task, the agent is in a
state s and can take action a.

Upon taking an action a, the agent goes to a state s’ and is given a
reward r. The reward r is

obtained from the reward distribution function

:R S A· ֏ R with

r

=

R s a ( ,

)

where S

and A are state and action spaces of dimensions dim(s) and dim(a)
respectively and R is the reward space which is a subset of ℝ (dim(x)
denotes the dimension of the vector x). The

goal of the agent in such tasks is to optimize its decisions with
respect to the obtained reward.

This problem becomes harder when the environment is not stationary and
the reward

distribution changes based on which context the environment is present
in. Mathematically,

this means that the reward distribution function is redefined as

:R S A C

֏ R and

=

r

R s a c ( ,

,

)

where C is the context space of dimension dim(c) and c is the context in
which

the agent is present. The problem is harder in this case since the agent
must identify the

context in which it is present and then choose the action accordingly.
This class of tasks are

termed as multi-context tasks. The problem of identifying a change in
context has been

studied in the change detection theory (Hartland, Baskiotis et
al. 2007). Given infinite

memory, the Page-Hinkley statistics (Hinkley 1970) can be shown to give
the minimum

expected time before detecting a change in context for rewards given
that the rewards come

from the exponential family of distributions (Lorden 1971, Hartland,
Gelly et al. 2006).

The rewards as defined above are not deterministic in general. The
multi-context tasks

defined above are a special case of the general multi-context problems
which have stochastic rewards. Mathematically, R is a probability
distribution over R and r is a sample drawn

from this distribution. While individually having multiple contexts or
stochasticity is

reasonably solvable, together they make the problem highly non-trivial.
This class of

problems belongs to stochastic multi context problems. Such problems can
be viewed as an

extension of contextual bandits (Langford and Zhang 2008) where the
context information is

not given to the agent.

Bayesian Model Formulation

· · bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

We defined the stochastic multi-context problem in the previous section.
In this section, we

present an algorithm to solve the problem. We consider a simpler version
of the problem but

the discussions can be extended to harder tasks. We consider a single
state so that the reward

only depends on the context and the action chosen. We look at a setting
where there are two

possible actions,

1a and

2a and two contexts 1c and 2c . Let

1a be the optimal action in 1c

and

2a in 2c . Also we restrict R to have 2 values-

successR and

failureR

. Since there are two

possible actions and contexts, we define a reward distribution matrix as
follows

R

 =  

r 11 r 21

r 12 r 22

  

where

ijr is the probability of getting a reward

successR while taking action

ja in context

ic .

We get

failureR

with a probability (1- ijr ) while taking action

ja in context

ic . With the help

of this, we define the reward distribution function as

R c a ( i

,

j

)

 =  

R R

success

failure

r with probability ij with probability 1

r ij

Having formulated the problem, we notice that solving the problem can be
reduced to the

estimation of the current context since we know the optimal action in
each context. Assuming

we choose action a and get a reward r, using Bayes Theorem, we have

P c (

=

c a r | , ) 1

=

P a r c ( ,

=

P a r c ( , c c P c ) ( 1 1

|

=

= c P c c ) ( ) 1 1 = + P a r c ( ,

)

P c (

=

c a r | , ) 2

=

P a r c ( ,

=

P a r c ( , c c P c ) ( 1 1

|

=

)

=

c P c c ) ( ) 2 2 = + P a r c ( ,

c P c ) ( 2

=

c 2

)

c P c ) ( 2

=

c 2

)

Eq. 1

Eq. 2

Assuming we do not have any initial knowledge of the current context, we
have

= P c c 1

(

)

=

= P c c 2

(

= ) 0.5

. Also

P c c a r 2

(

= - , ) 1

=

= (

P c c a r , ) 1

. Hence, we only need to

track Eq. 1 which can be reduced to,

-   

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

P c (

=

c a r , ) | 1

=

=

P a r c ( , | + = c ) 1

c ) 1 P a r c ( ,

P a r c ( ,

=

c 2

)

Eq. 3

We can now extend this to multiple trials by keeping track of the
history of action selection

and rewards obtained. At the thi

trial, let the action chosen be

ia and the reward obtained be

ir . We get at the

thn trial

n P c ((

=

1 c ),…,(

c 1

=

c 1

)| (

n a r ,

n

),…(

1 1 a r ,

))

=

n

P a r

((

,

n

),…(

1

1 a r ,

) | (

n

n

(( P a r = n c c 1

, ),…, (

),…( 1 c

1 a r , = c 1

1

) | ( + ))

=

n

c P a r

c 1 n ,

),…, ( n

1 c ),…(

((

=

c )) 1 1 1 a r ) | ( ,

Eq. 4

n

c

=

),…, (

1 c

c 2

=

c 2

))

and correspondingly for context 2 as well. Due to independence of
trials, Eq. 4 can be

simplified as

n P c ((

=

1 c ),…,(

c 1

=

c 1

)| (

n a r ,

n

),…(

1 1 a r ,

))

=

n

= 1

i

i P a r (

,

i

i

c

=

c 1

)

n

= 1

i

i P a r (

,

i

i

c

=

-   

c 1

)

n

= 1

i

i P a r (

,

i

i

c

=

c

2

)

Eq. 5

Instead of keeping the full history since beginning, we can consider a
sliding history for a

particular window length h, making Eq. 5,

n P c ((

=

1 c ),…,(

c 1

=

c 1

)| (

n a r ,

n

),…(

1 1 a r ,

))

=

(cid:213) (cid:213) (cid:213)

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

n

= - + i n h

1

i P a r (

,

i

i

c

=

c 1

)

n

= - + i n h

1

i P a r (

,

i

i

c

=

-   

c 1

)

n

= - + i n h

1

i P a r (

,

i

i

c

=

c 2

)

Eq. 6

These terms can be read from the reward distribution function. However,
the reward

distribution function is not accessible to the agent and this makes this
model unrealistic. We

thus need to estimate these terms which gives rise to the proposed
theoretical model.

Theoretical Model

The Bayesian model developed in the previous section seems to solve the
problem of

estimating the context in which the agent is present. However it uses

i i P a r c ,

(

i

c=

1

)

which

is not available to the agent. Thus, the next best option is to estimate
the context the agent is

in and then choose the actions accordingly. We denote the context
estimated by the agent

using ˆc. Following the same steps as above we get the expression for
the estimated context as

n ˆ P c ((

=

1 ˆ c ),…,(

ˆ c 1

=

ˆ c 1

)| (

n a r ,

n

),…(

1 1 a r ,

))

=

n

= - + i n h

1

i P a r (

,

i

i

ˆ c

=

ˆ c 1

)

n

= - + i n h

1

i P a r (

,

i

i

ˆ c

=

-   

ˆ c 1

)

n

= - + i n h

1

i P a r (

,

i

i

ˆ c

=

ˆ c 2

)

Eq. 7

Now we can get values for the terms in Eq. 7 since the agent knows which
context it

estimated it was in when taking the action. Using the information from
the preceding trials,

we can estimate the probability as,

(cid:213) (cid:213) (cid:213)

(cid:213) (cid:213) (cid:213)

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

i ˆ P a r c ,

(

i

i

=

=

ˆ c 1

)

i

i

N a r , (( ˆ N c (

) |

ˆ c ˆ c 1

=

ˆ c 1

)

)

Eq. 8

where

i N a r (( ,

i

) |

c c= ˆ ˆ 1

)

is the number of times the agent chose

ia when it estimated the

context as 1ˆc and got the reward

ir and

N c c= ˆ ˆ ( 1

)

is the number of times the agent

estimated its context as 1ˆc . This expression was derived so that agent
can estimate the

context it is in by looking at the term

n ˆ P c ((

=

1 ˆ c ),…,(

ˆ c 1

=

ˆ c 1

) | (

n a r ,

n

),…(

1 1 a r ,

))

.

But to calculate this, we need terms that imply that the agent has to
estimate the context and

choose actions accordingly. There is thus an inherent circularity in the
problem. To break this

circularity, we can solve the problem iteratively. We try to estimate
the reward distribution

function at trial number t and denote this as ˆ tR . In addition, we
keep track of another matrix

(cid:3) t N which has the number of times the agent chose a particular
action in a particular estimated

context. The two matrices are as follows

t

(cid:3) R

 =  

ˆ r ˆ r

t 11 t 21

ˆ r ˆ r

t 12 t 22

  

where ˆt

ijr represents the estimated probability of getting a reward

successR when choosing

action

ja in estimated context ˆic at trial t.

t

ˆ N

 =  

t ˆ n 11 t ˆ n 21

t ˆ n 12 t ˆ n 22

  

where ˆt

ijn represents the number of times the agent chose action

ja in estimated context ˆic

at trial t. For ease of notation, we also define

With this Eq. 7 becomes

=

i i ˆ L P a r c ( k

,

i

i

=

ˆ c k

)

and k varies from 1 to 2.

n ˆ P c ((

=

1 ˆ c ),…,(

ˆ c 1

=

ˆ c 1

)| (

n a r ,

n

),…(

1 1 a r ,

))

=

n

n

= - + i n h

1

-   

i L 1

i L 1

n

i L 2

= - + i n h

1

= - + i n h

1

Eq. 9

Since the reward probabilities are equally likely at the beginning of
the trial, we have

(cid:213) (cid:213) (cid:213)

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

(cid:3) 0 R

 =  

0.5 0.5 0.5 0.5

  

(cid:3) 0 N

 =  

0 0 0 0

  

0 ˆ P c (

=

ˆ c 1

)

=

0 ˆ P c (

=

= ) 0.5

ˆ c 2

In trial t, the agent estimates its current context ( ˆic ) based on its
estimate in the previous trial

and chooses the action (

ja ) as given in Eq. 10 and Eq. 11 respectively.

=

i

arg max

t ˆ P c (

1

=

ˆ c k

)

k

{1,2}

Eq. 10

j

{1,2} ˆ r ik

 arg max =  + 1 b  where ε denoted the probability of exploration and
~ b number 0 or 1 drawn with a probability (1-p) and p respectively. The
exploration ensures that

, where Ber(p) denotes a

ε with probability 1- ε with probability

(0.5)

Ber

k

Eq. 11

all the actions are sampled in the initial trials.

Based on the choice of ˆic and

ja , the agent can update the values of ˆ tR and

(cid:3) t N as given in

Eq. 12 and Eq. 13 respectively.

t ˆ n ij

n -= t ˆ ij

1

-   1

(

t ˆ n ij

1

*

=

t ˆ r ij

t ˆ r ij t ˆ n ij

1

-   

t

r

)

Eq. 12

Eq. 13

where

tr denotes the reward obtained at trial t.

Since ˆt

ijr represents the estimated probability of getting a reward

successR when choosing action

ja in estimated context ˆic at trial t, 1- ˆt

ijr represents the estimated probability of getting a

reward

failureR

. Thus

t

iL is given in

t L i



t ˆ r =  - ij ˆ1 r 

t ij

=

t

r

t r

R

success

R

fai

lure

Eq. 14

Substituting values of Eq. 14 in Eq. 9, we can get the estimates of the
context in trial t as given in,

-   ˛

˛

-   
-   

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

t

ˆ ( P c

=

ˆ c 1

)

=

t

t

= - + t h

f

1

-   

f L 1

f L 1

t

f L 2

= - + t h

f

1

= - + f t h

1

Eq. 15

Eq. 10 to Eq. 15 can be used to formulate an algorithm for the agent to
solve a stochastic multi context task as shown in Fig. 1

Fig. 1

Flowchart depicting steps to solve a stochastic multi context task.

Stochastic Reward Based Task Learning in Striatum

We proposed a theoretical model in the last section to solve stochastic
multi context tasks. In

this section we develop a biologically plausible model of the striatum
for these tasks. This

model is derived from an existing model of the basal ganglia proposed to
solve multi-context

problems (Shivkumar, Muralidharan et al. 2017). The center-surround
structures seen in the

striatum (Fig. 2A) are modeled using a layered SOM model. In a layered
SOM model, each

neuron in the center SOM layer projects to a secondary SOM layer.

(cid:213) (cid:213) (cid:213)

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

The center layer in the striatal model is the Strio-SOM, which maps the
state space and is

believed to model the striosomes. The neurons in the Strio-SOM project
to the Matri-SOM

which maps the action space and is believed to model the matrisomes Fig.
2B.

Given m1 x n1 neurons in the Strio-SOM and m2 x n2 neurons in the
Matri-SOM, the weights

of the Strio-SOM(W S) have dimension m1 x n1 x dim(s) where s is the
state vector.

Similarly, for an action vector a the weights of all the Matri-SOMs (W
M) are of dimension

m1 x n1 x m2 x n2 x dim(a) as each neuron in the Strio-SOM projects to a
Matri-SOM.

For a state input s, the activity for a neuron n in the Strio-SOM is
given in Eq. 16.

S

X

[

n

]

=

exp(

||

W

S

s

[

n

]

2

S

2

s

||

2

)

Eq. 16

where [n] represents the spatial location of the neuron n and σS
controls the spread of the

neuron activity. The complete activity of the Strio-SOM (XS) is the
combination of individual

activity of all the neurons. The neuron with the highest activity
(“winner”) for a state s is

* denoted by ns

.

Similarly, for an action input a corresponding to a state s, the
activity for a neuron n in the

Matri-SOM is given in Eq. 17.

M

X

*

[

n

s

]

[

n

]

=

exp(

M

||

W

[ n s

*

s

]

[]

n 2

M

2

a

||

2

)

Eq. 17

where σM controls the spread of the neuron activity. The complete
activity of the Matri-SOM

corresponding to neuron ns

* (

M

X

* ]

[ s n

) is the combination of individual activities of all the

neurons in the Matri-SOM corresponding to ns

*. The neuron with the highest activity

(“winner”) for an action a in a state s is denoted as ns,a

* .

The weight of a neuron n in the Strio-SOM for a state input s is updated
according to Eq. 18

W [

S n

]

W [

S n

]

-   

h

.exp(

S

*

2

]

||

2

n || [ ]

n s 2

[

S

s

).(

s

W [

S n

]

)

Eq. 18

-   
-   
-   
-   

‹

-   
-   

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

The weight of neuron n in the Matri-SOM for an action input a in a state
s is updated

according to Eq. 19.

W [

M * n s

][

n

]

W [

M * n

s

-   n ][]

h

.exp(

M

n || [ ]

*

2

]

||

2

[

n s a , 2

M

s

).(

a

W [

M * n

s

)

][

n

]

Eq. 19

These representations can be used to evaluate the states and actions and
guide the decision

making process. The schematic of our striatal model to solve stochastic
RL tasks is given in

Fig. 2C.

Fig. 2 A) Schematic of the centre-surround mapping of striosomes and
matrisomes in the striatum. The

white centre represents the striosomes and the surround orange
represents the matrisomes. B) Schematic of the layered SOM architecture
where each neuron in the Strio-SOM (Red) projects to the neurons in the
Matri-SOM (Green) C) Schematic diagram of the Striatum model where the
arrows indicate the connections and their types.

Let the agent performing the task be in state s. The striosome activity
gives us the

representation of the state in the striatum. This activity is modeled by
the Strio-SOM as given

in Eq. 16. Thus the activity is of dimension m1 x n1.

-   
-   ‹
-   

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

This activity of the Strio-SOM projects to the SNc and represents the
value for the state s in

our model (Eq. 20). The Striatal-SNc (WStr→SNc) are trained using the
signal from SNc which is representative of Temporal Difference (TD)
error (d ) (Eq. 21). The TD error is calculated

as

d

g= + r

V s

( ’)

V s ( )

where s’ is the new state after taking action a, r is the reward

obtained and g is the discount factor.

V s ( )

= ∑

Str

W ﬁ

SNc

S

X

n [ ]

[

n

]

n

Str

SNc

W

= n ]

[

h

Str

d SNc

S

X

[

n

]

Eq. 20

Eq. 21

where V(s) represents the value for state s, ηStr→SNc is the learning
rate for WStr→SNc.

The actions that can be performed in a state s are represented by the
matrisome activity

surrounding the striosome neuron for that state. This is given by the
activity of the Matri-

SOM corresponding to the neuron with the highest activity in the
Strio-SOM (ns

*) in our

model. The activity of a Matri-SOM neuron for an action a is given in
Eq. 17 and is of

dimension m2 x n2.

The Matri-SOM activity x for action a is projected to the action value
neurons as given in Eq.

22. If na is the action value neuron for the action a,

Q

nX [ a]

corresponds to the action value for

the action in the state s in our model. These connections are also
trained using TD error as

above and the update equation is given in Eq. 23

Q

X

[

n a

]

= ∑

W

n

St

r X (

)

m

Str Q (

)

M

X

*

[

n s

] [

n

]

*

[

n s

] [

n

]

Str X (

)

m

Str Q (

)

W

*

[

n

s

]

[

n

]

=

h

Str X (

)

m

d Str Q (

)

M

X

*

[

n

s

] [

n

]

Eq. 22

Eq. 23

where XQ represents the activity of the action value neurons,

h

St

r

(

X

)

m

Str

(

Q

)

is the learning rate

for

rW St

(

X

)

m

Str

(

Q

)

.

The activity of the action value neurons are used for action selection
by using a softmax

policy in our model (Eq. 24). We believe that this is carried out by the
dynamics of the STN-

GPe oscillations with the striatal action value neurons projecting to
the GPe. This is further

elaborated in the ‘Discussion’ section.

-   

"

ﬁ ﬁ D

ﬁ "

ﬁ ﬁ D

ﬁ ﬁ bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

P a s ( | )

=

exp(

b

Q

X

)

[

n a

]

∑

a

’

A

exp(

b

Q

.

X

)

[

n a

’

]

Eq. 24

where b is the inverse temperature and A denotes the action set for the
agent.

Exploiting the Striatal Modularity for solving context dependent tasks

The modular nature of the striatal anatomy has been proposed to be
responsible for solving

context dependent tasks using a modular RL framework (Shivkumar,
Muralidharan et al. 

2017). In this method, the agent allocates separate modules to separate
contexts. Each of the

modules has its own copy of the environment in a particular context,
represented by an

environment feature signal (ρ). This copy is used to generate a
responsibility signal, denoted

by λ, which indicates how close the current context is to the one
represented by the module.

Thus by identifying the module with the highest responsibility signal we
can follow the

policy developed in that module to solve the problem in an efficient
manner. We can extend

the model described above to incorporate the modular RL framework. The
schematic for the

extended model is given in Fig. 3.

˛

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

Fig. 3 A schematic of the extended model to handle modular RL tasks
showing the case with two striatal modules. The state representations of
the two modules are used to calculate their respective responsibilities
which are then used by the striatal interneurons to choose the
appropriate module.

We believe that context selection happens at the level of the striatum
and the context

modulated activity is projected to the action value neurons. For
clarity, we have expanded the

intra-nuclear activity of the striatum in the model schematic (Fig. 3).
Supposing there are K

modules denoted by M1, M2 …, MK. We now define the weights and
activities in the previous

sections for each module and denote {Mi} with each term associated with
module Mi. Thus,

for a module m, the following variables undergo a change in notation: XS
→ XS,{m} (Eq. 16),

XM → XM,{m} (Eq. 17), W S → W S,{m} (Eq. 18), W M → W M,{m} (Eq. 19),
V(s) → V{m}(s) (Eq.

20), W Str→SNc→ W Str→SNc,{m}

(Eq. 21),

W

Str X (

)

m

Str Q (

)

W

S

tr X (

)

m

Str Q (

m ),{ }

(Eq. 23).

We propose that in addition to the value of the state s, the activity of
the Strio-SOM also

projects to the SNc to represent the environment feature signal (ρ{m}).
The weights of these

projections are denoted as Wρ Str→SNc,{m} and are trained using the
signal from SNc which is representative of context prediction error (d
*). The corresponding equations are given in Eq.

25 and Eq. 26. The context prediction error is calculated as

d

*

= - r

r

{ }( ) m s

ﬁ ﬁ ﬁ

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

m { }

r

s ( )

= ∑

Wr

S

t r

SNc m ,{ } [

n

]

X

S m ,{ } [

n

]

n

St

r

W

r

SNc m

,{ } [

n

]

=

rh

S

tr

d SNc

*

X

S m

,{ } [

n

]

Eq. 25

Eq. 26

The responsibility signal for each module is denoted by λ{m} for module
m. In a given state s,

the module with the highest λ is chosen for deciding the action in that
state. Biologically, we

believe that this selection of the appropriate module for the context is
guided by the striatal

interneurons (Sullivan, Chen et al. 2008). Let the winning module in the
state s be denoted by

m*. The winning module projects to the action value neurons (Eq. 27)
following which the

processing is the same as in the previous section.

Q

X

[

n a

]

= ∑

W

n

Str

(

X

)

m

* Str Q m

),{

(

} [

*

n

s

]

[

n

]

M

,{

* m

X

} [

*

n

s

]

[

n

]

The dynamics of the responsibility signal is given in Eq. 28

-ɺ

l

l a d

(

l

* 2 )

Eq. 27

Eq. 28

where αλ controls the influence of context prediction error on the
responsibility signal and δ*

is the context prediction error.

Results

Performance of theoretical model on T-Maze tasks

The study of context dependent stochastic tasks is a reasonably
underexplored area owing to

the complexity of decision making involved in these tasks. However, some
of the earlier

results (Lloyd and Leslie 2013) make some predictions which we aim to
replicate with our

model.

The task performed by the agent is a T-maze task (Olton 1979) where the
agent has to choose

one of the arms in a maze. Upon choosing the arm, the agent gets a
reward Rmax with a given

probability (Psuccess) and a reward Rmin with a given probability
(Pfailure). The task can be

extended to a context-dependent problem by reversing the reward
distributions with trials.

ﬁ "

ﬁ ﬁ D

ﬁ "

-   

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

We study the performance with changing Rmax/Rmin and Psuccess/Pfailure.
Animals tend to choose

rewards which have a higher magnitude and greater rewards lead to faster
convergence (Fig.

4A). Similarly, with the same magnitude, animals tend to prefer
distributions which reward

with a higher probability (Fig. 4C). These effects are captured by our
model as shown in Fig.

4B and Fig. 4D respectively. The figures show the ratio of the correct
choices by the agent in 50 trials averaged over 50 sessions. The value
of exploration factor, ε (Eq. 11) was set as 0.1 and the window length,
h (Eq. 7) was chosen as 5.

Fig. 4 A) Demonstration of change in performance with varying reward
magnitudes (Figure adapted from (Lloyd

and Leslie 2013)) where the legend indicates the ratio of the magnitude
of rewards in the two arms. B) Performance of our model on the varying
reward magnitude task C) Demonstration of change in performance with
varying reward probabilities (Figure adapted from (Lloyd and Leslie
2013)) where the legend indicate the ratio of the probability of getting
a reward in the two arms. D) Performance of our model on the varying
reward probability task.

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

Fig. 5 A) Percentage of trials where the animal chooses the arm which is
non-profitable for the first 24 trials and becomes profitable following
that. (Figure adapted from (Lloyd and Leslie 2013)). B) Performance of
the model on the task described in A. We see that the model shows
similar trends where the definite reward tasks show faster reversal
learning. C) Percentage of trials where the animal chooses the arm which
was rewarding before 24 trials following which both arms are not
rewarded (Figure adapted from (Lloyd and Leslie 2013)). D) Performance
of the model on the task described in C where the model shows similar
trends. The unlearning for the deterministic reward condition is faster
than the stochastic reward conditions.

Experimental evidence (Brunswik 1939) shows that partial reinforcement
and stochastic

rewards have a significant effect on reversal learning. We consider a
task where the animal is

trained on a T-maze with different reward probabilities for 24 trials
and then the rewarding

probabilities are reversed. We look at the percentage of the trials
where the animal chooses

the arm which is unprofitable at first and becomes profitable after the
reversal. We can

observe that the model results (Fig. 5B) show similar trends to earlier
results (Fig. 5A). The

tasks with deterministic rewards showed quicker reversal as compared to
probabilistic

rewards that showed slower policy modulation by the agent.

Stochastic reward distributions also have an effect on extinction
(Miltenberger 2011) of a

learned policy. To test this, we consider a task where the animal on a
T-maze for 24 trials as

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

above. However, the rewards for both arms are set as 0 following the 24
trials and the rate of

unlearning is studied. We observe that definite rewarding tasks show
faster extinction as

compared to the tasks with stochastic rewards (Fig. 5C) which is
captured by the model (Fig.

5D).

Solving Stochastic Reward Based Tasks using the Striatum Model

In this section, we demonstrate that the proposed model of striatum
model is capable of

solving stochastic reward based tasks. We consider a cue based decision
making task where

the animal has to choose one of the cues displayed on the screen. This
task was first described

in (Pasquereau, Nadjar et al. 2007) and a schematic of the task is given
in Fig. 6A. The

animal is presented with two cues in each trial at two locations (Fig.
6A). Each shape is

associated with a different probability of reward. The agent has to
choose one of the shapes

and gets a reward according to the associated probability.

We show that our striatal model is able to solve this task. We consider
a 4 dimensional state

vector, where each dimension is 1 if the shape is shown and 0 otherwise.
The action vector is

also 4 dimensional with each dimension denoting the action that is
chosen by the agent. The

various parameters of the model are given in Table 2.

Table 2: Parameter values for cue based decision making task

Parameter

Value

Parameter

Value

Strio-SOM Dimension (m1xn1)

3x2

Matri-SOM Dimension (m2xn2)

3x3

σS

ηS

γ

0.01

0.4

σM

ηM

0.95

ηStr→SNc

ηStr(Xm)→Str(Q)

5x10-4

Β

αλ

0.8

ηρ

Str→SNc

0.1

0.4

0.05

50

0.1

The agent (model) is pre-trained where it is given various state and
action inputs. We show

that the representational maps developed have a center-surround
structure (Fig. 6C) when we

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

view the activity corresponding to all the actions for a particular
state. The ratio of correct

choices chosen in 200 trials averaged over 25 sessions in given in Fig.
6B. Thus, we can see

that the agent is able to solve stochastic reward based tasks.
Experimental evidence shows

that that the percentage of times the agent chooses the arm with reward
probability P1, when

the ratio of the reward probabilities is P1/(P1+P2), follows a sigmoid
activity with center at

0.5 which is well captured by the model (Fig. 6D).

Fig. 6

A)  Schematic of the cue based decision making task where the agent has
    to choose between the two shapes shown in the screen and each shape
    has a different probability of reward associated with it. B)
    Percentage of correct responses averaged over 25 sessions for 200
    trials. C) Mapping of the action inputs forms a center-surround
    structure when we view the combined activity of the Matri-SOM for
    all action inputs D) Ratio of choosing response 1 with associated
    probability P1 w.r.t to the sum P1+P2. The model follows a similar
    trend to the experimental plot adapted from (Pasquereau, Nadjar et
    al. 2007)

Comparing the Theoretical and Neural model

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

We have introduced both a theoretical model capable of solving
stochastic multi-context

tasks and a neural network model which provides a biologically plausible
mechanism for the

same task. Since there are no available experiments dealing with these
tasks (to the best of

our knowledge), we shall use the theoretical model to understand the
performances of the

neural model. In that regard, we use a stochastic two arm bandit task
which was the

underlying problem in both the tasks described beforehand. The reward
distributions is

reversed after 500 trials and the performance of the agent is
characterized by averaging

performances over 25 sessions. We also observe the performances for
different values of ε

which represents the probability of reward for the non-profitable arm.

Fig. 7A demonstrates the probability of context 1 estimated by the
theoretical model whereas

Fig. 7B gives the estimation by the neural network model. We observe
that the theoretical

model is able to identify the context even for larger values of ε.
However, the neural network

model is mostly able to identify the context for small values of ε but
fails for larger values. A

similar trend can be seen in Fig. 8A and Fig. 8B where we measure the
percentage of correct

choices by the agent. We observe that the theoretical model is able to
learn faster upon

context reversal for all values of ε but the neural model needs to
relearn for higher values of

ε.

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

Fig. 7 A) Probability of context 1 as estimated by the theoretical
model. The vertical red dotted lines indicate the trials

where the context changes. The solid black line shows the mean estimate
of the probability of context 1 across multiple sessions and the shaded
grey region represents the standard error associated with the estimate.
B) Probability of context 1 as estimated by the neural model. Similar to
A where the red lines indicate context change, black line indicates the
estimate of the probability of context 1 and the grey line the standard
error.

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

Fig. 8 A) Percentage of correct responses by the theoretical model. The
vertical red lines indicate the trials where the context changes and the
black line denotes the ratio of the correct responses averaged across
multiple sessions. B) Percentage of correct responses by the neural
model. The vertical red lines indicate the trials where the context
changes and the black line denotes the ratio of the correct responses
averaged across multiple sessions.

From the experimental results, we can conclude that the neural model is
able to follow the theoretical model only for low values of ε and
behaves like a single context agent for larger values. This can be
further seen in Fig. 9 which shows that the neural model performances
lie between the theoretical optimal and a single context model and could
be the biological mechanism used for solving stochastic multi context
tasks.

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

Fig. 9 Schematic of the extended model to handle modular RL tasks
showing the case with two striatal

modules. The state representations of the two modules are used to
calculate their respective responsibilities which are then used by the
striatal interneurons to choose the appropriate module.

Discussion

We have presented a theoretical model to solve stochastic multi-context
tasks. This is also

accompanied by a biologically plausible computational model of the
striatum which also

attempts to tackle these problems.

Adapting to changing contexts

The problem of identifying a change in context in the environment based
on the rewards

obtained in the previous trials has been extensively studied in the
field of change detection

(Basseville and Nikiforov). Given the past history of reward samples
upon taking a particular

action, Page Hinkley (PH) statistics (Hinkley 1970) is a popular method
for testing the

hypothesis that a change in context has occurred (Hartland, Gelly et
al. 2006, Hartland,

Baskiotis et al. 2007). Under the constraint that the rewards come from
the exponential

family of distributions, PH statistics guarantee minimal expected time
before change

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

detection (Lorden 1971). Our model uses similar ideas of accumulation of
mean of rewards in

the past trial to predict change in contexts but uses limited memory as
a realistic biological

constraint. In addition, the model predicts the probability of context
change in each trial as

opposed to only predicting the occurrence of context change. The model
uses information

about all the actions in the limited history as opposed to traditional
change detection

algorithms which assume that the rewards in the history were generated
from a single action.

Theoretical Model as a Constrained Version of the Full Bayesian Model

The inherent complexity of stochastic context-dependent problems
motivated a Bayesian

approach to solve these problems. While the full Bayesian model
attempted to give the best

possible bound for these tasks, the theoretical model aimed to give a
characterization of the

expected performance under some realistic constraints such as the ones
encountered by an

animal solving these tasks. One of the key constraints is the assumption
of a limited history.

Since the animal has finite memory, it can use information from only a
small and recent

history to guide its decision making (Todd, Niv et al. 2009).
Exploration in action selection is

a facet of RL and is also observed in earlier studies (Doya 2008). This
is also captured as a

constraint in our theoretical model (Eq. 11).

Striatal Microanatomy and Contextual Learning

Our striatal model is derived from a computational model of the basal
ganglia proposed for

handling context dependent tasks (Shivkumar, Muralidharan et al. 2017).
The model is based

on the assumption that the striosomes map the state space and the
matrisomes map the action

space. This is supported from earlier results that the striosomes
receive input from the

orbitofrontal cortex (Eblen and Graybiel 1995) known for coding reward
related states

(Wilson, Takahashi et al. 2014). Anatomical studies also show that
striosome medium spiny

neurons (MSNs) project directly to SNc (Lanciego, Luquin et al. 2012)
which could compute

state values as in our model.

Evidence suggests that similar to how projections from the striosomes
code for state value,

projections from the matrisomes code for action value (Doya 2002).
Experimental results

show the existence of such neurons in the striatum which code
specifically for action value

(Samejima, Ueda et al. 2005). This is well captured in our model as the
Matri-SOM projects

to action value neurons in out striatal model.

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

Action selection is done using the softmax policy (Eq. 24) following the
action value

computation in the striatum. This policy uses a parameter β which
controls the exploration of

the agent. We believe that this could be the role of STN, GPe and GPi
before action selection

is done at the level of the thalamus. This is supported by earlier
results which suggest that

the underlying stochasticity in the soft-max rule could be achieved
indirectly by the chaotic

dynamics of the STN-GPe loop (Kalva, Rengaswamy et al. 2012).

Comparing the Theoretical and the Neural Model

The two models proposed in our work were developed and validated
independently from

each other. However, they share some common features and we can observe
that the

performance of the neural model falls between the performance of the
theoretical model and

the neural model with a single module (Fig. 9).

The theoretical model acts as a lower bound to the performance of the
neural model for the

given stochasticity in the problem. The neural model is also able to
achieve performance on par with the theoretical model for low values of
ε but fails to do so for larger ε where it

becomes similar to a single module system. Thus, we predict that our
neural model can explain behavior in stochastic multi context tasks for
ε <0.3. This also allows us to bound

performance of the animal performing such tasks in highly stochastic
conditions which is

challenging from an experimental perspective owing to the large number
of trials required.

Another feature of our theoretical model is that it is a very simple
model with no assumptions

on the reward or the context distributions. However, despite its
simplistic formulation, the

model is quite powerful and can capture all the previous results
reasonably well. The modular

arrangement of identifying context and using it for task selection is
very similar to the

proposed striatal model. Thus, the striatal model could be a
biologically plausible neural

implementation of the theoretical model.

Acknowledgements

We would like to thank Vignesh Muralidharan for aiding in the
development of the neural model.

References

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

Amemori, K.-i., L. G. Gibb and A. M. Graybiel (2011). “Shifting
responsibly: the importance of striatal modularity to reinforcement
learning in uncertain environments.” Frontiers in human neuroscience 5:
47. Basseville, M. and I. V. Nikiforov Detection of abrupt changes:
theory and application. Brunswik, E. (1939). “Probability as a
determiner of rat behavior.” Journal of Experimental Psychology 25(2):
175. Chakravarthy, V. S., D. Joseph and R. S. Bapi (2010). “What do the
basal ganglia do? A modeling perspective.” Biological cybernetics
103(3): 237-253. Charpier, S. and J. Deniau (1997). “In vivo
activity-dependent plasticity at cortico-striatal connections: evidence
for physiological long-term potentiation.” Proceedings of the National
Academy of Sciences 94(13): 7036-7040. Doya, K. (2002). “Metalearning
and neuromodulation.” Neural Networks 15(4): 495-506. Doya, K. (2008).
“Modulators of decision making.” Nature neuroscience 11(4): 410-416.
Doya, K., K. Samejima, K.-i. Katagiri and M. Kawato (2002). “Multiple
model-based reinforcement learning.” Neural computation 14(6):
1347-1369. Eblen, F. and A. M. Graybiel (1995). “Highly restricted
origin of prefrontal cortical inputs to striosomes in the macaque
monkey.” Journal of neuroscience 15(9): 5999-6013. Flaherty, A. and A.
M. Graybiel (1994). “Input-output organization of the sensorimotor
striatum in the squirrel monkey.” Journal of Neuroscience 14(2):
599-610. Granger, R. (2006). “Engines of the brain: The computational
instruction set of human cognition.” AI Magazine 27(2): 15. Graybiel,
A., A. Flaherty and J.-M. Gimenez-Amaya (1991). Striosomes and
matrisomes. The basal ganglia III, Springer: 3-12. Graybiel, A. M.
(2005). “The basal ganglia: learning new tricks and loving it.” Current
opinion in neurobiology 15(6): 638-644. Hartland, C., N. Baskiotis, S.
Gelly, M. Sebag and O. Teytaud (2007). “Change point detection and
meta-bandits for online learning in dynamic environments.” CAp: 237-250.
Hartland, C., S. Gelly, N. Baskiotis, O. Teytaud and M. Sebag (2006).
“Multi-armed bandit, dynamic environments and meta-bandits.” Hinkley, D.
V. (1970). “Inference about the change-point in a sequence of random
variables.” Biometrika: 1-17. Joel, D., Y. Niv and E. Ruppin (2002).
“Actor–critic models of the basal ganglia: New anatomical and
computational perspectives.” Neural networks 15(4): 535-547. Kaelbling,
L. P., M. L. Littman and A. W. Moore (1996). “Reinforcement learning: A
survey.” Journal of artificial intelligence research 4: 237-285. Kalva,
S. K., M. Rengaswamy, V. S. Chakravarthy and N. Gupte (2012). “On the
neural substrates for exploratory dynamics in basal ganglia: a model.”
Neural Networks 32: 65-73. Kohonen, T. (1998). “The self-organizing
map.” Neurocomputing 21(1): 1-6. Lanciego, J. L., N. Luquin and J. A.
Obeso (2012). “Functional neuroanatomy of the basal ganglia.” Cold
Spring Harbor perspectives in medicine 2(12): a009621. Langford, J. and
T. Zhang (2008). The epoch-greedy algorithm for multi-armed bandits with
side information. Advances in neural information processing systems.
Lloyd, K. and D. S. Leslie (2013). “Context-dependent decision-making: a
simple Bayesian model.” Journal of The Royal Society Interface 10(82):
20130069. Lorden, G. (1971). “Procedures for reacting to a change in
distribution.” The Annals of Mathematical Statistics: 1897-1908.
Miltenberger, R. G. (2011). Behavior modification: Principles and
procedures, Cengage Learning. Olton, D. S. (1979). “Mazes, maps, and
memory.” American psychologist 34(7): 583.

bioRxiv preprint first posted online Sep. 30, 2017;

doi:

http://dx.doi.org/10.1101/196543

.

The copyright holder for this preprint (which was

not peer-reviewed) is the author/funder. It is made available under a

CC-BY-NC-ND 4.0 International license .

Pasquereau, B., A. Nadjar, D. Arkadir, E. Bezard, M. Goillandeau, B.
Bioulac, C. E. Gross and T. Boraud (2007). “Shaping of motor responses
by incentive values through the basal ganglia.” Journal of Neuroscience
27(5): 1176-1183. Samejima, K., Y. Ueda, K. Doya and M. Kimura (2005).
“Representation of action-specific reward values in the striatum.”
Science 310(5752): 1337-1340. Schultz, W. (2004). “Neural coding of
basic reward terms of animal learning theory, game theory,
microeconomics and behavioural ecology.” Current opinion in neurobiology
14(2): 139-147. Seo, M., E. Lee and B. B. Averbeck (2012). “Action
selection and action value in frontal-striatal circuits.” Neuron 74(5):
947-960. Shivkumar, S., V. Muralidharan and V. S. Chakravarthy (2017).
“A Biologically Plausible Architecture of the Striatum to Solve
Context-Dependent Reinforcement Learning Tasks.” Frontiers in neural
circuits 11. Sullivan, M. A., H. Chen and H. Morikawa (2008). “Recurrent
inhibitory network among striatal cholinergic interneurons.” Journal of
neuroscience 28(35): 8682-8690. Sutton, R. S. and A. G. Barto (1998).
Reinforcement learning: An introduction, MIT press Cambridge. Todd, M.
T., Y. Niv and J. D. Cohen (2009). Learning to use working memory in
partially observable environments through dopaminergic reinforcement.
Advances in neural information processing systems. Wilson, R. C., Y. K.
Takahashi, G. Schoenbaum and Y. Niv (2014). “Orbitofrontal cortex as a
cognitive map of task space.” Neuron 81(2): 267-279. Yu, A. and P. Dayan
“Expected and unexpected uncertainty: ACh and NE in the neocortex.”


