Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding
(Survey) Subba Reddy Oota, Manish Gupta, Raju S. Bapi, Gael Jobard,
Frédéric

Alexandre, Xavier Hinaut

To cite this version:

Subba Reddy Oota, Manish Gupta, Raju S. Bapi, Gael Jobard, Frédéric
Alexandre, et al.. Deep Neural Networks and Brain Alignment: Brain
Encoding and Decoding (Survey). 2023. ￿hal-04162064￿

HAL Id: hal-04162064

https://hal.science/hal-04162064

Preprint submitted on 14 Jul 2023

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding
(Survey)

Subba Reddy Oota1,2 , Manish Gupta3,4 , Raju S. Bapi3 , Gael Jobard2
Frederic Alexandre1,2 , Xavier Hinaut1,2 1INRIA, Bordeaux, France,
2University of Bordeaux, France, 3IIIT Hyderabad, India, 4Microsoft,
Hyderabad, India subba-reddy.oota@inria.fr, gmanish@microsoft.com,
raju.bapi@iiit.ac.in, gael.jobard@u-bordeaux.fr,
frederic.alexandre@inria.fr, xavier.hinaut@inria.fr

Abstract How does the brain represent different modes of information?
Can we design a system that au- tomatically understands what the user is
think- ing? Such questions can be answered by study- ing brain
recordings like functional magnetic res- onance imaging (fMRI). As a
first step, the neu- roscience community has contributed several large
cognitive neuroscience datasets related to passive
reading/listening/viewing of concept words, narra- tives, pictures and
movies. Encoding and decod- ing models using these datasets have also
been pro- posed in the past two decades. These models serve as
additional tools for basic research in cognitive science and
neuroscience. Encoding models aim at generating fMRI brain
representations given a stimulus automatically. They have several
practi- cal applications in evaluating and diagnosing neu- rological
conditions and thus also help design ther- apies for brain damage.
Decoding models solve the inverse problem of reconstructing the stim-
uli given the fMRI. They are useful for designing brain-machine or
brain-computer interfaces. In- spired by the effectiveness of deep
learning mod- els for natural language processing, computer vi- sion,
and speech, recently several neural encoding and decoding models have
been proposed. In this survey, we will first discuss popular
representations of language, vision and speech stimuli, and present a
summary of neuroscience datasets. Further, we will review popular deep
learning based encoding and decoding architectures and note their
benefits and limitations. Finally, we will conclude with a brief summary
and discussion about future trends. Given the large amount of recently
published work in the ‘computational cognitive neuroscience’ com-
munity, we believe that this survey nicely organizes the plethora of
work and presents it as a coherent story.

1 Introduction Neuroscience is the field of science that studies the
structure It and function of the nervous system of different species.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

43

42

56

55

52

47

48

50

57

54

53

51

49

involves answering interesting questions like the following1. (1) How
learning occurs during adolescence, and how it dif- fers from the way
adults learn and form memories. (2) Which 44 specific cells in the brain
(and what connections they form 45 with other cells), have a role in how
memories are formed? 46 (3) How animals cancel out irrelevant
information arriving from the senses and focus only on information that
matters. (4) How do humans make decisions? (5) How humans de- velop
speech and learn languages. Neuroscientists study di- verse topics that
help us understand how the brain and ner- vous system work. Motivation:
The central aim of neuroscience is to unravel how the brain represents
information and processes it to carry out various tasks (visual,
linguistic, auditory, etc.). Deep neu- ral networks (DNN) offer a
computational medium to cap- ture the unprecedented complexity and
richness of brain ac- tivity. Encoding and decoding stated as
computational prob- lems succinctly encapsulate this puzzle. As the
previous sur- veys systematically explore the brain encoding and decod-
ing studies with respect to only language [Cao et al., 2021;
Karamolegkou et al., 2023], this survey summarizes the latest efforts in
how DNNs begin to solve these problems and thereby illuminate the
computations that the unreachable brain accomplishes effortlessly. Brain
encoding and decoding: Two main tasks studied in cognitive neuroscience
are brain encoding and brain decod- ing, as shown in Figure 1. Encoding
is the process of learn- ing the mapping e from the stimuli S to the
neural activation F . The mapping can be learned using features
engineering or deep learning. On the other hand, decoding constitutes
learn- ing mapping d, which predicts stimuli S back from the brain
activation F . However, in most cases, brain decoding aims at predicting
a stimulus representation R rather than actually reconstructing S. In
both cases, the first step is to learn a se- mantic representation R of
the stimuli S at the train time. Next, for encoding, a regression
function e : R → F is trained. For decoding, a function d : F → R is
trained. These functions e and d can then be used at test time to pro-
cess new stimuli and brain activations, respectively. Techniques for
recording brain activations: Popular tech- niques for recording brain
activations include single Micro-

60

62

67

68

61

66

69

65

58

59

63

64

70

71

77

74

79

81

76

82

78

72

80

75

73

1https://zuckermaninstitute.columbia.edu/file/5184/download?

token=qzId8vyR

Figure 1: Computational Cognitive Neuroscience of Brain Encoding and
Decoding: Datasets & Stimulus Representations

83

84

85

86

87

88

89

90

91

92

93

94

95

96

97

98

99

100

101

102

103

104

105

106

107

108

109

110

111

112

113

Electrode (ME), Micro-Electrode array (MEA), Electro- Cortico Graphy
(ECoG), Positron emission tomography (PET), functional MRI (fMRI),
Magneto-encephalography (MEG), Electro-encephalography (EEG) and
Near-Infrared Spectroscopy (NIRS). These techniques differ in their
spatial resolution of neural recording and temporal resolution.

fMRIs enable high spatial but low time resolution. Hence, they are good
for examining which parts of the brain handle critical functions. fMRI
takes 1-4 seconds to complete a scan. This is far lower than the speed
at which humans can process language. On the other hand, both MEG and
EEG have high time but low spatial resolution. They can preserve rich
syn- tactic information [Hale et al., 2018] but cannot be used for
source analysis. fNIRS are a compromise option. Their time resolution is
better than fMRI, and spatial resolution is bet- ter than EEG. However,
this spatial and temporal resolution balance may not compensate for the
loss in both. Stimulus Representations: Neuroscience datasets contain
stimuli across various modalities: text, visual, audio, video and other
multimodal forms. Representations differ based on modality. Older
methods for text-based stimulus representa- tion include text corpus
co-occurrence counts, topic models, syntactic, and discourse features.
In recent times, both se- mantic and experiential attribute models have
been explored for text-based stimuli. Semantic representation models in-
clude distributed word embeddings, sentence representation models,
recurrent neural networks (RNNs), and Transformer- based language
models. Experiential attribute models rep- resent words in terms of
human ratings of their degree of association with different attributes
of experience, typically on a scale of 0-6 or binary. Older methods for
visual stim-

115

114

119

ulus representation used visual field filter bank and Gabor wavelet
pyramid for visual stimuli, but recent methods use models like
ImageNet-pretrained convolutional neural net- 116 works (CNNs) and
concept recognition methods. For audio 117 stimuli, phoneme rate and the
presence of phonemes have 118 been leveraged, besides deep learning
models like Sound- Net. Finally, for multimodal stimulus
representations, re- 120 searchers have used both early fusion and late
fusion deep 121 learning methods. In the early fusion methods,
information 122 across modalities is combined in the early steps of
process- 123 ing. While in late fusion, the combination is performed
only 124 at the end. We discuss stimulus representation methods in 125
detail in Sec. 2. 126 Naturalistic Neuroscience Datasets: Several
neuroscience datasets have been proposed across modalities (see Figure
2). These datasets differ in terms of the following criteria: (1) Method
for recording activations: fMRI, EEG, MEG, etc. (2) Repetition time
(TR), i.e. the sampling rate. (3) Character- istics of fixation points:
location, color, shape. (4) Form of stimuli presentation: text, video,
audio, images, or other mul- timodality. (5) Task that participant
performs during record- 134 ing sessions: question answering, property
generation, rating 135 quality, etc. (6) Time given to participants for
the task, e.g., 136 1 minute to list properties. (7) Demography of
participants: males/females, sighted/blind, etc. (8) Number of times the
re- sponse to stimuli was recorded. (9) Natural language associ- ated
with the stimuli. We discuss details of proposed datasets in Sec. 3.
Brain Encoding: Other than using the standard stimuli repre- 142
sentation architectures, brain encoding literature has focused 143 on
studying a few important aspects: (1) Which models lead 144

127

128

129

140

141

137

132

133

131

139

138

130

Stimulus Repr.EncodingDecodingNeuroscience DatasetsStimulus
RepresentationsFigure 2: Representative Samples of Naturalistic Brain
Dataset: (LEFT) Brain activity recorded when subjects are reading and
listening to the same narrative (Deniz et al. 2019), and (RIGHT) example
naturalistic image stimuli from various public repositories: BOLD5000
(Chang et al. 2019), SSfMRI (Beliy et al., 2019), and VIM-1 (Kay et al.,
2008).

145

146

147

148

149

150

151

152

153

154

155

156

157

158

159

160

161

162

163

164

165

(2) How to better predictive accuracy across modalities? can we
    disentangle the contributions of syntax and seman- tics from
    language model representations to the alignment between brain
    recordings and language models? (3) Why do some representations lead
    to better brain predictions? How are deep learning models and brains
    aligned in terms of their information processing pipelines? (4) Does
    joint encoding of task and stimulus representations help? We discuss
    these details of encoding methods in Sec. 5. Brain Decoding: Ridge
    regression is the most popular brain decoder. Recently, a fully
    connected layer [Beliy et al., 2019] or multi-layered perceptrons
    (MLPs) [Sun et al., 2019] have also been used. While older methods
    attempted to decode to a vector representation using stimuli of a
    single mode, newer methods focus on multimodal stimuli decoding
    [Pereira et al., 2016; Oota et al., 2022c]. Decoding using
    Transform- ers [Gauthier and Levy, 2019; Toneva and Wehbe, 2019;
    D´efossez et al., 2022; Tang et al., 2022], and decoding to ac- tual
    stimuli (word, passage, image, dialogues) have also been explored.
    We discuss details of these decoding methods in Sec. 6.

Figure 3: Alignment between deep learning systems and human brains
[Toneva et al. 2019].

Computational Cognitive Science (CCS) Research goals: CCS researchers
have primarily focused on two main ar- 167 eas [Doerig et al., 2022]
(also, see Figure 3). (1) Improving 168 predictive Accuracy. In this
area, the work is around the fol- 169 lowing questions. (a) Compare
feature sets: Which feature set provides the most faithful reflection of
the neural repre- sentational space? (b) Test feature decodability:
“Does neu-

171

172

170

166

218

219

220

221

222

227

223

degree of association with different attributes of experience, typically
on a scale of 0-6 [Anderson et al., 2019; Ander- son et al., 2020;
Berezutskaya et al., 2020; Just et al., 2010; Anderson et al., 2017b] or
binary [Handjaras et al., 2016; Wang et al., 2017]. Visual Stimulus
Representations: For visual stimuli, older methods used visual field
filter bank [Thirion et al., 2006; 224 Nishimoto et al., 2011] and Gabor
wavelet pyramid [Kay 225 et al., 2008; Naselaris et al., 2009]. Recent
methods use 226 models like CNNs [Du et al., 2020; Beliy et al., 2019;
Anderson et al., 2017a; Yamins et al., 2014; Nishida et al., 2020] and
concept recognition models [Anderson et al., 2020]. Audio Stimuli
Representations: For audio stimuli, phoneme rate and presence of
phonemes have been leveraged [Huth et 232 al., 2016]. Recently, authors
in [Nishida et al., 2020] used 233 features from an audio deep learning
model called SoundNet 234 for audio stimuli representation. Multimodal
Stimulus Representations: To jointly model the information from
multimodal stimuli, recently, various multimodal representations have
been used. These include processing videos using audio+image
representations like VGG+SoundNet [Nishida et al., 2020] or using
image+text 240 combination models like GloVe+VGG and ELMo+VGG 241 in
[Wang et al., 2020]. Recently, the usage of multimodal 242 text+vision
models like CLIP, LXMERT, and VisualBERT 243 was proposed in [Oota et
al., 2022d]. 244

238

239

228

231

230

229

237

236

235

3 Naturalistic Neuroscience Datasets

245

256

255

254

We discuss the popular text, visual, audio, video and other 246
multimodal neuroscience datasets that have been proposed 247 in the
literature. Table 1 shows a detailed overview of brain 248 recording
type, language, stimulus, number of subjects (|S|) 249 and the task
across datasets of different modalities. Figure 2 250 shows examples
from a few datasets. 251 Text Datasets: These datasets are created by
presenting 252 words, sentences, passages or chapters as stimuli. Some
of 253 the text datasets include Harry Potter Story [Wehbe et al.,
2014], ZUCO EEG [Hollenstein et al., 2018] and datasets proposed in
[Handjaras et al., 2016; Anderson et al., 2017a; Anderson et al., 2019;
Wehbe et al., 2014]. In [Handjaras et 257 al., 2016], participants were
asked to verbally enumerate in 258 one minute the properties (features)
that describe the entities 259 the words refer to. There were four
groups of participants: 5 260 sighted individuals were presented with a
pictorial form of the 261 nouns, 5 sighted individuals with a
verbal-visual (i.e., written 262 Italian words) form, 5 sighted
individuals with a verbal au- 263 ditory (i.e., spoken Italian words)
form, and 5 congenitally 264 blind with a verbal auditory form. Data
proposed by [An- 265 derson et al., 2017a] contains 70 Italian words
taken from 266 seven taxonomic categories (abstract, attribute,
communica- 267 tion, event/action, person/social role, location,
object/tool) in 268 the law and music domain. The word list contains
concrete 269 as well as abstract words. ZUCO dataset [Hollenstein et
al., 2018] contains sentences for which fMRIs were obtained for 3 tasks:
normal reading of movie reviews, normal reading of Wikipedia sentences
and task-specific reading of Wikipedia 273 sentences. For this dataset
curation, sentences were presented 274

272

270

271

Figure 4: Language Model

173

174

175

176

177

178

179

180

181

182

183

184

185

186

187

188

189

190

191

192

193

194

195

196

197

198

199

200

201

202

203

204

205

206

207

208

209

210

211

212

213

214

215

216

217

ral data Y contain information about features X?” (c) Build accurate
models of brain data: The aim is to enable simula- tion of neuroscience
experiments. (2) Interpretability. In this area, the work is around the
following questions. (a) Examine individual features: Which features
contribute most to neural activity? (b) Test correspondences between
representational spaces: “CNNs vs ventral visual stream” or “Two text
rep- resentations”. (c) Interpret feature sets: Do features X, gen-
erated by a known process, accurately describe the space of neural
responses Y? Do voxels respond to a single feature or exhibit mixed
selectivity? (d) How does the mapping relate to other models or theories
of brain function? We discuss some of these questions in Sections 5 and
6.

Older methods

2 Stimulus Representations In this section, we discuss types of stimulus
representations that have been proposed in the literature across
different modalities: text, visual, audio, video and other multimodal
stimuli. Text Stimulus Representations: for text-based stimuli
representation include text corpus co- occurrence counts [Mitchell et
al., 2008; Pereira et al., 2013; Huth et al., 2016], topic models
[Pereira et al., 2013], syn- tactic features and discourse features
[Wehbe et al., 2014]. In recent times, for text-based stimuli, both
semantic mod- els as well as experiential attribute models have been ex-
plored. Semantic representation models include word em- bedding methods
[Pereira et al., 2018; Wang et al., 2020; Pereira et al., 2016; Toneva
and Wehbe, 2019; Anderson et al., 2017a; Oota et al., 2018], sentence
representation models (see Figure 4) [Sun et al., 2020; Sun et al.,
2019; Toneva and Wehbe, 2019], RNNs [Jain and Huth, 2018; Oota et al.,
2019] and Transformer methods [Gauthier and Levy, 2019; Toneva and
Wehbe, 2019; Schwartz et al., 2019; Schrimpf et al., 2021a; Antonello et
al., 2021; Oota et al., 2022b; Aw and Toneva, 2022]. Popular word em-
bedding methods include textual (i.e., Word2Vec, fastText, and GloVe),
linguistic (i.e., dependency), conceptual (i.e., RWSGwn and ConceptNet),
contextual (i.e., ELMo). Pop- ular sentence embedding models include
average, max, con- cat of avg and max, SIF, fairseq, skip, GenSen,
InferSent, ELMo, BERT, RoBERTa, USE, QuickThoughts and GPT- 2.
Transformer-based methods include pretrained BERT with various NLU
tasks, finetuned BERT, Transformer-XL, GPT- 2, BART, BigBird, LED, and
LongT5. Experiential attribute models represent words in terms of human
ratings of their

Dataset Harry Potter

Authors [Wehbe et al., 2014]

t x e T

ZuCo 240 Sentences with Con- tent Words BCCWJ-EEG Subset Moth Radio Hour

Vim-1 Generic Object Decoder

l a u s i V

BOLD5000 Algonauts

NSD THINGS

The Moth Radio Hour

Narratives Natural Stories The Little Prince MEG-MASC BBC’s Doctor Who

[Handjaras et al., 2016] [Anderson et al., 2017a] [Hollenstein et al.,
2018] [Anderson et al., 2019]

[Oseki and Asahara, 2020] [Deniz et al., 2019] [Thirion et al., 2006]

[Kay et al., 2008] [Horikawa and Kamitani, 2017] [Chang et al., 2019]
[Cichy et al., 2019]

[Allen et al., 2022] [Hebart et al., 2022]

[Handjaras et al., 2016] [Huth et al., 2016] [Brennan and Hale, 2019]

[Anderson et al., 2020] [Nastase et al., 2021] [Zhang et al., 2020] [Li
et al., 2021] [Gwilliams et al., 2022] [Seeliger et al., 2019]

Table 1: Naturalistic Neuroscience Datasets

Stimulus

Lang. English Reading Chapter 9 of Harry Potter and the Sorcerer’s Stone

Type fMRI/ MEG fMRI fMRI EEG fMRI English Reading 240 active voice
sentences describing everyday situations

Italian Verbal, pictorial or auditory presentation of 40 concrete nouns,
four times Italian English Reading 1107 sentences with 21,629 words from
movie reviews

Reading 70 concrete and abstract nouns from law/music, five times

fMRI fMRI

-   
-   

Japanese Reading 20 newspaper articles for ∼30-40 minutes

EEG fMRI English Reading 11 stories fMRI

-   

Viewing rotating wedges (8 times), expanding/contracting rings (8
times), rotating 36 Gabor filters (4 times), grid (36 times) Viewing
sequences of 1870 natural photos Viewing 1,200 images from 150 object
categories; 50 images from 50 object categories; imagery 10 times
Viewing 5254 images depicting real-world scenes Viewing 92 silhouette
object images and 118 images of objects on natural background Viewing
73000 natural scenes Viewing 31188 natural images

-   
-   
-   
-   

fMRI fMRI/ MEG fMRI fMRI/ MEG fMRI fMRI English Listening eleven
10-minute stories EEG

English Listening Chapter one of Alice’s Adventures in Wonderland (2,129

words in 84 sentences) as read by Kristen McQuillan

fMRI English Listening one of 20 scenario names, 5 times fMRI English
Listening 27 diverse naturalistic spoken stories. 891 functional scans
fMRI English Listening Moth-Radio-Hour naturalistic spoken stories. fMRI
English Listening audiobook for about 100 minutes. MEG English Listening
two hours of naturalistic stories. 208 MEG sensors fMRI English Viewing
spatiotemporal visual and auditory videos (30 episodes). 120.8
whole-brain volumes (∼23 h) of single-presentation data, and 1.2 vol-
umes (11 min) of repeated narrative short episodes. 22 repetitions

Italian Verbal, pictorial or auditory presentation of 40 concrete nouns,
4 times

|S| Task 9

Story understanding

Imagine a situation with noun

20 Property Generation 7 12 Rate movie quality 14 Passive reading

40 Passive reading 9 9

Passive reading and Listening Passive viewing

2 5

Passive viewing Repetition detection

4 Passive viewing 15 Passive viewing

8 8

Passive viewing Passive viewing

20 Property Generation 7 Passive Listening 33 Question answering

Imagine personal experiencs

26 345 Passive Listening 19 Passive Listening 112 Passive Listening 27
Passive Listening 1

Passive viewing

Japanese Ads

[Nishida et al., 2020]

fMRI

Japanese Viewing 368 web and 2452 TV Japanese ad movies (15-30s). 7200
train

52 Passive viewing

Pippi Langkous

[Berezutskaya et al., 2020] ECoG Swedish/

Dutch

and 1200 test fMRIs for web; fMRIs from 420 ads. Viewing 30 s excerpts
of a feature film (in total, 6.5 min long), edited together for a
coherent story

37 Passive viewing

Algonauts Natual Short Clips Natual Short Clips 60 Concrete Nouns

[Cichy et al., 2021] [Huth et al., 2022] [Lahner et al., 2023] [Mitchell
et al., 2008] [Sudre et al., 2012]

fMRI English Viewing 1000 short video clips (3 sec each) fMRI English
Watching natural short movie clips fMRI English Watching 1102 natural
short video clips fMRI English Viewing 60 different word-picture pairs
from 12 categories, 6 times each MEG English Reading 60 concrete nouns
along with line drawings. 20 questions per

10 Passive viewing Passive viewing 5 10 Passive viewing 9 Passive
viewing 9 Question answering

Pereira

[Pereira et al., 2018]

fMRI English Viewing 180 Words with Picture, Sentences, word clouds;
reading 96

16 Passive viewing and reading

[Zinszer et al., 2018]

fNIRS English 8 concrete nouns (audiovisual word and picture stimuli):
bunny, bear,

24 Passive viewing and listening

kitty, dog, mouth, foot, hand, and nose; 12 times repeated.

Neuromod

[Cao et al., 2021] [Boyle et al., 2020]

fNIRS Chinese Viewing and listening 50 concrete nouns from 10 semantic
categories. fMRI English Watching TV series (Friends, Movie10)

7 6

Passive viewing and listening Passive viewing and listening

text passages; 72 passages. 3 times repeated.

noun lead to 1200 examples.

o i d u A

o e d i V

l a d o m

i t l u M

r e h t O

275

276

277

278

279

280

281

282

283

284

285

286

287

288

289

290

291

292

293

to the subjects in a naturalistic reading scenario. A complete sentence
is presented on the screen. Subjects read each sen- tence at their own
speed, i.e., the reader determines for how long each word is fixated and
which word to fixate next.

Visual Datasets: Older visual datasets were based on binary visual
patterns [Thirion et al., 2006]. Recent datasets con- tain natural
images. Examples include Vim-1 [Kay et al., 2008], BOLD5000 [Chang et
al., 2019], Algonauts [Cichy et al., 2019], NSD [Allen et al., 2022],
Things-data[Hebart et al., 2022], and the dataset proposed in [Horikawa
and Kami- tani, 2017]. BOLD5000 includes ∼20 hours of MRI scans per each
of the four participants. 4,916 unique images were used as stimuli from
3 image sources. Algonauts contains two sets of training data, each
consisting of an image set and brain activity in RDM format (for fMRI
and MEG). Training set 1 has 92 silhouette object images, and training
set 2 has 118 object images with natural backgrounds. Testing data con-
sists of 78 images of objects on natural backgrounds. Most of the visual
datasets involve passive viewing, but the dataset

in [Horikawa and Kamitani, 2017] involved the participant doing the
one-back repetition detection task.

294

295

298

296

297

Audio Datasets: Most of the proposed audio datasets are in English [Huth
et al., 2016; Brennan and Hale, 2019; Anderson et al., 2020; Nastase et
al., 2021], while there is one [Handjaras et al., 2016] on Italian. The
participants were involved in a variety of tasks while their brain
activations were measured: Property generation [Handjaras et al., 2016],
passive listening [Huth et al., 2016; Nastase et al., 2021], 302
question answering [Brennan and Hale, 2019] and imagining 303 themselves
personally experiencing common scenarios [An- 304 derson et al., 2020].
In the last one, participants underwent fMRI as they reimagined the
scenarios (e.g., resting, reading, writing, bathing, etc.) when prompted
by standardized cues. Narratives [Nastase et al., 2021] used 17
different stories as stimuli. Across subjects, it is 6.4 days worth of
recordings.

309

306

308

305

307

301

300

299

Video Datasets: Recently, video neuroscience datasets have also been
proposed. These include BBC’s Doctor Who [Seel- iger et al., 2019],
Japanese Ads [Nishida et al., 2020], Pippi

310

311

312

Figure 5: Evaluation Metrics for Brain Encoding and Decoding. (LEFT)
Pearson Correlation, (MIDDLE) 2V2 Accuracy [Toneva et al. 2020], and
(RIGHT) Pairwise Accuracy.

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

Langkous [Anderson et al., 2020] and Algonauts [Cichy et al., 2021].
Japanese Ads data contains data for two sets of movies were provided by
NTT DATA Corp: web and TV ads. There are also four types of cognitive
labels associated with the movie datasets: scene descriptions,
impression ratings, ad effectiveness indices, and ad preference votes.
Algonauts 2021 contains fMRIs from 10 human subjects that watched over
1,000 short (3 sec) video clips. Other Multimodal Datasets: Finally,
beyond the video datasets, datasets have also been proposed with other
kinds of multimodality. These datasets are audiovisual ([Zinszer et al.,
2018; Cao et al., 2021]), words associated with line drawings [Mitchell
et al., 2008; Sudre et al., 2012], pictures along with sentences and
word clouds [Pereira et al., 2018]. These datasets have been collected
using a variety of meth- ods like fMRIs [Mitchell et al., 2008; Pereira
et al., 2018], MEG [Sudre et al., 2012] and fNIRS [Zinszer et al., 2018;
Cao et al., 2021]. Specifically, in [Sudre et al., 2012], sub- jects
were asked to perform a QA task, while their brain ac- tivity was
recorded using MEG. Subjects were first presented with a question (e.g.,
“Is it manmade?”), followed by 60 con- crete nouns, along with their
line drawings, in a random or- der. For all other datasets, subjects
performed passive view- ing and/or listening.

4 Evaluation Metrics Two metrics are popularly used to evaluate brain
encoding models: 2V2 accuracy [Toneva et al., 2020; Oota et al., 2022b]
and Pearson Correlation [Jain and Huth, 2018], as shown in Figure 5.

They are defined as follows. Given a subject and a brain region, let N
be the number of samples. Let {Yi}N i=1 and { ˆYi}N i=1 denote the
actual and predicted voxel value vectors for the ith sample. Thus, Y ∈
RN ×V and ˆY ∈ RN ×V where V is the number of voxels in that region. 2V2
Accu- (cid:80)N −1 j=i+1 I[{cosD(Yi, ˆYi) + racy is computed as i=1
cosD(Yj, ˆYj)} < {cosD(Yi, ˆYj) + cosD(Yj, ˆYi)}] where cosD is the
cosine distance function. I[c] is an indicator func- tion such that I[c]
= 1 if c is true, else it is 0. The higher the 2V2 accuracy, the better.
Pearson Correlation is com- i=1 corr[Yi, ˆYi] where corr is the correla-
puted as PC= 1 N

1 NC2

(cid:80)N

(cid:80)n

tion function.

353

354

355

356

358

357

359

360

361

Brain decoding methods are evaluated using popular met- rics like
pairwise and rank accuracy [Pereira et al., 2018; Oota et al., 2022c].
Other metrics used for brain decod- ing evaluation include R2 score,
mean squared error, and us- ing Representational Similarity Matrix
[Cichy et al., 2019; Cichy et al., 2021]. Pairwise Accuracy To measure
the pairwise accuracy, the first step is to predict all the test
stimulus vector representa- tions using a trained decoder model. Let S =
[S0, S1,· · · ,Sn], ˆS = [ ˆS0, ˆS1,· · · , ˆSn] denote the “true”
(stimuli-derived) and 363 predicted stimulus representations for n test
instances resp. 364 Given a pair (i, j) such that 0 ≤ i, j ≤ n, score is
1 365 if corr(Si, ˆSi) + corr(Sj, ˆSj) > corr(Si, ˆSj) + corr(Sj, ˆSi),
else 0. Here, corr denotes the Pearson correlation. Fi- nal pairwise
matching accuracy per participant is the aver- age of scores across all
pairs of test instances. For com- puting rank accuracy, we first compare
each decoded vector 370 to all the “true” stimuli-derived semantic
vectors and ranked 371 them by their correlation. The classification
performance re- 372 flects the rank r of the stimuli-derived vector for
the correct word/picture/stimuli: 1 − value for each participant is the
average rank accuracy across all instances.

#instances−1 . The final accuracy 374

r−1

375

369

373

367

366

376

368

362

377

5 Brain Encoding Encoding is the learning of the mapping from the
stimulus 378 domain to the neural activation. The quest in brain
encoding 379 is for “reverse engineering” the algorithms that the brain
uses 380 for sensation, perception, and higher-level cognition. Recent
breakthroughs in applied NLP enable reverse engineering the language
function of the brain. Similarly, pioneering results have been obtained
for reverse engineering the function of ventral visual stream in object
recognition founded on the ad- vances and remarkable success of deep
CNNs. The overall schema of building a brain encoder is shown in Figure
6.

383

386

381

387

385

382

384

Initial studies on brain encoding focused on smaller data sets and
single modality of brain responses. Early mod- els used word
representations [Hollenstein et al., 2019]. 390 Rich contextual
representations derived from RNNs such 391 as LSTMs resulted in superior
encoding models [Jain and 392 Huth, 2018; Oota et al., 2019] of
narratives. The recent 393

389

388

Figure 6: Schema for Brain Encoding

394

395

396

397

398

399

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

efforts are aimed at utilizing the internal representations extracted
from transformer-based language models such as ELMo, BERT, GPT-2, etc
for learning encoding models of brain activation [Jat et al., 2020;
Caucheteux et al., 2021; Antonello et al., 2021]. High-grain details
such as lexical, compositional, syntactic, and semantic representations
of nar- ratives are factorized from transformer-based models and uti-
lized for training encoding models. The resulting models are better able
to disentangle the corresponding brain responses in fMRI [Caucheteux et
al., 2021]. Finally, is has been found that the models that integrate
task and stimulus representa- tions have significantly higher prediction
performance than models that do not account for the task semantics
[Toneva et al., 2020; Schrimpf et al., 2021a].

Similarly, in vision, early models focused on indepen- dent models of
visual processing (object classification) us- ing CNNs [Yamins et al.,
2014]. Recent efforts in visual en- coding models focus on using richer
visual representations derived from a variety of computer vision tasks
[Wang et al., 2019]. Instead of feed-forward deep CNN models, us- ing
shallow recurrence enabled better capture of temporal dy- namics in the
visual encoding models [Kubilius et al., 2019; Schrimpf et al., 2020].

Table 2 summarizes various encoding models proposed in the literature
related to textual, audio, visual, and multimodal stimuli. Figure 7
classifies the encoding literature along var- ious stimulus domains such
as vision, auditory, multimodal, and language and the corresponding
tasks in each domain. Linguistic Encoding: A number of previous works
have in- vestigated the alignment between pretrained language mod- els
and brain recordings of people comprehending language. Huth et
al. [2016] have been able to identify brain ROIs (Re- gions of Interest)
that respond to words that have a similar meaning and have thus built a
“semantic atlas” of how the human brain organizes language. Many studies
have shown accurate results in mapping the brain activity using neural
[Ander- distributed word embeddings for linguistic stimuli son et al.,
2017a; Pereira et al., 2018; Oota et al., 2018; Nishida and Nishimoto,
2018; Sun et al., 2019]. Unlike ear-

433

434

435

436

lier models where each word is represented as an indepen- dent vector in
an embedding space, [Jain and Huth, 2018] built encoding models using
rich contextual representations derived from an LSTM language model in a
story listen- ing task. With these contextual representations, they
demon- strated dissociation in brain activation – auditory cortex (AC)
and Broca’s area in shorter context whereas left Temporo- Parietal
junction (TPJ) in longer context. [Hollenstein et al., 2019] presents
the first multimodal framework for evaluat- ing six types of word
embedding (Word2Vec, WordNet2Vec, GloVe, FastText, ELMo, and BERT) on 15
datasets, includ- ing eye-tracking, EEG and fMRI signals recorded during
lan- guage processing. With the recent advances in contextual rep- 445
resentations in NLP, few studies incorporated them in relating 446
sentence embeddings with brain activity patterns [Sun et al., 447 2020;
Gauthier and Levy, 2019; Jat et al., 2020].

443

438

439

442

441

437

440

448

444

449

More recently, researchers have begun to study the align- ment of
language regions of the brain with the layers of lan- 450 guage models
and found that the best alignment was achieved 451 in the middle layers
of these models [Jain and Huth, 2018; 452 Toneva and Wehbe, 2019].
Schrimpf et al. [2021a] examined 453 the relationship between 43 diverse
state-of-the-art language 454 models. They also studied the behavioral
signatures of human 455 language processing in the form of self-paced
reading times, 456 and a range of linguistic functions assessed via
standard engi- 457 neering tasks from NLP. They found that
Transformer-based 458 models perform better than RNNs or word-level
embedding 459 models. Larger-capacity models perform better than smaller
460 models. Models initialized with random weights (prior to 461
training) perform surprisingly similarly in neural predictiv- 462 ity as
compared to final trained models, suggesting that net- work architecture
contributes as much or more than expe- rience dependent learning to a
model’s match to the brain. 465 Antonello et al. [2021] proposed a
“language representation 466 embedding space” and demonstrated the
effectiveness of the 467 features from this embedding in predicting fMRI
responses to linguistic stimuli. Disentangling the Syntax and Semantics:
The represen- 470 tations of transformer models like BERT, GPT-2 have
been 471

463

468

464

469

Table 2: Summary of Representative Brain Encoding Studies

Stimuli Authors

[Jain and Huth, 2018] [Toneva and Wehbe, 2019] [Toneva et al., 2020]
[Schrimpf et al., 2021b]

Lang.

Stimulus Representations

Dataset Type fMRI fMRI/ MEG English ELMo, BERT, Transformer-XL MEG
fMRI/ECoG English 43 language models (e.g. GloVe, ELMo, BERT,

English LSTM

English BERT

[Gauthier and Levy, 2019]

fMRI

English BERT, fine-tuned NLP tasks (Sentiment, Natural

GPT-2, XLNET)

[Deniz et al., 2019] [Jain et al., 2020] [Caucheteux et al., 2021]
[Antonello et al., 2021]

t x e T

[Reddy and Wehbe, 2021] [Goldstein et al., 2022]

[Oota et al., 2022b] [Oota et al., 2022a] [Merlin and Toneva, 2022]

fMRI fMRI fMRI fMRI

fMRI fMRI

fMRI fMRI fMRI

language inference), Scrambling language model

English GloVe English LSTM English GPT-2, Basic syntax features English
GloVe, BERT, GPT-2, Machine Translation, POS

tasks

English Constituency, Basic syntax features and BERT English GloVe,
GPT-2 next word, pre-onset, post-onset

word surprise English BERT and GLUE tasks English ESN, LSTM, ELMo,
Longformer English BERT, Next word prediction, multi-word semantics,

scrambling model

[Toneva et al., 2022] [Aw and Toneva, 2022]

fMRI / MEG English ELMo, BERT, Context Residuals fMRI

English BART, Longformer, Long-T5, BigBird, and corre-

6 9 9 20

7

9 6

|S| Dataset

Subset Moth Radio Hour Story understanding Question-Answering Neural
architecture of language

Imagine a situation with the noun

Ridge

Subset Moth Radio Hour Subset Moth Radio Hour

345 Narratives

6 Moth Radio Hour

8 8

82 82 8

8 8

Harry Potter ECoG

Pereira & Narratives Narratives Harry Potter

Harry Potter Passive reading

19, 12 Zhang

82

Narratives

8 MEG-MASC 12 12 5 4 7 4 4 4

Reading Sentences Pereira Pereira BOLD 5000 Algonauts BOLD 5000 BOLD
5000 BOLD 5000

6 Moth Radio Hour

sponding Booksum models as well Node Count

English, Chi- nese English Constituency, Dependency trees, Basic syntax
fea-

tures and BERT

English Basic syntax features, GloVe and BERT English BERT-Large, GPT-2
XL English BERT-Large, GPT-2 XL English BERT-Large, GPT-2 XL, Text
Perturbations

21 downstream vision tasks CNN models AlexNet, ResNet, DenseNet 21
downstream vision tasks CNN models AlexNet CNN models AlexNet

VQ-VAE)

English 5 basic and 25 deep learning based speech models (Tera, CPC,
APC, Wav2Vec2.0, HuBERT, DistilHu- BERT, Data2Vec

English Wav2Vec2.0 and SUPERB tasks English Merlo Reseve English 985D
Semantic Vector

English Wav2Vec2.0 English APC, AST, Wav2Vec2.0, and HuBERT English 19
Speech Models (e.g. DeepSpeech, Wav2Vec2.0,

345 Narratives

7 Moth Radio Hour Passive listening 19

Narratives Neuromod

82 5 5 Moth Radio Hour & Short Movie

Ridge Ridge Ridge

Clips

English CLIP, VisualBERT, LXMERT, CNNs and BERT English BriVL English
BridgeTower

5, 82 Periera & Narratives

5 Pereira & Short Movie Clips 5 Moth Radio Hour & Short Movie

Ridge Ridge Ridge

Clips

[Zhang et al., 2022b]

fMRI

[Oota et al., 2023a]

[Oota et al., 2023b] [Tuckute et al., 2023] [Kauf et al., 2023] [Singh
et al., 2023] [Wang et al., 2019] [Kubilius et al., 2019] [Dwivedi et
al., 2021] [Khosla and Wehbe, 2022] [Conwell et al., 2023] [Millet et
al., 2022] [Vaidya et al., 2022] [Tuckute et al., 2022]

[Oota et al., 2023c]

[Oota et al., 2023d] [Dong and Toneva, 2023] [Popham et al., 2021]

[Oota et al., 2022d] [Lu et al., 2022] [Tang et al., 2023]

fMRI

MEG fMRI fMRI fMRI fMRI fMRI fMRI fMRI fMRI fMRI fMRI fMRI

fMRI

fMRI fMRI fMRI

fMRI fMRI fMRI

l a u s i V

o i d u A

l a d o M

i t l u M

Model

Ridge Ridge Ridge Ridge

Ridge Ridge Ridge Ridge

Ridge

Ridge Ridge Ridge

Ridge Ridge

Ridge

Ridge

Ridge Ridge Ridge Ridge Ridge Ridge Ridge Ridge Ridge Ridge Ridge Ridge

Ridge

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

shown to linearly map onto brain activity during language comprehension.
Several studies have attempted to disentan- gle the contributions of
different types of information from word representations to the
alignment between brain record- ings and language models. Wang et
al. [2020] proposed a two-channel variational autoencoder model to
dissociate sentences into semantic and syntactic representations and
separately associate them with brain imaging data to find
feature-correlated brain regions. To separate each syntac- tic feature,
Zhang et al. [2022a] proposed a feature elim- ination method, called
Mean Vector Null space Projection. Compared with word representations,
word syntactic features (parts-of-speech, named entities, semantic
roles, dependen- cies) seem to be distributed across brain networks
instead of a local brain region. In the previous two studies, we do not
know whether all or any of these representations effectively

491

490

drive the linear mapping between language models (LMs) and 488 the
brain. Toneva et al. [2022] presented an approach to dis- 489 entangle
supra-word meaning from lexical meaning in lan- guage models and showed
that supra-word meaning is pre- dictive of fMRI recordings in two
language regions (anterior and posterior temporal lobes). Caucheteux et
al. [2021] pro- posed a taxonomy to factorize the high-dimensional
activa- tions of language models into four combinatorial classes: lex-
ical, compositional, syntactic, and semantic representations. They found
that (1) Compositional representations recruit a more widespread
cortical network than lexical ones, and en- compass the bilateral
temporal, parietal and prefrontal cor- tices. (2) Contrary to previous
claims, syntax and semantics are not associated with separated modules,
but, instead, ap- pear to share a common and distributed neural
substrate.

497

495

496

498

493

492

494

499

502

501

500

While previous works studied syntactic processing as cap-

503

Vision tasks and brains

Vision

Alignment between vi- sion models and brain

Auditory

Brain Encoding

Multimodal

Language

Visual properties in vision mod- els and brains

Speech tasks and brains

Alignment between pretrained speech models and brain

Language & Au- ditory and Vision

Incorporating Lan- guage into Vision

Alignment between pretrained language models and brain

NLP tasks in language mod- els and brains

Disentangling the Syntax and Semantics

Linguistic proper- ties in language models and brains

[Wang et al., 2019; Dwivedi et al., 2021]

[Kubilius et al., 2019; Conwell et al., 2023]

[Khosla and Wehbe, 2022]

[Tuckute et al., 2022; Oota et al., 2023d]

[Vaidya et al., 2022; Millet et al., 2022; Tuckute et al., 2022; Oota et
al., 2023c]

[Lu et al., 2022; Dong and Toneva, 2023]

[Oota et al., 2022d; Popham et al., 2021; Tang et al., 2023; Wang et
al., 2022]

[Huth et al., 2016; Anderson et al., 2017a; Jain and Huth, 2018; Toneva
and Wehbe, 2019; Deniz et al., 2019; Schrimpf et al., 2021a; Caucheteux
and King, 2020; Goldstein et al., 2022; Antonello et al., 2021; Oota et
al., 2022a]

[Gauthier and Levy, 2019; Schwartz et al., 2019; Toneva et al., 2020;
Oota et al., 2022b]

[Wang et al., 2020; Zhang et al., 2022a; Caucheteux et al., 2021; Toneva
et al., 2022; Reddy and Wehbe, 2021; Toneva et al., 2021; Oota et al.,
2023a]

[Kumar et al., 2022; Aw and Toneva, 2022; Merlin and Toneva, 2022; Oota
et al., 2022e; Tuckute et al., 2023; Kauf et al., 2023]

Figure 7: Brain Encoding Survey Tree

504

505

506

tured through complexity measures (syntactic surprisal, node count, word
length, and word frequency), very few have stud- ied the syntactic
representations themselves. Studying syn-

tactic representations using fMRI is difficult because: (1) representing
syntactic structure in an embedding space is a non-trivial computational
problem, and (2) the fMRI signal

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

is noisy. To overcome these limitations, Reddy et al. [2021] proposed
syntactic structure embeddings that encode the syn- tactic information
inherent in natural text that subjects read in the scanner. The results
reveal that syntactic structure- based features explain additional
variance in the brain activity of various parts of the language system,
even after control- ling for complexity metrics that capture the
processing load. Toneva et al. [2021] further examined whether the
represen- tations obtained from a language model align with different
language processing regions in a similar or different way. Linguistic
properties in LMs and brains: Understanding the reasons behind the
observed similarities between lan- guage comprehension in LMs and brains
can lead to more insights into both systems. Several works [Schwartz et
al., 2019; Kumar et al., 2022; Aw and Toneva, 2022; Merlin and Toneva,
2022; Oota et al., 2022b] have found that using a fine-tuned BERT leads
to improved brain predictions. How- ever, it is not clear what type of
information in the fine-tuned BERT model led to the improvement. It is
unclear whether and how the two systems align in their information
processing pipeline. Aw and Toneva [2022] used four pre-trained large
language models (BART, Longformer Encoder Decoder, Big- Bird, and
LongT5) and also trained them to improve their narrative understanding,
using the method detailed in Fig- ure 8. However, it is not understood
whether prediction of the next word is necessary for the observed brain
alignment or simply sufficient, and whether there are other shared mech-
anisms or information that is similarly important. Merlin and Toneva
[2022] proposed two perturbations to pretrained lan- guage models that,
when used together, can control for the ef- fects of next word
prediction and word-level semantics on the alignment with brain
recordings. Specifically, they find that improvements in alignment with
brain recordings in two lan- guage processing regions–Inferior Frontal
Gyrus (IFG) and Angular Gyrus (AG)–are due to next word prediction and
word-level semantics. However, what linguistic information actually
underlies the observed alignment between brains and language models is
not clear. Recently, Oota et al. [2022e] tested the effect of a range of
linguistic properties (surface, syntactic and semantic) and found that
the elimination of each linguistic property results in a significant
decrease in brain alignment across all layers of BERT. Visual Encoding:
CNNs are currently the best class of mod- els of the neural mechanisms
of visual processing [Du et al., 2020; Beliy et al., 2019; Oota et al.,
2019; Nishida et al., 2020]. How can we push these deeper CNN models to
cap- ture brain processing even more stringently? Continued ar-
chitectural optimization on ImageNet alone no longer seems like a viable
option. Kubilius et al. [2019] proposed a shal- low recurrent anatomical
network CORnet that follows neu- roanatomy more closely than standard
CNNs, and achieved the state-of-the-art results on the Brain-score
benchmark. It has four computational areas, conceptualized as analogous
to the ventral visual areas V1, V2, V4, and IT, and a linear cate- gory
decoder that maps from the population of neurons in the model’s last
visual area to its behavioral choices.

Despite the effectiveness of CNNs, it is difficult to draw specific
inferences about neural information processing us- ing CNN- derived
representations from a generic object-

571

588

576

572

574

575

573

577

589

classification CNN. Hence, Wang et al. [2019] built encoding 569 models
with individual feature spaces obtained from 21 com- 570 puter vision
tasks. One of the main findings is that features from 3D tasks, compared
to those from 2D tasks, predict a distinct part of visual cortex.
Auditory Encoding: Speech stimuli have mostly been rep- resented using
encodings of text transcriptions [Huth et al., 2016] or using basic
features like phoneme rate, the sum of squared FFT coefficients [Pandey
et al., 2022], etc. Text transcription-based methods ignore the raw
audio-sensory in- 578 formation completely. The basic speech feature
engineering 579 method misses the benefits of transfer learning from
rigor- 580 ously pretrained speech DL models.

581 Recently, several researchers have used popular deep 582 583

learning models such as APC [Chung et al., 2020], Wav2Vec2.0 [Baevski et
al., 2020], HuBERT [Hsu et al., 584 2021], and Data2Vec [Baevski et al.,
2022] for encoding 585 speech stimuli. Millet et al. [2022] used a
self-supervised 586 learning model Wav2Vec2.0 to learn latent
representations 587 of the speech waveform similar to those of the human
brain. They find that the functional hierarchy of its transformer lay-
ers aligns with the cortical hierarchy of speech in the brain, 590 and
reveals the whole-brain organisation of speech processing 591 with an
unprecedented clarity. This means that the first trans- 592 former
layers map onto the low-level auditory cortices (A1 593 and A2), the
deeper layers (orange and red) map onto brain 594 regions associated
with higher-level processes (e.g. STS and 595 IFG). Vaidya et al. [2022]
present the first systematic study 596 to bridge the gap between recent
four self-supervised speech 597 representation methods (APC, Wav2Vec,
Wav2Vec2.0, and 598 HuBERT) and computational models of the human
auditory 599 system. Similar to [Millet et al., 2022], they find that
self- 600 supervised speech models are the best models of auditory ar-
eas. Lower layers best modeled low-level areas, and upper- middle layers
were most predictive of phonetic and semantic areas, while layer
representations follow the accepted hier- 604 archy of speech
processing. Tuckute et al. [2022] analyzed 605 19 different speech
models and find that some audio models 606 derived in engineering
contexts (model applications ranged 607 from speech recognition and
speech enhancement to audio 608 captioning and audio source separation)
produce poor predic- 609 tions of auditory cortical responses, many
task-optimized au- dio speech deep learning models outpredict a standard
spec- trotemporal model of the auditory cortex and exhibit hierar-
chical layer-region correspondence with auditory cortex. Multimodal
Brain Encoding: Multimodal stimuli can be 614 best encoded using
recently proposed deep learning based 615 multimodal models. Oota et
al. [2022d] experimented with 616 multimodal models like Contrastive
Language-Image Pre- 617 training (CLIP), Learning Cross-Modality Encoder
Repre- 618 sentations from Transformers (LXMERT), and VisualBERT 619 and
found VisualBERT to the best. Similarly, Wang et 620 al. [2022] find
that multimdoal models like CLIP better pre- dict neural responses in
visual cortex, since image captions 622 typically contain the most
semantically relevant information 623 in an image for humans. [Dong and
Toneva, 2023] present a 624 systematic approach to probe multi-modal
video Transformer model by leveraging neuroscientific evidence of
multimodal information processing in the brain. The authors find that
in-

610

603

611

602

601

612

613

627

626

625

621

Figure 8: Comparison of brain recordings with language models trained
on web corpora (LEFT) and language models trained on book stories
(RIGHT) [Aw and Toneva, 2022].

628

629

630

631

632

633

634

635

636

637

638

639

640

641

642

643

644

645

646

647

648

649

650

651

652

653

termediate layers of a multimodal video transformer are bet- ter at
predicting multimodal brain activity than other layers, indicating that
the intermediate layers encode the most brain- related properties of the
video stimuli. Recently, [Tang et al., 2023] investigated a multimodal
Transformer as the encoder architecture to extract the aligned concept
representations for narrative stories and movies to model fMRI responses
to nat- uralistic stories and movies, respectively. Since language and
vision rely on similar concept representations, the authors perform a
cross-modal experiment in which how well the lan- guage encoding models
can predict movie-fMRI responses from narrative story features (story →
movie) and how well the vision encoding models can predict narrative
story-fMRI responses from movie features (movie → story). Overall, the
authors find that cross-modality performance was higher for features
extracted from multimodal transformers than for lin- early aligned
features extracted from unimodal transformers.

6 Brain Decoding Decoding is the learning of the mapping from neural
activa- tions back to the stimulus domain. Figure 9 depicts the typical
workflow for building an image/language decoder. Decoder Architectures:
In most cases, the stimulus repre- sentation is decoded using typical
ridge regression models trained on each voxel and its 26 neighbors in 3D
to pre- dict each dimension of the stimulus representation. Also,
decoding is usually performed using the most informative

656

654

655

voxels [Pereira et al., 2018]. In some cases, a fully con- nected layer
[Beliy et al., 2019] or a multi-layered percep- tron [Sun et al., 2019]
has been used. In some studies, when decoding is modeled as multi-class
classification, Gaus- 657 sian Na¨ıve Bayes [Singh et al., 2007; Just et
al., 2010] and 658 SVMs [Thirion et al., 2006] have also been used for
decod- 659 ing. Figure 10 summarizes the literature related to various
decoding solutions proposed in vision, auditory, and language domains.
Decoding task settings: The most common setting is to per- form decoding
to a vector representation using a stimuli of 664 a single mode (visual,
text or audio). Initial brain decoding 665 experiments studied the
recovery of simple concrete nouns 666 and verbs from fMRI brain activity
[Nishimoto et al., 2011] 667 where the subject watches either a picture
or a word. Sun 668 et al. [2019] used several sentence representation
models to 669 associate brain activities with sentence stimulus, and
found 670 InferSent to perform the best. More work has focused on de-
671 coding the text passages instead of individual words [Wehbe et al.,
2014].

661

662

660

663

673

672

Some studies have focused on multimodal stimuli based 674 675

decoding where the goal is still to decode the text represen- tation
vector. For example, Pereira et al. [2018] trained the 676 decoder on
imaging data of individual concepts, and showed 677 that it can decode
semantic vector representations from imag- 678 ing data of sentences
about a wide variety of both concrete and abstract topics from two
separate datasets. Further, Oota

679

680

Figure 9: Schema for Brain Decoding. LEFT: Image decoder [Smith et
al. 2011], RIGHT: Language Decoder [Wang et al. 2019]

Table 3: Summary of Representative Brain Decoding Studies

Stimuli Authors

Lang.

Stimulus Representations

|S| Dataset

[Pereira et al., 2018] [Wang et al., 2020] [Oota et al., 2022c] [Tang et
al., 2022]

[Beliy et al., 2019]

Dataset Type fMRI fMRI fMRI fMRI

fMRI

English English English English

[Takagi and Nishimoto, 2022] [Ozcelik and VanRullen, 2023] [Chen et al.,
2023b] [D´efossez et al., 2022]

fMRI fMRI fMRI MEG,EEG English

t x e T

l a u s i V

o i d u A

Word2Vec, GloVe, BERT BERT, RoBERTa GloVe, BERT, RoBERTa GPT, fine-tuned
GPT on Reddit comments and au- tobiographical stories End-to-End
Encoder-Decoder, Decoder-Encoder, AlexNet Latent Diffusion Model, CLIP
VDVAE, Latent Diffusion Model Latent Diffusion Model, CLIP MEL
Spectrogram, Wav2Vec2.0

17 Pereira Pereira 6 17 Pereira 7 Moth Radio Hour

5 Generic Object Decoding, ViM-1

4 NSD 7 NSD 3 HCP fMRI-Video-Dataset

169 MEG-MASC

Model

Ridge Ridge Ridge Ridge

Ridge

Ridge Ridge, CLIP

[Gwilliams et al., 2022]

MEG

English

Phonemes

7 MEG-MASC

Vision

Video reconstruction

Image reconstruction

[Nishimoto et al., 2011; Chen et al., 2023b]

[Naselaris et al., 2009; Beliy et al., 2019; Takagi and Nishimoto, 2022;
Ozcelik and VanRullen, 2023; Chen et al., 2023a]

Brain Decoding

Auditory

Speech re- construction

[Anumanchipalli et al., 2019; D ´efossez et al., 2022]

Language

Reconstructing continuous language

Decoding word/sentence vector

Figure 10: Brain Decoding Survey Tree

[Affolter et al., 2020; Tang et al., 2022]

[Pereira et al., 2018; Sun et al., 2019; Gauthier and Levy, 2019; Abdou
et al., 2021; Oota et al., 2022c]

681

682

683

684

685

686

et al. [2022c] propose two novel brain decoding setups: (1) multi-view
decoding (MVD) and (2) cross-view decoding (CVD). In MVD, the goal is to
build an MV decoder that can take brain recordings for any view as input
and predict the concept. In CVD, the goal is to train a model which
takes brain recordings for one view as input and decodes a seman-

tic vector representation of another view. Specifically, they 687 study
practically useful CVD tasks like image captioning, im- 688 age tagging,
keyword extraction, and sentence formation.

689

To understand application of Transformer models for de- 690 coding
better, Gauthier et al. [2019] fine-tuned a pre-trained 691 BERT on a
variety of NLU tasks, asking which lead to im- 692

693

694

695

696

697

698

699

700

701

702

703

704

705

706

707

708

709

710

711

712

713

714

715

716

717

718

719

720

721

722

723

724

725

726

727

728

729

730

731

732

733

734

735

736

737

738

739

740

741

742

743

744

745

746

747

provements in brain-decoding performance. They find that tasks which
produce syntax-light representations yield signif- icant improvements in
brain decoding performance. Toneva et al. [2019] study how
representations of various Trans- former models differ across layer
depth, context length, and attention type.

Some studies have attempted to reconstruct words [Affolter et al.,
2020], continuous language [Tang et al., 2022], im- ages [Du et al.,
2020; Beliy et al., 2019; Fang et al., 2020; Lin et al., 2022], speech
[D´efossez et al., 2022] or question- answer speech dialogues [Moses et
al., 2019] rather than just predicting a semantic vector representation.
Lastly, some studies have focused on reconstructing personal imagined
ex- periences [Berezutskaya et al., 2020] or application-based decoding
like using brain activity scanned during a picture- based mechanical
engineering task to predict individuals’ physics/engineering exam
results [Cetron et al., 2019] and reflecting whether current thoughts
are detailed, correspond to the past or future, are verbal or in images
[Smallwood and Schooler, 2015]. Table 3 aggregates the brain decoding
liter- ature along different stimulus domains such as textual, visual,
and audio.

7 Conclusion, Limitations, and Future Trends

Conclusion In this paper, we surveyed important datasets, stimulus
representations, brain encoding and brain decoding methods across
different modalities. A glimpse of how deep learning solutions throw
light on putative brain computations is given. Limitations Naturalistic
datasets of passive reading/listening offer ecologically realistic
settings for investigating brain function. However, the lack of a task
(as in a controlled psycholinguistic experiment) that probes the
participant’s un- derstanding of the narrative limits the inferences
that can be made on what the participant’s brain is actually engaged in
while passively following the stimuli. This becomes even more important
when multi-lingual, multiscriptal participants process stimuli in L2
language or script – it is unclear if the brain activity reflects the
processing of L2 or active suppres- sion L1 while focusing on L2
[Malik-Moraleda et al., 2022]. Future Trends Some of the future areas of
work in this field are as follows: (1) While there is work on the text,
under- standing the similarity in information processing between vi-
sual/speech/multimodal models versus natural brain systems (2) Decoding
to actual multimodal remains an open area. stimuli seems feasible thanks
to recent advances in generation using deep learning models. (3) Deeper
understanding of the degree to which damage to different parts of the
human brain could lead to the degradation of cognitive skills. (4) How
can we train artificial neural networks in novel self-supervised ways
such that they compose word meanings or comprehend images and speech
like a human brain? (5) How can we lever- age improved neuroscience
understanding to suggest changes in proposed artificial neural network
architectures to make them more robust and accurate? We hope that this
survey motivates research along the above directions.

References [Abdou et al., 2021] Mostafa Abdou, Ana Valeria Gonz´alez,
Mariya Toneva, Daniel Hershcovich, and Anders Søgaard. Does injecting
linguistic structure into lan- arXiv preprint guage models lead to
better alignment with brain recordings? arXiv:2101.12608, 2021.

[Affolter et al., 2020] Nicolas Affolter, Beni Egressy, Damian Pascual,
and Roger Wattenhofer. Brain2word: Decoding brain activity for language
generation. arXiv preprint arXiv:2009.04765, 2020.

[Allen et al., 2022] Emily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L
Breedlove, Jacob S Prince, Logan T Dowdle, Matthias Nau, Brad Caron,
Franco Pestilli, Ian Charest, et al. A massive 7t fmri dataset to bridge
cognitive neuroscience and artifi- cial intelligence. Nature
neuroscience, 25(1):116–126, 2022.

[Anderson et al., 2017a] Andrew J Anderson, Douwe Kiela, Stephen Clark,
and Mas- simo Poesio. Visually grounded and textual semantic models
differentially decode brain activity associated with concrete and
abstract nouns. TACL, 5:17–30, 2017.

[Anderson et al., 2017b] Andrew James Anderson, Jeffrey R Binder,
Leonardo Fer- nandino, Colin J Humphries, Lisa L Conant, Mario Aguilar,
Xixi Wang, Donias Doko, and Rajeev DS Raizada. Predicting neural
activity patterns associated with sentences using a neurobiologically
motivated model of semantic representation. Cerebral Cortex,
27(9):4379–4395, 2017.

[Anderson et al., 2019] Andrew James Anderson, Jeffrey R Binder,
Leonardo Fer- nandino, Colin J Humphries, Lisa L Conant, Rajeev DS
Raizada, Feng Lin, and Edmund C Lalor. An integrated neural decoder of
linguistic and experiential mean- ing. Journal of Neuroscience,
39(45):8969–8987, 2019.

[Anderson et al., 2020] Andrew James Anderson, Kelsey McDermott, Brian
Rooks, Kathi L Heffner, David Dodell-Feder, and Feng V Lin. Decoding
individual identity from brain activity elicited in imagining common
experiences. Nature communica- tions, 11(1):1–14, 2020.

[Antonello et al., 2021] Richard Antonello, Javier S Turek, Vy Vo, and
Alexander Huth. Low-dimensional structure in the space of language
representations is re- flected in brain responses. NeurIPS,
34:8332–8344, 2021.

[Anumanchipalli et al., 2019] Gopala K Anumanchipalli, Josh Chartier,
and Edward F Chang. Speech synthesis from neural decoding of spoken
sentences. Nature, 568(7753):493–498, 2019.

[Aw and Toneva, 2022] Khai Loong Aw and Mariya Toneva.

models for deeper understanding improves brain alignment.
arXiv:2212.10898, 2022.

Training language arXiv preprint

[Baevski et al., 2020] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,
and Michael Auli. wav2vec 2.0: A framework for self-supervised learning
of speech representations. NeurIPS, 33:12449–12460, 2020.

[Baevski et al., 2022] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun
Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for
self-supervised learning in speech, vision and language. In ICML, pages
1298–1312. PMLR, 2022.

[Beliy et al., 2019] Roman Beliy, Guy Gaziv, Assaf Hoogi, Francesca
Strappini, Tal Golan, and Michal Irani. From voxels to pixels and back:
Self-supervision in natural-image reconstruction from fmri. arXiv
preprint arXiv:1907.02431, 2019.

[Berezutskaya et al., 2020] Julia Berezutskaya, Zachary V Freudenburg,
Luca Ambro- gioni, Umut G¨uc¸l¨u, Marcel AJ van Gerven, and Nick F
Ramsey. Cortical network responses map onto data-driven features that
capture visual semantics of movie frag- ments. Scientific reports,
10(1):1–21, 2020.

[Boyle et al., 2020] Julie A Boyle, Basile Pinsard, A Boukhdhir, S
Belleville, S Bram- batti, J Chen, J Cohen-Adad, A Cyr, A Fuente, P
Rainville, et al. The courtois project on neuronal modelling: 2020 data
release. In OHBM, 2020.

[Brennan and Hale, 2019] Jonathan R Brennan and John T Hale.
Hierarchical struc- ture guides rapid linguistic predictions during
naturalistic listening. PloS one, 14(1):e0207741, 2019.

[Cao et al., 2021] Lu Cao, Dandan Huang, Yue Zhang, Xiaowei Jiang, and
Yanan Chen. Brain decoding using fnirs. In AAAI, volume 35, pages
12602–12611, 2021.

[Caucheteux and King, 2020] Charlotte Caucheteux and Jean-R´emi King.
Language processing in brains and deep neural networks: computational
convergence and its limits. BioRxiv, 2020.

[Caucheteux et al., 2021] Charlotte Caucheteux, Alexandre Gramfort, and
Jean-Remi King. Disentangling syntax and semantics in the brain with
deep networks. In ICML, pages 1336–1348. PMLR, 2021.

[Cetron et al., 2019] Joshua S Cetron, Andrew C Connolly, Solomon G
Diamond, Vicki V May, and James V Haxby. Decoding individual differences
in stem learning from functional mri data. Nature communications,
10(1):1–10, 2019.

[Chang et al., 2019] Nadine Chang, John A Pyles, Austin Marcus, Abhinav
Gupta, Michael J Tarr, and Elissa M Aminoff. Bold5000, a public fmri
dataset while view- ing 5000 visual images. Scientific data, 6(1):1–18,
2019.

748

749 750 751 752

753 754 755

756 757 758 759

760 761 762

763 764 765 766 767

768 769 770 771

772 773 774 775

776 777 778

779 780 781

782 783 784

785 786 787

788 789 790

791 792 793

794 795 796 797

798 799 800

801 802 803

804 805

806 807 808

809 810 811

812 813 814

815 816 817

818 819 820

821 822 823

824 825

826 827 828 829 830

831 832 833 834

835 836 837 838

839 840 841

842 843 844 845

846 847 848 849

850 851 852

853 854 855

856 857 858

859 860

861 862

863 864 865 866

867 868 869 870

871 872 873

874 875 876 877

878 879 880 881 882

883 884 885

886 887 888

889 890 891

892 893 894 895

896 897 898

899 900

901 902

903 904 905

906 907 908

909 910 911

912 913 914

915 916 917 918

919 920 921

922 923 924

[Chen et al., 2023a] Xuhang Chen, Baiying Lei, Chi-Man Pun, and Shuqiang
Wang. Brain diffuser: An end-to-end brain image to brain network
pipeline. arXiv preprint arXiv:2303.06410, 2023.

[Horikawa and Kamitani, 2017] Tomoyasu Horikawa and Yukiyasu Kamitani.
Generic decoding of seen and imagined objects using hierarchical visual
features. Nature communications, 8(1):1–15, 2017.

[Chen et al., 2023b] Zijiao Chen, Jiaxin Qing, and Juan Helen Zhou.
Cinematic mindscapes: High-quality video reconstruction from brain
activity. arXiv preprint arXiv:2305.11675, 2023.

[Chung et al., 2020] Yu-An Chung, Hao Tang, and James Glass.
Vector-quantized au-

toregressive predictive coding. Interspeech, pages 3760–3764, 2020.

[Cichy et al., 2019] Radoslaw Martin Cichy, Gemma Roig, Alex Andonian,
Kshitij Dwivedi, Benjamin Lahner, Alex Lascelles, Yalda Mohsenzadeh,
Kandan Ramakr- ishnan, and Aude Oliva. The algonauts project: A platform
for communication between the sciences of biological and artificial
intelligence. arXiv e-prints, pages arXiv–1905, 2019.

[Cichy et al., 2021] Radoslaw Martin Cichy, Kshitij Dwivedi, Benjamin
Lahner, Alex Lascelles, Polina Iamshchinina, M Graumann, A Andonian, NAR
Murty, K Kay, Gemma Roig, et al. The algonauts project 2021 challenge:
How the human brain makes sense of a world in motion. arXiv preprint
arXiv:2104.13714, 2021.

[Conwell et al., 2023] Colin Conwell, Jacob S. Prince, Kendrick N. Kay,
George A. Alvarez, and Talia Konkle. What can 1.8 billion regressions
tell us about the pres- bioRxiv, sures shaping high-level visual
representation in brains and machines? 2023.

[Hsu et al., 2021] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,
Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert:
Self- supervised speech representation learning by masked prediction of
hidden units. TASLP, 29:3451–3460, 2021.

[Huth et al., 2016] Alexander G Huth, Wendy A De Heer, Thomas L
Griffiths, Fr´ed´eric E Theunissen, and Jack L Gallant. Natural speech
reveals the semantic maps that tile human cerebral cortex. Nature,
532(7600):453–458, 2016.

[Huth et al., 2022] Alexander G Huth, Shinji Nishimoto, An T Vu, Dupre
la Tour T,

and Gallant JL. Gallant lab natural short clips 3t fmri data. G-Node,
2022.

[Jain and Huth, 2018] Shailee Jain and Alexander G Huth. Incorporating
context into

language encoding models for fmri. In NIPS, pages 6629–6638, 2018.

[Jain et al., 2020] Shailee Jain, Vy Vo, Shivangi Mahto, Amanda LeBel,
Javier S Interpretable multi-timescale models for predicting

Turek, and Alexander Huth. fmri responses to continuous natural speech.
NeurIPS, 33:13738–13749, 2020.

[Jat et al., 2020] S Jat, H Tang, P Talukdar, and T Mitchel. Relating
simple sentence representations in deep neural networks and the brain.
In ACL, pages 5137–5154, 2020.

[D´efossez et al., 2022] Alexandre D´efossez, Charlotte Caucheteux,
J´er´emy Rapin, Ori Kabeli, and Jean-R´emi King. Decoding speech from
non-invasive brain recordings. arXiv preprint arXiv:2208.12266, 2022.

[Just et al., 2010] Marcel Adam Just, Vladimir L Cherkassky, Sandesh
Aryal, and Tom M Mitchell. A neurosemantic theory of concrete noun
representation based on the underlying brain codes. PloS one,
5(1):e8622, 2010.

[Deniz et al., 2019] Fatma Deniz, Anwar O Nunez-Elizalde, Alexander G
Huth, and Jack L Gallant. The representation of semantic information
across human cerebral cortex during listening versus reading is
invariant to stimulus modality. Journal of Neuroscience,
39(39):7722–7736, 2019.

[Doerig et al., 2022] Adrien Doerig, Rowan Sommers, Katja Seeliger,
Blake Richards, Jenann Ismael, Grace Lindsay, Konrad Kording, Talia
Konkle, Marcel AJ Van Ger- ven, Nikolaus Kriegeskorte, et al. The
neuroconnectionist research programme. arXiv preprint arXiv:2209.03718,
2022.

[Dong and Toneva, 2023] Dota Tianai Dong and Mariya Toneva. Interpreting
multi- modal video transformers using brain recordings. In ICLR 2023
Workshop on Mul- timodal Representation Learning: Perks and Pitfalls,
2023.

[Du et al., 2020] Changde Du, Changying Du, Lijie Huang, and Huiguang
He. Con- ditional generative neural decoding with structured cnn feature
prediction. In AAAI, pages 2629–2636, 2020.

[Dwivedi et al., 2021] Kshitij Dwivedi, Michael F Bonner, Radoslaw
Martin Cichy, and Gemma Roig. Unveiling functions of the visual cortex
using task-specific deep neural networks. PLoS computational biology,
17(8):e1009267, 2021.

[Fang et al., 2020] Tao Fang, Yu Qi, and Gang Pan. Reconstructing
perceptive images

from brain activity by shape-semantic gan. NeurIPS, 33:13038–13048,
2020.

[Gauthier and Levy, 2019] Jon Gauthier and Roger Levy. Linking
artificial and human

neural representations of language. arXiv preprint arXiv:1910.01244,
2019.

[Goldstein et al., 2022] Ariel Goldstein, Zaid Zada, Eliav Buchnik,
Mariano Schain, Amy Price, Bobbi Aubrey, Samuel A Nastase, Amir Feder,
Dotan Emanuel, Alon Cohen, et al. Shared computational principles for
language processing in humans and deep language models. Nature
neuroscience, 25(3):369–380, 2022.

[Gwilliams et al., 2022] Laura Gwilliams, Graham Flick, Alec Marantz,
Liina Pylkka- nen, David Poeppel, and Jean-Remi King. Meg-masc: a
high-quality magneto- encephalography dataset for evaluating natural
speech processing. arXiv preprint arXiv:2208.11488, 2022.

[Hale et al., 2018] John Hale, Chris Dyer, Adhiguna Kuncoro, and
Jonathan Brennan. Finding syntax in human encephalography with beam
search. In ACL, pages 2727– 2736, 2018.

[Handjaras et al., 2016] Giacomo Handjaras, Emiliano Ricciardi, Andrea
Leo, Alessandro Lenci, Luca Cecchetti, Mirco Cosottini, and Giovanna
Marotta. How concepts are encoded in the human brain: a modality
independent, category-based cortical organization of semantic knowledge.
Neuroimage, 135:232–242, 2016.

[Hebart et al., 2022] Martin N Hebart, Oliver Contier, Lina Teichmann,
Adam Rock- ter, Charles Y Zheng, Alexis Kidder, Anna Corriveau, Maryam
Vaziri-Pashkam, and Chris I Baker. Things-data: A multimodal collection
of large-scale datasets for in- vestigating object representations in
brain and behavior. bioRxiv, pages 2022–07, 2022.

[Hollenstein et al., 2018] Nora Hollenstein, Jonathan Rotsztejn, Marius
Troendle, An- dreas Pedroni, Ce Zhang, and Nicolas Langer. Zuco, a
simultaneous eeg and eye- tracking resource for natural sentence
reading. Scientific data, 5(1):1–13, 2018.

[Hollenstein et al., 2019] Nora Hollenstein, Antonio de la Torre,
Nicolas Langer, and Ce Zhang. Cognival: A framework for cognitive word
embedding evaluation. In CoNLL, pages 538–549, 2019.

[Karamolegkou et al., 2023] Antonia Karamolegkou, Mostafa Abdou, and
Anders arXiv preprint

Søgaard. Mapping brains with language models: A survey.
arXiv:2306.05126, 2023.

[Kauf et al., 2023] Carina Kauf, Greta Tuckute, Roger Levy, Jacob
Andreas, and Evelina Fedorenko. Lexical semantic content, not syntactic
structure, is the main contributor to ann-brain similarity of fmri
responses in the language network. bioRxiv, pages 2023–05, 2023.

[Kay et al., 2008] Kendrick N Kay, Thomas Naselaris, Ryan J Prenger, and
Jack L Gal- lant. Identifying natural images from human brain activity.
Nature, 452(7185):352– 355, 2008.

[Khosla and Wehbe, 2022] Meenakshi Khosla and Leila Wehbe. High-level
visual ar- eas act like domain-general filters with strong selectivity
and functional specializa- tion. bioRxiv, 2022.

[Kubilius et al., 2019] Jonas Kubilius, Martin Schrimpf, Kohitij Kar,
Rishi Rajaling- 925 ham, Ha Hong, Najib Majaj, Elias Issa, Pouya
Bashivan, Jonathan Prescott-Roy, 926 Kailyn Schmidt, et al. Brain-like
object recognition with high-performing shallow 927 recurrent anns.
NIPS, 32:12805–12816, 2019. 928

[Kumar et al., 2022] Sreejan Kumar, Theodore R Sumers, Takateru
Yamakoshi, Ariel Goldstein, Uri Hasson, Kenneth A Norman, Thomas L
Griffiths, Robert D Hawkins, and Samuel A Nastase. Reconstructing the
cascade of language processing in the brain using the internal
computations of a transformer-based language model. BioRxiv, pages
2022–06, 2022.

[Lahner et al., 2023] Benjamin Lahner, Kshitij Dwivedi, Polina
Iamshchinina, Monika Graumann, Alex Lascelles, Gemma Roig, Alessandro
Thomas Gifford, Bowen Pan, SouYoung Jin, N Apurva Ratan Murty, et
al. Bold moments: modeling short visual events through a video fmri
dataset and metadata. bioRxiv, pages 2023–03, 2023.

[Li et al., 2021] Jixing Li, Shohini Bhattasali, Shulin Zhang, Berta
Franzluebbers, Wen-Ming Luh, R Nathan Spreng, Jonathan R Brennan, Yiming
Yang, Christophe Pallier, and John Hale. Le petit prince: A multilingual
fmri corpus using ecological stimuli. Biorxiv, pages 2021–10, 2021.

[Lin et al., 2022] Sikun Lin, Thomas Christopher Sprague, and Ambuj
Singh. Mind reader: Reconstructing complex images from brain activities.
In NeurIPS, 2022.

[Lu et al., 2022] Haoyu Lu, Qiongyi Zhou, Nanyi Fei, Zhiwu Lu, Mingyu
Ding, Jingyuan Wen, Changde Du, Xin Zhao, Hao Sun, Huiguang He, et
al. Multi- modal foundation models are better simulators of the human
brain. arXiv preprint arXiv:2208.08263, 2022.

[Malik-Moraleda et al., 2022] Saima Malik-Moraleda, Dima Ayyash, Jeanne
Gall´ee, Josef Affourtit, Malte Hoffmann, Zachary Mineroff, Olessia
Jouravlev, and Evelina Fedorenko. An investigation across 45 languages
and 12 language families reveals a universal language network. Nature
Neuroscience, 25(8):1014–1019, 2022.

[Merlin and Toneva, 2022] Gabriele Merlin and Mariya Toneva. Language
models and brain alignment: beyond word-level semantics and prediction.
arXiv preprint arXiv:2212.00596, 2022.

[Millet et al., 2022] Juliette Millet, Charlotte Caucheteux, Pierre
Orhan, Yves Boubenec, Alexandre Gramfort, Ewan Dunbar, Christophe
Pallier, and Jean-Remi King. Toward a realistic model of speech
processing in the brain with self-supervised learning. arXiv:2206.01685,
2022.

929 930 931 932 933

934 935 936 937

938 939 940 941

942 943

944 945 946 947

948 949 950 951

952 953 954

955 956 957 958

959 960 961

962 963 964

965 966 967

968 969 970 971

972 973 974

975 976 977

978 979 980

981 982 983

984 985 986

987 988 989 990

991 992 993 994

995 996

997 998 999

1000 1001 1002

1003 1004 1005

1006 1007 1008

1009 1010

1011 1012 1013

1014 1015

1016 1017 1018

1019 1020 1021

1022 1023 1024

1025 1026 1027

[Mitchell et al., 2008] Tom M Mitchell, Svetlana V Shinkareva, Andrew
Carlson, Kai- Min Chang, Vicente L Malave, and Robert A Mason.
Predicting human brain activ- ity associated with the meanings of nouns.
Science, 320(5880):1191–1195, 2008.

[Moses et al., 2019] David A Moses, Matthew K Leonard, Joseph G Makin,
and Ed- ward F Chang. Real-time decoding of question-and-answer speech
dialogue using human cortical activity. Nature communications,
10(1):1–14, 2019.

[Naselaris et al., 2009] Thomas Naselaris, Ryan J Prenger, Kendrick N
Kay, Michael Oliver, and Jack L Gallant. Bayesian reconstruction of
natural images from human brain activity. Neuron, 63(6):902–915, 2009.

[Nastase et al., 2021] Samuel A Nastase, Yun-Fei Liu, Hanna Hillman,
Asieh Zad- bood, Liat Hasenfratz, Neggin Keshavarzian, Janice Chen,
Christopher J Honey, Yaara Yeshurun, Mor Regev, et al. Narratives: fmri
data for evaluating models of naturalistic language comprehension.
bioRxiv, pages 2020–12, 2021.

[Nishida and Nishimoto, 2018] Satoshi Nishida and Shinji Nishimoto.
Decoding nat- uralistic experiences from human brain activity via
distributed representations of words. Neuroimage, 180:232–242, 2018.

[Nishida et al., 2020] Satoshi Nishida, Yusuke Nakano, Antoine Blanc,
Naoya Maeda, Masataka Kado, and Shinji Nishimoto. Brain-mediated
transfer learning of convo- lutional neural networks. In AAAI, pages
5281–5288, 2020.

[Nishimoto et al., 2011] Shinji Nishimoto, An T Vu, Thomas Naselaris,
Yuval Ben- jamini, Bin Yu, and Jack L Gallant. Reconstructing visual
experiences from brain activity evoked by natural movies. Current
biology, 21(19):1641–1646, 2011.

[Pereira et al., 2018] Francisco Pereira, Bin Lou, Brianna Pritchett,
Samuel Ritter, 1028 Samuel J Gershman, Nancy Kanwisher, Matthew
Botvinick, and Evelina Fedorenko. 1029 Toward a universal decoder of
linguistic meaning from brain activation. Nature com- 1030 munications,
9(1):1–13, 2018. 1031

[Popham et al., 2021] Sara F Popham, Alexander G Huth, Natalia Y
Bilenko, Fatma 1032 Deniz, James S Gao, Anwar O Nunez-Elizalde, and Jack
L Gallant. Visual and 1033 linguistic semantic representations are
aligned at the border of human visual cortex. 1034 Nature neuroscience,
24(11):1628–1636, 2021. 1035

[Reddy and Wehbe, 2021] Aniketh Janardhan Reddy and Leila Wehbe. Can
fmri re- 1036 veal the representation of syntactic structure in the
brain? NeurIPS, 34:9843–9856, 1037 2021. 1038

[Schrimpf et al., 2020] Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib
J Majaj, 1039 Rishi Rajalingham, Elias B Issa, Kohitij Kar, Pouya
Bashivan, Jonathan Prescott- 1040 Roy, Franziska Geiger, et
al. Brain-score: Which artificial neural network for object 1041
recognition is most brain-like? BioRxiv, page 407007, 2020. 1042

[Schrimpf et al., 2021a] Martin Schrimpf, Idan Blank, Greta Tuckute,
Carina Kauf, 1043 Eghbal A Hosseini, Nancy Kanwisher, Joshua Tenenbaum,
and Evelina Fedorenko. 1044 The neural architecture of language:
Integrative reverse-engineering converges on a 1045 model for predictive
processing. PNAS, Vol:To appear, 2021. 1046

[Schrimpf et al., 2021b] Martin Schrimpf, Idan Asher Blank, Greta
Tuckute, Carina 1047 Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua B
Tenenbaum, and Evelina 1048 Fedorenko. The neural architecture of
language: Integrative modeling converges on 1049 predictive processing.
PNAS, 118(45), 2021. 1050

[Oota et al., 2018] Subba Reddy Oota, Naresh Manwani, and Raju S Bapi.
fMRI Se- mantic Category Decoding Using Linguistic Encoding of Word
Embeddings. In ICONIP, pages 3–15. Springer, 2018.

[Schwartz et al., 2019] Dan Schwartz, Mariya Toneva, and Leila Wehbe.

Inducing 1051 brain-relevant bias in natural language processing models.
NIPS, 32:14123–14133, 1052 2019. 1053

[Oota et al., 2019] Subba Reddy Oota, Vijay Rowtula, Manish Gupta, and
Raju S Bapi. Stepencog: A convolutional lstm autoencoder for
near-perfect fmri encoding. In IJCNN, pages 1–8. IEEE, 2019.

[Seeliger et al., 2019] K Seeliger, RP Sommers, Umut G¨uc¸l¨u, Sander E
Bosch, and 1054 MAJ Van Gerven. A large single-participant fmri dataset
for probing brain responses 1055 to naturalistic stimuli in space and
time. bioRxiv, page 687681, 2019. 1056

[Oota et al., 2022a] Subba Reddy Oota, Frederic Alexandre, and Xavier
Hinaut. Long- term plausibility of language models and neural dynamics
during narrative listening. In Proceedings of the Annual Meeting of the
Cognitive Science Society, volume 44, 2022.

[Oota et al., 2022b] Subba Reddy Oota, Jashn Arora, Veeral Agarwal,
Mounika Marreddy, Manish Gupta, and Bapi Raju Surampudi. Neural language
taskonomy: arXiv preprint Which nlp tasks are the most predictive of
fmri brain activity? arXiv:2205.01404, 2022.

[Oota et al., 2022c] Subba Reddy Oota, Jashn Arora, Manish Gupta, and
Raju S Bapi. Multi-view and cross-view brain decoding. In COLING, pages
105–115, 2022.

[Oota et al., 2022d] Subba Reddy Oota, Jashn Arora, Vijay Rowtula,
Manish Gupta, In COLING, pages 116–133,

and Raju S Bapi. Visio-linguistic brain encoding. 2022.

[Oota et al., 2022e] Subba Reddy Oota, Manish Gupta, and Mariya Toneva.

Joint processing of linguistic properties in brains and language models.
arXiv preprint arXiv:2212.08094, 2022.

[Oota et al., 2023a] Subba Reddy Oota, Mounika Marreddy, Manish Gupta,
and Bapi Raju Surampud. Syntactic structure processing in the brain
while listening. arXiv preprint arXiv:2302.08589, 2023.

[Oota et al., 2023b] Subba Reddy Oota, Trouvain Nathan, Frederic
Alexandre, and Xavier Hinaut. Meg encoding using word context semantics
in listening stories. In Interspeech, 2023.

[Oota et al., 2023c] Subba Reddy Oota, Khushbu Pahwa, Mounika Marreddy,
Manish Gupta, and Raju Surampudi Bapi. Neural architecture of speech. In
ICASSP, 2023.

[Oota et al., 2023d] Subba Reddy Oota, Agarwal Veeral, Marreddy Mounika,
Gupta Manish, and Raju Surampudi Bapi. Speech taskonomy: Which speech
tasks are the most predictive of fmri brain activity? In 24th
INTERSPEECH Conference, 2023.

[Oseki and Asahara, 2020] Yohei Oseki and M Asahara. Design of
bccwj-eeg: Bal- anced corpus with human electroencephalography. In LREC,
pages 189–194, 2020.

[Ozcelik and VanRullen, 2023] Furkan Ozcelik and Rufin VanRullen.
Brain-diffuser: Natural scene reconstruction from fmri signals using
generative latent diffusion. arXiv preprint arXiv:2303.05334, 2023.

[Pandey et al., 2022] Pankaj Pandey, Gulshan Sharma, Krishna P
Miyapuram, Ra- manathan Subramanian, and Derek Lomas. Music
identification using brain re- sponses to initial snippets. In ICASSP,
pages 1246–1250, 2022.

[Pereira et al., 2013] Francisco Pereira, Matthew Botvinick, and Greg
Detre. Using wikipedia to learn semantic feature representations of
concrete concepts in neu- roimaging experiments. Artificial
intelligence, 194:240–252, 2013.

[Singh et al., 2007] Vishwajeet Singh, Krishna P. Miyapuram, and Raju S.
Bapi. De- 1057 In 1058 1059

tection of cognitive states from fmri data using machine learning
techniques. Manuela M. Veloso, editor, IJCAI, pages 587–592, 2007.

[Singh et al., 2023] Chandan Singh, Aliyah R Hsu, Richard Antonello,
Shailee Jain, 1060 Alexander G Huth, Bin Yu, and Jianfeng Gao.
Explaining black box text modules in 1061 natural language with language
models. arXiv preprint arXiv:2305.09863, 2023. 1062

[Smallwood and Schooler, 2015] Jonathan Smallwood and Jonathan W
Schooler. The 1063 science of mind wandering: empirically navigating the
stream of consciousness. 1064 Annual review of psychology, 66:487–518,
2015. 1065

[Sudre et al., 2012] Gustavo Sudre, Dean Pomerleau, Mark Palatucci,
Leila Wehbe, 1066 Alona Fyshe, Riitta Salmelin, and Tom Mitchell.
Tracking neural coding of percep- 1067 tual and semantic features of
concrete nouns. NeuroImage, 62(1):451–463, 2012. 1068

[Sun et al., 2019] Jingyuan Sun, Shaonan Wang, Jiajun Zhang, and
Chengqing Zong. 1069 In AAAI, 1070 1071

Towards sentence-level brain decoding with distributed representations.
pages 7047–7054, 2019.

[Sun et al., 2020] Jingyuan Sun, Shaonan Wang, Jiajun Zhang, and
Chengqing Zong. 1072 IEEE 1073 1074

Neural encoding and decoding with distributed sentence representations.
TNNLS, 32(2):589–603, 2020.

[Takagi and Nishimoto, 2022] Yu Takagi and Shinji Nishimoto.
High-resolution im- 1075 age reconstruction with latent diffusion models
from human brain activity. bioRxiv, 1076 pages 2022–11, 2022. 1077

[Tang et al., 2022] Jerry Tang, Amanda LeBel, Shailee Jain, and
Alexander G Huth. 1078 Semantic reconstruction of continuous language
from non-invasive brain recordings. 1079 bioRxiv, pages 2022–09, 2022.
1080

[Tang et al., 2023] Jerry Tang, Meng Du, Vy A Vo, Vasudev Lal, and
Alexander G 1081 Huth. Brain encoding models based on multimodal
transformers can transfer across 1082 language and vision. arXiv
preprint arXiv:2305.12248, 2023. 1083

[Thirion et al., 2006] Bertrand Thirion, Edouard Duchesnay, Edward
Hubbard, Jessica 1084 Dubois, Jean-Baptiste Poline, Denis Lebihan, and
Stanislas Dehaene. Inverse retino- 1085 topy: inferring the visual
content of images from brain activation patterns. Neuroim- 1086 age,
33(4):1104–1116, 2006. 1087

[Toneva and Wehbe, 2019] Mariya Toneva and Leila Wehbe. Interpreting and
improv- 1088 ing natural-language processing (in machines) with natural
language-processing (in 1089 the brain). arXiv preprint
arXiv:1905.11833, 2019. 1090

[Toneva et al., 2020] Mariya Toneva, Otilia Stretcu, Barnab´as P´oczos,
Leila Wehbe, 1091 and Tom M Mitchell. Modeling task effects on meaning
representation in the brain 1092 via zero-shot meg prediction. NIPS, 33,
2020. 1093

[Toneva et al., 2021] Mariya Toneva, Jennifer Williams, Anand B,
Christoph Dann, 1094 1095

and Leila Wehbe. Same cause; different effects in the brain. In CLeaR,
2021.

[Pereira et al., 2016] Francisco Pereira, Bin Lou, Brianna Pritchett,
Nancy Kanwisher, Matthew Botvinick, and Ev Fedorenko. Decoding of
generic mental representations from functional mri data using word
embeddings. bioRxiv, page 057216, 2016.

[Toneva et al., 2022] Mariya Toneva, Tom M Mitchell, and Leila Wehbe.
Combining 1096 computational controls with natural text reveals aspects
of meaning composition. 1097 Nature Computational Science,
2(11):745–757, 2022. 1098

1099 1100 1101

1102 1103 1104 1105

1106 1107 1108

1109 1110 1111

1112 1113 1114

1115 1116 1117

1118 1119 1120

1121 1122 1123 1124

1125 1126 1127 1128

1129 1130 1131

1132 1133 1134

1135 1136 1137 1138 1139

1140 1141 1142 1143

[Tuckute et al., 2022] Greta Tuckute, Jenelle Feather, Dana Boebinger,
and Josh H McDermott. Many but not all deep neural network audio models
capture brain re- sponses and exhibit hierarchical region
correspondence. bioRxiv, 2022.

[Tuckute et al., 2023] Greta Tuckute, Aalok Sathe, Shashank Srikant,
Maya Taliaferro, Mingye Wang, Martin Schrimpf, Kendrick Kay, and Evelina
Fedorenko. Driving and suppressing the human language network using
large language models. bioRxiv, 2023.

[Vaidya et al., 2022] Aditya R Vaidya, Shailee Jain, and Alexander G
Huth. Self- supervised models of audio effectively explain human
cortical responses to speech. arXiv preprint arXiv:2205.14252, 2022.

[Wang et al., 2017] Jing Wang, Vladimir L Cherkassky, and M Adam Just.
Predicting the brain activation pattern associated with the
propositional content of a sentence: Modeling neural representations of
events and states. HBM, 10:4865–4881, 2017.

[Wang et al., 2019] Aria Wang, Michael Tarr, and Leila Wehbe. Neural
taskonomy: Inferring the similarity of task-derived representations from
brain activity. NeurIPS, 32:15501–15511, 2019.

[Wang et al., 2020] Shaonan Wang, Jiajun Zhang, Haiyan Wang, Nan Lin,
and Chengqing Zong. Fine-grained neural decoding with distributed word
representa- tions. Information Sciences, 507:256–272, 2020.

[Wang et al., 2022] Aria Yuan Wang, Kendrick Kay, Thomas Naselaris,
Michael J Tarr, and Leila Wehbe. Incorporating natural language into
vision models improves pre- diction and understanding of higher visual
cortex. BioRxiv, pages 2022–09, 2022.

[Wehbe et al., 2014] Leila Wehbe, Brian Murphy, Partha Talukdar, Alona
Fyshe, Aa- ditya Ramdas, and Tom Mitchell. Simultaneously uncovering the
patterns of brain regions involved in different story reading
subprocesses. PloS one, 9(11):e112575, 2014.

[Yamins et al., 2014] Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan
A Solomon, Darren Seibert, and James J DiCarlo. Performance-optimized
hierarchical models predict neural responses in higher visual cortex.
PNAS, 111(23):8619–8624, 2014.

[Zhang et al., 2020] Yizhen Zhang, Kuan Han, Robert Worth, and Zhongming
Liu. Connecting concepts in the brain by mapping cortical
representations of semantic relations. Nature communications,
11(1):1–13, 2020.

[Zhang et al., 2022a] Xiaohan Zhang, Shaonan Wang, Nan Lin, Jiajun
Zhang, and Chengqing Zong. Probing word syntactic representations in the
brain by a feature elimination method. AAAI, 2022.

[Zhang et al., 2022b] Xiaohan Zhang, Shaonan Wang, Nan Lin, and
Chengqing Zong. Is the brain mechanism for hierarchical structure
building universal across lan- In Proceedings of the 2022 Con- guages?
an fmri study of chinese and english. ference on Empirical Methods in
Natural Language Processing, pages 7852–7861, 2022.

[Zinszer et al., 2018] Benjamin D Zinszer, Laurie Bayet, Lauren L
Emberson, Ra- jeev DS Raizada, and Richard N Aslin. Decoding semantic
representations from functional near-infrared spectroscopy signals.
Neurophotonics, 5(1):011003– 011003, 2018.


