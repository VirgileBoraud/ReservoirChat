A Simple Reservoir Model of Working Memory with Real Values Anthony
Strock, Nicolas P. Rougier, Xavier Hinaut

To cite this version:

Anthony Strock, Nicolas P. Rougier, Xavier Hinaut. A Simple Reservoir
Model of Working Mem- ory with Real Values. Third workshop on advanced
methods in theoretical neuroscience, Jun 2018, Göttingen, Germany.
￿hal-01861784￿

HAL Id: hal-01861784

https://inria.hal.science/hal-01861784

Submitted on 25 Aug 2018

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

A Simple Reservoir Model of Working Memory with Real Values

MNEMOSYNE Team

first-name.last-name@inria.fr

Anthony Strock2,1,3, Nicolas Rougier1,2,3, Xavier Hinaut1,2,3

1.  Inria Bordeaux Sud-Ouest, Talence, France.
2.  LaBRI, UMR 5800, CNRS, Bordeaux INP, Université de Bordeaux,
    Talence, France.
3.  Institut des Maladies Neurodégénératives, UMR 5293, CNRS, Université
    de Bordeaux, Bordeaux, France.

Abstract Prefrontal cortex is known to be involved in many high-level
cognitive functions, in particular working memory. Here, we study to
what extent a group of randomly connected units can store and maintain
(as output) an arbitrary real value from a streamed input, i.e. how such
system act as a sustained working memory module without being distracted
by the input stream.

Furthermore, we explore to what extent such an architecture can take
advantage of the stored value in order to produce non-linear
computations. Systematic comparison between different architectures
(with and without feedback, with and without a working memory unit)
shows that explicit memory is required.

With Principal Component Analyses (PCA) we show that the reservoir state
is encoding time and the memorized value in different ways depending if
a supplementary task is required. Moreover, theses memory states are
similar to attractors in an input-driven system [3], and in particular,
similar to a noisy line attractor [6].

In this study, we did not try to find the optimal number of reservoir
units needed for each task. Conversely, we voluntary limited the size of
the reservoir to 100 neurons in order to see if such rather small
reservoirs were sufficiently competitive.

Materials & Methods Echo State Networks [1] Update equation of the
reservoir (recurrent layer) and the readout (output layer):

Matrices Win and W are randomly generated.

Training of the output weights with ridge regression

Results: Model comparison

Main results for task 2 (RMSE)

No WM unit: With WM unit:

3.03e-1 7.26e-4

Detailed results for tasks 1 & 2

Task 1: store value sync. with last trigger

Task 2: scale input with last trigger sync. value

Materials & Methods

Value

V

Trigger

T

ﬁxed plastic

Working Memory

M

Value

V

Trigger

T

ﬁxed plastic

Working Memory

M

Global Results

Testing the simple architecture for untrained scenarios

Need for WM unit (see model comparison panel for details)

(Left) PCs explaining most variance. (Right) PCs most correlated with WM
unit.

(Left) PCs explaining most variance. (Right) PCs most correlated with WM
unit.

PCA analyses

Discussion

We have shown how a small group of randomly connected units is able to
maintain an arbitrary value at an arbitrary time from a streamed input.
It is to be noted that the model has not been trained at memorizing
every possible value since there is virtually an infinite number of
values between -1 and +1. What the model has actually learned is to gate
an input value into a placeholder, a.k.a. a working memory (WM) unit.
After training, this WM unit act like a gated memory: information enters
while the gate is opened and is kept constant once the gate is closed.
It can then be used to solve more complex tasks.

We have also demonstrated how such explicit working memory is critical
in solving a scaling (i.e. multiplication) task. The same model deprived
of the explicit working memory fails at solving the task and exhibits
very bad performances (error of 3.03e-1 instead of 7.26e-4). This
demonstrates the criticality of the presence of the working memory unit.

Future work will investigate similar mechanisms when online learning
algorithms are used (e.g. reward-modulated rules [4] and FORCE learning
variants [6, 7]), and investigate how temporal patterns such as
conceptors [2] could be stored in WM units in a similar way.

xxxxx

References [1] H. Jaeger, “The “echo state” approach to analysing and
training recurrent neural networks” Bonn, Germany: German National
Research Center for Information Technology GMD Technical Report,
vol. 148, p. 34, 2001. [2] H. Jaeger. Controlling recurrent neural
networks by conceptors. arXiv preprint arXiv:1403.3369, 2014. [3] R
Pascanu and H Jaeger. A neurodynamical model for working memory. Neural
networks, 24(2):199– 207, 2011. [4] G M Hoerzer, R Legenstein, and W
Maass. Emergence of complex computational structures from chaotic neural
networks through reward-modulated hebbian learning. Cerebral cortex,
24(3):677–690, 2012. [5] Barak, O., & Tsodyks, M. (2014). Working models
of working memory. Current opinion in neurobiology, 25, 20-24. [6] Beer,
C., & Barak, O. (2018). Dynamics of dynamics: following the formation of
a line attractor. arXiv preprint arXiv:1805.09603. [7] Sussillo, D., &
Abbott, L. F. (2009). Generating coherent patterns of activity from
chaotic neural networks. Neuron, 63(4), 544-557. [8] J. Bergstra, D.
Yamins, and D. D. Cox, “Hyperopt: A python library for optimizing the
hyperparameters of machine learning algorithms,” in Proceedings of the
12th Python in Science Conference, pp. 13–20. 2013. [9] P S
Goldman-Rakic. Circuitry of primate prefrontal cortex and regulation of
behavior by representational memory. Comprehensive Physiology, 1987.
[10] M Rigotti, O Barak, M R Warden, X-J Wang, N D Daw, E K Miller, and
S Fusi. The importance of mixed selectivity in complex cognitive tasks.
Nature, 497(7451): 585, 2013. [11] H Jaeger. Long short-term memory in
echo state networks: Details of a simulation study. Technical report,
Jacobs University Bremen, 2012. [12] J Dambre, D Verstraeten, B
Schrauwen, and S Massar. Information processing capacity of dynamical
systems. Scientific reports, 2:514, 2012. [13] P F Dominey. Complex
sensory-motor sequence learning based on recurrent state representation
and reinforcement learning. Biological cybernetics, 73(3):265–274, 1995.

Links Paper:

https://hal.inria.fr/hal-01803594v1

Source code:

https://github.com/anthony-strock/ijcnn2018

Related paper Anthony Strock, Nicolas Rougier, Xavier Hinaut. A Simple
Reservoir Model of Working Memory with Real Values. IJCNN 2018 -
International Joint Conference on Neural Networks, Jul 2018, Rio de
Janeiro, Brazil. 〈hal-01803594〉


