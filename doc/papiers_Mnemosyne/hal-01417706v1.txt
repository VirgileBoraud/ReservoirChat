Using Natural Language Feedback in a Neuro-inspired Integrated
Multimodal Robotic Architecture Johannes Twiefel, Xavier Hinaut, Marcelo
Borghetti, Erik Strahl, Stefan

Wermter

To cite this version:

Johannes Twiefel, Xavier Hinaut, Marcelo Borghetti, Erik Strahl, Stefan
Wermter. Using Natural Language Feedback in a Neuro-inspired Integrated
Multimodal Robotic Architecture. 25th IEEE International Symposium on
Robot and Human Interactive Communication (RO-MAN), Aug 2016, New York
City, United States. pp.52 - 57, ￿10.1109/ROMAN.2016.7745090￿.
￿hal-01417706￿

HAL Id: hal-01417706

https://inria.hal.science/hal-01417706

Submitted on 15 Dec 2016

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

Using Natural Language Feedback in a Neuro-inspired Integrated
Multimodal Robotic Architecture

Johannes Twiefel1, Xavier Hinaut1,2, Marcelo Borghetti,1 Erik Strahl1
and Stefan Wermter1

Abstract— In this paper we present a multi-modal human robot interaction
architecture which is able to combine infor- mation coming from
different sensory inputs, and can generate feedback for the user which
helps to teach him/her implicitly how to interact with the robot. The
system combines vision, speech and language with inference and feedback.
The system environment consists of a Nao robot which has to learn
objects situated on a table only by understanding absolute and relative
object locations uttered by the user and afterwards points on a desired
object to show what it has learned. The results of a user study and
performance test show the usefulness of the feedback produced by the
system and also justify the usage of the system in a real-world
applications, as its classiﬁcation accuracy of multi-modal input is
around 80.8%. In the experiments, the system was able to detect
inconsistent input coming from different sensory modules in all cases
and could generate useful feedback for the user from this information.

I. INTRODUCTION

In human robot interaction (HRI), substantial effort is spent on
improving sensory classiﬁers which provide in- formation to trainable
knowledge databases. By employing state-of-the-art sensory classiﬁers it
is still not guaranteed that the given input can be processed and
learned, as there is still the possibility of incorrect input generated
by the user. Also, when using a multimodal architecture, the inputs
provided to the system can be inconsistent, possibly by incorrect input
coming from the user or a misclassiﬁcation performed by a classiﬁer. To
implicitly train the user how to provide consistent input to the system,
inconsistent inputs have to be classiﬁed as such and feedback has to be
provided to change the user’s way of teaching. Inconsistent inputs are
e.g. teaching a robot that an object is at a speciﬁc position while it
is in fact not present. Also, the user could confuse his egocentric
perspective with the intrinsic perspective of the robot, which means
confusing “left” and “right”, which is also a common problem in
human-human interaction. We state that this kind of input information
should not only be rejected, but also used to provide feedback to the
user what

*This work was partially supported by a Marie Curie Intra European
Fellowship within the 7th European Community Framework Programme:
EchoRob project (PIEF-GA-2013- 627156).

1Johannes

Twiefel, Xavier Hinaut, Marcelo Borghetti,

Erik Strahl, and Stefan Wermter are with Knowledge Technology Group,
Informatik, Universit¨at Hamburg, Department 22527 Hamburg, {twiefel,
hinaut, borghetti, strahl, Germany wermter}@informatik.uni-hamburg.de

2Xavier Hinaut

is with Inria Bordeaux Sud-Ouest, Talence, France; LaBRI, UMR 5800,
CNRS, Bordeaux INP, Universit´e de Bordeaux, des Maladies
Neurod´eg´en´eratives, Talence, UMR 5293, CNRS, Universit´e de Bordeaux,
Bordeaux, France xavier.hinaut@inria.fr

France;

Institut

and

kind of inconsistency occurred, so that the user is able to change his
strategy of teaching.

In this paper we present a multi-modal architecture which employs
vision, language and speech, to teach a robot in a home scenario. Our
system also contains a novel rule- based inference system which can
identify the source of an inconsistent situation and provide feedback to
the user. Also, the robot possesses a trainable knowledge base which it
can consult to perform different actions and learn different objects by
referring to other objects it knows already.

The study of Vollmer et al. [1] also discusses the inﬂuence of feedback
provided by a robot to a human tutor. In their scenario a robot is
instructed to imitate or emulate human actions which consist of moving
different objects on a table; the human tutor and the robot sit together
at a table like in the given study. The results of the study indicate
that the feedback provided by the robot directly inﬂuences the future
way of teaching. In contrast to our study, no learning is involved,
which leads to a stateless communication on the robot’s side, while the
feedback of our robot is also inﬂu- enced by the knowledge the robot
gained during the dialog. Also, in that study only a unimodal (visual)
input is provided to the system, so there is no requirement for
identifying inconsistent inputs coming from different modalities.

II. SCENARIO

In this paper our system is tested in an HRI scenario con- taining a
table, which is divided into the absolute positions (left, right and
middle). Our robot (a Nao) is situated on one side of the table, the
user on the other side to create a conversational atmosphere. The goal
of the user is to help the robot to attach labels to objects
(banana,box,cup, see Fig. 1) which are positioned on the table. In the
beginning, the robot can only distinguish between different objects, but
has no labels (like “banana”) for them. The user can employ semi-free
speech to teach the robot by providing information about the relative or
absolute position of the object on the table. To test if the robot
actually learnt, the user can instruct the robot to point to a speciﬁc
object it has learnt. Fig. 1 shows the scenario.

III. APPROACH

Our integrated system consists of two modalities, which are speech and
vision. The task that the system can perform is mapping objects to
names. At the beginning, the system is initialized with the two sensory
modules being trained independently. The visual system is trained on a
set of objects which it can distinguish between, but cannot map

Fig. 1. The left images show a user communicating with the robot, the
right image depicts the three objects (cup, box, banana).

these objects to lingual identiﬁers. Also, the visual system can
determine the position of the recognized objects. The speech recognition
system is able to recognize domain- speciﬁc utterance and to create word
representations from them. To interpret these sentence hypotheses, a
natural language processing system is trained to create machine-
readable predicate representations. To process the predicates and the
position of the recognized objects, a rule-based inference system was
developed which is able to detect inconsistent information provided by
the sensory module and so can provide detailed feedback for the user. If
the input information is consistent with the context, the information is
added to the knowledge base of the robot. By triggering speciﬁc
commands, the robot can be encouraged to use the knowledge it has to
provide information to the user by giving information about a speciﬁc
object position the user asked for. Fig. 2 contains the architecture of
our integrated system.

Fig. 2. The architecture of the system, showing the different modules
and input modalities. Arrows indicate the ﬂow of information. Vision and
speech recognition (via language understanding) provide information to
the inference module, which uses this information to verify the
consistency of the inputs, while using previously learned knowledge from
the knowledge base if necessary. Feedback is provided via motion or
synthesized speech. While only consistent information is stored in the
knowledge base, incon- sistent input is rejected and the user is
informed about the reasons using the feedback module.

A. Vision Module

To perform the classiﬁcation of the objects, we ﬁrst segment them from
the environment, since this produces distinct clusters of points with
reduced noise. For simplicity, we are considering that the objects are
on plane surfaces such as table, ﬂoor or wall. The environment is
represented as a point cloud captured by an RGB-D device. To select the
objects we take into account the fact that the z axis points to the
direction of the objects. According to [2], for segmentation the scene
is reoriented in a way that the z axis

Fig. 3. Convolutional Neural Network Architecture composed of two
convolutional layers, each followed by a pooling layer (max pooling).
The tuple (C, w, h) on the top of each image is composed of C channels
(3 for RGB), width w and height h. The convolutional layers extract
features from the images and combine them in posterior layers. The max
pooling layer is used for scale and translation invariance. In this
ﬁgure, the convolutional ﬁlters employed have sizes (5, 5) for and the
pooling ﬁlters employed have sizes (2, 2).

becomes orthogonal to the normal vector of the plane. Thus, we identify
all objects that have y coordinate values higher than the average value
of the y coordinate of the points that compose the plane. Finally, we
consider only objects located within a tolerance distance from the
center of mass of the segmented plane.

To classify the objects, a Convolutional Neural Network [3] is employed
(see Fig. 3), because this model already performed well in object
recognition tasks. To train the neural network, we captured samples of
isolated objects and segmented them from the environment according to
the approach described earlier. To increase the number of samples we
rotated the objects in different orientations. We applied to each sample
5 rotations in the x axis and 10 rotations in the y axis. These numbers
of rotation in x and y were deﬁned to simulate different viewpoints from
the top. The sequence of rotations around the y axis can also be seen as
sequential captures around the object when the camera is moving
clockwise or counter-clockwise.

All the samples produced are then projected into the x-y plane and are
used to train the neural network. The output of the convolutional layers
is a vector of features that are used as input to a multi-layer
perceptron. We are using the same parameters described in [2]. The
neural network is trained using Backpropagation Algorithm. The whole
architecture can be seen in Figure 3. We are using two feature maps
layers followed by pooling layers. The size of the input image is 50 ×
50 pixels. For the feature maps layer we use a ﬁlter of size 5 × 5 and
for the pooling we use a ﬁlter of size 2 × 2. In the ﬁrst layer, 20
feature maps are generate and in the second layer 10 feature maps are
generated. Different values for the number of feature maps, size of the
ﬁlters, and internal parameters of the multi-layer perceptron were
tested and we selected the parameters that performed best.

B. Speech Recognition Module

As we are working in a speciﬁc scenario, our speech module is based on a
domain-dependent approach (DOCKS) developed by Twiefel et al. (2014) [4]
which can be re- stricted to a scenario and improve the recognition
accuracy, because less errors are possible. The approach employs

SpeechRecognitionLanguageUnderstandingVisionKnowledge
BaseInferenceFeedback byMotionFeedback by SpeechUserTableClassifier(3,
50, 50)(10, 46, 46)(10, 23, 23)(20, 19, 19)(20, 9, 9)Object ClassFeature
Maps LayerPooling LayerObject 2DFeature Maps LayerPooling LayerGoogle’s
speech recognition system [5], which is domain- independent and
optimized for web-searches and dictation the tasks and employs a
post-processing step to restrict produced text hypotheses to the given
scenario. A list of possible sentences the user could utter is provided
to the system, containing sentences like “this is the box”, “the box is
left to the cup”, “show me the banana”, etc. The text hypotheses from
Google are converted to phoneme sequences by consulting a pre-trained
grapheme-to-phoneme converter [6], as the pronunciation of the
hypotheses may be similar to the given set of expectable sentences while
the grapheme representation is not. To measure the distance between the
Google hypotheses (we take the 10-best) and the expectable sentences, we
employ the Levenshtein distance [7]:

leva,b(i, j) =

  

max(i, j)  

min

if min(i, j) = 0

leva,b(i − 1, j) + 1 leva,b(i, j − 1) + 1 leva,b(i − 1, j − 1) +
1(ai(cid:54)=bj )



otherwise

(1) with a being a phoneme sequence from Google’s hypothe- ses and b
    being a phoneme sequence from the predeﬁned expectable sentence
    list. We extend this distance measure by adding a conﬁdence value c
    using

c = max(0, 1 − min(levag,bs (|ag|, |bs|)/|bs|))

(2) 

with ag being the g-th phoneme sequence of Google’s hypothesis and bs
being the s-th phoneme sequence of the sentence list. We also developed
an Android application which performs the speech recognition and
replaced the PC version from before [4]. The system does not need any
training, we only provide a list of sentences. To extend the list with
new objects, we generate sentences containing these objects with a
grammar-controlled sentence generator.

C. Language Understanding Module

The language module (θ-RARes) has been adapted from previous experiments
on a neuro-inspired model for sentence comprehension using recurrent
neural networks [8] and its application to HRI [9]. This model is based
on an Echo State Network [10] (ESN) with leaky integrator neurons.

The model (see Figure 4) is trained to learn the mapping of the semantic
words (SW) (e.g. nouns, verbs) of a sentence onto the different slots
(the thematic roles: e.g. action, location) of a basic event structure
in a predicate form like action(object, location). This predicate
representation en- ables to easily integrate this model into a robotic
architecture [9].

As depicted in Figure 4, the system processes inputs as follows: from a
sentence (i.e. sequence of words) as input (upper left) the model
outputs (middle right) a predicate that can be post-processed by the
system. The output predicate can represent a command (e.g. “Show me the
banana” → show(banana)) or an information on the state of the world
(e.g. “The banana is in the middle” → middle(banana)). Before entering
the recurrent neural network, words are preprocessed. This preprocessing
transforms the sentence

The θ-RARes language module. Sentences are ﬁrst transformed Fig. 4. into
a sentence structure, i.e. all semantic words (SW) are replaced by an SW
marker. The ESN (i.e. reservoir) is given the sentence structure word by
word. Each word activates a different input unit. During training, the
connections to the readout layer are modiﬁed in order to learn the
mapping between the sentence structure and the predicate-argument
meaning. After training, the most active units are bound with the SW
kept in the SWs memory to form the resulting predicate. Figure adapted
from [9].

into a sentence structure (or grammatical construction): SW, i.e. nouns
and verbs that have to be assigned a thematic role, are replaced by the
SW item. The processing of the grammatical construction is sequential:
words are given one at a time. The ﬁnal estimation of the thematic roles
for each SW are read-out at the end of the sentence.

The model processes grammatical constructions, not sen- tences per se,
thus permitting to bind a virtually unlimited number of sentences on
these sentence structures. Based training corpus this enables to process
only on a small future sentences with currently unknown SW. Therefore,
it is also suited for modelling developmental language acquisition [11],
[12], [13]. Here are some input/output transformations that the language
module performs. For the given “input sen- tences”, we provide the
corresponding → output predicates:

• “This is the banana” → this(banana) • “Show me the banana” →
show(banana) • “The banana is in the middle” → middle(banana) • “The
banana is on the right of the cup”

→ right(banana, cup)

• “Right of the cup is the banana”

→ right(banana, cup)

In these examples one can see that the system can robustly transform
different types of sentences using a simple predi- cate representation.
The last two sentences are quite different (the order of words is
different) but the system can learn to provide an identical predicate
representation. Hereafter are two sentences implying the execution of
two consecutive actions: the order of the predicates indicates the order
in which the actions have to be performed:

• “First show me the banana and then point to the box”

→ show(banana); point(box)

• “Before pointing to the box show me the banana”

→ show(banana); point(box)

theontoandSW…Sentence:PALput (toy, left)P(A, L)Semantic words
(SWs)Recurrent NeuralNetworkSW1SW2SW3P: PredicateA: AgentL:
LocationMeaning: P (A, L)Read-outLayerInputLayeron the left put the toy
.SW1SW2SW3putlefttoySWs MemoryFunction
words(FWs)PALPALleftputtoyInactive connectionActive connectionLearnable
connectionsFixed connectionsConnectionSW: Semantic word
item(e.g. toy)thisIn the last sentence, one can see that the system
learns to produce the correct chronological order of actions, even if
the order of the SW are very different. Thus, the operations performed
by the neural network could not be reduced to a simple set of rules
based on the order of the words in the sentence. One major advantage of
this neural network language module is that no parsing grammar has to be
deﬁned a priori: the system learns only from the examples given in the
data. Another interesting aspect of the system is that the language
model does not need to be retrained, it is only trained once during the
initialization. Moreover, testing a new sentence is computationally fast
(linear to the number of words in the sentence). The language module
architec- ture is ﬂexible enough to allow incremental learning (some
preliminary work has been done in this direction [12]) thus enabling the
system to learn during short to long periods of execution. The module
has other capabilities such as learning several languages at the same
time, and correctly processing sentences with out-of-vocabulary words
[13]. From the HRI point of view, the aim of using this neural network
based model is to gain adaptability because the system is trained on
examples (no need to predeﬁne a parser for each language), to be able to
process natural language sentences instead of stereotypical sentences
(i.e. ”put cup left”), and to be able to generalize to unknown sentences
(not in the training data set). Moreover, this model seems quite ﬂexible
when changing the output predicate representations [8]. From the
computational neuroscience and developmental robotics point of view, the
aim of having an architecture working with robots is to use them to
model and test hypotheses about child learning processes of language
acquisition [11]. Another beneﬁt of using an ESN-based model is the
constant execution time so that even long sentences can be processed in
real-time.

D. Knowledge Representation and Database

there is a map projecting object

For the given scenario, the knowledge representation of the system is an
abstract representation of the table. For this, the table is separated
into three positions, called left, right and middle, which is seen from
the robot’s intrinsic perspective. For the given scenario, the table
representation looks like this: table = (l, m, r) where l, m, r ∈ P with
P = {“banana”, “box”, “cup”, “unknown”, “empty”}. Also, labels to object
indices: t → i with i ∈ {−1, 0, 1, 2} and t ∈ T and T = P  {“empty”}.
This map is ﬁlled by teaching the robot and is empty in the beginning.
The representation (“banana”, “unknown”, “empty”) means that there is a
banana in the left position, a yet unknown object in the middle and no
object in the right position. The mapping “banana” → 0 means, that the
object identiﬁed with index 0 by the vision module was mapped to the
label “banana”.

E. Inference Module

To ensure that the knowledge database is always kept consistent, we
propose a rule-based inference module which is able to identify and
reject inconsistent information coming from the sensory modules while
accepting valid inputs. I.e.

the system receives the following information from the vision module:
(−1, 0, 1), the module converts this information to a table
representation by converting the index −1 to “empty” and replacing the
other indices by the labels it has in the database (in this case it has
”banana” → 0) and the rest with the “unknown” tag which results in the
following table representation: (“empty”, “banana”, “unknown”). In this
case, it receives the predicate right(banana, box), and evaluates the
validity of the predicate by determining the po- sition of the reference
object (banana), afterwards identifying the referred relative position
(the one right to the banana). Then, the module checks if there is an
“unknown” object in this position; if this is the case, it learns the
projection “box” → 1 and will label index 1 with the label “box” in the
future. The new table representation looks like this: (“empty”,
“banana”, “box”). As this mapping is indepen- dent from the position of
the objects, an object identiﬁed as 1 will always be labelled as a “box”
even if it is detected in a different position of the table in the
future. If, i.e. the system receives the information that the object in
the middle is a cup, and the table representation derived from the
vision module looks like this: (“banana”, “empty”, “unknown”), the
inference module will reject the information as there is no object
located in the middle of the table. This information can be used to
provide feedback to the user and encourage him to change the instruction
he uttered to the system. The following list of cases among others can
be identiﬁed and converted to useful feedback to the user.

• middle(cup), (“banana”, “empty”, “unknown”)

→ ”no object in position”

• middle(banana), (“banana”, “unknown”, “empty”)

→ ”banana already known”

• this(banana), (“unknown”, “unknown”, “empty”)

→ ”multiple unknown objects”

• right(cup, box), (“banana”, “cup”, “empty”)

→ ”object known as banana”

• right(banana, box), (“banana”, “cup”, “empty”)

→ ”described position outside of table”

• lef t(banana, box), (“unknown”, “cup”, “empty”)

→ ”reference object unknown”

• show(banana), (“unknown”, “cup”, “empty”)

→ ”no banana on table”

The cases of inconsistency are then transformed to natural language,
i.e. “I know what a banana is but I cannot see one on the table” or “I
do not know what a cup is, you have to teach me ﬁrst”. Also, the
feedback system is supported by arm motion, as the robot is pointing to
the position the new object should be located in, to help the user to
correct errors he/she made by confusing his/her intrinsic perspective of
the robot with his intrinsic perspective. Also, the infer- ence module
is able to perform mutual exclusion. For the given situation this(cup),
(“banana”, “box”, “unknown”), it identiﬁes the unknown object correctly
and maps the label “cup” to it, as this is the only object that could be
meant. Also, the module is able to perform two commands in a row, as
pointing to one object ﬁrst and to another one afterwards.

IV. STUDY DESIGN AND EXPERIMENTS

To measure the degree of usefulness of the feedback provided by our
inference module, we performed a user study with 12 participants, who
had to perform four tasks using the system. The following list contains
the table situation and the task:

1)  (“empty”,“box”,“banana”) - “Teach the robot what a

banana is and make it show it to you.”

2)  (“cup”,“box”,“empty”) - “Teach the robot what a cup and a box are
    and make it ﬁrst show you the cup and then show you the box.”

3)  (“empty”,“banana”,“cup”) - “Teach the robot what a

cup is and make it show it to you.”

4)  (“box”,“banana”,“empty”) - “Teach the robot what a box and a banana
    are and make it ﬁrst show you the box and then show you the banana.”

The participants had 5 minutes time per task and were restricted to use
a ﬁxed list of commands:

• show me the dog • this is the dog • the dog is left of the cat • the
dog is right of the cat • the dog is in the middle • ﬁrst show me the
dog, then show me the cat

The words cat and dog were placeholders for the object names, to reduce
the bias.

A. Study Design - Degree of Usefulness

As mentioned in section III-E, the inference module is able to detect
inconsistent input and provide user feedback. To test the degree of
usefulness, we divided the participant group into two subgroups. Two
different variants of our system were used, the verbose (V) system,
which provides feedback to the user and the non-verbose (NV) variant
which only provides information if a given command was accepted or
rejected. The V group performed task 1 and 2 with the V system, task 3
and 4 with the NV system. The NV group had to use the NV system for task
1 and 2 and the V system for task 3 and 4. Both groups had to use both
systems (V and NV) to show that participants using the NV variant ﬁrst
are still able to learn how to use the system faster when using the V
variant. Also, the V group did not receive feedback in task 3 and 4 to
show that they already learned how to use the system in the previous
tasks. We measured the number of commands and the time needed to perform
each task. Additionally, we provided a survey containing questions which
could be answered by giving numbers between 1 and 10, i.e. “The feedback
of the verbose system helped me to faster understand how to instruct the
robot.” “1: not at all” “10: deﬁnitely”

B. Experiment - System Performance

To be able to provide feedback for inconsistent inputs, an input has to
be classiﬁed correctly, even if the outputs cannot be used to add new
information to the knowledge base or perform a desired action.
Accordingly, we measured the

performance of all sensory modules, the inference module and also the
user. The performance was measured while the participants used the
system to perform their tasks.

V. RESULTS

The participants needed 354 commands in total to perform the tasks. The
speech module achieved an accuracy of 92.4%, together with the language
module, which only failed when the speech module produced an incorrect
hypothesis. The vision module attained a classiﬁcation accuracy of
87.3%, which results only of not recognizing an object at all, but never
confusing different objects. The accuracy of the multi- modal
integration is 80.8%, which means that vision, speech and language
module performed a correct classiﬁcation. 52.5% of the instructions
produced by the user were incon- sistent with the context, and could be
identiﬁed as this by the inference module. For all inconsistent cases,
the correct feedback could be created by the inference module, although
it only got provided to the user in the V variant of the system. 75% of
the users confused the perspective they had to use to describe the
relative position of an object, which means, that an object is i.e. left
of another object from the intrinsic perspective of the robot. The
results in Fig. 5 show that the V group needed less instructions and
time for task 1 and 2, which is also supported by a one-tailed, paired
t-test, with p = 0.029 for time and p = 0.011 for instructions. Also,
the V group needed signiﬁcantly less instructions (p = 0.001) and time
(p = 0.005) for all tasks.

Fig. 5. The graphs show the performance of the users using the verbose
and non-verbose system ﬁrst. The average number of instruction (right)
and the average time needed (left) to perform each task was measured.

VI. DISCUSSION

It took the participants some time to understand that there are relative
(left, right) and absolute (middle) descriptions of object locations.
The only way to perform a task was ﬁrst to teach the object in the
middle, as this is the only absolute description possible using the
provided commands, then relatively describing the object left or right
to it. Participants often tried to refer to the object in the middle,
which was not working, as the robot does not know any object labels in
the beginning. Another difﬁculty was the correct usage of “left of” and
“right of”, where the participants described the object from the wrong
perspective. By providing motion feedback and pointing to the empty
position, the participants immediately learned that they have to
describe the scene from the robot’s intrinsic perspective. The feedback
helped the user in all cases to understand immediately how to use

Task1234Seconds0100200300Verbose
FirstNon-VerboseFirstTask1234Instructions
needed051015Verbose-FirstNon-VerboseFirstOptimumrelative descriptions,
while for the NV variant it took the user more trials to ﬁnd out the
correct perspective. The results show that participants using the V
variant ﬁrst, learned faster to use the system correctly and needed less
instructions to perform the task. The minimum possible number of
instructions needed to complete a task was 3, and while performing task
3, the V group already had learned how to use the system and did not
need feedback anymore, while the NV group, which started to use the V
variant in task 3, could use the feedback to perform the task quickly
and with less instructions. The survey revealed that all participants
found the feedback system useful in some way and determined an improved
usability. They also noted that they were able to understand how to use
the robot faster in most cases.

VII. CONCLUSION

The performance measured in the user study clearly shows that the system
can be used in a real-world robot scenario. The results measured by the
feedback module justify the development and integration of a feedback
providing infer- ence module. Instead of only providing information
about success or failure of processing an instruction, the user can
learn implicitly to use the system correctly by interacting with the
robot in a natural dialog. The results of the survey emphasize the value
of the given module and we suggest to include a feedback provider in
other HRI scenarios, too. Also, the user could be encouraged to change
his/her strategy by providing detailed feedback much faster than by only
providing feedback about error or success of a given instruction. This
behaviour was supported by the fact that no error occurred during the
recognition but the user provided inconsistent input in a given
situation. Our ﬁndings support the hypothesis of Vollmer et al. [1] (see
Sec. I) that feedback coming from the robot directly inﬂuences the
future way of teaching of the user, and also indicate that this
hypothesis is true for multi-modal scenarios with possibly inconsistent
inputs. Also, it is important to produce correct outputs for
inconsistent input to be able to provide feedback, as 52.5% of the
instructions provided by the user were incorrect. The feedback module
could provide correct feedback in all cases if the inputs were classiﬁed
correctly (80.8%), which shows that a high classiﬁcation accuracy
directly inﬂuences the feedback, which then inﬂuences the performance of
the user. Due to the fact that all modules are loosely coupled, it is
easy to extend the system with other objects, other tags or new phrase
structures and predicates. At the moment, the system is not able to
perform online learning in the vision module, it is only able to
recognize pre-trained objects, and only the names are attached to them
by the learning process. If new objects are added, only the vision
module has to be retrained. To add new labels, new sentences containing
new labels have to be created for the speech module, which can be
performed easily by a grammar generator. As the language modules only
relies on the phrase structure and is independent from the entities
itself, it only has to be retrained when new phrase structures have to
be detected. Also, the

system could be used in a more complex scenario using continuous instead
of discrete location representations. For this, the knowledge
representation and the inference module have to be adjusted, which can
be done easily. We plan to extend the system to process completely free
speech, by using an n-gram-based post-processing model which allows
unknown words [4]. Also, the vision model could be used to train the new
object representations online. We also plan to employ the speech and
language module for a different language, which was already shown by
Hinaut et al. [13]. To illustrate how the robot interaction works a
video can be seen at https://youtu.be/FpYDco3ZgkU [14] with additional
information on the different modules.

ACKNOWLEDGMENT

We thank Francisco Cruz and Nilay Avs¸ar for their help

in the study and all participants who took part.

REFERENCES

[1] A.-L. Vollmer, M. M¨uhlig, J. J. Steil, K. Pitsch, J. Fritsch, K. J.
Rohlﬁng, and B. Wrede. Robots show us how to teach them: Feedback from
robots shapes tutoring behavior during action learning. PloS one,
9(3):e91349, 2014.

[2] M. B. Soares, P. Barros, G. I. Parisi, and S. Wermter. Learning
Objects from RGB-D Sensors Using Point Cloud-based Neural Networks. In
ESANN 2015, pages 439–444, 2015.

[3] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based
learn- ing applied to document recognition. Proc. of the IEEE,
86(11):2278– 2324, 1998.

[4] J. Twiefel, T. Baumann, S. Heinrich, and S. Wermter.

Improving domain-independent cloud-based speech recognition with domain-
dependent phonetic post-processing. In Twenty-Eighth AAAI. Qu´ebec City,
Canada, pages 1529–1535, 2014.

[5] J. Schalkwyk, D. Beeferman, F. Beaufays, B. Byrne, C. Chelba, M.
Cohen, M. Kamvar, and B. Strope. “your word is my command”: In Advances
in Speech Google search by voice: A case study. Recognition, pages
61–90. Springer, 2010.

[6] M. Bisani and H. Ney.

Joint-sequence models for grapheme-to- phoneme conversion. Speech
Communication, 50(5):434–451, 2008. [7] V. I. Levenshtein. Binary codes
capable of correcting deletions, insertions, and reversals. Soviet
Physics – Doklady, 10(8):707–710, 2 1966.

[8] X. Hinaut and P. F. Dominey. Real-time parallel processing of
grammatical structure in the fronto-striatal system: a recurrent network
simulation study using reservoir computing. PloS one, 8(2):e52946, 2013.

[9] X. Hinaut, M. Petit, G. Pointeau, and P. F. Dominey. Exploring the
acquisition and production of grammatical constructions through
Frontiers in human-robot interaction with echo state networks.
Neurorobotics, 8, 2014.

[10] H. Jaeger. The “echo state” approach to analysing and training
recurrent neural networks-with an erratum note. Bonn, Germany: German
National Research Center for Information Technology GMD Technical
Report, 148:34, 2001.

[11] M. Tomasello. Constructing a language: A usage based approach to
language acquisition. Cambridge, MA: Harvard University Press, 2003.

[12] X. Hinaut and S. Wermter. An incremental approach to language In
acquisition: Thematic role assignment with echo state networks. ICANN
2014, pages 33–40. Springer, 2014.

[13] X. Hinaut, J. Twiefel, M. Petit, P. Dominey, and S. Wermter. A
recurrent neural network for multiple language acquisition: Starting In
NIPS 2015 Workshop on Cognitive with english and french. Computation:
Integrating Neural and Symbolic Approaches, 2015.

[14] X. Hinaut, J. Twiefel, M. Borghetti Soares, P. Barros, L. Mici, and
S. Wermter. Humanoidly speaking – learning about the world and language
with a humanoid friendly robot. In Video competition, IJCAI, Buenos
Aires, Argentina. https://youtu.be/FpYDco3ZgkU, 2015.


