ReScience C: A Journal for Reproducible Replications in
Computational Science
Nicolas P. Rougier, Konrad Hinsen

To cite this version:

Nicolas P. Rougier, Konrad Hinsen. ReScience C: A Journal for Reproducible Replications in Com-
putational Science. RRPR 2018 - 2nd International Workshop on Reproducible Research in Pattern
Recognition, Aug 2018, Pékin, China. pp.150-156, ￿10.1007/978-3-030-23987-9\_14￿. ￿hal-02199854￿

HAL Id: hal-02199854

https://inria.hal.science/hal-02199854

Submitted on 31 Jul 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

ReScience C: a Journal for Reproducible
Replications in Computational Science

Nicolas P. Rougier1,2,3[0000−0002−6972−589X]
and Konrad Hinsen4,5[0000−0003−0330−9428]

1 INRIA Bordeaux Sud-Ouest, Talence, France
Nicolas.Rougier@inria.fr
2 Institut des Maladies Neurod´eg´en´eratives, Universit´e de Bordeaux,
CNRS UMR 5293, Bordeaux, France
3 LaBRI, Universit´e de Bordeaux, Bordeaux INP, CNRS UMR 5800, Talence, France
4 Centre de Biophysique Mol´eculaire, CNRS UPR 4301, Orl´eans, France
Konrad.Hinsen@cnrs.fr
5 Synchrotron SOLEIL, Division Exp´eriences, Gif sur Yvette, France

Abstract. Independent replication is one of the most powerful methods
to verify published scientiﬁc studies. In computational science, it requires
the reimplementation of the methods described in the original article by
a diﬀerent team of researchers. Replication is often performed by scien-
tists who wish to gain a better understanding of a published method,
but its results are rarely made public. ReScience C is a peer-reviewed
journal dedicated to the publication of high-quality computational repli-
cations that provide added value to the scientiﬁc community. To this end,
ReScience C requires replications to be reproducible and implemented
using Open Source languages and libraries. In this article, we provide
an overview of ReScience C’s goals and quality standards, outline the
submission and reviewing processes, and summarize the experience of
its ﬁrst three years of operation, concluding with an outlook towards
evolutions envisaged for the near future.

Keywords: Open science · Computational science · Reproducibility

1

Introduction

The question of how to attain reliable outcomes from unreliable components
pervades many aspects of life. Scientiﬁc research is no exception. Individual re-
search contributions are prone to mistakes, and sometimes fraud, and therefore
error detection and correction mechanisms are required to reach a higher level of
reliability at the collective level. The two main methods for error detection are
critical inspection, starting with peer review of article submissions but contin-
uing well after publication, and independent replication of published work. But
replication is more than a veriﬁcation technique. For the researchers performing
the replication, it yields a level of understanding and insight that is impossible to
achieve by other means. This is in fact the main motivation for much replication
work, veriﬁcation being merely a side eﬀect.

2

N.P. Rougier and K. Hinsen

The power but also the limitations of replication as an approach to veriﬁca-
tion are best illustrated by the recent discussion of replication crises in various
scientiﬁc domains [4, 2, 6, 5], which are all based on the observation of frequent
failures to replicate published scientiﬁc ﬁndings. However, a replication failure
does not necessarily mean that the original study is ﬂawed. First of all, it could
well be the replication work that is at fault. But it is also possible that both the
original and the replication work are of excellent quality and yet yield diﬀerent
conclusions, if some important factor has escaped everyone’s attention and ac-
cidentally diﬀers between the two studies (see [9, 7] for a recent example that
led to a seven-year search for the cause of the disagreement). In this situation,
independent replication can become the starting point of completely new lines
of research.

Replication is thus an important contribution to science, and its ﬁndings
should be shared with the scientiﬁc community. Unfortunately, most journals do
not accept replication studies for publication because originality is one of their
selection criteria. For this reason, we launched ReScience in 2015 (now called
ReScience C for reasons explained later) as a journal dedicated to replications
of computational research. In this article, we outline its mode of operation and
summarize our experience from the ﬁrst few years. A more complete account,
also containing more background references, has been published recently [8].

2 Terminology: reproducible replications

The replication crisis has given rise to an active debate in various domains of
science, in which some terms, in particular “reproducible” and “replicable”, are
used with very diﬀerent meanings. We therefore explain the deﬁnitions that we
are using in this article and more generally in ReScience C. Our deﬁnitions are
formulated in the speciﬁc context of computational science, and are not easily
transferable to experimental science [3].

A computation is reproducible if the code and input data is available to-
gether with suﬃcient instructions for someone else to re-do or reproduce the
computation. The only point in reproducing the computation is to verify its re-
producibility, which in turn is evidence that the archived code and data is (1)
complete and (2) indeed the code and data that was used in the original pub-
lished study. A failed reproduction means that the description of the original
code and data is incomplete or inaccurate. A frequent form of incompleteness
is the lack of a detailed description of the computational environment, i.e. the
infrastructure software (operating system, compiler, ...) or code dependencies (li-
braries, ...) that were used in the original work. Reproducible computations are
the most detailed and accurate possible description of a computational method
within the current state of the art of computational science.

A replication of computational work involves writing and then running new
software, using only the description of a method published in a journal article,
i.e. without using or consulting the software used by the original authors, which
may or may not be available. Successful replication conﬁrms that the method

The ReScience C Journal

3

description is complete and accurate, and signiﬁcantly reduces the probability
of an error in either implementation. A replication failure can be caused by
such errors or by an inexact or incomplete method description. It requires fur-
ther investigation which, as explained above, can even lead to new directions of
scientiﬁc inquiry.

A reproducible replication is a replication whose code and data has been
archived and documented for reproducibility. It is especially useful in the still
dominant situation that the target of the replication was not published repro-
ducibly. In that case, the replication provides not only veriﬁcation, but also the
missing code and data.

3 ReScience C

The deﬁnition of a replication given above should be suﬃcient to show that
performing replications is a useful activity for a researcher. Moreover, whether
successful or not, a replication yields additional insight into the problem that are
worth sharing with the scientiﬁc community. For example, minor omissions or
inaccuracies are inevitable in the narratives that make up for most of a journal
article, meaning that replication authors have to do some detective work whose
results are of use to others.

Unfortunately, the vast majority of scientiﬁc journals would not consider
such work for publication, with the possible exception of a failed replication of
particularly important ﬁndings, because novelty is for them an important selec-
tion criterion. Moreover, the reviewing process of traditional scientiﬁc journals,
designed in the 20th century for experimental and theoretical but not for com-
putational work, cannot handle the technical challenges posed by a veriﬁcation
of reproducibility and successful replication. For these reasons we created the
ReScience C journal (at the time called simply ReScience) in September 2015 as
a state-of-the-art venue for the publication of reproducible replication studies in
computational science.

The criteria that a submission must fulﬁll for acceptance by ReScience C are

the following:

– It must aim at reproducing all or a signiﬁcant part of the ﬁgures and tables

in an already published scientiﬁc study.

– The text of the article must discuss which results were successfully replicated
and which, if any, could not be replicated. It should also provide a description
of problems that were encountered, e.g. additional assumptions that had to
be made.

– The complete source code of the software used for the replication must be
provided, and should have only Open Source software as dependencies in
order to allow full inspection of the complete software stack.

– In order to ensure the independence of the replication, its authors should
not include any authors of the original study, nor their close collaborators.

A newly submitted replication is assigned to a member of the editorial
board, which at this time is composed of 12 scientists from diﬀerent research

4

N.P. Rougier and K. Hinsen

domains. The handling editor recruits two reviewers from a pool of currently
nearly 100 volunteers. The reviewing process consists of a dialog between the
reviewers, the authors, and the handling editor whose goal is to improve the
submission to the point that it can be accepted. In particular, the reviewers
verify that they can reproduce the results from the supplied code and data, and
judge if the replication claims made by the authors are valid subject to the cri-
teria of their scientiﬁc domain. The entire reviewing process is openly conducted
on the GitHub platform, meaning that contributions are open to read for any-
one, and anyone with a GitHub account can participate by leaving a comment.
Once the submission is deemed acceptable, it is added to the table of contents
and to the ReScience archive, with links to the submission repository, the re-
view, and a PDF version which permits the article to handled like a standard
scientiﬁc paper in personal and institutional databases and bibliography man-
agement software. An additional copy is deposited on Zenodo [1], which, being
an archiving platform, makes stronger promises about long-term preservation
than GitHub, whose primary goal is to support dynamic development processes.
An additional advantage is that Zenodo issues a DOI that serves as a persistent
reference.

The outstanding feature of this reviewing process, even compared to other
journals practicing open peer review, is the rapid interaction between reviewers
and authors that does not require the constant intervention of the handling
editor. This rapid exchange has turned out to be essential in the quick resolution
of the technical issues that inevitably arise when dealing with software and data.
Another outstanding feature of ReScience C is its reliance on no other in-
frastructure than two digital platforms, GitHub and Zenodo, which are both
free to use. Considering that editors and reviewers as well as authors are unpaid
volunteers, this means that ReScience C has so far been able to operate without
any budget at all, and thereby avoid being subjected to any political pressure.
We note however that this may not always be true for the individual volunteers
contributing to ReScience C because the open reviewing process provides no
anonymity. It is therefore imaginable that authors or reviewers of a ReScience
article pointing out a mistake in prior work by an inﬂuential scientist could be
exposed to sanctions by that scientist in grant or tenure reviews.

4 Learning from the past to prepare the future

After three years of operation, our original ideas for ReScience C have turned
into concrete practical experience which has mostly conﬁrmed our expectations.
It has also shown a few weaknesses, most of which concern technical details,
which we are currently addressing in an overhaul of the ReScience C publishing
workﬂow. In the following we summarize this experience and the conclusions we
have drawn from it, referring to the full account [8] for the details.

ReScience C has so far published 27 articles. Most submissions are from
computational neuroscience, the other represented domains are neuroimaging,
computational ecology, and computer graphics. No submission was ever rejected.

The ReScience C Journal

5

All submitted replications were successful, but this is probably due to a selection
bias: publishing a failed replication is equivalent to publicly accusing the authors
of the target work of having made a mistake, which is a potential source of
conﬂict. One idea we have put forward to alleviate this obstacle is pre-publication
replication. In that scenario, researchers submit their original work to a new type
of journal, for which we use the name CoScience to indicate that we imagine it
as the successor of ReScience. The journal then invites other scientists to do a
replication, and publishes the original work and the replication together as a
single joint work by the original authors and the replication team.

Achieving reproducibility has been much more challenging than expected. It
is the reviewers’ task to verify reproducibility, but our experience has shown that
this is not suﬃcient to ensure that someone else can reproduce the work as well.
Reviewers typically work in the same ﬁeld as the authors and are likely to have
similar software installation on their computers, meaning that unlisted depen-
dencies can easily go unnoticed. There are a few approaches that would improve
reproducibility, but each has its downsides as well. IPOL [?] provides online ex-
ecution via its Web site, which is extremely convenient for both reviewers and
readers. However, it is feasible only because IPOL’s narrow domain scope (signal
processing) makes the restriction to a small number of computational environ-
ments (C/C++, Python, Matlab) acceptable. We could also impose higher tech-
nical reproducibility guarantees on authors, e.g. the submission of an archived
environment in the form of a virtual machine or a container image, which would
also open the door to online execution via services such as Binder [?]. Such
a requirement might, however, also become an additional barrier discouraging
researchers from publishing their replication work.

The open reviewing process has overall worked very well. The exchanges
between reviewers and authors have been constructive and courteous without
exception. The handling editor intervenes mainly at the beginning, by inviting
reviewers, and at the end, by judging if the reviewers’ feedback is satisfactory for
publication. Occasionally, reviewers or authors ask the handling editor for help
with speciﬁc, mostly technical, issues. Another common task for the handling
editor is to gently nudge authors or reviewers towards completing their tasks
within reasonable delays. It is rare for third parties to intervene, but in one case
a reviewer suggested asking the author of the target study for the permission to
re-use some data, which he did by commenting directly on the GitHub platform.
An unexpected and so far unresolved consequence of the open reviewing
process is the impossibility to handle replications that process conﬁdential data.
In some ﬁelds of science, conﬁdentiality is inevitable, be it for ethical reasons
(e.g. in medical research) or for commercial ones (e.g. data on stock market
transactions not freely available). This is an issue of wider concern for the Open
Science community, and we hope that satisfactory solutions will emerge in the
near future.

The use of the GitHub platform has turned out to be a good choice over-
all. Since a ReScience C submission combines a narrative and source code, with
the code taking center stage during the reviewing process, a platform designed

6

N.P. Rougier and K. Hinsen

for collaborative software development and code reviewing is a better match
than the traditional manuscript management platforms used by scientiﬁc jour-
nals, which have no provision at all for reviewing code. We are, however, cur-
rently revising several technical details. Submissions currently take the form of
a pull request to the ReScience repository, which is counterintuitive for an ar-
ticle submission. More importantly, the ﬁnal steps of publishing in our current
workﬂow are laborious and not automated, causing too much hassle mainly for
the handling editor. In the future workﬂow, articles are submitted as individual
repositories of which ReScience retains a fork upon acceptance.

Finally, an evolution that has motivated the name change from ReScience to
ReScience C is the imminent launch of ReScience X, a new journal dedicated to
replications of experimental research, under the auspices of Etienne Roesch and
Nicolas Rougier. We hope that it will be able to proﬁt from the experience gained
with ReScience C, although the challenges it will face are of a quite diﬀerent na-
ture. ReScience C will continue to focus on improving computational research,
joining forces with the wider Reproducible Research community wherever pos-
sible. For example, we envisage proposing the publication of dedicated issues
to reproducibility-related workshops such as the Reproducible Research on Pat-
tern Recognition workshop [?] (part of the International Conference on Pattern
Recognition) or the Enabling Reproducibility in Machine Learning workshop
(part of the Internatiponal Conference on Machine Learning).

References

1. Zenodo. http://www.zenodo.org/
2. Baker, M.: 1,500 scientists lift the lid on reproducibility. Nature 533(7604), 452–454

(May 2016). https://doi.org/10.1038/533452a

3. Hinsen, K.:

Scientiﬁc

software

is

diﬀerent

from lab

equipment.

http://blog.khinsen.net/posts/2018/05/07/scientiﬁc-software-is-diﬀerent-from-
lab-equipment/ (May 2018)

4. Ioannidis, J.P.A.: Why Most Published Research Findings Are False. Plos Med 2(8),

e124 (2005). https://doi.org/10.1371/journal.pmed.0020124

5. Iqbal,

S.A., Wallach,

J.D., Khoury, M.J.,

J.P.A.: Reproducible Research Practices
Literature. PLOS Biology
Biomedical
https://doi.org/10.1371/journal.pbio.1002333

Schully,
and Transparency
14(1),

e1002333

S.D.,

across

Ioannidis,
the
2016).

(Jan

6. Munaf`o, M.R., Nosek, B.A., Bishop, D.V.M., Button, K.S., Chambers, C.D., Percie
du Sert, N., Simonsohn, U., Wagenmakers, E.J., Ware, J.J., Ioannidis, J.P.A.: A
manifesto for reproducible science. Nature Human Behaviour 1(1), 0021 (Jan 2017).
https://doi.org/10.1038/s41562-016-0021

7. Palmer, J.C., Haji-Akbari, A., Singh, R.S., Martelli, F., Car, R., Panagiotopoulos,
A.Z., Debenedetti, P.G.: Comment on “The putative liquid-liquid transition is a
liquid-solid transition in atomistic models of water” [I and II: J. Chem. Phys. 135,
134503 (2011); J. Chem. Phys. 138, 214504 (2013)]. The Journal of Chemical Physics
148(13), 137101 (Apr 2018). https://doi.org/10.1063/1.5029463

8. Rougier, N.P., Hinsen, K., Alexandre, F., Arildsen, T., Barba, L.A., Benureau, F.C.,
Brown, C.T., de Buyl, P., Caglayan, O., Davison, A.P., Delsuc, M.A., Detorakis,

The ReScience C Journal

7

G., Diem, A.K., Drix, D., Enel, P., Girard, B., Guest, O., Hall, M.G., Henriques,
R.N., Hinaut, X., Jaron, K.S., Khamassi, M., Klein, A., Manninen, T., Marchesi,
P., McGlinn, D., Metzner, C., Petchey, O., Plesser, H.E., Poisot, T., Ram, K., Ram,
Y., Roesch, E., Rossant, C., Rostami, V., Shifman, A., Stachelek, J., Stimberg, M.,
Stollmeier, F., Vaggi, F., Viejo, G., Vitay, J., Vostinar, A.E., Yurchak, R., Zito,
T.: Sustainable computational science: The ReScience initiative. PeerJ Computer
Science 3, e142 (Dec 2017). https://doi.org/10.7717/peerj-cs.142

9. Smart, A.G.: The war over supercooled water. Physics Today (Aug 2018).

https://doi.org/10.1063/PT.6.1.20180822a

