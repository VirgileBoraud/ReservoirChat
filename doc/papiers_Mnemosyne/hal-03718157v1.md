Multi-task learning with modular reinforcement learning
Jianyong Xue, Frédéric Alexandre

To cite this version:

Jianyong Xue, Frédéric Alexandre. Multi-task learning with modular reinforcement learning. SAB
2022 - 16th International Conference on the Simulation of Adaptive Behavior, Sep 2022, Cergy-
Pontoise / Virtual, France. ￿hal-03718157￿

HAL Id: hal-03718157

https://inria.hal.science/hal-03718157

Submitted on 8 Jul 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Multi-task learning with modular reinforcement
learning

Jianyong Xue1,2,3[0000−0003−3245−8852] and Frédéric
Alexandre1,2,3[0000−0002−6113−1878]

1 Inria Bordeaux Sud-Ouest, 33405 Talence, France
2 LaBRI, Université de Bordeaux, Bordeaux INP
CNRS, UMR 5800, Talence, France
3 Institut des Maladies Neurodégénératives, Université de Bordeaux
CNRS, UMR 5293, Bordeaux, France
{jianyong.xue, frederic.alexandre}@inria.fr

Abstract. The ability to learn compositional strategies in multi-task
learning and to exert them appropriately is crucial to the development
of artificial intelligence. However, there exist several challenges: (i) how
to maintain the independence of modules in learning their own sub-tasks;
(ii) how to avoid performance degradation in situations where modules’
reward scales are incompatible; (iii) how to find the optimal composite
policy for the entire set of tasks. In this paper, we introduce a Modular
Reinforcement Learning (MRL) framework that coordinates the competi-
tion and the cooperation between separate modules. A selective update
mechanism enables the learning system to align incomparable reward
scales in different modules. Furthermore, the learning system follows a
“joint policy” to calculate actions’ preferences combined with their re-
sponsibility for the current task. We evaluate the effectiveness of our
approach on a classic food-gathering and predator-avoidance task. Re-
sults show that our approach has better performance than previous MRL
methods in learning separate strategies for sub-tasks, is robust to mod-
ules with incomparable reward scales, and maintains the independence
of the learning in each module.

Keywords: Multi-task learning · Modular reinforcement learning · In-
comparable reward scale · Compositionality policy · Model-based rein-
forcement learning.

1

Introduction

Multi-task learning is popularly observed in humans and several other intelli-
gent animal species, and the ability to tackle with diverse tasks has gradually
increased with the enrichment of skills that are learned from experience. For
example, in a wild natural environment, most animals must pursue two tasks
simultaneously: look for food and avoid predators. Meanwhile, long-time evolu-
tion has allowed animals to generate a more sensitive olfactory system to find
food rapidly and to be more alert to predators, as well as to run faster.

2

J. Xue and F. Alexandre

A hallmark of multi-task learning benefits from the reusage of similar patterns
underlying a set of regularities across tasks, in order to improve performance on
any other single task [18,15]. The goal of multi-task learning is to find an optimal
solution for solving the entire set of tasks in parallel. However, there exist several
challenges: (i) how to maintain the independence of modules in learning their
own sub-tasks; (ii) how to avoid performance degradation in situations where
modules’ reward scales are incompatible; (iii) how to find the optimal composite
policy for the entire set of tasks.

These challenges have inspired the emergence of modular reinforcement learn-
ing (MRL) approaches, which decompose a multi-task problem into a collection
of concurrently running RL modules, each of which learns a separate policy
(or sub-policy) to solve a portion of the original problem [13,6]. Accordingly,
these joint modules implicitly form a larger composite RL problem, the goal
is to find the optimal policy for this composite RL [16]. Considering the per-
formance degradation in the composability of modules that have incomparable
reward scales, [13] introduced a special module named "command arbitrator",
and proposed the architecture Arbi-Q to reformulate the MRL. Specifically, the
arbitrator’s policy assigns modular preferences given the observation of the state,
then the selected module’s preferred action will be used to interact with the en-
vironment. After each interaction, the state-abstraction function in the selected
module transforms the world observation into a module-specific subset of the
world states, in which modules are associated with the world that they operated
in and not coupled to other modules or to an arbitrator.

Nevertheless, the Arbi-Q model has the following limitations: (i) all the mod-
ules need to be evaluated at each time-step without considering prior knowledge
of modular selection, which slows down the learning process and the decision-
making; (ii) learning system’s action preference always comes from one single
module, which may lead to dictatorship in decision-making; (iii) function ap-
proximation needs to be improved for complex continuous tasks (or the complex
hybrid of discrete and continuous tasks).

Focused on these limitations, in this article, we adopt the idea of multiple
model-based reinforcement learning from [3] and present a new MRL architec-
ture named "Inverse Arbi-Q" to coordinate the competition and the cooperation
between separate modules. A selective update mechanism enables the learning
system to align incomparable reward scales in different modules. Furthermore,
the learning system follows a “joint policy” to calculate actions’ preferences com-
bined with their responsibility for the current task.

2 Related work

Growing evidence from behavioral and biological neuroscience notes that most
animals’ brain parses ongoing behavior into discrete, bound segments [5]. Specif-
ically, internal models in separated areas of the cerebellum are responsible for
given tasks via a competitive and cooperative way [14,1]. In cases when a new
task appears, multiple models learn it through competition, but only one or a

Multi-task learning with modular reinforcement learning

3

small part of them will be recruited for this task. Meanwhile, human behaviors
are hierarchically organized: actions cohere into sub-task sequences, which fit
together to achieve overall task goals [8,2].

In order to scale up the RL to address multiple goals at once, an intuitive
approach is to train one RL module to handle each of the sub-goals, then the
learned policies for each module can be used to fairly distribute control among
the modules. However, these component policies are usually sub-optimal in the
context of the composite tasks. Focusing on this problem, [16] replace the Q-
learning rule within each module with a SARSA(0) learning rule, and propose
a model of GM-Sarsa(0) for finding approximate solutions to multiple-goal RL
problems. In order to better coordinate competitions between modules, the mix-
ture of experts architecture from [11,7] introduced a learning system that is
composed of a list of "experts" network, plus a gating network that determines
which of the experts should be responsible for the training case. However, this
modular architecture relies heavily on the performance of the gating network.

An alternative family of approaches [10,3,12] employs a state predictor within
each expert (or module), which enables the environmental dynamics to be pre-
dictable. The controllers associated with these experts are decided by the predic-
tion error: the one that has the smallest prediction error will be recruited at any
particular moment. Nevertheless, models based on this idea mainly emphasize
the competition between modules to choose one single expert that is the most
suitable for a specific task (or sub-task), but the cooperation between modules
as a compositional strategy for composable complex tasks is relatively lacking.
As an improvement, [3] propose a model of multiple model-based reinforcement
learning (MMBRL). In the MMBRL, a complex task is decomposed into mul-
tiple modules, particularly, the “responsibility signal" was introduced to weight
the outputs of multiple modules, as well as to gate the learning of the prediction
models and the RL controllers. However, the parallel update strategy degrades
its performance when modules has incomparable reward scales, and the com-
posite policy will be far from optimal as the complexity of the learning problem
increases.

Generally, the original MRL supports not only the decomposition of complex
tasks into modules, but also the composability of separately learned modules as
new strategies for tasks that were never solved before [4,13]. Focusing on the
optimality of the composite strategy for the entire task and the independence of
learning in separate modules, [12] introduced the specific concept of “modular
reward", which comes from the actual reward after each interaction plus a bonus
for passing the task on a proper module. Specifically, this bonus is calculated
from the modular value function and the temporal difference in the module
gating signal, which propagates the reward toward the entire task achievement
between modules. In situations where the tasks require to perform the sub-tasks
concurrently, [4] propose an hierarchical RL approach to learn both compound
and composable policies within the same learning process, by which exploiting
the off-policy data generated by the compound policy. The results show that
the experience collected with the compound policy permits not only to solve

4

J. Xue and F. Alexandre

the complex task but also to obtain useful composable policies that successfully
perform in their corresponding sub-tasks.

Aiming at modules that have conflicting goals and misaligned reward scales,
[6] presented the framework GRACIAS that assigns fine-grained importance to
different modules and enables composable decision making based on modern
deep RL methods. More precisely, the arbitrator learns the relative importance
of each module in a given state, combined with action-value of the individual
modules to calculate the joint action preferences. However, since the number of
modules in this architecture is preset in advance, which will gradually limit its
performance when the complexity of the task continues to increase. Moreover,
when the arbitrator is trained with little prior knowledge, the performance will
be initially far from optimal.

In the present work, the idea of Inverse Arbi-Q model is trying to address all
the major concerns mentioned above. First, it’s different from [7] where experts
are determined by a gating network. Instead, modules in the present architecture
learn their own behavioral policy and solve their own sub-tasks by competition.
Second, it’s also different from [13] where the action selection follows a "joint
policy". Here action preferences come from the combination of all MBRL mod-
ules, rather than from one single preferred module. Furthermore, different from
[3] where not all modules are involved in the update procedure after each inter-
action, instead, only the most related module will be targeted for amelioration.
Moreover, the update of parameters in the selected module is gated by a dy-
namic signal, the “responsibility signal" [3,12] , which comes from the gaussian
softmax function of prediction error in each module and determines how much
the module is responsible for the current situation, rather than only according
to a fixed parameter of learning rate as in [13,4] as we will explain in section 5.1.

3 Preliminaries

3.1 Reinforcement Learning

Reinforcement Learning (RL) paradigm focuses on learning an optimal policy π∗
that enables the agent to gain the maximum accumulated rewards from future
steps [17]. At each time step t, the agent stays in state st ∈ S and follows the
current policy π(a|s) to execute an action at ∈ A; as a result, it receives an im-
mediate feedback rt+1 from the environment and transitions stochastically to the
next state st+1 ∈ S. Note that performing this policy requires either knowing the
underlying state transition model P (st+1|st, at) and reward function R(st, at),
or estimating the state value V (st) through rewards received in trajectories.

Based on differences in learning optimal policy, RL approaches could be di-
vided into two categories: (a) model-free RL, which focuses on learning a policy
or value function, and (b) model-based RL, which aims at learning a dynamic
model and a reward function. In the model-free RL approaches, policy learning
only focuses on interaction trajectories, and their parameters are mainly updated
by the TD-error between state-values within two (or several) steps. Obviously,

Multi-task learning with modular reinforcement learning

5

it requires a large amount of samples to achieve good performance, and typi-
cally only learns a single task at a time; also it usually suffers from high sample
complexity.

However, in the model-based RL system, a dynamic model is used to make
predictions of the probabilities of transition to the next state st+1, and of a re-
ward function to provide the expected values r′, given the current state st and
the action at selected by the current policy π(a|s). Model-based reinforcement
learning algorithms are generally regarded as being more sample efficient [9].
However, to achieve good sample efficiency, model-based RL algorithms conven-
tionally use relatively simple function approximators, which fails to generalize
well in complex tasks, or with probabilistic dynamics models in complicated and
high-dimensional domains.

3.2 Modular reinforcement learning

The Modular Reinforcement Learning (MRL) framework is composed of several
concurrently running RL modules, each of which learns a specific policy to solve
a simpler sub-problem of the main RL problem. In particular, each module has
access to the observation of the environment and shares a common action space
[16]. During each interaction, all modules are required to execute the same action,
while observing the state transition and a reward signal specific to the module
[13]. The goal of MRL is to learn the optimal policy for the entire problem, in
order to maximize the accumulative global reward in the long run.

Furthermore, action selection in MRL follows a “joint policy", in which action
preferences come from the combination of all (or partially related) RL controllers
in each module. At each time step, the candidate actions are evaluated by the
weighted summation of their value expectations of future rewards from the RL
controller in each module.

n
(cid:88)

Q(s, a) =

Qi(s, a)wi

(1)

i=1
where the weight wi assigned to each module indicates the responsibility of the
module in the current task.

4 The Inverse Arbi-Q architecture

The basic idea of the Inverse Arbi-Q architecture lies in the learning system
that decomposes a multi-task learning problem into a collection of RL modules,
each of which learns a separate strategy for a specific sub-task concurrently.
Figure 1 presents the overall organization of Inverse Arbi-Q architecture, which
is composed of a list of n model-based RL (MBRL) modules. Each of MBRL
modules shares a common inputs (state space and action space) and has an
identical structure that consists of three components: the reward predictor, the
RL controller and the state predictor, which provide the elementary realization
of the module for learning its own behavioral policy and its own designated
sub-task.

6

J. Xue and F. Alexandre

Fig. 1. Schematic diagram of Inverse Arbi-Q architecture.

5

Implementation

In this section, we explain the details for the implementation of Inverse Arbi-Q
architecture. Subsection 5.1 introduces the state predictor and the calculation
of responsibility signal for each module. Subsection 5.2 is about the reward
predictor, we used a specific “modular reward" [12] for rewarding the module
that is more suitable for the current sub-task, which encourages the architecture
to be more concentrated to update to modules that better fit the current task. In
subsections 5.3 and 5.4, we are more focused on the state-value function and the
state-action function in the RL controller, as well as the action selection based
on these two functions. Additionally, in order to make decisions more coherent,
we adopted eligibility traces for each state, which are also introduced in this
subsection.

5.1 State predictor

The dynamic function Fi(st+1, st, at) in the state predictor of module i gives the
probability distribution of the newly observed state st+1 based on the previous
state st and the action at performed by the agent at step t.

Fi(st+1, st, at : θi) = Pi(st+1|st, at : θ)

(2)

where st ∈ {1, . . . , N } and at ∈ {1, . . . , M } are discrete states and actions,
i ∈ {1, . . . , n} is the index of modules. Specifically, this dynamic function is
realized through a neural network with three fully-connected layers, where the
vector of parameters θ represents the weights of the network. This network was

(cid:262)(cid:952)(cid:176)(cid:87)(cid:76)(cid:169)(cid:87)(cid:76)RewardpredictorRL controllerStatepredictorENV-xxΣ Reward rtactionataction atResponsibility signal (cid:176)1nxnext statest’(cid:87)(cid:76) state stprediction errors(cid:169) (cid:87)(cid:76) Multi-task learning with modular reinforcement learning

7

trained by a collection of pairs of inputs (st, at) and their corresponding output
labels s′
t+1 from trajectories τ = {s0, a0, . . . , sT −2, aT −2, sT −1} of length T [9].
The initial parameters in this neural network were set as a small random value
with uniform distribution between 0.0 and 1.0. With the newly observed state
st+1 , the state prediction error δi

st of module i is calculated as follows:

δi
st = Fi(j, st, at : θi) − c(j, st+1), j ∈ {1, . . . , N }

c(j, st+1) =

(cid:40)

1 j = st+1,
0 otherwise.

(3)

(4)

Along with state prediction errors in each module, the responsibility signal λi
t
of each module for the current sub-task is formulated as the gaussian softmax
function,

λi
t =

2σ2 δj

st

2

st

j=1

ˆλi
2σ2 δi
te− 1
ˆλi
(cid:80)n
te− 1
λi
j=1 λj

(cid:80)n

t−1

ρ

t−1

ρ

ˆλi
t =

2

(5)

(6)

where the responsibility predictor ˆλi
t represents the prior knowledge or belief
about module selection, which is used to maintain the temporal continuity of
the selected module. Parameter ρ (0 < ρ < 1) controls the effects of past module
selections on current decision-making. Parameter vector θ is updated as follows:

θi = θi + αλi

tδi
st

∂Fi(st+1, st, at : θi)
∂θi

(7)

where 0 < α < 1 is the learning rate.

5.2 Reward predictor

The reward function Ri(r′, st, at) provides an expected reward r′
t based on the
previous state st and the action at selected at step t. Traditionally, parameters in
the reward predictor are updated by the reward prediction error, which comes
from the error between reward prediction r′
t and actually received immediate
reward rt after each interaction.

As described in the previous section, the responsibility signal represents how
responsible the modules are for the current situation; the temporal difference
of the responsibility signals between two steps could be used to reflect whether
the current module is more sensitive than the previously selected one. Based on
this idea, we utilize a “modular reward" [12], which comes from the immediately
received reward and an extra bonus for rewarding the module that better fits
the current sub-task. Specifically, this bonus is calculated from the conduct of
modular value function and the temporal difference in the responsibility signal,
which encourage the learning system to be more focused on the modules they

8

J. Xue and F. Alexandre

are responsible for, and lead to the optimality of the composite strategy for the
entire task. Basically, the modular reward is calculated as:

ˆri
t = rt + (λi

t+1 − λi

t)V i(st+1)

and we have the reward prediction error δi

rt as follows:

rt = ˆri
δi

t − Ri(r′, st, at)

(8)

(9)

Weighted by the responsibility signal and the learning rate, parameters in the
reward predictor will be updated as follows:

Ri(r′, st, at) = Ri(r′, st, at) + αλi

tδi
rt

(10)

5.3 RL controller

The RL controller in module i provides two functions: the state-value func-
tion V i(st) and the state-action-value function Qi(st, at). The value function
V i(st) gives the reward expectation of state st at step t and the state-action-
value function Qi(st, at) provides the reward expectation of action at in state
st. Technically, the goal of reinforcement learning is to improve the policy so
that a maximal accumulative reward will be obtained in the long run [17]. The
basic strategy of reinforcement learning is to use this value function to estimate
the state value under the current policy, and then to improve the policy based
on the value function. We define the value function of the state s(t) under the
current policy π as:

V i
π(st) = Eπ

(cid:34) ∞
(cid:88)

(cid:35)
γkr(t + k)

k=0
= Eπ(Rt+1 + γV i

π(st+1)|st)

(11)

where γ is the discount factor that controls the effects of future rewards. Tra-
ditionally, Rt+1 + γV i
π(st+1) is also called the “target value" of state st. Com-
bined with the state transition function Fi(st+1, st, at) and the reward function
Ri(r′, st, at), we have the full Bellman equation [17] of value function:

V i
π(st) =

(cid:88)

at∈A

t(at|st)[Ri(r′
πi

t, st, at) + γ

(cid:88)

st+1∈S

Fi(st+1, st, at : θi)V i

π(st+1)]

(12)
In order to make decisions in each step more coherent with previous ones, we
recorded the eligibility traces ei
(cid:40)

ts for each state s, which are updated as:

ei
t(s) =

ηγei
ηγei

t−1(s)
t−1(s) + 1

if s ̸= st
if s = st

(13)

The temporal-difference (TD) error δi
target value and the evaluated value:

vt comes from the deviation between the

vt = ˆri
δi

t + γV i(st+1) − V i(st)

(14)

Multi-task learning with modular reinforcement learning

9

where ˆri
responsibility signal as:

t is the modular reward and the state value V i(st) will be updated with

V i(st) = V i(st) + αλi

tδi

vtei

t(s)

(15)

5.4 Action selection

At each step, the action preferences follow a “joint policy", in which state-action
values derive from the combination of RL controllers in each module. Specifically,
action is decided based on the weighted summation of its value expectation
Qi(st, at) of future rewards in each module at state st. For each candidate action
aj ∈ A, (j = 1, . . . , M ), their values are calculated as:

Q(st, aj) =

n
(cid:88)

i=1

t[Ri(r′, st, aj) + γ
λi

(cid:88)

j∈X

Fi(sj, st, aj : θi)V (sj)]

(16)

where X is the set of possible states that the agent observes after taking action
aj in state s. In a greedy policy, the action that has the largest value will be
selected:

at = arg max
aj ∈A
Furthermore, we use a softmax function to explore action space in a stochastic
way, where the action at is selected by

Q(st, aj)

(17)

Pi(aj|st) =

eβQ(st,aj )
k=1 eβQ(st,ak)

(cid:80)M

(18)

where β is set as trials/500 and controls the stochasticity of action selection.

6 Simulation

6.1 Settings

In order to investigate the effectiveness of Inverse Arbi-Q architecture, we use a
food-gathering and predator-avoidance task derived from [16,13]. In this carrot-
rabbit-predator task, the environment is designed as a 5x5 grid world (as shown
in Figure 2. The blue rectangle , purple triangle and green circle represent the
rabbit, the predator, and the carrot respectively. Specifically, the rabbit searches
carrots with one of eight possible one step actions: {north (N), east (E), south
(S), west (W), northeast (NE), southeast (SE), northwest (NW), southwest
(SW)} while avoiding being caught by the predator. When the rabbit moves
in the location of any of the carrots, it receives a reward of 1.0 and a new carrot
will be allocated to an otherwise position. In each step where the rabbit does not
find any carrot, it gets a reward of -0.1 to represent increasing hunger. Mean-
while, the predator moves one step directly toward the rabbit while the rabbit
moves two steps. If the rabbit avoids the predator, it will receive a reward of
0.5, and a reward of -1.0 if it is caught by the predator, which also means the
termination of this trial. In this environment, the observation state of the rabbit
and the predator are their absolute coordinates respectively.

10

J. Xue and F. Alexandre

Fig. 2. The food-gathering and predator-avoidance world.

6.2 Results

At the begining, we’d like to compare the performance of Inverse Arbi-Q with
previous MRL approaches. Fig. 3(a) shows the average number of steps during
5000 trial epochs in 20 simulation runs. It can be seen that the Inverse Arbi-Q
model works significantly better than Arbi-\* approaches (Arbi-Q, Arbi-MBRL
and Arbi-SARSA(0)) and GM-MBRL.

(a) Averaged step per trial.

(b) Averaged score per trial.

Fig. 3. Performance comparison with other MRL approaches.

Furthermore, we utilize another performance metric of "score" from [13] to
compare Inverse Arbi-Q with different models. Specifically, the calculation of a
score follows this method: for each time that the rabbit finds a carrot, the score
is defined as 1.0, however, if it was caught by the predator, the score is 0.0, in
situations that the rabbit avoids the predator while no carrot was eaten, the
score is set as 0.5. Fig. 3(b) shows the scores between Inverse Arbi-Q with other
approaches. We can find that Inverse Arbi-Q receives more scores that other
models.

agentfoodpredatorMulti-task learning with modular reinforcement learning

11

Moreover, we’d like to know the performance of Inver Arbi-Q with incompara-
ble reward scales. Here we increase the avoidance reward by 10 times to observe
the performance of Inverse Arbi-Q in the incomparable rewards. As shown in
Fig. 4, Inverse Arbi-Q is robust across different incomparable reward scales.

Fig. 4. Performance of Inverse Arbi-Q with incomparable reward scales.

7 Conclusions and future work

In summary, we present the simple yet effective architecture Inverse Arbi-Q to
scale up the RL to learn compositional strategies and to exert them appropriately
in multi-task learning. Particularly, the responsibility signal, the outcome of
a gaussian softmax function of prediction error in each module, was used to
weight the outputs of multiple modules, as well as to gate the learning of each
module. Furthermore, action preferences are calculated by a "joint policy", which
comes from the combination of state-action values in each module multiplied by
its responsibility signal. Moreover, the selective update mechanism enables the
learning system to align incomparable reward scales between different modules.
Our ultimate goal with modular reinforcement learning is to facilitate the
integration of multiple modules into a developmental way, in order to learn
compositional strategies, as well as to generate appropriate behaviors accord-
ing to the specific contexts. Nevertheless, a number of questions stands out as
important targets for the next stage of research, such as: (i) the improvement
of planning strategy or the proactive way in the evaluation of modular reward;
(ii) enabling module selection and responsibility calculation in high dimension-
ality situations; (iii) the design of bidirectional (top-down and bottom-up) solu-
tions for task decomposition and compositionality. Inspirations from continual
learning and meta-learning approaches will be a promising direction for learning
tremendous tasks effectively without creating additional modules.

12

J. Xue and F. Alexandre

References

1. Bernard, J.A.: Don’t forget the little brain: A framework for incorporating the cere-
bellum into the understanding of cognitive aging. Neuroscience & Biobehavioral
Reviews p. 104639 (2022)

2. Botvinick, M.M.: Hierarchical models of behavior and prefrontal function. Trends

in cognitive sciences 12(5), 201–208 (2008)

3. Doya, K., Samejima, K., Katagiri, K.i., Kawato, M.: Multiple model-based rein-

forcement learning. Neural computation 14(6), 1347–1369 (2002)

4. Esteban, D., Rozo, L., Caldwell, D.G.: Hierarchical reinforcement learning for con-
current discovery of compound and composable policies. In: 2019 IEEE/RSJ In-
ternational Conference on Intelligent Robots and Systems (IROS). pp. 1818–1825.
IEEE (2019)

5. Gatti, D., Rinaldi, L., Ferreri, L., Vecchi, T.: The human cerebellum as a hub of

the predictive brain. Brain Sciences 11(11), 1492 (2021)

6. Gupta, V., Anand, D., Paruchuri, P., Kumar, A.: Action selection for composable
modular deep reinforcement learning. In: Proceedings of the 20th International
Conference on Autonomous Agents and MultiAgent Systems. pp. 565–573 (2021)
7. Jacobs, R.A., Jordan, M.I., Nowlan, S.J., Hinton, G.E.: Adaptive mixtures of local

experts. Neural computation 3(1), 79–87 (1991)

8. Logan, G.D., Crump, M.J.: Hierarchical control of cognitive processes: The case for
skilled typewriting. In: Psychology of learning and motivation, vol. 54, pp. 1–27.
Elsevier (2011)

9. Nagabandi, A., Kahn, G., Fearing, R.S., Levine, S.: Neural network dynamics
for model-based deep reinforcement learning with model-free fine-tuning. In: 2018
IEEE International Conference on Robotics and Automation (ICRA). pp. 7559–
7566. IEEE (2018)

10. Narendra, K.S., Balakrishnan, J., Ciliz, M.K.: Adaptation and learning using mul-
tiple models, switching, and tuning. IEEE control systems magazine 15(3), 37–51
(1995)

11. Nowlan, S.J., Hinton, G.E.: Evaluation of adaptive mixtures of competing experts.

In: NIPS. vol. 3, pp. 774–780 (1990)

12. Samejima, K., Doya, K., Kawato, M.: Inter-module credit assignment in modular

reinforcement learning. Neural Networks 16(7), 985–994 (2003)

13. Simpkins, C., Isbell, C.: Composable modular reinforcement learning. In: Proceed-
ings of the AAAI conference on artificial intelligence. vol. 33, pp. 4975–4982 (2019)
14. Smith, B.J., Read, S.J.: Modeling incentive salience in pavlovian learning more
parsimoniously using a multiple attribute model. Cognitive, Affective, & Behavioral
Neuroscience pp. 1–14 (2021)

15. Sodhani, S., Zhang, A., Pineau, J.: Multi-task reinforcement learning with context-
based representations. In: International Conference on Machine Learning. pp.
9767–9779. PMLR (2021)

16. Sprague, N., Ballard, D.: Multiple-goal reinforcement learning with modular sarsa

(0) (2003)

17. Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduction. MIT press

(2018)

18. Wang, J.X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J.Z., Munos, R.,
Blundell, C., Kumaran, D., Botvinick, M.: Learning to reinforcement learn. arXiv
preprint arXiv:1611.05763 (2016)

