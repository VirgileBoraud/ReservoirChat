Deciphering the contributions of episodic and working memories in
increasingly complex decision tasks Snigdha Dagar, Frédéric Alexandre,
Nicolas P. Rougier

To cite this version:

Snigdha Dagar, Frédéric Alexandre, Nicolas P. Rougier. Deciphering the
contributions of episodic and working memories in increasingly complex
decision tasks. IJCNN 2021 - International Joint Conference on Neural
Networks, Jul 2021, Shenzhen, China. pp.1-6,
￿10.1109/IJCNN52387.2021.9534315￿. ￿hal- 03465820￿

HAL Id: hal-03465820

https://inria.hal.science/hal-03465820

Submitted on 3 Dec 2021

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International
License

Deciphering the contributions of episodic and working memories in
increasingly complex decision tasks

Snigdha Dagar Inria Bordeaux Sud-Ouest Univ. Bordeaux, CNRS
snigdha.dagar@inria.fr

Frederic Alexandre Inria Bordeaux Sud-Ouest Univ. Bordeaux, CNRS
Frederic.Alexandre@inria.fr

Nicolas Rougier Inria Bordeaux Sud-Ouest Univ. Bordeaux, CNRS
Nicolas.Rougier@inria.fr

Abstract—Augmenting the representation of the current state of the
external world with internal states corresponding to working and
episodic memories has been proposed as a bio- inspired solution to apply
models of reinforcement learning to non-Markovian tasks. But,
transposing these results to behavioral and experimental neuroscience,
it is not completely clear how each of these memories can contribute to
learning the augmented representations and when they must act in
association for more complex tasks. Choosing an elementary
implementation of these memories and experimental tasks of decision
making in rodents, we explore these pivotal situations and make concrete
the underlying mechanisms and criteria. We also specify cases where
additional mechanisms must be envisaged.

Index Terms—reinforcement

learning, working memory,

episodic memory

I. INTRODUCTION

Learning and decision making are fundamental aspects of cognition and
are closely linked. They have long been studied in animals through
various levels of complexity in behavioral tasks. Reinforcement Learning
(RL) provides a theoretical frame- work for modeling tasks in which
agents interact with their environment and learn rules by receiving
reward signals upon taking actions, and has an undeniable biological
basis [1]. It is nonetheless constrained by the Markovian property,
stating that the decision can be directly made from the present state,
not consistent with known characteristics of animal behavior in real
world situations or in even cognitive tasks.

This class of partially observable Markov decision prob- lems (POMDPs)
has been solved by extending the present state (representing the state
of the environment) with internal representations [2] that might
correspond to memories built from previous experiences. A basic version
of this principle has been proposed in [3] and related to biological
basis, with the distinction between a working memory (associated with
the prefrontal cortex) [4], where a given cue can be kept present in
memory for some time, even if it disappears from the experienced world,
and an episodic memory (associated with the hippocampus), where a
previous episode (series of

This work is partly supported by the project Ecomob, co-funded by Inria

and by the french region Nouvelle Aquitaine.

steps) can be recalled by similarity from the present state and
manipulated as a virtual state.

On the computational and experimental neuroscience sides, a biological
neural network framework was proposed [5] to explain how working memory
representations in the PFC may be updated and maintained. This concept
of gating models was also used to study rule acquisition in rats [6], by
comparing the ability of two RL algorithms to replicate rat behavior.
The ability and limitations of such a model in a common human behavioral
task has also been demonstrated [7]. More recently, a simpliﬁcation of
this model has been extended [8] to include a bias that better explains
the performance of rats in a spatial alternation task.

On the machine learning side, learning and exploiting these forms of
memory have been adapted to RL [9], introducing complex representational
and computational mechanisms that have also been compared in more
details with human brain circuitry, thus introducing meta-RL [10] and
episodic-RL [11] as new paradigms for addressing non Markovian problems.
But this impressive level of performance is at the price of complex
computations, requiring an often prohibitive training time (and
correspondingly corpus size) and resulting in ob- scure computing
phenomena, difﬁcult to interpret and analyse in terms of functional
contributions of the respective memory mechanisms.

What we propose in this paper is to design a study where a basic RL
agent is extended with a minimal version of working memory and of
episodic memory, that can be trained quickly and without
hyper-parameters, and to deﬁne tasks where the usefulness of each memory
can be analysed in details. Particularly, what we want to understand and
share with our experimental neuroscientiﬁc colleagues are the conditions
where one or the other kind of memory are needed and where they are not
sufﬁcient and should be complemented with more complex mechanisms. In
other words, this explanatory study is the premise for predictions to be
conﬁrmed in forthcoming experimental studies in neuroscience and for
precise speciﬁca- tion to be implemented in more powerful learning
algorithms for machine learning.

II. METHODS AND TASKS

A. Computational model and architecture

a)  Basic RL agent: A tabular, actor critic temporal difference learning
    architecture with (cid:15) greedy exploration was used. The agent
    maintains a table of state values V , where V (s) is the agent’s
    current estimation of expected, temporally discounted reward that
    will follow state s. The agent also maintains a table of action
    values Q, where Q(s, a) is the value of taking action a in state s.
    The actor part of the architecture follows a simple policy where the
    agent picks the action with the highest Q-Value, except with a
    probability 0 ≤ (cid:15) ≤ 1 the agent selects an action at random.
    In the critic part of the architecture, the TD error δ is computed
    when the agent takes an action a in state s and transitions to state
    s(cid:48) after receiving a scalar reward r :

δ = r + γV (s(cid:48)) − V (s)

(1) 

where 0 ≤ γ ≤ 1 is a temporal discounting factor. The old

state value and action value are then updated as :

V (s) ← V (s) + αδ

Q(s, a) ← Q(s, a) + αδ

where α is a learning rate parameter.

(2) 
(3) 

The agent acts on the state space S = SL where SL are all the possible
location states an agent can ﬁnd itself in (numbered squares in a
T-shape grid world), by taking motor actions A = up, down, right, lef t
which can result in a change of the location state.

b)  RL agent with Working Memory:

: To include a working memory representation, the state space was aug-
mented with an extra memory element. A factored state space
representation was used : S : SLXSW M with each tuple of the form (SL,
SW M )(grid world location, working memory contents) having its own set
of action values. The total number of possible WM states is one more
than the number of location states (one extra for empty memory at the
start of the episode) i.e. |SW M | = |SL| + 1. The memory element is
”hidden” as the agent can only access this state using the update action
- which sets the working memory state to the current sensory location
state, until it is overwritten when the next memory action is taken.

c)  RL agent with Episodic Memory:

: We include an abstraction of the episodic memory system in the model,
which is content addressable and temporally indexed. The model maintains
a history of the agent’s n most recently visited states, in order. After
each action that changes the agent’s location state, the previous state
is added to the end of the list, and the oldest state in the list is
removed (no state is removed for the ﬁrst n steps). The model now has a
3 element tuple for state representation. The factored state space S =
SLXSEP XSW M . The episodic memory state SEP can take either one state
from the episodic memory list or an

additional state representing ”nothing recalled”. To interact with this
memory system, the agent can take two actions - ”cue retrieval” to ﬁnd
the most recent instance of the agent’s current state in the list (and
set SEP to that state) or ”advance retrieval” to set SEP to the state
following its current state in the list. This kind of abstraction allows
the model to retrieve a speciﬁc episode from its past and replay the
memory from the retrieved point.

B. Tasks

a)  Task A : Discrimination: In the ﬁrst, tactile discrim- ination task,
    the agent receives a sensory stimulus or a ”cue” at the starting
    state. In this version of the task, it was a tactile cue about the
    surface, which could be rough on the right or left (represented by
    different states, as in Figure 1.B) and was indicative of the
    rewarding arm i.e. if the surface was rough on the right, the reward
    was placed at the end of the right arm while if it was rough on the
    left, the reward was placed at the end of the left arm. Thus, the
    agent’s choice depended on learning this contextual or sensory rule.
    Another version of this task could be one where instead of a tactile
    stimulus, the agent could receive an auditory or odor stimulus in
    the starting state [12]. The important point is that this kind of
    sensory cue allows the rat or agent to form a distinct
    representation of the state

b)  Task B : Alternation: The second task is a spatial alternation task,
    in the environment as shown in Figure 1.A. This class of tasks is
    widely used to study hippocampal and working memory (PFC) functions
    [13]. The agent begins in the bottom of the central hallways (square
    marked with a black dot) and proceeds up to the choice point. On the
    ﬁrst (sample) trial, the agent can either turn left or right and
    receives the end of the arm (squares marked a positive reward at
    highlighted in black). The agent then continues along the return arm
    and back to the starting point (where it is prevented from entering
    the side hallway by a barrier). Following the ﬁrst trial, the agent
    receives a positive reward if it chooses the opposite direction as
    on the previous trial, otherwise it receives a negative reward.

c)  Task 3 : Radial Maze: In this task, there are three task conditions
    or contexts as represented in Figure 2 (left). In each task
    condition, the agent has the option of choosing only between 2 arms,
    with the rest of the arms blocked (the visual barriers being the
    contextual or sensory cue). For each of the contexts (A,B,C or D),
    the agent has to learn the alternating rule. In the trial phase,
    each context is presented once - A, B then C. The agent is free to
    go into any of the 2 open arms, and receives a reward at the end of
    the arm. Following the trial phase, the contexts are presented at
    random and the agent only receives a reward if it chooses the arm
    that it had not picked in the previous trial of the same context.

A. Need for episodic memory

III. RESULTS

We ﬁrst show, individually through the discrimination and is unable to

the simple RL agent

alternation tasks,

that

learn these tasks, remaining at chance performance for the
discrimination task, and below chance for the alternation task.
Performance in this task is plotted in Figure 3 (a) and (b). The RL
agent with a working memory reaches an optimal performance by
appropriately updating its internal memory at the right time. The agent
learned different policies for each item that may be stored in the
working memory. Accordingly, this can be interpreted as learning
behaviors for a given context. In the discrimination task, the agent
does this by buffering the sensory cue it received at the start state
(or central arm), and maintaining it in memory until the choice point,
thus disambiguating the trial type. In the alternation task, the agent
had to buffer a location following the choice, and maintain this in
memory until the choice point in the succeeding trial. In the
simulations, we could often observe that the agent could ﬁnd the
corresponding strategy and could learn to update and maintain the memory
with the previous choice (for alter- nation) or the cue (for
discrimination), whether left or right (accordingly, we call this
strategy ”remember both”) (Figure 5 Right, Figure 6 Right).
Interestingly enough, we could also observe sometimes the elaboration of
another strategy, which is valid even if based on a side effect. In this
”remember one” strategy, it is sufﬁcient for the agent to remember only
one choice (for alternation) or one cue (for discrimination) (Figure 5
Left, Figure 6 Left) and to simply label the other case by clearing the
working memory. What is important at the end is to learn to associate
the good action with a non ambiguous encoding of the state. The strategy
adopted by the agent as a percentage over 100 runs is shown in Figure 4.

Next, we demonstrate the agent’s behavior when the task is made more
complex, and the agent has to learn the same behavioral rule of
alternating between it’s choices, but under 3 different contexts. We use
the radial maze environment and show that a simple augmentation of
working memory is not enough to learn this task. While in a task with
only two contexts, the RL agent with one working memory element may be
able to perform the task at slightly above chance per- formance,
increasing the number of tasks quickly deteriorates the learning. A
separate memory mechanism, or the episodic memory is needed to learn the
presented task, as shown in Figure 3 (c). To perform this task
correctly, the agent depends on its episodic memory to retrieve the last
presentation of the current context, advance one step in the memory to
discover which direction it had previously chosen, and then choose the
opposite direction. It may be possible to solve this task by increasing
the number of memory elements, but this would exponentially scale the
state space, making value learning increasingly difﬁcult. Learning to
use working memory is an implicit kind of learning, which evolves over
time as knowing what and knowing when to update and maintain while
making the use of episodic memory is an explicit learning of recall.

B. Need for performance monitoring / Transfer of learning

We analyzed the ability of the agent to adapt in a non- stationary
environment. We tested if the agent is able to acquire the two distinct
rules of task A (discrimination) and

Fig. 1: A The alternation task requires for an agent to alternate its
choice between two different options (A & B). In the displayed setup,
the environment is a classical T-maze where the agent starts from the
bottom location in order to reach A or B. The ﬁrst choice is free and
the reward is obtained after each alternation between A and B. After
each trial, the agent restarts from the initial location. On this
example, the 5 trials can be written as ABBAB and only transitions AB
and BA are rewarded. This task implies for the agent to remember its
previous choice across trials. B The discrimination task requires for an
agent to learn which cue (out of two) is associated with a reward. The
agent has to choose between the two different options (A & B) depending
on a cue that is presented at the entrance of the T-Maze. This task
implies for the agent to remember the initial cue until it reached the
corresponding target during a single trial. C One example for the
discrimination task where the agent has learned to memorize the location
at the entrance. D One example for the alternation task where the agent
has learned to memorize the location after its choice has been made.

Fig. 2: The radial maze task corresponds to a contextualized alternation
task with four different contexts (A, B, C, D) as illustrated on the
left part of the ﬁgure. At any time, only one context X is open such
that the agent has only to choose between X0 or X1. The difﬁculty
however is to maintain a memory for a given context when the context is
changed. To be able to successfully solve this global task, the agent
has thus to maintain simultaneously four different memories
corresponding to the four different states of the context.

rewardedrewardednot rewardedrewardedrewardedrewardednot
rewardedrewardednot rewardedrewardedAlternation task(inter-trial working
memory)Discrimination task(inner-trial working memory)start + cuestart +
cueback to startTrial 1Trial 2Trial 3Trial 4Trial 5back to
startABBBBAAAAAAAAABBBB8745632187456321store state 1( = store cue)store
state 7( = store decision)CDBAC0A0C1A1D0B0D1B1Task A(State: A1)Task
D(State: D0)Task C(State: C1)Task B(State:
B0)A1A0A1B0B1C1C0C1A0A1A0B0B1D1D0D1switchswitchswitchswitchswitchswitchHold
A1Hold A1Hold B1Hold
C1…STARTA1A0A1B0B1C1C0C1A0A1D1D0D1B0B1A01010110101101010TrialsRadial
maze task(inter-task working memory)start(a)

(b) 
(c) 

Fig. 3: Performance of the agent in the (a) alternation task - a simple
RL agents performs worse than chance as repeated visits to the same arm
result in a negative reward (b) discrim- ination task (c) radial maze
task. Each plot corresponds to the mean performance of 100 individual
agents.

Fig. 4: Percentage of strategy types learned in the Alternation and
Discrimination tasks. The proportion of trials in the ﬁnal block in
which the previous choice (alternation) or the cue (discrimination) was
in memory at the choice point was calculated. A threshold of 2/3. was
used, for example, (a) the number of trials in which cue1 was in memory
/ number of trials with cue1 was greater than threshold, strategy was
”remember cue1” (similarly for cue2) and (b) number of trials is which
left was in memory / number of trials when previous choice was left was
greater than threshold, strategy was ”remember left” (similarly for
right); if proportions for both were above threshold, the strategy was
”remember both”; and if both were below threshold, it was ”remember
other”.

agent learned to ’pay attention’ (i.e store in memory) to the presented
cues. Nonetheless, due to the variation in the type of strategy used by
the agent, it is able to reach a sub-optimal, but above chance level of
performance. In the two rules we consider, the state spaces are only
partially overlapping, and the difference in the policies is about when
to update, thus the agent’s performance doesn’t drastically drop after
the second switch. However in a situation where the rules are reversed
[6] (for example, for the discrimination task - initially if cue1
rewards the right choice and cue2 rewards the left choice, a rule
reversal would mean cue1 rewards the left choice and cue2 rewards the
right choice), using the presented model would show such a sudden drop.
This is because (a) the same set of Q-values are learned and updated
continuously as the rules change, and (b) the model uses the same agent
for learning motor and memory policies (as opposed to some multi-agent
RL models). In any case, this model has the limitation of being unable
to recall a previously learned rule. Hence, the best it can do is to
identify a change in the environment, ’forget’ its currently held
policy, or adapt its exploitation/exploration, and relearn its action
preferences.

IV. DISCUSSION

task B (alternation) by switching the underlying rule from one to the
other, starting with task A, in the order ABAB. We show that after
learning the ﬁrst task, the agent is able to learn the second task
(alternation), but not to an optimal level of performance (as shown in
Figure 7). This is due to contextual interference from the ﬁrst task, in
which the

The current study explored the ability and constraints of a RL model,
augmented with working and episodic memories, to model rule learning
under increasing levels of task complexity. Our focus for this work was
to identify the limits of a minimal RL model, and increasingly
complement it with biologically plausible mechanisms to explain learning
behavior.

01020304050Trial Block0.00.10.20.30.40.50.60.70.8Success RateTask 2 :
Alternationsimple RLRL with WM01020304050Trial
Block0.00.20.40.60.81.0Success RateTask 1 : Discriminationsimple RLRL
with WM0255075100125150175200Trial Block0.20.30.40.50.60.70.80.9Success
RateAlternation in Radial Maze with contextRL with WMRL with WM and
EMDiscriminationAlternation7839195833Remember bothRemember oneother(a)

(c) 
(d) 
(e) 
(f) 
(g) 

Fig. 5: Examples of ”remember one”(left) and ”remember both”(right)
strategies learned by the model in the discrim- ination task. (a) and
(b) Probability over blocks of different possible memory contents
(cue1/2, central arm or empty) at the choice point. Since on average,
half of the trials begin with an observation of cue1 and the other with
an observation of cue2, the maximum proportion of trials that either of
these can be in memory is around 50% (c) and (d) Probability over blocks
of choosing to turn right or left as a function of different possible
memory contents. Probabilities are derived from Q-values at the end of
each block. (e) and (f) Probability over blocks of updating or
maintaining memory contents conditional on (1) observing the cue (at the
start state) or (2) having it already in memory (before the choice
point). Probabilities are derived from Q-values at the end of each block

Even though the three tasks considered here are not really ecological,
they are indeed experimented with rodents by our neuroscientiﬁc
colleagues and could be used to replicate the present ﬁndings and
explore possible predictions. Particularly, they allow us to illustrate
the necessity of having multiple memory systems (working memory and
episodic memory) in this context. For the simple discrimination task,
which shares a number of similarities with a regular delay match to
sample (DMTS) task, we explained that the Markovian Property forbids a
basic RL agent to solve the task and an additional hidden state (working
memory) is necessary to reach the optimal behavior. The alternating task
shares a lot of features with the discrimination task but the main
difference concerning the working memory is not ”what to store” but
”when to store”. We also showed that it is necessary for the model to
store the state after the decision has been taken, that

(a) 
(b) 
(c) 
(d) 
(e) 
(f) 

Fig. 6: Examples of ”remember one”(left) and ”remember both”(right)
strategies learned by the model in the alter- nation task. Probabilities
plotted are the same as described in Figure 5, with observations of cue1
and cue2 replaced with observations of turning left or right in the
previous trial

Fig. 7: The agent learns task A (discrimination) for the ﬁrst 50
episodes, then task B (alternation) for the next 50 episodes, followed
by a switch to task A and then back to task B. The presentation of the
cues interferes with the learning of the task. A simple method of
performance monitoring to identify a rule change can be maintaining a
running mean of the rewards obtained over the last n episodes

is, when the agent is close to one of the two end states. We also
noticed that the model can efﬁciently learn to update and maintain
working memory representations which can be intra- trial, inter-trial
and inter-task. the

radial maze

Finally,

implies

setup

our

(in

01020304050Trial Block0.100.150.200.250.300.35Probability of memory
state at choice
pointmemory=C1memory=C2memory=centralmemory=empty01020304050Trial
Block0.00.10.20.30.40.5Probability of memory state at choice
pointmemory=C1memory=C2memory=centralmemory=empty01020304050Trial
Block0.00.20.40.60.8Probability of choosing left or right
armP(left|memory=C1)P(left|memory=C2)P(right|memory=C1)P(right|memory=C2)01020304050Trial
Block0.00.20.40.60.81.0Probability of choosing left or right
armP(left|memory=C1)P(left|memory=C2)P(right|memory=C1)P(right|memory=C2)01020304050Trial
Block0.20.30.40.50.60.70.80.9Probability
Update/MaintainP(update|observeC1)P(update|observeC2)P(maintain|memory=C1)P(maintain|memory=C2)01020304050Trial
Block0.30.40.50.60.70.80.91.0Probability
Update/MaintainP(update|observeC1)P(update|observeC2)P(maintain|memory=C1)P(maintain|memory=C2)01020304050Trial
Block0.00.10.20.30.40.50.60.7Probability of memory state at choice
pointmemory=Lmemory=Rmemory=centralmemory=empty01020304050Trial
Block0.00.10.20.30.40.50.60.7Probability of memory state at choice
pointmemory=Lmemory=Rmemory=centralmemory=empty01020304050Trial
Block0.10.20.30.40.50.60.7Probability of choosing left or right
armP(left|memory=L)P(left|memory=R)P(right|memory=L)P(right|memory=R)01020304050Trial
Block0.00.20.40.60.81.0Probability of choosing left or right
armP(left|memory=L)P(left|memory=R)P(right|memory=L)P(right|memory=R)01020304050Trial
Block0.10.20.30.40.50.60.70.80.9Probability
Update/MaintainP(update|observeL)P(update|observeR)01020304050Trial
Block0.20.40.60.81.0Probability
Update/MaintainP(update|observeL)P(update|observeR)Trial
Block0.20.40.60.81.0Success RateDiscrimination -
Alternation025507510012515017520005[7] M. Todd, Y. Niv, and J. D.
Cohen, “Learning to use working memory in partially observable
environments through dopaminergic reinforcement,” Advances in neural
information processing systems, vol. 21, pp. 1689– 1696, 2008.

[8] D. B. Kastner, A. K. Gillespie, P. Dayan, and L. M. Frank, “Memory
alone does not account for the way rats learn a simple spatial
alternation task,” Journal of Neuroscience, vol. 40, no. 38,
pp. 7311–7317, 2020. [9] M. Botvinick, S. Ritter, J. X. Wang, Z.
Kurth-Nelson, C. Blundell, and D. Hassabis, “Reinforcement learning,
fast and slow,” Trends in cognitive sciences, vol. 23, no. 5,
pp. 408–422, 2019.

[10] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R.
Munos, C. Blundell, D. Kumaran, and M. Botvinick, “Learning to
reinforcement learn,” arXiv preprint arXiv:1611.05763, 2016.

[11] S. Ritter, J. X. Wang, Z. Kurth-Nelson, and M. Botvinick, “Episodic
control as meta-reinforcement learning,” bioRxiv, p. 360537, 2018. [12]
N. J. Broadbent, L. R. Squire, and R. E. Clark, “Rats depend on habit
memory for discrimination learning and retention,” Learning & Memory,
vol. 14, no. 3, pp. 145–151, 2007.

[13] L. M. Frank, E. N. Brown, and M. Wilson, “Trajectory encoding in
the hippocampus and entorhinal cortex,” Neuron, vol. 27, no. 1, pp. 169–
178, 2000.

[14] D. Stuss, B. Levine, M. Alexander, J. Hong, C. Palumbo, L. Hamer,
K. Murphy, and D. Izukawa, “Wisconsin card sorting test performance in
patients with focal frontal and posterior brain damage: effects of
lesion location and test structure on separable cognitive processes,”
Neuropsychologia, vol. 38, no. 4, pp. 388–402, 2000.

[15] B. C. Da Silva, E. W. Basso, A. L. Bazzan, and P. M. Engel,
“Dealing with non-stationary environments using context detection,” in
Proceed- ings of the 23rd international conference on Machine learning,
2006, pp. 217–224.

[16] K. Doya, K. Samejima, K.-i. Katagiri, and M. Kawato, “Multiple
model- based reinforcement learning,” Neural computation, vol. 14, no.
6, pp. 1347–1369, 2002.

[17] K. Lloyd and D. S. Leslie, “Context-dependent decision-making: a
simple bayesian model,” Journal of The Royal Society Interface, vol. 10,
no. 82, p. 20130069, 2013.

[18] A. Collins and E. Koechlin, “Reasoning, learning, and creativity:
frontal lobe function and human decision-making,” PLoS Biol, vol. 10,
no. 3, p. e1001293, 2012.

[19] H. Eichenbaum, “Memory: organization and control,” Annual review of

psychology, vol. 68, pp. 19–45, 2017.

[20] A. Shenhav, J. D. Cohen, and M. M. Botvinick, “Dorsal anterior
cingulate cortex and the value of control,” Nature neuroscience,
vol. 19, no. 10, pp. 1286–1291, 2016.

implementation) a different kind of memory (episodic memory) such as to
be able to ”upload” the right context to the working memory. An
alternative implementation could have been to have four different
working memories, one for each context. But even in such case, the core
”routing” problem remains the same: depending on the context, the agent
needs to store the memory at the right place.

If we now go back to behavior, we think that the radial maze task is
actually quite representative of our day to day behavior. An agent
already engaged in a given task may ”pause” it in favor of another task
provided it perceives some cue indicating that a new task can be
advanced or completed. This kind of behavior requires in fact the
temporal organization of behavior. Furthermore, even though the task is
already quite complex, we nonetheless keep it simple by explicitly cuing
the agent with the unambiguous identiﬁcation of the context, contrarily
to tasks such as the Wisconsin Card Sorting Task (WCST) [14] that
require an effective cognitive effort to monitor performance to decide
if the context has changed or not. It thus comes as no surprise that our
human brains possess dedicated areas in the frontal cortex to monitor,
select and instantiate such high level rules.

Much work has been done on dealing with non-stationary environments, in
model-free algorithms [15] in multi-arm bandits, or model-based RL
frameworks such as that proposed by [16] but the neural underpinnings
for these are not yet clear. On the other hand, models for rule-learning
and rule- switching have been proposed that rely on bayesian inference
[17] [18], and while they provide a unifying theory for these concepts,
their complexity makes them infeasible for guiding and testing
fundamental hypothesis in experiments. Coming back to biological
inspiration and building upon working memory and episodic memory a
biologically inspired cognitive control [19] [20] could be an
interesting way to deﬁne a more ﬂexible decision making agent, able to
quickly adapt in an uncertain and changing world, as we will study in
the near future.

REFERENCES

[1] D. Lee, H. Seo, and M. W. Jung, “Neural basis of reinforcement
learning and decision making,” Annual review of neuroscience, vol. 35,
pp. 287– 308, 2012.

[2] L. Peshkin, N. Meuleau, and L. Kaelbling, “Learning policies with

external memory,” arXiv preprint cs/0103003, 2001.

[3] E. A. Zilli and M. E. Hasselmo, “Modeling the role of working memory
and episodic memory in behavioral tasks,” Hippocampus, vol. 18, no. 2,
pp. 193–209, 2008.

[4] R. C. Wilson, Y. K. Takahashi, G. Schoenbaum, and Y. Niv, “Or-
bitofrontal cortex as a cognitive map of task space,” Neuron, vol. 81,
no. 2, pp. 267–279, 2014.

[5] R. C. O’Reilly and M. J. Frank, “Making working memory work: a
computational model of learning in the prefrontal cortex and basal
ganglia,” Neural computation, vol. 18, no. 2, pp. 283–328, 2006. [6] K.
Lloyd, N. Becker, M. W. Jones, and R. Bogacz, “Learning to use working
memory: a reinforcement learning gating model of rule acquisition in
rats,” Frontiers in computational neuroscience, vol. 6, p. 87, 2012.


