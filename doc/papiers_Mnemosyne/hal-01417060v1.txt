Recurrent Neural Network for Syntax Learning with Flexible
Representations Xavier Hinaut

To cite this version:

Xavier Hinaut. Recurrent Neural Network for Syntax Learning with
Flexible Representations. IEEE ICDL-EPIROB Workshop on Language
Learning, Dec 2016, Cergy, France. ￿hal-01417060￿

HAL Id: hal-01417060

https://inria.hal.science/hal-01417060

Submitted on 15 Dec 2016

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

Recurrent Neural Network for Syntax Learning with Flexible
Representations

Xavier Hinaut1

Abstract— We present a Recurrent Neural Network (RNN), namely an Echo
State Network (ESN), that performs sentence comprehension and can be
used for Human-Robot Interaction (HRI). The RNN is trained to map
sentence structures to meanings (e.g. predicates). We have previously
shown that this ESN is able to generalize to unknown sentence structures
in English and French. The meaning representations it can learn to
produce are ﬂexible: it enables one to use any kind of “series of slots”
(or more generally a vector representation) and are not limited to
predicates. Moreover, preliminary work has shown that the model could be
trained fully incrementally. Thus, it enables the exploration of
language acquisition in a developmental approach. Furthermore, an
“inverse” version of the model has been also studied, which enables to
produce sentence structure from meaning representations. Therefore, if
these two models are combined in a same agent, one can inves- tigate
language (and in particular syntax) emergence through agent-based
simulations. This model has been encapsulated in a ROS module which
enables one to use it in a cognitive robotic architecture, or in a
distributed agent simulation.

I. INTRODUCTION

How do children learn language? In particular, how do they associate the
structure of a sentence to its meaning? This question is linked to the
more general issue: how does the brain associate sequences of symbols to
internal symbolic or sub-symbolic representations? We propose a
framework to understand how language is acquired based on a simple and
generic neural architecture (Echo State Networks) [1] which is not
hand-crafted for a particular task.

First, we present the general ESN architecture. Then, we detail the
reservoir sentence processing model. Finally we give some perspectives.

II. METHODS & RESULTS

A. Echo State Networks

The language module is based on an ESN [1] with leaky neurons: inputs
are projected to a random recurrent layer and a linear output layer
(called “read-out”) is modiﬁed by learning (which can also be done in an
online fashion). The units of the recurrent neural network have a leak
rate (α) hyper-parameter which corresponds to the inverse of a time
constant. These equations deﬁne the update of the ESN:

x(t + 1) = (1 − α)x(t) + αf (W inu(t + 1) + W x(t)) (1)

y(t) = W outx(t)

(2) 

1X. Hinaut is with Inria Bordeaux Sud-Ouest, Talence, France; LaBRI, UMR
5800, CNRS, Bordeaux INP, Universit´e de Bordeaux, Talence, France; and
Institut des Maladies Neurod´eg´en´eratives, UMR 5293, CNRS, Univer-
sit´e de Bordeaux, Bordeaux, France xavier.hinaut@inria.fr

with x(t), u(t) and y(t) the reservoir (i.e. hidden) state, the input
and the read-out (i.e. output) states respectively at time t, α the leak
rate, W , W in and W out the reservoir, the input and the output
matrices respectively and f the tanh activation function. After the
collection of all reservoir states the following equation deﬁnes how the
read-out (i.e. output) weights are trained:

W out = Y d[1; X]+

(3) 

with Y d the concatenation of the desired outputs, X the concatenation
of the reservoir states (over all time steps for all train sentences)
and M + the Moore-Penrose pseudo- inverse of matrix M . Hyper-parameters
that can be used for this task are the following: spectral radius: 1,
input scaling: 0.75, leak rate: 0.17, number of reservoir units: 100.

Sentences are converted to a sentence structure by replacing Fig. 1.
semantic words by a SW marker. The ESN is given the sentence structure
word by word. Each word activates a different input unit. During
training, the connections to the readout layer are modiﬁed to learn the
mapping between the sentence structure and the arguments of the
predicates. When a sentence is tested, the most active units are bound
with the SW kept in the SWs memory to form the resulting predicate.
(Adapt. from [2].)

B. Reservoir Sentence Processing Model

The reservoir sentence processing model has been adapted from previous
experiments on a neuro-inspired model for sentence comprehension using
ESN [3] and its application to HRI [2]. The model learns the mapping of
the semantic words (SW; e.g. nouns, verbs) of a sentence onto the
different slots (the thematic roles: e.g. action, location) of a basic
event structure (e.g. action(object, location)). As depicted in Fig. 1,
the system processes a sentence as input and generates corresponding
predicates. Before being fed to the

theontoandSW…Sentence:PALput (toy, left)P(A, L)Semantic words
(SWs)Recurrent NeuralNetworkSW1SW2SW3P: PredicateA: AgentL:
LocationMeaning: P (A, L)Read-outLayerInputLayeron the left put the toy
.SW1SW2SW3putlefttoySWs MemoryFunction
words(FWs)PALPALleftputtoyInactive connectionActive connectionLearnable
connectionsFixed connectionsConnectionSW: Semantic word
item(e.g. toy)thisESN, sentences are transformed into a sentence
structure (or grammatical construction) semantic words (SW), i.e. nouns,
verbs and adjectives that have to be assigned a thematic role, are
replaced by the SW item. The processing of the grammatical construction
is sequential (one word at a time) and the ﬁnal estimation of the
thematic roles for each SW is read-out at the end of the sentence.

By processing constructions [4] and not sentences per se, the model is
able to bind a virtually unlimited number of sentences to these sentence
structures. Based only on a small training corpus (a few tens of
sentences), this enables the model to process future sentences with
currently unknown semantic words if the sentence structures are similar.
One major advantage of this neural network language module is that no
parsing grammar has to be deﬁned a priori: the system learns only from
the examples given in the training corpus. Here are some input/output
transformations that the language model performs:

• “Please give me the mug” → give(mug, me) • “Could you clean the table
with the sponge?”

→ clean(table, sponge)

• “Find the chocolate and bring it to me” → ﬁnd(chocolate),
bring(chocolate, me)

As shown, the system can robustly transform different types of
sentences. Recently, we have explored how ﬂexible our system is: we
found that it can handle various kinds of representations at the same
time. For instance, it allows to use nouns as main elements of a
predicate, and use its arguments to ﬁll in adjectives:

• “Search for the small pink object”

→ search(object), object(small, pink)

It also allows to process various kinds of complex sentences: • “Bring
me the newspaper which is on the table in the kitchen” →
bring(newspaper, me), newspaper(on, table), table(in, kitchen)

C. Perspectives

In Hinaut et al. [5], it has been shown that the model can learn to
process sentences with out-of-vocabulary words. Moreover, we
demonstrated that it can generalize to unknown constructions in both
French and English at the same time. To illustrate how the robot
interaction works, a video can be seen at youtu.be/FpYDco3ZgkU [6][7].
The source code, implemented as a ROS module, is available at github.
com/neuronalX/EchoRob. With an ESN of 100 units, the training of 200
sentences takes about one second on a laptop computer. Testing a
sentence is of the order of 10 ms. This ROS module could be employed to
process various hypotheses generated by a speech recognition system
(like in [7]), then returning the retrieved predicates for each
hypothesis, thus, enabling a semantic analyser or world sim- ulator to
choose the predicates with the highest likelihood. Preliminary work has
shown that the model could be trained fully incrementally (i.e. changing
weights at each time step) [8].Moreover, an “inverse” version of the
model has been also studied, which enables to learn mapping from meaning

representations to sentence structure [2]. The representations may not
be limited to “series of slots”: neural networks are able to deal with
hierarchical structures when they are embedded in a vector
representation (e.g. Fluid Construction Grammar using Holographic
Reduced Representations like in [9]. With these properties, one can
investigate language syntax emergence through agent-based simulations.

In a nutshell, the objectives of this model are to improve HRI and
provide models of language acquisition. From the HRI point of view, the
aim of using this neural network- based model is (1) to gain
adaptability because the system is trained on corpus examples (no need
to predeﬁne a parser for each language), (2) to be able to process
natural language sentences instead of stereotypical sentences (i.e. “put
cup left”), and (3) to be able to generalize to unknown sentence
structures (not in the training data set). Moreover, this model is quite
ﬂexible when changing the output predicate repre- sentations, as we have
shown here. From the computational neuroscience and developmental
robotics point of view, the aim of this architecture is to model and
test hypotheses about child learning processes of language acquisition
[10].

ACKNOWLEDGMENT

I thank William Schueller for interesting discussions and

ideas.

REFERENCES

[1] H. Jaeger. The “echo state” approach to analysing and training re-
current neural networks. Bonn, Germany: German National Research Center
for Information Technology GMD Technical Report, 148:34, 2001.

[2] X. Hinaut, M. Petit, G. Pointeau, and P. F. Dominey. Exploring the
acquisition and production of grammatical constructions through
Frontiers in interaction with echo state networks. human-robot
Neurorobotics, 8, 2014.

[3] X. Hinaut and P.F. Dominey. Real-time parallel processing of gram-
matical structure in the fronto-striatal system: a recurrent network
simulation study using reservoir computing. PloS one, 8(2):e52946, 2013.

[4] A.E. Goldberg. Constructions: A construction grammar approach to

argument structure. University of Chicago Press, 1995.

[5] X. Hinaut, J. Twiefel, M. Petit, P. Dominey, and S. Wermter. A
recurrent neural network for multiple language acquisition: Starting In
NIPS 2015 Workshop on Cognitive with english and french. Computation:
Integrating Neural and Symbolic Approaches, 2015. [6] X. Hinaut, J.
Twiefel, M. Borghetti Soares, P. Barros, L. Mici, and S. Wermter.
Humanoidly speaking – learning about the world and language with a
humanoid friendly robot. In IJCAI Video competition, Buenos Aires,
Argentina. https://youtu.be/FpYDco3ZgkU, 2015. [7] J. Twiefel, X.
Hinaut, M. Borghetti, E. Strahl, and S. Wermter. Using Natural Language
Feedback in a Neuro-inspired Integrated Multimodal In Proc. of RO-MAN,
New York City, USA, Robotic Architecture. 2016.

[8] X. Hinaut and S. Wermter. An incremental approach to language
acquisition: Thematic role assignment with echo state networks. In Proc.
of ICANN 2014, pages 33–40, 2014.

[9] Y. Knight, M. Spranger, and L. Steels. A vector representation of
Fluid Construction Grammar using Holographic Reduced Representations.
2015.

[10] M. Tomasello. Constructing a language: A usage based approach to
language acquisition. Cambridge, MA: Harvard University Press, 2003.


