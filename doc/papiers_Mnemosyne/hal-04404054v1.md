Sustainable Deep Learning for Time-series: Reservoir
Computing
Andrea Ceni, Claudio Gallicchio, Xavier Hinaut, Gianluca Milano, Abigail

Morrison

To cite this version:

Andrea Ceni, Claudio Gallicchio, Xavier Hinaut, Gianluca Milano, Abigail Morrison. Sustainable
Deep Learning for Time-series: Reservoir Computing. ECML-PKDD 2023 - European Conference
on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, Sep 2023,
Turino, Italy. ￿hal-04404054￿

HAL Id: hal-04404054

https://inria.hal.science/hal-04404054

Submitted on 18 Jan 2024

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Tutorial Proposal on
Sustainable Deep Learning for Time-series:
Reservoir Computing

Andrea Ceni, Claudio Gallicchio, Xavier Hinaut, Gianluca Milano, and Abigail Morrison

Title: Sustainable Deep Learning for Time-series: Reservoir Computing

Abstract

Reservoir Computing (RC) is a promising approach for time-series forecasting that offers
efficient and sustainable deep learning alternatives to traditional neural networks. This
tutorial provides an introduction to sustainable deep learning with RC,
including its
theoretical foundations, practical implementation, and advantages over other approaches.
The tutorial starts with an overview of the RC framework, including the Echo State Network
(ESN) and Liquid State Machine (LSM) models. The fundamental principles of these models
are explored, including their unique network structure, the learning mechanisms, and the
fundamentals of RC mathematical background. The tutorial then proceeds to explore the
strong links with computational neuroscience, and the exciting recent advances in
neuromorphic hardware implementations of reservoirs. Finally, the tutorial covers practical
implementation aspects, existing software libraries, as well as successful application
examples in several domains including automotive, avionics, health, and robotics.

In addition to providing practical guidance, this tutorial also emphasizes the importance of
sustainability in deep learning. The environmental impact of training deep neural networks is
substantial, and RC offers a more energy-efficient approach that is particularly well-suited for
time-series forecasting. By using RC, researchers can reduce the carbon footprint of their
research while achieving state-of-the-art results.

Overall, this tutorial provides a comprehensive introduction to sustainable deep learning with
RC for
is suitable for both beginners and advanced
researchers looking to explore this exciting area of machine learning.

time-series forecasting, and it

Description of the tutorial

Reservoir Computing (RC)
is a paradigm for designing recurrent neural models for
sequential data processing, distinctly featured by efficient training algorithms. The approach
is rooted in the analysis of the dynamical components of the neural architecture under the
perspective of dynamical systems theory, and proposes to use fixed (possibly randomized)
connections under asymptotic stability conditions. The resulting reservoir dynamics provide a
input signals, enabling to reduce the learning problem to a
rich encoding of the external
simple regression performed by a linear model. The approach stands out in the context of
Machine Learning and Deep Learning research for its distinctive and striking advantage in
terms of computational efficiency, ensuring fast and sustainable training algorithms for
time-series AI applications. Apart
from the undoubted computational advantage, RC is
appealing for a number of reasons, including its neurobiological plausibility, its amenability to
implementations in unconventional neuromorphic hardware (e.g., in photonics), and the fact
that it leads to clean mathematical analysis of the architectural biases of recurrent models.
Intriguingly, RC gives a refreshed perspective over diverse topics in Deep Learning
research, including learning beyond Backpropagation, stable neural architectures, the lottery
ticket hypothesis, tiny ML, and neural architecture search.
The tutorial will offer a comprehensive view of the field, from basics to the most recent
advances on the machine learning side,
trends of RC in
Computational Neuroscience, the recent developments in neuromorphic hardware, and will
survey current libraries of RC with a detailed introduction to ReservoirPy with Python
Notebooks available online.

the starting and current

This tutorial is targeted to both researchers and practitioners that are interested in setting up
fastly-trained recurrent neural networks for structured data. It will open their mind to a
computing paradigm different from the most well-known ones, which offers state-of-the-art
performances on several applications. Such a paradigm should be considered as its own
and to implement mixtures of approaches in order to tackle seriously the environmental
challenges. It is also recognized in neuroscience as an important principle to interpret and
model brain computations. Basic concepts on Machine Learning and Deep Neural Networks
are suggested prerequisites.

Relevance to the ECML community

This tutorial will provide an intriguing and broad overview on the emerging topic of efficiently
trainable neural networks architectures, ranging from the enabling mathematical theory and
the available software libraries, to computational neuroscience evidences and neuromorphic
the diverse
implementations in unconventional physical substrates.
backgrounds of the presenters, along with the designed format for the tutorial (featuring
relatively short presentations), will be key factors in providing a stimulating and effective
introduction to the subject.
The focus on energy expenditure, environmental and economic impact related to the
development of deep learning algorithms is becoming increasingly important in the European
and global
landscape. We thereby believe that the issues addressed in this tutorial are
extremely relevant to the ECML community.

In this sense,

Outline of the tutorial

This tutorial is structured in chapters, according to the following organization. Note that a 30
minutes break is expected between Chapters 3 and 4.

● Chapter 1: Introduction (10 minutes)

Preliminaries on Deep Learning for sequential data and Recurrent Neural Networks
(RNNs); green AI and the problem of designing sustainable Deep Learning
algorithms.

● Chapter 2: Basics (30 minutes)

Randomization in Deep Neural Networks; Echo State Networks, Liquid State
Machines, and deep architectures; quality of dynamics and network topologies;
training algorithms; Reservoir Computing for graphs.

● Chapter 3: Mathematical foundations (40 minutes)

Recurrent Neural Networks as input-driven dynamical systems. Stability of recurrent
neural networks; Memory-nonlinearity trade-off; Edge-of-chaos dynamics.

● Chapter 4: Computational Neuroscience of Reservoir Computing (40 minutes)

Recent advances in Reservoir Computing in Neuroscience; Paradigm shift in the last
decade from brain data analysis towards the RC paradigm of “decoding” complex
non-linear computations.

● Chapter 5: Neuromorphic Hardware implementations (40 minutes)

Principles of neural networks in unconventional and neuromorphic hardware;
Physical substrates for neuromorphic reservoirs: photonics, memristors, spintronics,
and mechanical systems.

● Chapter 6: Software libraries and applications (40 minutes)

Python frameworks for Reservoir Computing; hyperparameter exploration; building
complex architectures; example applications (e.g., sound classification with implicit
the trade. Short hands-on lab demonstration
segmentation) and tricks of
(ReservoirPy):
time-series
classification.

Implementing Echo State Networks in Python for

● Chapter 7: Quo vadis? (10 minutes)

Conclusions and outlook.

Instructors

Andrea Ceni
Affiliation: Department of Computer Science, University of Pisa, Italy;
email: andrea.ceni@di.unipi.it
Presenter for: Chapter 3 (Mathematical Foundations)
Short bio: Andrea Ceni received the M.Sc. degree in Mathematics cum laude from Università
degli Studi di Firenze, Italy, in 2017, and the Ph.D. degree from the Department of Computer
Science of the University of Exeter, UK, in 2021. He has been a Postdoctoral Research

Associate at CEMPS, University of Exeter. Currently, he is a Research Fellow at the
Department of Computer Science of the University of Pisa, Italy. His research interests
include recurrent neural networks, deep learning, computational neuroscience, chaos theory,
and complex systems.

Claudio Gallicchio
Affiliation: Department of Computer Science, University of Pisa, Italy;
email: claudio.gallicchio@unipi.it
Presenter for: Chapters 1 (Introduction), 2 (Basics), 6 (Quo vadis?)
Short bio: Claudio Gallicchio is a Tenured-track Assistant Professor of Machine Learning at
the Department of Computer Science of the University of Pisa, Italy. He received his PhD
from the University of Pisa, where he focused on Reservoir Computing models and theory
for structured data. His research is based on the fusion of concepts from Deep Learning,
Recurrent Neural Networks, and Randomized Neural Systems. He is the founder and former
chair of the IEEE CIS Task Force on Reservoir Computing, and a member of the IEEE CIS
Data Mining and Big Data Analytics Technical Committee, of the IEEE CIS Task Force on
Deep Learning, and of
the IEEE Task Force on Learning for structured data. He
(co-)authored several publications at major ML conferences, including ICML, AAAI, and
IJCNN/WCCI. He has (co-)organized several events (special sessions, workshops, and
tutorials)
focused on Reservoir Computing and Randomized Neural Networks at ML
international conferences (including AAAI, IJCNN/WCCI, ICANN, ESANN). He serves as a
member of several program committees of conferences and workshops in ML and AI
(including NeurIPS,
ICONIP,
ICLR,
IJCNN/WCCI). He served as guest editor for Special Issues on topics related to Reservoir
Computing in ML journals including IEEE Transactions on Neural Networks and Learning
Systems (TNNLS) and Cognitive Computation (Springer). Currently, he is an Academic
Editor of PLOS ONE, and an Associate Editor of TNNLS.

IJCAI-PRICAI, ECML-PKDD,

ICML, AISTATS,

Xavier Hinaut
Affiliation: Inria, Bordeaux, France;
email: xavier.hinaut@inria.fr
Presenter for: Chapter 6 (Software libraries and applications)
Short bio: Xavier Hinaut is Research Scientist in Computational Neuroscience at Inria,
Bordeaux, France. His work is at the frontier of neurosciences, machine learning, robotics
from the modeling of human sentence processing to the analysis of
and linguistics:
birdsongs and their neural correlates. He manages the DeepPool ANR project on human
sentence modeling with Deep Reservoirs architectures. He leads ReservoirPy development:
a new Python library for Reservoir Computing. https://github.com/reservoirpy/reservoirpy

Gianluca Milano
Affiliation: INRiM, Torino, Italy;
email: g.milano@inrim.it
Presenter for: Chapter 5 (Neuromorphic Hardware implementations)
Short bio: Gianluca Milano is currently a permanent researcher at the Italian National
Institute of Metrological Research (INRiM). He received a Ph.D. in Physics cum laude from
Politecnico di Torino, Italy, in collaboration with the Italian Institute of Technology (IIT). His
main research interests and activities focus on: i) the investigation of electronic and ionic
transport properties and physicochemical phenomena in nanodevices and low dimensional

is

synthesis

from material

systems; and ii) memristive devices and architectures for memory and neuromorphic
applications,
to device characterization, modeling, and
implementation of unconventional and brain-inspired computing paradigms in neuromorphic
project EMPIR MEMQuD
the
architectures. He
(https://memqud.inrim.it/)
including Universities,
research centers and industries, that focus on the development of memristive devices
working in the quantum regime for quantum and neuromorphic applications. He is member
of the Editorial advisory Board for the journal APL Machine Learning. For his work on
in-materia implementation of reservoir computing in self-organizing networks of nano objects
he has received the NEST prize for Nanoscience 2021.

involves 15 european partners,

coordinator
that

european

of

Abigail Morrison
Affiliation: Institute of Neuroscience and Medicine (INM-6), Research Centre Jülich, and
Computer Science 3 - Software Engineering, RWTH Aachen, Germany
email: morrison@fz-juelich.de
Presenter for: Chapter 4 (Computational Neuroscience of Reservoir Computing)
Short bio: Abigail Morrison has a background in physics and AI, which led her ineluctably to
the field of computational neuroscience. Her primary research focus is uncovering the brain's
computational principles, with particular emphasis on representation and learning, through
the development of spiking neural network models. Her recent work examines the limits and
potential of reservoir computing as a model for understanding cortical computation. A
secondary focus is on high-performance simulation technology and its associated software
ecosystem. Following research positions at the Bernstein Center Freiburg (Germany) and
RIKEN Brain Science Institute (Japan), she held professorships at the University of Freiburg
and the Ruhr University of Bochum (Germany), before being appointed to the Department of
Computer Science at RWTH Aachen in 2020.

Previous Venues

Similar tutorials/seminars, covering a part of the topics of the proposed one for ECML, have
been previously given in the following venues.

[AAAI 2021] “Deep Randomized Neural Networks”, Tutorial of the 35th AAAI Conference on
Intelligence (AAAI), 2-9 February 2021, Virtual Conference. Speaker: Claudio
Artificial
Gallicchio. Audience size: around 50 people. Duration: 1.5 hours. Conference program link
and tutorial material:
https://aaai.org/Conferences/AAAI-21/aaai21tutorials/#mq3 and
https://sites.google.com/site/cgallicch/resources/tutorial\_DRNN

[IJCNN 2021] “Reservoir Computing: Randomized Recurrent Neural Networks”, Tutorial of
the International Joint Conference on Neural Networks (IJCNN), 18-22 July 2021, Virtual
Conference. Speaker: Claudio Gallicchio. Audience size: around 50 people. Duration: 2
hours. Conference program link: https://www.ijcnn.org/tutorials Tutorial
recording on
YouTube: https://www.youtube.com/watch?v=XJg7VdN7g-0

[NMP2021 - 11th Optoelectronics and Photonics Summer School] “Neuromorphic
Computing for Neural Networks”, 20-26 June 2021, Monte Bondone, Italy. Speaker: Claudio

Gallicchio. Audience size: around 100 people. Duration: 2 hours. Program of the school:
https://event.unitn.it/nmp2021/.

Technical equipment

The tutorial
microphones, and usb-c or hdmi cables for slides presentation.

requires standard conference equipment,

including a video projector,

