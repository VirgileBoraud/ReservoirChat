Recurrent Neural Networks Models for Developmental
Language Acquisition: Reservoirs Outperform LSTMs
Xavier Hinaut, Alexandre Variengien

To cite this version:

Xavier Hinaut, Alexandre Variengien. Recurrent Neural Networks Models for Developmental Lan-
guage Acquisition: Reservoirs Outperform LSTMs. SNL 2020 - 12th Annual Meeting of the Society
for the Neurobiology of Language, Oct 2020, Virtual Edition, Canada. ￿hal-03146558￿

HAL Id: hal-03146558

https://inria.hal.science/hal-03146558

Submitted on 19 Feb 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Recurrent Neural Networks Models for Developmental Language Acquisition: 
Reservoirs Outperform LSTMs

Xavier Hinaut1,2,3\*, Alexandre Variengien1,2,3,4
1Inria, Bordeaux, France. 2LaBRI, UMR 5800, Talence, France. 3Neurodegeneratives Diseases Institute, 
UMR 5293, Bordeaux, France. 4Ecole Normal Supérieure de Lyon, France. \*xavier.hinaut@inria.fr

INTRODUCTION

METHODOLOGY

RESULTS

RESULTS

for

previously

developed

cortico-striatal models

sentence
We
comprehension (Hinaut & Dominey 2013) and sentence production (Hinaut
et al. 2015). The sentence comprehension model is based on the reservoir
computing principle (Jaeger & Haass 2004): a random recurrent neural
network (a reservoir) provides a rich recombination of sequential word
inputs (e.g. a piece of prefrontal cortex), and an output layer (e.g. striatum)
learns to "reads-out" the roles of words in the sentence from the internal
recurrent dynamics. The model has several interesting properties, like the
ability to predict the semantic roles of words during online processing.
corpus
Additionally, we demonstrated its
complexities, in different languages (Hinaut & Twiefel 2019), and even its
ability to work with bilingual inputs.

to various

robustness

ABSTRACT

In this study, we propose to (1) use the model in a new task related to a
developmental language acquisition (i.e. Cross-Situational Learning) (Juven
& Hinaut 2020; Dinh & Hinaut 2020), (2) provide a quantitative comparison
with one of the best performing neural networks for sequential tasks (a
LSTM), and (3) provide a qualitative analysis on the way reservoirs and
LSTMs solve the task. This new Cross-Situational Task is as follows: for a
given sentence, the target output provided often contains more detailed
features than what is available in the sentence. Thus, the models have not
only to learn how to parse sentences to extract useful information, but also
to statistically infer which word is associated with which feature. While
reservoir units are modelled as leaky average firing rate neurons, LSTM
units (Hochreiter & Schmidhuber, 1997) are engineered to gate information
using a costly and biologically implausible learning algorithm (Back-
Propagation Through Time).

RESULTS

sizes

vocabulary

We found that both models were able to successfully learn the task: the
LSTM reached slightly better performance for the basic corpus, but the
reservoir was able to significantly outperform LSTMs on more challenging
corpora with increasing
given set of
hyperparameters). We analysed the hidden activations of internal units of
both models. Despite the deep differences between both models (trained
or fixed internal weights), we were able to uncover similar inner dynamics:
the most useful units (with strongest weights to the output layer) seemed
tuned to keep track of several specific words in the sentence. Because of its
learning algorithm, it is predictable to see such behaviour in a LTSM but not
in a reservoir; in fact, the LSTM contained more tuned-like units than the
reservoir.

(for

a

CONCLUSION

The qualitative and quantitative differences between LSTMs and reservoirs
highlights the gap between classical Deep Learning approaches (based on
back-propagation algorithm) and more plausible brain learning mecha-
nisms. First, the reservoir is more efficient in terms of training time and cost
(the LSTM needs several passes on the training data, while the reservoir
uses only one). Secondly, only the reservoir model seems to scale to larger
corpora without the need to adapt specifically the hyper-parameters of the
model. Finally, the presence of more tuned units in the LSTM compared to
the reservoir might be an explanation of why the LSTM overfits to the
training data. More generally, a LSTM seems to have more limited
generalization capabilities than a reservoir when available data is limited.

The Reservoir Computing principle as cortico-striatal model
Only the output connections (red dashed) are trained, the input (green) and
recurrent connections (black) are randomly generated and kept fixed. Non-
inputs
linear dynamics
recombinations” from which interesting features could be “read out” from
the outputs. The reservoir used here is an Echo State Network (ESN) [3].

inside the reservoir

create “a pool of

Inp(cid:88)(cid:87)(cid:86)

O(cid:88)(cid:87)p(cid:88)(cid:87)(cid:86)

Random (cid:90)eigh(cid:87)s

Learned (cid:90)eigh(cid:87)s

Ac(cid:87)i(cid:89)a(cid:87)ion (cid:87)hro(cid:88)gh (cid:87)ime

Model and set-up
The model has to reconstruct an imagined scene from the sentence given
word by word. The simulated vision creates a perceptual representation
corresponding to the full description of objects in the scene. This
representation is used as target outputs for the reservoir, even if the
sentence only partially describes the objects in the scene, or if it describes
only one object. Thus the model has to learns statistically which word is
linked to which feature. This particular set-up creates cross-situational
learning conditions similar to the ones children are facing. The set-up, input
and target outputs were the same for the LSTM experiments.

(cid:90)ord

Inp(cid:88)(cid:87) ac(cid:87)i(cid:89)a(cid:87)ion (cid:87)hro(cid:88)gh (cid:87)ime

Lo(cid:90) ac(cid:87)i(cid:89)a(cid:87)ion(cid:172)

High ac(cid:87)i(cid:89)a(cid:87)ion(cid:172)

"The c(cid:88)(cid:83) i(cid:86) (cid:82)(cid:81) (cid:87)he (cid:85)igh(cid:87)"

(cid:87)he

i(cid:86)

c(cid:88)p

I(cid:80)ag(cid:76)(cid:81)ed
(cid:86)ce(cid:81)e

<(cid:87)i(cid:86)(cid:86)(cid:88)e(cid:86)>
<(cid:79)ef(cid:87)>



<(cid:85)igh(cid:87)>



<(cid:85)ed>

<(cid:82)b(cid:77) 1>

<(cid:82)b(cid:77) 2>

Hea(cid:85)d (cid:86)e(cid:81)(cid:87)e(cid:81)ce
de(cid:86)c(cid:85)(cid:76)b(cid:76)(cid:81)g (cid:87)(cid:75)e
(cid:86)(cid:76)(cid:87)(cid:88)a(cid:87)(cid:76)(cid:82)(cid:81)

.
.
.

Win

Reservoir
ESN
= ESN

Wo(cid:88)t

I(cid:80)ag(cid:76)(cid:81)ed
(cid:83)e(cid:85)ce(cid:83)(cid:87)(cid:88)a(cid:79) (cid:85)e(cid:83)(cid:85)e(cid:86)e(cid:81)(cid:87)a(cid:87)(cid:76)(cid:82)(cid:81):
(cid:82)(cid:88)(cid:87)(cid:83)(cid:88)(cid:87) af(cid:87)e(cid:85) (cid:87)(cid:75)e (cid:79)a(cid:86)(cid:87) (cid:87)(cid:76)(cid:80)e (cid:86)(cid:87)e(cid:83)

Outputs of the reservoir after training
x-axis is time, y-axis is the output neurons activities. We can see a jump in
the related output after a keyword is seen. Which means that after each
word the model updates the outputs. This is an interesting property of the
model because it was only trained to give the output during the whole
sentence, not when a specific word appeared. In the Object 2 outputs (left),
we see that “orange” is first recognized as an object (noun), but when “cup”
arrives it is then recognized as a color (adjective).

Object 1

Object 2






1.0

0.5

0.0






1.0

0.5

0.0

B E GIN

on

the
right

there is

the

and

orange
blue

a

orange

cup is on

the
right

E N D

B E GIN

on

the
right

there is

the

and

orange
blue

a

orange

cup is on

the
right

E N D





1.0

0.5

0.0





1.0

0.5

0.0

B E GIN

on

the
right

there is

the

and

orange
blue

a

orange

cup is on

the
right

E N D

B E GIN

on

the
right

there is

the

and

orange
blue

a

orange

cup is on

the
right

E N D






1.0

0.5

0.0






1.0

0.5

0.0

B E GIN

on

the
right

there is

the

and

orange
blue

a

orange

cup is on

the
right

E N D

B E GIN

on

the
right

there is

the

and

orange
blue

a

orange

cup is on

the
right

E N D

RESULTS

Testing the generalisation capabilities of reservoirs and LSTMs
Averaged errors for test data set for the reservoir (=ESN) and different
sizes of LSTMs (20/40/80 units). Hyperparameters were optimized for all
models with 4 objects in the dataset.

"The c(cid:88)(cid:83) i(cid:86) (cid:82)(cid:81) (cid:87)he (cid:85)igh(cid:87)"

Va(cid:79)id ?

E(cid:91)ac(cid:87) ?

REREFENCES

1. Hinaut & Dominey, Plos ONE, 2013 .
2. Hinaut et al., Brain & Language, 2015.
3.
Jaeger & Haas, Science, 2004.
4. Hinaut & Twiefel, IEEE TCDS, 2019.
Juven & Hinaut, IJCNN, 2020.
5.
6. Dinh & Hinaut, ICANN, 2020.
7. Hochreiter & Schmidhuber, Neural Computation, 1997.

Detailed of internal neuron activations for a LSTM and a reservoir
(Left) LSTM: (top) Activity variation of all 20 neurons of a LSTM. (bottom) Detail of 1 neuron particularly sensitive to the word “and”.
(Right) Reservoir: (top) Activity variation of a selection of neurons. (bottom) Raw activity of a selection of neurons in the reservoir and sum of variations.

Absolute variation of the activation of reservoir neurons during the processing of the sentence

reservoir unit 838 
reservoir unit 563 
reservoir unit 122 
reservoir unit 966 
reservoir unit 424 

s
n
o
r
u
e
n
r
i
o
v
r
e
s
e
r

f
o
n
o
i
t
a
v
i
t
c
a

e
h
t

f
o

n
o
i
t
a
i
r
a
v

l

e
t
u
o
s
b
A

0.030

0.025

0.020

0.015

0.010

0.005

0.000

Sim(cid:88)la(cid:87)ed (cid:89)i(cid:86)i(cid:82)(cid:81)

+ dW

<(cid:87)i(cid:86)(cid:86)(cid:88)e(cid:86)>
<(cid:79)ef(cid:87)>



<(cid:85)igh(cid:87)>



<(cid:85)ed>

Pe(cid:85)ce(cid:76)(cid:89)ed
(cid:86)ce(cid:81)e

CNN

Pe(cid:85)ce(cid:83)(cid:87)(cid:88)a(cid:79) (cid:85)e(cid:83)(cid:85)e(cid:86)e(cid:81)(cid:87)a(cid:87)(cid:76)(cid:82)(cid:81):
c(cid:85)ea(cid:87)ed (cid:90)(cid:76)(cid:87)(cid:75) a fa(cid:78)e (cid:89)(cid:76)(cid:86)(cid:76)(cid:82)(cid:81)
(cid:80)(cid:82)d(cid:88)(cid:79)e

<(cid:82)b(cid:77) 1>

<(cid:82)b(cid:77) 2>

B E GIN

a

blu e

glass

is

o n

th e

left

a n d

th at

is

a

ora n g e

cu p

E N D

Activation of reservoir neurons during the processing of the sentence

reservoir unit 648 
reservoir unit 2 
reservoir unit 866 
reservoir unit 834 
reservoir unit 844 
reservoir unit 122 
reservoir unit 304 
reservoir unit 703 
reservoir unit 996 
reservoir unit 887 
Sum of the absolute variations of 
the activations of the reservoir units

0.15

0.1

0.05

0.0

n
o
i
t
a
v
i
t
c
a

t
i
n
u
r
i
o
v
r
e
s
e
R

-0.05

-0.1

BE GIN

a

blue

glass

is

on

the

left

and

that

is

a

orange

cup

EN D

0.3

0.2

0.1

0.0

S
u
m
o
f

t
h
e

a
b
s
o
u
t
e

l

v
a
r
i
a
t
i
o
n

o
f

t
h
e

r
e
s
e
r
v
o
i
r
u
n
i
t
s
'

a
c
t
i
v
a
t
i
o
n

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
