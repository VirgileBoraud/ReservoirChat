Language Acquisition with Echo State Networks: Towards Unsupervised
Learning Thanh Trung Dinh, Xavier Hinaut

To cite this version:

Thanh Trung Dinh, Xavier Hinaut. Language Acquisition with Echo State
Networks: Towards Unsu- pervised Learning. ICDL 2020 - IEEE
International Conference on Development and Learning, Oct 2020,
Valparaiso / Virtual, Chile. ￿hal-02926613￿

HAL Id: hal-02926613

https://inria.hal.science/hal-02926613

Submitted on 31 Aug 2020

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

Language Acquisition with Echo State Networks: Towards Unsupervised
Learning

Thanh Trung Dinh 1. INRIA Bordeaux Sud-Ouest. 2. LaBRI, Bordeaux INP,
CNRS, UMR 5800. 3. Institut des Maladies Neurod´eg´en´eratives,
Universit´e de Bordeaux, CNRS, UMR 5293. Bordeaux, France.
orcid.org/0000-0003-0249-2080

Xavier Hinaut 1. INRIA Bordeaux Sud-Ouest. 2. LaBRI, Bordeaux INP, CNRS,
UMR 5800. 3. Institut des Maladies Neurod´eg´en´eratives, Universit´e de
Bordeaux, CNRS, UMR 5293. Bordeaux, France.
orcid.org/0000-0002-1924-1184

Abstract—The modeling of children language acquisition with robots is a
long quest paved with pitfalls. Recently a sentence parsing model
learning in cross-situational conditions has been proposed: it learns
from the robot visual representations. The model, based on random
recurrent neural networks (i.e. reser- voirs), can achieve signiﬁcant
performance after few hundreds of training examples, more quickly that
what a theoretical model could do. In this study, we investigate the
developmental plausibility of such model: (i) if it can learn to
generalize from single-object sentence to double-object sentence; (ii)
if it can use more plausible representations: (ii.a) inputs as sequence
of phonemes (instead of words) and (ii.b) outputs fully independent from
sentence structure (in order to enable purely unsupervised
cross-situational learning). Interestingly, tasks (i) and (ii.a) are
solved in a straightforward fashion, whereas task (ii.b) suggest that
that learning with tensor representations is a more difﬁcult task

Index Terms—Reservoir Computing, Echo State Networks, Unsupervised
Learning, Cross-Situational learning, Robot, Lan- formation and symbol
ground- guage Learning, Concept ing/emergence, Language acquisition,
Language and semantic reasoning.

I. INTRODUCTION

Numerous studies investigate various aspects of children language
acquisition. During the ﬁrst year of life, children already combine
numerous steps [1], as well as various mechanisms such as sensorimotor
imitation learning [2] within their language acquisition process.
Developmental psychology studies focus on the developmental aspects of
how different steps enable to stack upon one another, on models
investigating how the various mechanisms can be combined to manage all
these steps one after another [3], and on subsequent integration of such
models in robots [4]. Some psychological studies investigate the bases
of interaction created between an infant and its caregiver [5],
necessary to root verbal communication and the formation of abstract
symbols [6]. Harnad stated the famous Symbol Grounding Problem [7] and
shown the importance for a system manipulating symbol to “anchor” their
meanings to “raw” perceptions: the so-called symbol grounding that
several studies experimented in robots [8], [9]. From a developmental
perspective, a bottom-up approach that let symbols emerge seems
relatively feasible and more

appropriate [10]. Within the family of symbol grounding research, some
studies investigate how biologically plausible neural-based mechanism
could model the language processing in brain [11]. However, few studies
directly investigate how to model developmental aspects of symbol
grounding learn- ing. Thus, our study is motivated towards this research
axe, extending a recent model proposed by Juven et al. [12] for language
grounding with reservoir computing.

II. RELATED WORK

Our study is inspired by the work of Juven [12] on lan- guage
acquisition model with reservoir computing via cross- situational
learning. In his work, Juven adapted the ResPars model extensively
described in [11] jointly with its neurobio- logical foundations. The
model is able to capture semantics in a complex sentence by
co-occurrences of words and perform sentence-level comprehension.
Juven’s is trained with teacher signals provided by a simulated vision.
The model is trained via cross-situational learning: sentences given at
input only describe partially the simulated vision (i.e. object features
or entire objects can be omitted in the sentence), while teacher signals
contain information on complete visual perception. Therefore, Juven’s
model could theoretically enable robots to learn language basis
(i.e. description of objects and their features given non trivial
sentences) by themselves without any supervision.

Nevertheless, Juven’s model has limits. The ﬁrst limit is that words are
used as input to the model. Some works have shown that words could be
extracted directly from speech (i.e. word discovery [13]). However due
to the natural ambiguity of speech, contextual information is often
necessary to correctly segment words from continuous speech. In
addition, devel- opmental studies have shown that children ﬁrst
discriminate phonemes before learning to recognize words and their mean-
ings [1]. Thus, whether used for Human-Robot Interaction ap- plications
or children-linguistic developmental research, word acquisition from raw
audio signals would be a ﬁrst prerequisite for Juven’s model.
Importantly, Juven’s output representation depends on the number and the
chronological order in which objects are described. Thus, teacher
signals still depend on

aimed both to model brain cortical areas in the computa- tional
neuroscience ﬁeld, and to overcome gradient vanishing problem suffered
by RNNs trained with Back Propagation Through Time (BPTT) in the machine
learning ﬁeld. In the RC paradigm, RNN hidden units and input weights
are immutable and randomly initialized; only output weights
(i.e. readout layer) are trainable. Training a RNN within RC paradigm
only requires to update readout layer instead of training the whole
network with BPTT. Figure 1 shows how RNN model with RC paradigm is used
in the work of Juven.

In our study, we focus on Echo State Network [14] (ESN), an instance of
RC paradigm. Equations (1) and (2) compute internal states and outputs
of the ESN respectively for each time step. In general, each input
”step” of the sequence is projected to a high-dimensional space inside
the reservoir via the input weights and then the recurrent internal
connections: this captures non-linear relations between different parts
of the input sequence. Then, the readout layer maps the internal space
to output space, where only interesting relations are kept during
training. In the equations below: W, Win and Wout are the reservoir
internal, input and output weights matrices; xt, ut and yt denote
internal states, input and output at time t. xt = (1 − α)xt−1 + α tanh
(cid:0)Wxt−1 + Winut

(1) 

(cid:1)

rt =

(cid:19)

(cid:18) 1 xt

yt = Woutrt

(2) 

Similarly as how human learns language, we use an iterative and gradual
online learning. In our study, we use FORCE learning [15], an online
learning method for reservoirs. The general idea of FORCE is to ﬁt the
readout layer weights so as to keep model prediction error small and
stable with any new data. FORCE learning employs a modiﬁed version of
Recurrent Least Square (RLS) algorithm to update readout layer Wout for
each time step. Equations (3) and (4) computes e− (i.e. prediction error
at time t) and Pt (i.e. correlation t matrix between current state and
history states) respectively. Readout weights are updated via equation
(5) at the end of each time step.

t

t = ypredict e−

t

− ytarget t

= Wout

t−1rt − ytarget

t

P0 =

I α

δPt = −

Pt−1rtr(cid:62) 1 + r(cid:62)

t Pt−1 t Pt−1rt

Wout

0 = 0

δWout

t = −e−

t r(cid:62)

t Pt

(3) 
(4) 
(5) 

Since FORCE learning uses RLS under the hood, regulariza- tion parameter
α plays an important role. Used for initializing matrix P and acting as
learning rate, α has to be selected appropriately for speciﬁc task.
Small α results in fast learning, but also makes weights change so
quickly that the model may become unstable. By contrast, a model with
large α has a slower learning rate, but sometime unable to keep up with
rapid change in data dynamics.

The ﬁgure represents an entire cross-situational learning pipeline, Fig.
1. containing a vision model and a language model. As can be seen below
the legend (on the top left), the ResPars model is composed of an Echo
State Network (ESN) which receives a sequence of words (i.e. a sentence)
as input and is trained to recognize the objects described in the
sentence. While training the ESN, only the output weights Wout are
modiﬁed. Visual perception of the scene (described by the sentence) is
provided by the vision part and converted into teacher output for the
language model. In Juven’s work and also our study, simulated vision
replaces the real vision model. The output representation for language
model proposed by Juven is structured by the chronological order in
which objects are described in the sentence. If only one object is
described, this object should be represented in < obj1 >; in such case
simulated vision would still ﬁll in the teacher output with a random
object for < obj2 >; Juven’s model successfully learns to ignore such <
obj2 > in such case. Image comes from [12].

sentence structure, even if the model can be trained with cross-
situational learning, which prevents the model from learning in a fully
unsupervised fashion.

Our study focuses on exploring the same architecture for language
acquisition model proposed by Juven [12] (ﬁgure 1), under the
developmental point of view, as well as different conditions to overcome
the limits of Juven’s work. We inves- tigate the model for: (a) its
ability to generalize from short single-object sentences to longer
double-object sentences, (b) its capability to handle sequences of
phonemes, (c) a new output representation independent of the order that
objects are described in the sentence, thus allowing fully unsupervised
cross-situational learning.

III. METHODS

A. Reservoir computing and FORCE learning

Reservoir Computing (RC) is a paradigm for training Re- current Neural
Networks (RNN). It is simple, efﬁcient and only requires
low-computational resources. Originally, it was

“The cup is on the right”…theiscupWinWout+ dWImaginedperceptual
representation:output after the last time stepPerceptual
representation:created with a fake visionmoduleCNNESNPerceivedsceneHeard
sentencedescribing thesituationSimulated visionImaginedsceneInput
activation through timewordLow activation High activation To implement
reservoir models in our study, we used ReservoirPy v0.2 [16]. It is an
efﬁcient library to design ESNs, which already supports ofﬂine and
online training, as well as computational parallelization, fast
reservoir initialization and other necessary utilities, such as
optimized parameters search with hyperopt [17]. For supporting FORCE
learning, ReservoirPy is extensively used in our study to facilitate
implementation effort.

In order to explore if reservoir model for language ground- ing is
capable to deal with phonemes, we experimented on Ju- ven’s model. Words
in the corpus are converted into sequences of phonemes based on the
Carnegie Mellon University word- phoneme dictionary (CMUdict v0.07)2 as
described in [18]. Then, using the same way for input encoding as with
words (i.e. one hot encoding for each phoneme), we train the model on
corpus of phonemes and evaluate its performance.

B. Corpus and teacher output generation

D. Generalization property

Corpus contains sentences given to the model at input. In our work, the
corpus are generated under our designed grammar (ﬁgure 2). Comparatively
to the grammar introduced in Juven’s work, we use 4 objects, 6 colors
and 3 positions (instead of 4, 4, 4 respectively). In addition, we add
one more form for sentences: there is the (< color >?) < object > on the
< position >.

Teacher outputs are expected outputs after processing input sentences,
used to train the model. In our study, teacher output represents exactly
objects described in the sentence. More concretely, if the sentence
describes an object without color, teacher output shows an object with
unknown-color feature. We had also performed experiments without this
unknown category on our proposed model, but we did not manage to obtain
good enough performances. Further solutions would be explored in future
work.

OBJ → cup | bowl | apple | spoon COL → red | orange | yellow | green |
blue | magenta POS → left | middle | right THE → a | the THIS → (this |
that) SENTENCE-1-OBJ → THIS is THE (COL)? OBJ

THE OBJ (on the POS)? is COL
THE (COL)? OBJ is on the POS
there is THE (COL)? OBJ on the POS
on the POS (there)? is THE (COL)? OBJ

SENTENCE-2-OBJ → SENTENCE-1-OBJ

SENTENCE-1-OBJ and SENTENCE-1-OBJ

Fig. 2. Grammar used to generate the corpus. The grammar can generates
sentences describing one or two objects depending on the scenario. The
total number of different sentences that could be generated is 952976 (=
9762).

C. Phoneme inputs

One of the limits in Juven’s model [12] is the use of words as input,
because it is not natural for HRI applications, as well as for modeling
linguistic development in children. Meanwhile, phoneme seems more
biologically plausible, since children develops their capacity to
recognize phonemes1 be- fore learning words.

1A phone is the smallest acoustic unit in human language, and a phoneme
is the smallest invariant unit (a phoneme can include several variants
of phones: e.g. different variants of pronouncing word ”a”).

Experimenting on Juven’s model, we discovered an in- teresting property
showing the developmental plausibility of reservoir-based language
models. In general, the model, trained on single-object sentences
(e.g. ”there is a red cup on the left”), is capable of recognizing
object features described in longer sentences. In our case, we tested
with double-object sentences, where each sentence is a concatenation of
2 single- object sentences and the word “and” (e.g. ”the red apple is on
the right and there is blue bowl on the left”). Given that all object
features are already encountered by the model when it is trained on
single-object sentences, the model can still recognize those features
and ignore the words like “and”, which do not play an important role in
describing objects, in longer sentences even though the model has never
seen those sentences before.

Despite having this very interesting property, the output representation
in Juven’s model depends on the number of objects predeﬁned in the
scene, which is tightly coupled with the sentences describing it. If the
model was trained on single- object sentences, it can only return one
object at output. Thus, the object features recognized in double-object
sentences are mixed up and hardly separable in order to extract them
from the output representation.

E. Output representation

Important limits in Juven’s model stem from its output representation,
which is dependent on each utterance and does not allow to train the
model in a purely unsupervised fashion. Hence, we propose a new output
representation which is independent of the order in that objects are
mentioned, as well as the number of objects in the scene.

In our experiments, each object has 3 features: category (e.g. cup),
color (e.g. red) and position (e.g. right). Thus, we use a 3D tensor to
represent output, where each axis represents one feature. Provided that
there is no two identical objects (with the exact same features) in a
sentence, this output can represent all objects mentioned in the
sentence at once. Further work will explore if updated output
representations can handle cardinality (i.e. number of the same objects)
in the output activation level.

In our study, we choose to use a sparse encoding scheme for output, so
as to speed-up computations. In our proposed representation, each cell
(i.e. smallest unit inside the tensor) identiﬁes an object with all of
its features (i.e. the coordinates

2CMUdict can be found at http://www.speech.cs.cmu.edu/cgi-bin/cmudict.

of the cell). The teacher output is, therefore, generated by ﬁrstly
initializing the whole tensor with zeros, then setting all the cells
corresponding to objects described in the sentence with 1. This is the
so-called object activated (or cell activated) encoding scheme. Figure 3
shows an example of this scheme. Consistent to this encoding scheme,
given the number of objects in the scene, the results are extracted from
the tensor representation by selecting that same number of cells with
highest values among others.

set contains sentences that the model has never encountered during
training.

This section is structured as following: (i) experiment on the model
capability to generalize from single-object to double- object sentences,
(ii) experiment on model’s performance with phonemes input, (iii)
experiments on model’s performance with tensor output representation
under various conditions, including: different number of sentences for
training, different reservoir sizes. The experiment derivation tree is
listed below, based on different studies on reservoir language model

• Input representation

– word (experiment I, II, III) – phoneme (experiment II)

• Output representation

– Juven’s model (experiment I, II) – tensor

∗ object activated (experiment III) ∗ feature activated ∗ lateral
inhibition

1)  Experiment I: Generalization property: In this experi- ment, we
    evaluated Juven’s model for its capability to gener- alize from
    single-object to double-object sentences. The model is trained on
    1000 single-object sentences and then tested on double-object
    sentences. The output is plotted to show how the object features are
    activated at output during the whole sentence. In the plot, y axis
    illustrates the activation intensity, while x axis shows the words
    in the sentence. The plot is provided and discussed in subsection
    IV-A.

2)  Experiment II: Phonemes input: In this experiment, we compare the
    performance of Juven’s model on input with phonemes and words. The
    model is evaluated on 2 scenar- ios: single-object and double-object
    corpus. According to the grammar described in ﬁgure 2, there are
    much less single- object sentences than double-object sentences.
    Thus for single- object corpus, we used a same size of 400 sentences
    for train and test set, while in the second scenario corpus size
    increases to 1000. Results are provided in subsection IV-B.

3)  Experiment III: Tensor representation: In this experi- ment, we
    evaluate the model’s performance with tensor rep- resentation. With
    hyperopt, the optimal values for reservoir hyperparameters are:
    spectral radius λ = 0.3, regularization coefﬁcient α = 10−8, leak
    rate = 0.15, input scaling = 1.0, reservoir sparsity = 0.1. The
    model is then trained and evalu- ated on double-object corpus, using
    those optimal values for reservoir hyperparameters. In this
    experiment, we investigate the model’s performance with respect to
    (w.r.t)

(a) Number of sentences for training: The reservoir size is ﬁxed at 1000
    neurons. The model is trained on 10,000 sentences in total. For each
    step of 1000 sentences, the model is tested on 300 sentences. The
    model’s perfor- mance is measured and plotted. Results are provided
    and discussed in subsection IV-C. The optimal number of sentences
    for training is used in the next experiment.
(b) Reservoir size (i.e. number of recurrent hidden units, rep-
    resenting memory capacity and non-linear computational

Fig. 3. Example of teacher output for provided sentence with our
proposed output representation. The 3D tensor is projected alongside 2
axis, showing the association of object features described in the
sentence.

In our study, we also explored other encoding schemes for tensor
representation, though the model does not achieve signiﬁcant performance
with them.

• Feature activated (or plan activated): the tensor is initial- ized
with zeros. Then, for each feature mentioned in the sentence, a “weight”
is added to the values of the cells representing objects sharing that
feature in common (i.e. the cells on a same “feature plan”). Finally,
the same weight is added for the cells representing exactly the objects
described in the sentence.

• Lateral

taking

inhibition:

from Self- Organising Maps [19], each cell representing exactly the
object is positively activated, while all the plans sharing these cells
are negatively activated.

inspiration

F. Evaluation

The valid evaluation metric as presented in the work of Juven is adopted
to measure how well our model performs. An output is considered valid
when it contains at least all objects and their features described in
the sentence. For example, a valid output for sentence ”there is a cup
on the left” can be (< cup >, < lef t >, < red >).

G. Experiments

This part summarizes the experiments conducted during our study. For all
experiments, the model is trained with FORCE the end of each learning
and readout sentence. Optimial values for reservoir hyparameters were
obtained with hyperopt beforehand. In the experiments where the model’s
performance is measured (experiment II, III), test

layer is updated at

1-object scenario (train = 400 / valid = 400) 2-object scenario (train
= 1000 / valid = 1000)

Phoneme input Word input

0.0 (±0.0)

0.0 (±0.0)

0.6 (±0.39)

1.6 (±0.66)

TABLE I ERROR ON VALID METRIC FOR SINGLE-OBJECT AND DOUBLE-OBJECT
SCENARIOS WITH PHONEMES OR WORDS INPUT

Fig. 4. Capability to generalize from single-object to double-object
sentences. Y axis: activation intensity, X axis: words in the sentence.

Fig. 5. Error (evaluated with valid metric) of model w.r.t number of
sentences for training.

power of reservoir models): The model is trained on 5,000 sentences and
also tested on other 300 sentences. The performance is plotted and
discussed in subsection IV-D.

IV. RESULTS

A. Experiment I: Generalization property

Figure 4 shows how the model behaves with double-object sentences when
being trained on single-object sentences only. The plot shows the
activation of object features at output. From the ﬁgure, we can observe
that correct object features are activated when the model encounters the
words describing them. Since the teacher output is only given to the
model at the end of the sentence, this interesting property indicates
that the model is able to intrinsically “compare” the sentences given at
input and teacher signals at output to learn to know which word
describes which object feature. Moreover, the model never encountered
double-object sentences during training, but it can still trigger
correct features of second object at output. Thus, this behavior
suggests that the model is able to learn and “remember” the mapping of
words and object features that those words describe from simple
single-object sentences and generalize to longer sentences.
Consequently, when a double- object sentence is given to the model,
correct features are activated corresponding to the words describing
them.

B. Experiment II: Phonemes input

Table I compares model error evaluated with valid metric (subsection
III-F) on phonemes and words input. As we can observe from the table,
the model has similar performance with phonemes as well as with words:
both achieves an error of around 1%.

C. Experiment IIIa: Number of training sentences

Figure 5 shows the evolution of model’s error rate w.r.t the number of
sentences used for training. The plot shows a steep decrease in error
rate from 85% to 35% when the number of sentences increases from 1000 to
2000. The trend continues, but more slightly, to 5000 sentences before
ﬂuctuating around 17% of error rate. Since our model needs more
sentences for training in comparison with Juven’s model before reaching
a reasonable performance, it suggests that learning with tensor
representation is a more difﬁcult task. Based on those results, we
select 5,000 sentences to train our models for the rest of our
experiments.

D. Experiment IIIb: Number of reservoir hidden units

Figure 6 shows model’s error rate in relation to the reservoir size. As
we can see from the plot, a reservoir with more neurons performs better
(i.e. having a lower error rate). Indeed, the number of neurons is
considered as memory capacity and non-linear computational power of the
reservoir. As the experiments IIIa suggest that learning with tensor
output is more difﬁcult, more neurons can help to improve the global
performance of reservoir models. However, the performance does not
improve much when the number of neurons exceeds 2000, this may suggest
that other hyperparameters have to be modiﬁed so that the model can
achieve better performance. Hence, a reservoir of 2000 neurons and
trained with 5,000 sentences probably achieves its best performance in
our study. Supplementary experiments and results can be found at

https://github.com/neuronalX/DinhHinaut2020-ICDL

V. DISCUSSION Despite developmental language evidences and robot learn-
ing progress [4], the modeling of children-like language ac-

also more difﬁcult to learn for LSTMs Finally, we want to use a
hierarchical-task reservoir [20] in order to decompose this difﬁcult
task in two sub-tasks. A ﬁrst reservoir would associate sequence of
phonemes to individual concepts/features (i.e. object features), and a
second reservoir would merge the concepts together within a complete
object representation.

Currently, as a follow-up to this study, we are investigat- ing a new
representation, which is also independent of the sentence structure but
has a much smaller size than tensor representation. More importantly,
this novel representation scales because its size is proportional to the
number of object features, and because of its ability to exploit
effectively and extensively the generalization property.

REFERENCES

[1] P. K. Kuhl. Early language acquisition: cracking the speech code.
Nature

Reviews Neuroscience, 5(11):831–843, Nov 2004.

[2] S. Pagliarini et al. Vocal Imitation in Sensorimotor Learning
Models: a Comparative Review. HAL Preprint hal-02317144, October 2019.
[3] E. Dupoux. Cognitive science in the era of artiﬁcial intelligence: A
roadmap for reverse-engineering the infant language-learner. Cognition,
173:43–59, April 2018.

[4] A. Cangelosi et al.

Integration of action and language knowledge: A roadmap for
developmental robotics. IEEE Transactions on Autonomous Mental
Development, 2(3):167–195, 2010.

[5] I. Nomikou et al. Taking up an active role: emerging participation
in early mother–infant interaction during peekaboo routines. Frontiers
in psychology, 8:1656, 2017.

[6] J. Raczaszek-Leonardi et al. Language development from an ecological
perspective: Ecologically valid ways to abstract symbols. Ecological
Psychology, 30(1):39–73, January 2018.

[7] S. Harnad. The symbol grounding problem. Physica D: Nonlinear

Phenomena, 42(1-3):335–346, 1990.

[8] M. Spranger et al. Open-ended procedural semantics. grounding in
robots, pp. 153–172. Springer, 2012.

In Language

[9] X. Hinaut and M. Spranger. Learning to parse grounded language using
reservoir computing. In Proc. of ICDL-Epirob. IEEE, August 2019. [10] T.
Taniguchi et al. Symbol emergence in robotics: a survey. Advanced

Robotics, 30(11-12):706–728, 2016.

[11] X. Hinaut and P. Dominey. Real-time parallel processing of
grammatical structure in the fronto-striatal system: a recurrent network
simulation study using reservoir computing. PLoS ONE, 8(2):e52946, 2013.
[12] A. Juven and X. Hinaut. Cross-Situational Learning with Reservoir
In Proc. of IJCNN

Computing for Language Acquisition Modelling. 2020).

[13] T. Taniguchi et al. Double articulation analyzer with deep sparse
autoencoder for unsupervised word discovery from speech signals.
Advanced Robotics, 30(11-12):770–783, April 2016.

[14] H. Jaeger. The ”echo state” approach to analysing and training
recurrent neural networks. Technical Report 148, German National
Research Center for Information Technology GMD, Bonn, Germany, 1 2001.
[15] D. Sussillo and L. F. Abbott. Generating coherent patterns of
activity from chaotic neural networks. Neuron, 63(4):544–557, Aug 2009.

[16] N. Trouvain et al.

Library to Design Echo State Networks.
https://github.com/neuronalX/reservoirpy.

ReservoirPy: an Efﬁcient and User-Friendly In ICANN, 2020.

[17] J. Bergstra et al. Hyperopt: A python library for optimizing the
hyperparameters of machine learning algorithms. In Proceedings of the
12th Python in Science Conference, pp. 13–20, 2013.

[18] X. Hinaut. Which input abstraction is better for a robot syntax
acquisition model? phonemes, words or grammatical constructions? In
Proc. of ICDL-Epirob. IEEE, 2018.

[19] T. Kohonen. The self-organizing map. Proceedings of

the IEEE,

78(9):1464–1480, 1990.

[20] L. Pedrelli and X. Hinaut. Hierarchical-task reservoir for anytime
pos tagging from continuous speech. In Proc. of IJCNN 2020, 2020. [21]
T.-Y. Lin et al. Microsoft coco: Common objects in context. In European

conference on computer vision, pp. 740–755. Springer, 2014.

Fig. 6. Error (evaluated with valid metric) of model w.r.t reservoir
size.

quisition in robots seems still in an early stage. Juven et al. [12]
proposed a sentence parsing model learning in cross- situational
conditions (a new version of ResPars model [11]): it learns from the
visual scene a robot perceives. In this study, we investigated the
developmental plausibility of this model and proposed new
representations for outputs based on tensors. These new representations
did not rely on the input sentence structure, enabling the model to
learn purely from unsupervised cross-situational learning.

The reservoir-based model

is able to learn on the new proposed representations, even though the
task becomes more difﬁcult. Indeed, twice more neurons and a bigger
training cor- pus are needed to obtain signiﬁcant performance. We
veriﬁed this increased task difﬁculty with extensive hyperparameter
search using hyperopt [17] and the graphical tool provided in
ReservoirPy [16]. The fact that the representation proposed in Juven’s
model is dependent on the sentence structure (i.e. the order in which
words appear) is not speciﬁc to the task, nor to the ResPars model: it
is a common way of representing the role of words in a sentence like in
Semantic Role Labelling. Thus, searching solutions for the
sentence-structure independence is not speciﬁcally limited to our study,
but would be useful in general for developmental language models.

Interestingly, we discovered two abilities of Juven’s model, which were
conﬁrmed also for the tensor representations. Firstly, the model is able
to process sentences composed of se- quence of phonemes with the same
performance as with words, thus making the models not dependent on word
segmentation and categorization. Secondly, the model is able to
generalize to double-object, or probably multiple-object sentences when
it is only trained with single-object sentences. These new abilities
demonstrate more developmentally plausible mechanisms and behaviors. In
particular, this generalization capability indicates that the ResPars
model has intrinsic properties for scaffolding learning.

The new tensor representations is our ﬁrst attempt to reach a version of
ResPars that could learn in fully unsupervised cross-situational
learning conditions, in order to approach how humans, especially
children, learn language. Several tracks can be followed to pursue this
work. For example, experiment with LSTMs on similar tasks to see if the
tensor representations are

SUPPLEMENTARY MATERIAL

We made similar experiments as presented above with our model
(i.e. reservoir model with tensor representation) with the same
conditions, as well as under various changes in teacher output
generation and input types. Same values for hyperparameters (obtained
from hyperopt and experiments in subsection III-G3 on tensor
representation) are used to favor comparison. The experiments provide
interesting results that may be extended in future work.

A. Experiment I-sup: Generalization from 1 to 2 objects

In this experiment, we investigate the capability of our model to
generalize from single-object to double-object sen- tences. The output
of our model is a 3D object-feature tensor with object-activated
encoding scheme. In this experiment, we selectively project the output
tensor alongside individual feature axes to observe how the model
behaves with the features described in the sentence. Results are shown
in ﬁgure 7. In addition, we also project the tensor on a combination of
2 features, in order to see whether the features are correctly
associated together as described in the sentence. The projection is done
with max operation (i.e. taking the max values over all cells that
represent the objects sharing the shame features to be projected on).
The results are given in ﬁgure 8.

Figure 7 shows the same output activation plot as in experiment 4, but
with tensor representation. The model is also trained on single-object
sentences before being tested with double-object sentences. As we can
observe from the plot, our model has the same generalization property,
but the activation intensity is much lower. Besides, unrelated object
features are slightly positively triggered at non-functional words (i.e.
words that do not represent any object features), before being
negatively triggered at functional words. Those differences are
subjected to further study for better understanding on how our proposed
model behaves.

Figure 8 shows a more detail view on the output rep- resentation of our
model with the same sentence in ﬁgure 7. The ﬁgure shows that (< cup obj
>, < right pos >, < green col >) is the most activated cell, correctly
corre- sponding to the green cup on the right. However, the model did
not recognize the red apple on the left. Indeed, cells associated <
apple obj > and < lef t pos > features only have a maximum value of 0.1,
whereas most activated cells associated to < apple obj > come with <
right pos > feature, but they only have a slightly higher intensity at
0.11. This implies that future research to improve the model’s
performance on tensor representation should focus on resolving
correcting these kinds of wrong association.

B. Experiment III-sup: Number of object categories

Fig. 7. Generalization capacity from 1-object to 2-object sentences of
our model. Y axis: activation intensity, X axis: words in the sentence.

Fig. 8. Output tensor projected onto 2 axis, showing the characteristics
associated with objects described in the same sentence as in Figure 7.

model proposed in the work of Juven [12] as baseline for comparison. The
error of such model is computed via equation (6), provided that the
total number of potential colors and positions are predeﬁned (i.e. ncol
= 6 for red, orange, yellow, green, blue, magenta and npos = 3 for left,
middle, right). We adapt the equation with the grammar used for
generating our corpus.

errth(nobj, ntrain) =

1 −

(cid:18)

1 244 × nobj

(cid:19)2×ntrain

(6) 

We investigate the scalability of model with tensor rep- resentation by
controlling the number of objects (i.e. object names). Indeed, more
number of objects result in larger corpus vice versa. In this
experiment, we evaluated the error of our proposed model w.r.t different
number of object categories, varying from 5 to 30. In addition, we also
use a theoretical

Figure 9 illustrates the performance of our model with different number
of object categories (i.e. orange solid line) in comparison with the
error of theoretic model (i.e. grey dashed line). The plot shows that
our model is still not as good as the theoretical model. However, the
error evolution line has similar trend w.r.t number of object
categories.

various conditions, for example different weight added for object
features and entire objects.

Result: 32.9 ± 1.81

COMPLEMENTARY STUDIES

In this section, we introduce a way to ﬁlter the corpus so that the
objects described in the sentences are more realistic, as well as
experimented the new corpus with Juven’s model. Beside, we also tested a
possibility to extend Juven’s model only by changing teacher output
encoding scheme.

F. Vision-biased corpus

We conducted an experiment on Juven’s model with a vision-biased corpus
to explore further the model’s learning capability via cross-situational
learning. Exploiting training labels of COCO dataset for object
segmentation task [21], we extract and ﬁlter only real possible values
for object features available in COCO images when generating corpus.
Thus, the generated corpus is less equally distributed among object
features (e.g. there is no sentence describing a magenta apple), but
closer to human utterances. The experiment is conducted to measure the
error of Juven’s model under the same con- ditions and reservoir
hyperparameters as used in experiment comparing phonemes and words input
(subsection III-G2) for the double-object corpus and words input.
Comparing the result with table I, the model error is higher when
training with this vision-biased corpus. However, an error rate of 2%
demonstrates that the model can still perform well under realistic
conditions of fully cross-situational learning.

Result: 2.1 ± 0.63

G. Abstract-ranking encoding

As an idea to overcome the limit on sentence-structure dependency of the
output representation in Juven’s model, we exploit the tensor
representation to sort objects in the scene with an arbitrary rank (when
the tensor is ﬂatten) before encoded it in teacher output. Thus, the
order that objects are encoded in teacher output does not depend on the
sentence describing the scene anymore, but on how the tensor repre-
sentation is designed. Juven’s model is trained and evaluated under the
same conditions and reservoir hyperparameters as in the experiment
above.

The result shows a high error rate. However, the experiment is still
interesting, since it demonstrates that learning such an arbitrary
ranking is a difﬁcult task for reservoir models. Hence, the lower
performance that we obtained with our tensor representation (compared to
the original model proposed in Juven’s work) is probably due to this new
constraint (for the ResPars model) to have an output representation
completely independent of the structure of the sentence. This is
reassuring in a way, highlighting a potentially inherent issue rather
than the consequences of our speciﬁc representation choices.

Result: 67.4 ± 0.63

Fig. 9. Valid error metric of model w.r.t number of object categories.

C. Phonemes input

We conducted the same experiment on phonemes with our proposed model.
The result demonstrates that our model has the same level of performance
with phonemes as with words, thus can be used to study linguistic
development in children, as well as applied for other HRI applications.

Result with words: 23.1 ± 0.72 Result with phonemes: 19.0 ± 1.52

D. Tensor representation without unknown features

is possible to adapt

We studied whether it

the tensor representation without unknown values on the feature axes
when generating teacher outputs, so that the model allows fully
unsupervised cross-situational learning. Our naive approach is to set
cells representing objects combined with all possible values of the
unknown feature to 1. Literally, it is equivalent to telling the model
that all colors are possible for a cup, when the sentence describes that
cup with unknown color.

However, the result shows that model has higher error rate. Indeed,
teacher outputs generated in that way are unbalanced, because the number
of cells set to 1 depends on the utterance and is inﬂuenced by the
statistics of the training data.

Result: 52.0 ± 1.89

E. Feature-activated (plan-activated) encoding scheme

In this experiment, we studied feature-activated encoding scheme for
generating teacher output. As described in sub- section III-E, this
encoding schemes helps generate a teacher output where a cell with
higher value has more chance to describe the right object. Indeed, this
scheme results in more cells with non-zero values in the tensor,
compared to object- activated encoding scheme, and their valuestr are
proportional to the chance that they describe the right objects in the
scene. We can see from the result that the error decreases by a large
margin compared to previous experiment on representation without unknown
features. Even though the error is still higher than the representation
with unknown features and object- activated encoding scheme. This way of
encoding teacher output interesting for future work, since it allows
fully unsupervised learning on our model. However, more study is needed
to investigate the model’s behaviors under

is still


