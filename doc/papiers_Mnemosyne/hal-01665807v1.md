Teach Your Robot Your Language!
Trainable Neural Parser for Modelling Human Sentence Processing:
Examples for 15 Languages

Xavier Hinaut1,2 and Johannes Twiefel2

Abstract— We present a Recurrent Neural Network (RNN)
that performs thematic role assignment and can be used for
Human-Robot Interaction (HRI). The RNN is trained to map
sentence structures to meanings (e.g. predicates). Previously,
we have shown that the model is able to generalize on English
and French corpora. In this study, we investigate its ability to
adapt to various languages originating from Asia or Europe. We
show that it can successfully learn to parse sentences related to
home scenarios in ﬁfteen languages: English, German, French,
Spanish, Catalan, Basque, Portuguese, Italian, Bulgarian, Turk-
ish, Persian, Hindi, Marathi, Malay and Mandarin Chinese.
Moreover, in the corpora we have deliberately included variable
complex sentences in order to explore the ﬂexibility of the
predicate-like output representations. This demonstrates that
(1) the learning principle of our model is not limited to a
particular language (or particular sentence structures), but
more generic in nature, and (2) it can deal with various kind
of representations (not only predicates), which enables users
to adapt it to their own needs. As the model is inspired from
neuroscience and language acquisition theories, this generic and
language independent aspect makes it a good candidate for
modelling human sentence processing. It is especially relevant
when this model
is implemented in grounded multimodal
robotic architectures.

I. INTRODUCTION

Communicating with the present day robots is a chal-
lenging task for humans. It involves either learning a pro-
gramming language or using complex interfaces. The most
sophisticated robots, that recognise commands given orally,
are often limited to a pre-programmed set of stereotypical
sentences such as “Give me cup”. In this study, we propose
an approach that allows one to use natural language when
interacting with robots in languages originating from Asia
and Europe. Our architecture enables the users to train a
sentence parser easily on potentially many different contexts,
including home scenarios (e.g. grasping remote objects,
cleaning furnitures, ...) [1]. It can also enable them to directly
teach new sentences to the robot [2].

\*We gratefully acknowledge support by a Marie Curie Intra European
Fellowship within the 7th European Community Framework Programme:
EchoRob project (PIEF-GA-2013-627156). We also thank support for the
project “LingoRob - Learning Language in Developmental Robots” by
Campus France PHC Procope Project 37857TF and by DAAD Project
57317821 in the F¨orderprogramm “Projektbezogener Personenaustausch
Frankreich”.

1X. Hinaut is with Inria Bordeaux Sud-Ouest, Talence, France; LaBRI,
UMR 5800, CNRS, Bordeaux INP, Universit´e de Bordeaux, Talence, France;
and Institut des Maladies Neurod´eg´en´eratives, UMR 5293, CNRS, Univer-
sit´e de Bordeaux, Bordeaux, France. xavier.hinaut@inria.fr
2X. Hinaut and J. Twiefel are with Knowledge Technology Group,
Department Informatik, Universit¨at Hamburg, 22527 Hamburg, Germany.
surname@informatik.uni-hamburg.de

Current approaches typically involve developing methods
speciﬁcally aimed at — or only tested on — the English
language. Speciﬁc parsers then need to be designed for other
languages, often adapting the algorithms and methods to
their speciﬁcities. In order to overcome these limitations we
propose a simple and ﬂexible way of training a sentence
parser for an arbitrary language, without requiring any new
programming effort. This enables one to easily create parsers
even for languages which do not have a signiﬁcant corpus
available. We believe that fast
trainable parsers that are
simple and ﬂexible are a key component to make home
robotics more efﬁcient and provide social adaptation to the
users in particular contexts.

In the current paper, we present a previously developed
neural model [3], how it works, what its main properties are
and brieﬂy mention an available ROS module that could be
used within robotic architectures. Then, we demonstrate that
this model is able to learn more ﬂexible representations than
what was previously thought. In particular, these representa-
tions could be easily deﬁned by users and adapted to various
languages. This is done using home scenario corpora with
sentences and representations of various complexities. More
importantly, we show that this is not limited to a particular
language or language family, but works for a variety of
European and Asian languages: English, German, French,
Spanish, Catalan, Basque, Portuguese, Italian, Bulgarian,
Turkish, Persian, Hindi, Marathi, Malay and Mandarin Chi-
nese (traditional and simpliﬁed). Note that the all corpora
have been written by the respective language experts and
not using online or other automatic translator. Afterwards,
we discuss the results and give examples of particular
linguistic cases that are challenging to learn. Finally, we
elaborate why the model is not limited to learn predicate-
like representations, but should be able to learn multimodal
grounded ones.

II. MODEL OF HUMAN SENTENCE PROCESSING FOR HRI
A. Motivations

In this subsection, we explain why we choose the Echo
State Network (ESN) architecture as the core part of the neu-
ral parser: from biological inspiration and language acquisi-
tion theories, to its advantages for Human-Robot Interaction
(HRI). We propose it as one of the tools useful to study
how robot grounded representations could be manipulated in
syntactic sequence of symbols, regardless of the language
used.

Firstly, since brain processes involved in language ac-
quisition and sentence comprehension are still poorly un-
derstood, modelling is the key to make further steps in
understand these processes further. Our neural parser aims at
modelling brain processes of sentence comprehension while
using language acquisition theories [3]. Reservoir Computing
paradigm (such as ESNs) is considered to be a biologically
plausible model of “canonical circuits” (i.e. generic pieces
of cortex) and originates from computational neuroscience
[4][5]. For instance, compared to more classical methods of
RNN training, Reservoir Computing (RC) learning mecha-
nism does not unfold time, which makes it closer to learning
processes in the brain than Back-Propagation Through Time
(BPTT) [6]. More recently, it was also used in neuroscience
studies to decode the neural activity of primate prefrontal
cortex [7], probably because its non-linear computations
in high-dimensional space are in some way similar to the
dynamics of prefrontal cortex.
Secondly, we believe that

the HRI applications need
modules that can be trained quickly (e.g. via one-shot ofﬂine
learning), executed in tens of milliseconds, and can process
inputs in an incremental fashion [8][9] (word by word).
Additionally, our neural parser is interesting in the context of
HRI because there is no need to predeﬁne a parser for each
language; just a training corpus is needed. It is able to process
natural language sentences instead of stereotypical sentences
(like “put cup left”); and it generalizes to unknown sentence
structures (that are not present in the training data set).

Thirdly, as we aim to model cognitive language process-
ing, particularly in robotic architectures, we want the model
– and the symbols it manipulates – to be able to interact
with multimodal and grounded representations [10][11] and
enable them to emerge [12] “through” it. The model has
several features that could facilitate this: online processing
(process word by word the sentence) [3], anytime algorithm
(ability to have a partial or complete answer before the end
of a sentence [3]), ability to train it in a fully incremental
fashion (time step by time step [13]), ability to process
distributed representation of words instead of localist rep-
resentations (unpublished work). This link with multimodal
and grounded representations will be further detailed in
the discussion. Finally, some studies investigated the ability
of such networks to be embodied: a physical version of
Reservoir Computing (the framework containing ESNs) was
shown to be able to perform embodied or morphological
computation [14][15].

B. Reservoir Sentence Processing Model

How do children learn language? In particular, how do
they associate the structure of a sentence to its meaning?
This question is linked to a more general issue: how does the
brain associate sequences of symbols to internal symbolic or
sub-symbolic representations? Generic neural architectures
are needed to help the modelling such general processes.
Echo State Networks (ESNs) [16] are not hand-crafted for
a particular task, but on the contrary can be used for a
broad range of applications. ESNs are neural networks with

a random recurrent layer and a single linear output layer
(called “read-out”) modiﬁed by online or ofﬂine learning.
ESNs are not primarily aimed at symbolic processing, but as
a general architecture, they permit such application. Much
research has been done on language processing with neural
networks [17][18][19] and in particular with ESNs [20][21].
The tasks used were diverse, from predicting the next symbol
(i.e. word) in a sentence to speech processing. In this paper,
the task we perform is thematic role assignment.

Mapping the surface form (sequence of words) onto the
deep structure (meaning) of a sentence is not an easy task
since making word associations is not sufﬁcient. For instance,
a system relying only on the order of semantic words (cat,
scratch, dog) to assign thematic roles is not enough for the
following simple sentences, because even if cat and dog
appear in the same order they have different roles: “The cat
scratched the dog” and “The cat was scratched by the dog”.
It was shown that infants are able to quickly extract and
generalize abstract rules [22], and we want to take advantage
of this core ability without going through all the steps of
abstractions, but rather start at a certain level of abstraction
to build “meta-abstractions” on top. Thus, to be able to
learn such a mapping (from sequence of words to meaning)
with a good generalization ability, we rely on abstractions of
sentences rather than sentences themselves.

In order to teach the model to extract the meaning of a sen-
tence, we base our approach on the notion of construction:
the mapping between a sentence’s form and its meaning. The
hypothesis that children learn such constructions comes from
developmental theories of language acquisition [23][24][25].
It assumes children construct abstract linguistic categories
and schemas during language acquisition. Constructions are
an intermediate level of meaning between the smaller con-
stituents of a sentence and the full sentence itself. For
instance, a simple construction that a child could learn is
“Give me X.” when he/she wants to obtain any object X
out of reach. Constructions could be of variable size and
complexity from morpheme (e.g. anti-, pre-, -ing) to passive
sentences (“The X was Y by the Z”) [24]. Based on the
cue competition hypothesis of Bates and MacWhinney [26],
we make the assumption that most of the mapping between
a given sentence and its meaning can rely on the order of
words, and particularly on the pattern of function words and
morphemes [27].

The reservoir sentence processing model was used in
previous experiments studying it properties to model human
sentence processing [3] and its application to HRI [2]. The
model learns the mapping of the semantic words (i.e. content
words like nouns, verbs, adjectives, adverbs) of a sentence
onto different slots (the thematic roles: e.g. action, location)
of a basic event structure (e.g. action(object, location)). In
other words, the task to ﬁnd the correct predicates for a sen-
tence is equivalent to ﬁnding all the correct roles of the SW
of this sentence. As depicted in Fig. 1, the system processes
a sentence as input and generates corresponding predicates.
Words that are not Semantic Words (SW) are Function Words
(FW), which means that they have a particular syntactic

function in a sentence (eg: “Give the ball to the dog”: here
“to” indicates the recipient of an action).

In other words, most of the time it is possible to ﬁnd the
role of a SW without knowing that word or any other SW
in the sentence (see [27][3] for more details). This is one of
the reasons why it works well with many languages. As we
can see in Figure 1, before being fed to the ESN, sentences
are transformed into a sentence structure (or grammatical
construction [23]). The SWs (i.e. the words that have to
be assigned a thematic role) are replaced by the SW item
before entering the ESN. The processing of the grammatical
construction is sequential (one word at a time) and the ﬁnal
estimation of the thematic roles for each SW is read-out at
the end of the sentence. A current estimation of the predicted
roles is however available at each time step.

Fig. 1.
Sentences are converted to a sentence structure by replacing
semantic words by a SW marker. The ESN is given the sentence structure
word by word. Each word activates a different input unit. During training,
the connections to the readout layer are modiﬁed to learn the mapping
between the sentence structure and the arguments of the predicates. When
a sentence is tested, the most active units are bound with the SW kept in
the SWs memory to form the resulting predicate. (Adapt. from [2].)

C. Advantages of processing constructions

By processing grammatical constructions [23] and not
sentences per se, the model is able to bind a virtually unlim-
ited number of sentences to these sentence structures. Based
only on a small training corpus (a few tens of sentences),
this enables the model
to process future sentences with
currently unknown semantic words if the sentence structures
are similar. Considering a robot that could learn several new
semantic words each day is an interesting advantage: the
robot could learn the SW independently of the sentence.
For instance, it learns some sentences on the 1st day; on
the 2nd day it learns to associate visual features with an
object’s name (i.e. grounding the given name); on the 3rd
day it is able to understand new sentences with these SW as
long as they have appeared in similar context than other SW.
Otherwise it still can learn sentences about new contexts.

D. Usage of the ROS Module

The proposed model was encapsulated in a ROS module
[28]. It is coded in the Python language and uses the rospy

library. The source code, implemented as a ROS module,
is available at github.com/neuronalX/EchoRob On
this repository, one can ﬁnd the different versions of the
models developed and more corpora are available.

When running the program, the reservoir model is trained
using a text ﬁle and a ROS service is initialized. The training
text ﬁle contains sentences along with their corresponding
predicates which make it easily editable. If predicates repre-
sent multiple actions that have to be performed in a particular
order, the predicates have to be speciﬁed in chronological
order:

• bring coffee me, clean table ;

bring me the coffee before you clean
the table

• bring coffee me, cleaning table ;

before cleaning the table bring me
the coffee

This predicate representation enables one to easily integrate
this model into a robotic architecture because motor primi-
tives can be represented in this way. It enables the users to
deﬁne which kind of predicate representation they want to
use. By convention the ﬁrst word is the predicate (i.e. which
plays the role of a kind of function in the programming
sense), and the following words are the arguments (any
number of arguments could be given).

Once initialized, a request could be sent

to the ROS
service: it accepts a sentence (text string) as input and returns
an array of predicates in real time. With an ESN of 100 units,
training the neural parser to learn 200 sentences takes about
one second on a laptop computer. Testing a sentence is of
the order of 10 ms.

E. Previous results

The ability of the model to scale to larger copora depends
on the similarity of the sentence structures in the corpus. If
sentence structures are different variants of similar expres-
sions, then there is no need to use a high number of units
in the reservoir. For instance in [3] we have shown that a
5,000 units reservoir could generalize up to 85% on unkown
sentence structures on a corpus of size 90,000.

In Hinaut et al. (2015) [29], it has been shown that the
model can learn to process sentences with out-of-vocabulary
words [30] (i.e. words that are not in the speech recog-
nizer’s vocabulary). Moreover, we demonstrated that it can
generalize to unknown grammatical constructions in both
French and English at the same time. To illustrate how the
robot interaction works, a video is available at youtu.be/
FpYDco3ZgkU [31][32].

In [32] we have integrated this ROS module in a robotic
architecture with speech and visual object recognition. Once
this ROS module is integrated in such a system, it could
be employed to process various hypotheses generated by
the speech recognizer: e.g. for one utterance it generates
the 10 most probable sequence of words, then the ROS
module returns the retrieved predicates for each hypothesis,
ﬁnally allowing a semantic analyser or world simulator to
choose the predicates with the highest likelihood. Preliminary

theontoandSW…Sentence:PALput (toy, left)P(A, L)Semantic words (SWs)Recurrent NeuralNetworkSW1SW2SW3P: PredicateA: AgentL: LocationMeaning: P (A, L)Read-outLayerInputLayeron the left put the toy .SW1SW2SW3putlefttoySWs MemoryFunction words(FWs)PALPALleftputtoyInactive connectionActive connectionLearnable connectionsFixed connectionsConnectionSW: Semantic word item(e.g. toy)thiswork has shown that the model could be trained in a fully
incremental fashion [13]: we plan to add this feature to the
ROS module in the future.

One may argue that the main drawback of the architecture
is the fact that it only relies on function words to interpret a
sentence: this may cause some ambiguities when an identical
sentence structure corresponds to different meanings (see
[3] for more details). However, relying on function words
is also its strength since it enables to generalize to new
inputs with only tens of sentence structures in the training
corpus [3][2]. We believe this is not a limitation of the ar-
chitecture itself since some current unpublished experiments
show that real semantic words (distributed representations
using word2vec [33]) could be used instead of SW markers
(i.e. ﬁllers).

III. METHODS

A. Echo State Networks

The neural parser is based on an ESN [16] – a particular
kind of recurrent neural network (RNN) – with leaky neu-
rons: inputs are projected to a random recurrent layer and a
linear output layer (called “read-out”) is modiﬁed by learning
(which can also be done in an online fashion).

Compared to other RNNs, the input layer and the recurrent
layer (called “reservoir”) do not need to be trained. For other
RNNs, the structure of the recurrent layer evolves in most
cases by gradient descent algorithms like Backpropagation-
Through-Time [6], which is not biologically plausible and
is adapted iteratively to be able to hold a representaion
of the input sequence. In contrast, the random weights of
the ESN’s reservoir are not trained, but adapted to possess
the echo state property [16] or at least suitable dynamics
(e.g. “edge of chaos”) to generalize, which includes a non-
linear transformation of the input that can be learned by
a linear classiﬁer. The weights are adapted by scaling the
weights based on the maximum absolute eigenvalue (also
called spectral radius), which is a hyperparameter speciﬁc
to the task. The states of the reservoir are linearly separable
and can be mapped to the output layer by a computationally
cheap linear regression, as no gradient descent is necessary.
The weights of the input layer can be scaled by the input
scaling hyperparameter, which also depends on the nature
of the inputs. The size of the reservoir is another task-
speciﬁc hyperparameter, which can be ﬁxed for hyperparam-
eter search, as the other hyperparameters scale with a larger
reservoir [34].

The units of the recurrent neural network have a leak rate
(α) hyper-parameter which corresponds to the inverse of a
time constant. These equations deﬁne the update of the ESN:

x(t + 1) = (1 − α)x(t) + αf (Winu(t + 1) + Wx(t)) (1)

y(t) = Woutx(t)

(2)

where u(t), x(t) and y(t) are the input, the reservoir (i.e.
hidden) state and the read-out (i.e. output) states respectively
at time t. α is the leak rate. Win , W and Wout are the
input, the reservoir, and the read-out matrices respectively.

f is the tanh activation function. After the collection of all
reservoir states, the following equation deﬁnes how the read-
out (i.e. output) weights are trained:

Wout = Yd[1; X]+

(3)

where Yd is the concatenation of the desired outputs, X
is the concatenation of the reservoir states (over all time
steps for all trained sentences) and M+ is the Moore-Penrose
pseudo-inverse of matrix M.

B. Preprocessing

For some languages like Spanish, Basque, Turkish, etc., to
understand a sentence one may need to extract some word
sufﬁxes (or preﬁxes) from semantic words. In Spanish for
instance, “sirve -lo” is usually written as one word (“sirvelo”)
even if its translation to English is composed of two words
(“pour it”; “it” being a cup). This sufﬁx extraction may not
be mandatory for all the languages (e.g. French, English).

As we want the model to process sentence structures (e.g.
“The X was X-ed by the X.”1) instead of true sentences (e.g.
“The dog was scratched by the cat.”), we need to replace all
the content words (i.e. semantic words – SW) by a common
marker (X was used in the example here). This preprocessing
can include or not the extraction of preﬁxes and sufﬁxes (e.g.
-ed) depending on the language. In this study, we extracted
sufﬁxes for a given language only when it seemed useful
for the model to learn2. This preprocessing to distinguish
SW from FW can be done in two ways, one may be more
convenient than the other depending on the application:

• deﬁne a list of FWs (i.e. all the words that are not in
the meaning representation part of the construction) and
consider any other word as SW;

• deﬁne a list of SWs (i.e. words that are in meaning
representations), and consider any other word as FW.

the remaining words,

In the current experiments, we used the latter by deﬁning
as semantic words (SW), all the words that were in the
meaning parts of each corpus (i.e. the predicates). Then
all
including grammatical markers
(sufﬁxes beginning with “-”) and commas (for German and
Bulgarian), were deﬁned as function words (FW). In a few
cases, for some languages we had to slightly modify the list
of FW3 as we did not ask corpus translators to optimize their
translation for the model – we wanted the corpora to be close
to what any future user of the system could do. Of course,
the boundary between FW and SW is not well deﬁned and
depending on the context one may want to include it in the
meaning part or not. As we state in the Discussion (the use of
word “on” as both SW and FW) this could be an advantage
because it shows the system can learn these particular cases.

1In this study sufﬁxes were not extracted for English corpus. This sentence

is just given an example for a matter of understanding.

2This extraction of sufﬁxes was done manually given the smallness of the
corpus and because exploring automatic extraction of sufﬁxes for all these
different languages is out of the scope of this study.
FW used

for
github.com/neuronalX/EchoRob/blob/master/corpora/
2017\_TCDS\_15languages\_\_function-words-FW-used.py

available

corpus

3The

each

list

of

at

is

As explained is subsection II-C,

this preprocessing is
interesting for HRI because it enables the parser to not
rely on semantic words, which enables the robot system to
use newly learned semantic words with previously learned
data. Currently, there was no need to use part-of-speech
(POS) tagging, but it may help to add this complementary
information in the input representation of words.

C. Training the Neural Parser

First, an instance of ESN is created by generating the
random weights for the input connections and the recurrent
connections. Then, the neural model was trained indepen-
dently for each corpus. The training of the read-out (i.e.
output) connections was done using one shot linear regres-
sion with bias. The training is done on the full corpus (21
sentence-meaning pairs) and tested on the same corpus. We
use the same corpus for training and testing because we
are not interested in the generalization abilities, but on the
learnability of each corpus; the question is: Is it possible
that a relatively small reservoir ﬁnds a way to map inputs
(sentence structures) and outputs (predicate-like meanings)
for all languages?

Hyper-parameters that can be used for this task are as
follows: spectral radius: 1, input scaling: 0.75, leak rate:
0.167. We did not optimise these hyper-parameters to try
to have optimal performance with the lowest number of
neurons. We simply used values taken from previous similar
experiments [2][3][32]: these hyper-parameters are robust
to modiﬁcations. The number of neurons in the reservoir
(i.e. the recurrent layer) was either 50 or 75 depending on
the corpus: we did not search for the minimum number of
neurons in detail, we just changed it by a 25 unit step.

IV. HOME CORPORA IN MULTIPLE LANGUAGES

A. Introduction

interest

In this section, we give the full corpus for English
(21 meaning-sentence pairs), the corresponding corpus with
constructions giving an idea of how the reservoir model
“perceives” the inputs and outputs, and some particular
for ﬁfteen other corpora4). Some
sentences of
languages are written in their alphabet of origin and
some are using Roman alphabet. All corpora are avail-
at github.com/neuronalX/EchoRob/blob/
able
master/corpora/2017\_TCDS\_15languages.txt .
Each corpus is organised as follows. Each line contains a
sentence together with its corresponding meaning represen-
tation: left part of the semi-column is the meaning, right part
is the sentence:

1) open door ; open the door
2) take coffee, pour coffee into mug ; take the coffee and

pour it into the mug

One can see that the way to write predicates is fairly
intuitive and can be done without prior knowledge of a
predeﬁned output structure. The words in the left part (i.e.

4There are sixteen corpora in total, corresponding to ﬁfteen different

languages, and two versions of Mandarin (modern and traditional).

meaning) may not be a particular form of a verb (noun with
plural, verb with conjugation, ...), this is because each these
words has to be written the same way as in the right part
(i.e. sentence) for the preprocessing to know which word is
refereed to.

B. English

1) open door ; open the door
2) answer phone ; answer the phone
3) water plants ; water the plants
4) clear table ; clear the table
5) take coffee, pour coffee into mug ; take the coffee and

pour it into the mug

6) clean table, put mug on table ; clean the table and put

the mug on it

7) put mug on left ; put the mug on the left
8) get phone, bring phone me ; get the phone and bring

it to me

9) go to bathroom ; go to the bathroom

10) make tea me ; make me some tea
11) tell joke me ; tell me a joke
12) make sandwhich me, sandwhich tomatoes ; make me

a sandwhich with tomatoes

13) bring newspaper me, newspaper on table,

table in
kitchen ; bring me the newspaper which is on the table
in the kitchen

14) bring dress me, dress blue, dress in closet ; bring me

the blue dress that is in the closet

15) bring pen me, pen blue, pen beside cup, cup red ; bring

me the blue pen beside the red cup

16) dance for me, clean ﬂoor, clean same time ; dance for

me and clean the ﬂoor at the same time

17) switch off light ; switch off the light
18) ﬁnd bottle, bottle water ; ﬁnd the bottle of water
19) search object, object small pink ; search for the small

pink object

20) search recipe internet, recipe tiramisu ; search the

recipe of tiramisu on internet

21) check if, if husband home, husband my, if ﬁve ; check

if my husband is at home at ﬁve

C. English constructions

In the following, we show what the English corpus become
after preprocessing; i.e. the sentence structures on the right
parts are input to the reservoir model. To make them more
human-friendly we wrote for instance “W the X and M it N
the O”, but actually the true input given to the neural network
is “SW the SW and SW it SW the SW”: there is no difference
between the semantic words inputs, they are all represented
as a SW marker. All the capital variables are arbitrary names:
W, X, Y, ... In order to make them more readable, their are
organised as groups always in the same order (and at the
same position in the predicate-slot), in order to represent the
four possible predicate-like parts of the meaning (i.e. right-
hand of each line): [W X Y Z], [M N O P], [I J K L],
[A B C D]. Sometimes a semantic word is used in several
predicates (for a same given construction), it will then have

the same letter in all these slots. For instance, this is the
case for construction 8, where X is at position 2 of the 1st
predicate and at position 2 of the 2nd predicate: “W X, M
X O ; W the X and M it to O”. Given these “precautions”
one can easily see some patterns appearing in the corpus.

1) W X ; W the X
2) W X ; W the X
3) W X ; W the X
4) W X ; W the X
5) W X, M X O P ; W the X and M it O the P
6) W X, M N O X ; W the X and M the N O it
7) W X Y Z ; W the X Y the Z
8) W X, M X O ; W the X and M it \* O
9) W X Y ; W X the Y
10) W X Y ; W Y some X
11) W X Y ; W Y a X
12) W X Y, X N ; W Y a X with N
13) W X Y, X N O, O J K ; W Y the X which is N the

O J the K

14) W X Y, X N, X J K ; W Y the N X that is J the K
15) W X Y, X N, X J K, K B ; W Y the N X J the B K
16) W X Y, M N, M J K ; W X Y and M the N at the J

K

17) W X Y ; W X the Y
18) W X, X N ; W the X of N
19) W X, X N O ; W \* the N O X
20) W X Y, X N ; W the X of N \* Y
21) W X, X N O, N J, X B ; W X J N is at O at B

One can see the following symbol in the corpus “\*” which
needs an explanation. Some words could be considered both
as semantic and function word by the language experts whom
wrote a given corpus: in practice this means that the word
would sometime appear on the meaning (left) part, and
sometimes not. Consequently these words need to be always
considered as semantic word by the model, in order to let
it bind a thematic role to them. Thus even if the word do
not appear in the left-hand part, it needs to be a SW marker
in the right-hand part5. In this case we noted this word with
“\*” symbol.

11) Marathi
12) Hindi
13) Basque
14) Persian
15) Portuguese
16) Bulgarian

a) Sentence 2: This sentence is a good example for a
simple mapping. By learning this mapping, the model is now
able to generate predicates for all kind of these sentences,
which are often used in simple HRI scenarios, like “open
the door”, “water the plants” or “clear the table”. It is not
necessary for the model to know the words “open”, “door”,
“water”, “plants”, “clear”, or “table”.

1) answer phone ; answer the phone
2) geh an Telefon ; geh an das Telefon
3) contesta tel´efono ; contesta el tel´efono
4) r´eponds t´el´ephone ; r´eponds au t´el´ephone
5) rispondi telefono ; rispondi al telefono
6) contesta tel`efon ; contesta el tel`efon
7) 接 电话 ; 接 那 -通 电话
8) 接 電話 ; 接 那 -通 電話
9) jawab telefon ; jawab -kan panggilan telefon itu

10) cevap ver telefon ; telefon -a cevap ver
11) uchal phone ; phone uchal
12) dho javaab phon ; phon ka javaab dho
13) telefono erantzun ; telefono -a erantzun
14) javab telfon ; telfon ra javab bede
15) atenda telefone ; atenda o telefone
16) вдигни телефон ; вдигни телефон -а

b) Sentence 6: In this example, a little more complex
mapping with two predicates is given. As it can be seen, the
original order of the tasks to be performed is preserved by
the order of the predicates. Moreover, the possibility of using
FWs as SWs is shown, like the word “on”, “auf”, “sobre”,
“dessus”, ...

1) clean table, put mug on table ; clean the table and put

the mug on it

2) reinige Tisch, stell Tasse auf Tisch ; reinige den Tisch

und stell die Tasse auf ihn

D. Examples from the Corpora

3) limpia mesa, pon taza sobre mesa ; limpia la mesa y

The examples provided in this subsection (sentence 2, 6,
8, 13, 19 and 21) are available (by order) in the following
languages:

1) English
2) German
3) Spanish
4) French
5) Italian
6) Catalan
7) Simpliﬁed Mandarin
8) Traditional Mandarin
9) Malay
10) Turkish

5That is why the words “for”, “on” and “to” are not in the close class

word list in the supplementary data.

pon la taza sobre ella

4) nettoie table, met tasse dessus table ; nettoie la table

et met la tasse dessus
tavolo, metti

5) pulisci

tazza sopra tavolo ; pulisci

il

tavolo e metti -ci sopra la tazza

6) neteja taula, posa tassa sobra taula ; neteja la taula i

posa la tassa a sobra

7) 清理 桌子, 放 杯子 上 桌子 ; 清理 那 -张 桌子 再

把 那 -个 杯子 放 在 桌子 上

8) 清理 桌子, 放 杯子 上 桌子 ; 清理 那 -張 桌子 再

把 那 -個 杯子 放 在 桌子 上

9) kemas meja, letak cawan ke atas meja; kemas -kan

meja itu dan letak -kan cawan itu ke atas nya

10) temizle masa, koy ﬁncan ¨ust¨unde masa ; masa -yi

temizle -yip ﬁncan -i ¨ust¨unde koy

11) kar saf table, thev pela tya ; table saf kar ani pela tya

-wr thev

12) saaf karo mez, rakho mez mug ; mez saaf karo aur us

par mug rakho

13) mahai garbitu, kiker ezarri mahai gainean ; mahai -a

garbitu eta kiker -a ezarri gainean

14) tamiz miz, gzar livan rooy miz ; miz ra tamiz kon va

livan ra rooy -e an bo- gzar

15) limpe mesa, coloque caneca nela mesa ; limpe a mesa

e coloque a caneca nela

16) почисти маса, сложи чаша на маса ; почисти

маса -та и сложи чаша -та на нея
c) Sentence 8: This sentence illustrates the problem of
personal pronouns like “me”, “mir”, “-me”, “moi”, ... which
are often sufﬁxes of other words in several languages.

1) get phone, bring phone me ; get the phone and bring

it to me

2) nimm Telefon, bring Telefon mir ; nimm das Telefon

und bring es zu mir

3) toma tel´efono, trae tel´efono -me ; toma el tel´efono y

trae -me -lo

4) prends t´el´ephone, am`ene t´el´ephone moi ; prends le

t´el´ephone et am`ene le moi

5) prendi telefono, da telefono -mme ; prendi il telefono

e da -mme -lo

6) pren tel`efon, porta tel`efon ‘m ; pren el tel`efon i porta

‘m -ho

7) 取 手机, 拿 手机 我 ; 取 手机 再 把 它 拿 给 我
8) 取 手機, 拿 手機 我 ; 取 手機 再 把 它 拿 給 我
9) dapat telefon, ambil telefon saya; dapat -kan telefon

itu dan ambil -kan -nya untuk saya

10) al telefon, getir telefon bana ; telefon -u al -ip bana

getir

11) ghe phone, aan phone mazajaval

; phone ghe ani

mazajaval aan

12) lo phon, lay aao mere phon ; phon lo aur mere paas

lay aao

13) telefono hartu, neri ekarri ; telefono -a hartu eta neri

ekarri

14) gir telfon, avar telfon man ; telfon ra be- gir va an ra

baray -e man bi- avar

15) pegue telefone, traga telefone -me ; pegue e traga -me

o telefone

16) вземи телефон, донеси телефон ми ; вземи теле-

фон -а и ми го донеси
d) Sentence 13: Here, three predicates can be extracted
along with personal pronouns. Note that
the predicates
are linked by one another by one word, making a simple
branching tree structure (newspaper -on- table -in- kitchen).
Additionally, it demonstrates one aspect of the ﬂexibility of
the system: nouns could be used as localisation predicates,
indicating where these objects are.

1) bring newspaper me, newspaper on table,

table in
kitchen ; bring me the newspaper which is on the table
in the kitchen

2) bring Zeitung mir, Zeitung auf Tisch ; bring mir die

Zeitung , die auf dem Tisch ist

3) trae peri´odico -me, peri´odico sobre mesa, mesa en
cocina ; trae -me el peri´odico que est´a sobre la mesa
en la cocina

4) apporte journal moi,

table dans
cuisine ; apporte moi le journal qui est sur la table
dans la cuisine

journal sur table,

5) prendi giornale -mi, giornale trova sul tavolo, tavolo in
cucina ; prendi -mi il giornale che si trova sul tavolo
in cucina

6) porta peri`odic ‘m, peri`odic sobre taula, taula en cuina ;
porta ‘m el peri`odic que est`a sobre la taula en la cuina
7) 取 报纸 我, 报纸 上 桌子, 桌子 里 厨房 ; 给 我 取

在 厨房 里 桌子 上 的 报纸

8) 取 報紙 我, 報紙 上 桌子, 桌子 裡 廚房 ; 給 我 取

在 廚房 裡 桌子 上 的 報紙

9) ambil surat khabar saya, surat khabar atas meja, meja
dalam dapur; ambil -kan saya surat khabar itu yang
berada di atas meja di dalam dapur

10) getir gazete bana, gazete ¨ust¨unde masa, masa -da
mutfak ; mutfak -da olan masa -nin ¨ust¨unde -ki gazete
bana getir

11) aan wartapatra, wartapatra table, table svayampakgruh;
jo wartapatra svayampakgruh -at table -wr ahe to aan
rasoyi ghar

12) lavo mujhay akhbaar, mez akhbaar,

mez;mujhay rasoyi ghar may mez par akhbaar lavo
13) egunkari ekar diezadazu, egunkari mahai gainean, ma-
hai sukalde ; sukalde -ko mahai gainean den egunkari
-a ekar diezadazu

14) avar roozname man, roozname rooy miz, miz dar
ashpazkhane; roozname -yi ra ke rooy -e miz dar
ashpazkhane ast ra baray -e man bi- avar

15) traga jornal -me, jornal na mesa, mesa na cozinha ;
traga -me o jornal que est´a na mesa na cozinha
16) донеси вестник ми, вестник на маса, маса в
кухня ; донеси ми вестник -а , който е на маса
-та в кухня -та

e) Sentence 19: This sentence demonstrates another
aspect of the ﬂexibility of these predicates. Entities like
“object” can be predicate with its arguments playing the role
of describers or modiﬁers (“small”, “pink”).

1) search object, object small pink ; search for the small

pink object

2) such Objekt, Objekt kleine pinke ; such das kleine

pinke Objekt

3) busca objeto, objeto rosado peque˜no ; busca el objeto

rosado peque˜no

4) cherche objet, objet petit rose ; cherche le petit objet

rose

5) cerca oggetto, oggetto piccolo rosa ; cerca il piccolo

oggetto rosa

6) cerca objecte, objecte petit rosat ; cerca l’ objecte petit

7) 寻找 物体, 物体 小件 粉红色 ; 寻找 那 -个 小件 的

i rosat

粉红色 物体

8) 尋找 物體, 物體 小件 粉紅色 ; 尋找 那 -個 小件 的

粉紅色 物體

9) cari objek, objek kecil merah jambu; cari -kan objek

V. RESULTS

kecil yang berwarna merah jambu itu

10) al s¸ey, s¸ey pembe k¨uc¸ ¨uk ; k¨uc¸ ¨uk pembe s¸ey -i al
11) shodh vastu, vastu chhoti gulabi ; chhoti gulabi vastu

shodh

12) doondo vasthu, chotay vasthu, gulaabi vasthu;chotay

gulaabi vasthu doondo

13) gauz bilatu, gauz txiki arros ; gauz -a txiki eta arros

-a bilatu

14) gard shey, shey koochak soorati ; donbal -e shey -e

koochak -e soorati be- gard

15) procure objeto, objeto pequeno rosa ; procure pelo

pequeno objeto rosa

16) намери предмет, предмет малк розов ; намери

малк -ия розов предмет

f) Sentence 21: This most challenging meaning con-
tains four predicates with a complex tree kind of structure:
“if” appears two times as a predicate in non-consecutive
slots, separated by a “modiﬁer” predicate.

1) check if, if husband home, husband my, if ﬁve ; check

2)

if my husband is at home at ﬁve
¨uberpr¨ufe ob, ob Mann Hause, Mann mein, ob f¨unf ;
¨uberpr¨ufe , ob mein Mann um f¨unf zu Hause ist
3) chequea si, si marido casa, marido mi, si cinco ;

chequea si mi marido est´a en casa a las cinco

4) v´eriﬁe si, si mari maison, mari mon, si heures cinq ;
v´eriﬁe si mon mari est `a la maison `a cinq heures
5) controlla se, se marito casa, marito mio, se cinque ;

controlla se mio marito ´e a casa alle cinque

6) comprova si, si marit casa, marit meu, si cinc ; com-

prova si el meu marit est`a a casa a les cinc

7) 检查 是不是, 是不是 丈夫 家, 丈夫 我的, 是不是 五
点钟 ; 检查 我的 丈夫 是不是 五点钟 有 在 家
8) 檢查 是不是, 是不是 丈夫 家, 丈夫 我的, 是不是 五
點鐘 ; 檢查 我的 丈夫 是不是 五點鐘 有 在 家
9) memeriksa jikalau, jikalau suami rumah, suami saya,
jikalau lima; memeriksa -kan jikalau suami saya berada
di rumah pada pukul lima

10) kontrol et, koca ev, koca -m, saat bes¸ ; koca -m ev -de

saat bes¸ -te olacagina kontrol et

11) tapas jr, jr pati ghari, pati maze, pach wajta ; jr maze

pati pach wajta ghari astil ka he tapas

12) jaanch agar, agar ghar pathi, mera pathi, agar paanch
bajay;jaanch agar mera pathi paanch bajay ghar par
hain

13) egiaztatu, senar etxean bada, senar nere, bost bada ;

egiaztatu nere senar -a etxean bada bost -etan

14) barrasi agar, agar shohar khane, shohar man, agar panj
; barrasi kon agar shohar -e man saat -e panj dakhel
-e khane bashad

15) veriﬁque se, se marido casa, marido meu, se cinco ;
veriﬁque se meu marido estar´a em casa `as cinco
16) провери дали, дали съпруг вкъщи, съпруг мой,
дали пет ; провери дали мой съпруг е вкъщи в
пет

A. Learning corpora of various languages

In the following, we show that the network is able to learn
to parse correctly the sentences related to home scenarios. In
particular, similar networks (with the same hyper-parameters)
can learn to map sentences to predicates in sixteen corpora.
The hyper-parameters were taken from previous experiments
and were not optimized for these new corpora. This demon-
strates the robustness of this neural parser. Moreover,
it
shows that, even with all the particularities of each language
(discussed in subsection IV-D), a generic neural model is
able to learn each of them without being adapted and despite
having a random recurrent network as it core part.

This offers one more clue to reject Chomsky’s suggestion
that children are born with a preprogrammed Universal
Grammar in their brain which would allow them to acquire
language just by tuning the parameters of a particular lan-
guage based on a limited exposure [35].

An ESN of 75 neurons is able to learn a set of 21 sentence-
predicate pairs in any of these languages (i.e. reproduce
exactly the same predicates for each sentence), but Hindi
corpora. For Hindi we obtain one error with 75 neurons6.
Conversely, a network of 50 units is able to learn any
of English, French, Catalan, Persian or Portuguese corpus.
The difference of units needed is not surprising since some
language speciﬁcities may be more difﬁcult than others to
learn. This is due to the fact that the dataset is tiny and
highly variable.

We did not investigate generalization concerning these
corpora, even if we already demonstrated that
the same
network could generalize on both English and French at the
same time [29]. However, we focused on the ﬂexibility of
the meaning representations, and in particular in robot home
scenarios. Furthermore, the corpora are of tiny size (only 21
sentences for each language) with a high variability in their
structure. Consequently, each sentence is nearly unique in
its structure (and its use of function words), which does not
enable generalization as shown in previous studies.

In the following, we give some remarks regarding lan-
guages speciﬁcities to show why it should be difﬁcult to
have a generic system that could learn all of them.

B. General remarks

All sentences in different languages correspond to the
translation from English sentences, but there are some speci-
ﬁcities in each language which make the predicates not a
direct translation word by word7. For instance, in the German
sentence “geh an Telefon ; geh an das Telefon” (“answer
phone ; answer the phone”), the “an” is not present in the
English predicate because it is a “verb particle” (a German
speciﬁcity). In a similar way, for the Spanish sentence “toma

6For the sentence-predicate pair #2. “dho javaab phon;phon ka javaab

dho”.

7As one can see in the corpora, one amazing word which is very similar
across languages is “tomato”. This is probably because tomatoes come from
South America, thus many languages have borrowed the word t¯omatl from
the Nahuatl language, an Uto-Aztecan language.

el telefono y trae -me -lo” (“get the phone and bring it to
me”): the direct translation of “to me” would be “a mi”, but
it is more natural in Spanish to attach “-me” to the verb.
When grammatical markers such as “-me” are used in the
meaning representation, we use “-” to indicate it is not a word
by itself but a preﬁx or sufﬁx, and the direction indicates to
which word this marker should be attached: i.e. “preﬁx- word
-sufﬁx”.

Some actions or verbs appear in two words: this is for
instance the case in English for “switch off” (which is usually
one word in other languages) and in Marathi for “kar saf”
(clean). In such cases, one of the part of the verb was
considered as the 1st argument in the meaning structure:
“switch off light”, here off is the ﬁrst argument, and “light” is
shifted as the 2nd argument. This does not prevent the neural
model
to learn this particular case and demonstrate that
ﬂexible meaning representations can be learned. In Marathi,
the meaning is “kar saf table” and the sentence was “table
saf kar”: the order of the words composing the verb was
different. This is due to the coherency of word order in this
language.

Some additional features could be added to allow a more
precise way of coding the predicates. For instance when
two words should be assigned the same role – consider the
action “switch off” –, a more complex coding of predicates
should be used. As we will see in the Results section, such
particularity does not prevent the model to learn all the
English corpus. In future versions of the parser, we could
add a particular sign like “ ” linking the two words that
would indicate that these two words should be learnt to be
having the same role.

In this German subsample of the corpus, learning sentence
1 or 3 would be enough to “generalize” to the other. We
see that even if two languages are close in the family
of languages, they are some differences in theirs sentence
structures.

D. Speciﬁc remarks about Hindi

In the Hindi corpus, while using a different word order in
the predicates than English, the coherency is kept. As we see,
a user creating a training corpus in its own language could
use a particular word order for the predicates. It should be
noted that, English and Hindi do not use the same general
word order: English is a “SVO” language and Hindi is a
“SOV” language8. This is one reason why the word order
in predicates for Hindi seemed more adequate in the above
mentioned way.

In the following, we illustrate the difference in the deﬁni-
tion of predicates between English and Hindi. Considering
the word order of predicates in English as a reference, Hindi
word order of predicate where 1st, 3rd, 2nd arguments. Here
are some examples of English predicate word order:

• sentence #10: make me some tea
• predicate #10: make tea me {verb direct-object indirect-

object}

• sentence #18: ﬁnd the bottle of water
• predicate #18: ﬁnd bottle, bottle water {verb direct-

object, direct-object object-complement}

In comparison, here are the corresponding examples of Hindi
predicate word order:

• sentence #10: mujhay thoda chai banao
• predicate #10: banao mujhay chai {verb indirect-object

C. Comparing English and German constructions

direct-object}

In subsection IV-C, one can see that some English
constructions are equivalent for different sentence-meaning
pairs. It is obviously the case two times in the corpus: for
sentence-meanings pairs 1, 2, 3 and 4: “W X ; W the X”;
and sentence-meaning pairs 9 and 17: “W X Y ; W X the
Y”. This means that the model only needs to learn one of the
following sentences to be able to “generalize” to the other
sentences:

1) open door ; open the door
2) answer phone ; answer the phone
3) water plants ; water the plants
4) clear table ; clear the table
Converselly this is not the case for the equivalent German

sentences:

¨offne T¨ur ; ¨offne die T¨ur

1)
2) geh an Telefon ; geh an das Telefon
3) gieße Pﬂanzen ; gieße die Pﬂanzen
4) leere Tisch ; leere den Tisch
Which produce the following constructions:
1) W X ; W die X
2) W X Y ; W X das Y
3) W X ; W die X
4) W X ; W den X

• sentence #18: paani ka bothal doondo
• predicate #18: doondo bothal, paani bothal {verb direct-

object, object-complement direct-object}

As one can see, in sentence #10 the order of direct-object
and indirect-object are reversed in Hindi compared to En-
glish. Similarly, in sentence #18 direct-object and object-
complement are reversed.

E. Flexibility of the meaning representation learnt

Previously we reported that the system could learn chrono-
logical ordering. We showed that for sequential motor action,
it could organize the predicate in chronological order [2].
An example is shown in the subsection II-D about the ROS
module.

In the corpora proposed in this study, the meaning repre-
sentations used were very variable and are of different nature.
We have seen in the sentence examples given that predicates
could be of different forms: action, entity localisation, entity
modiﬁer, conditional (“if”), ... Moreover, some details (i.e.
some arguments in a predicate) could be omitted when not

8In linguistics, people usually refer to the main word order in a language
by attributing one of the 6 possible word orders of the Subject, the Verb
and the Object (SVO, SOV, VSO, VOS, OSV, OVS). English, Mandarin,
Malay or Latin languages are SVO for instance.

necessary: this doesn’t prevent the neural model from learn-
ing. The ability of the neural model to learn such different
representations, in several different languages, shows that
no particular output seems to be required. For instance,
for the Hindi corpus, an organisation of the elements in
the predicates was following a VOS (verb-object-subject)
structure instead of a VSO structure like in English corpus
(see subsection V-D on Hindi corpus for more details). It
seems that as long as the output representation is coherent it
can be learnt by the neural parser.

F. Ambiguous constructions and conﬂicts

is looking at

Some sentences could be ambiguous: they have several
possible interpretations. This is the case with the sentence
“The girl
the man with a telescope”. The
telescope could be held by the girl or by the man. This
ambiguity means that there are two possible syntactic trees
that could be found by a parser. A similar kind of ambiguity
can happen with constructions, when a sentence structure can
have two possible meanings. In [3] we have shown that the
neural parser is able to output the two possible answers at
the same time.

There are some other cases which are difﬁcult (or even
impossible to learn) by the current system because of some
exceptions that conﬂict with other sentence structures. For
instance, this sentence-meaning pair in Hindi (and its corre-
sponding pair in English):

• #2. dho javaab phon ; phon ka javaab dho
• (answer phone ; answer the phone)

is kept as it is because the use of “dho” here is not the
common way. Usually “dho” is used like this:

• #3. dho paani paudhon;paudhon ko paani dho
• (water plants ; water the plants)

Due to this exception, this prevents the learning of another
sentence-meaning pair, as a collateral side effect:

• #18. doondo bothal, paani bothal

; paani ka bothal

doondo

• (ﬁnd bottle, bottle water ; ﬁnd the bottle of water)
Here is another special case of sentence-meaning pair

which could have impaired the learning, but it hasn’t:

• #17. bujhaa dho batthi ; batthi bujhaa dho
• (switch off light ; switch off the light)

These examples demonstrate that the system is able to learn
some exceptions but not
the ones that conﬂict with the
regular sentence-meaning pairs.

G. A truly generic neural parser?

When talking about languages with people at an open air,
you may have heard “Chinese language has no grammar.”.
Even if this may have been claimed by some linguists, we
were previously wondering if our neural parser would be
ever able to parse such a language. In fact, as we have shown
in this study, Mandarin has enough grammatical markers to
allow the model to learn the sentence-meaning pairs.

We believe that

is a particularly interesting result,
because the system was not adapted or particularly tuned

it

to process speciﬁcally the Mandarin language. As it was not
tuned to process the other 14 languages. The current study
thus demonstrates the capability of this neural parser to be
truly generic across languages and reinforce the language
learning assumptions it is based on.

VI. DISCUSSION

We proposed an approach that allows people to use natural
language when interacting with robots. Our architecture
enables to train a sentence parser easily on home scenarios:
no parsing grammar has to be deﬁned a priori, it only relies
on the particular corpus used for training. Moreover, we
showed that this neural parser is not limited to one language,
but was able to learn corpora in 15 different
languages
of different parts of the world and of different linguistic
families. This demonstrates that this neural parser is not
tuned for a particular language, and that it is a ﬂexible tool
for studies in Human-Robot Interaction across the world.

In future work, we plan to merge this neural parser to
a new speech recognition system that would employ the
syntactic reanalysis model of Twiefel et al. [36] in order to
overcome the issues of non grammatical sentences produced
by speech recognizers. This model also uses the concept of
structures of sentence.

The way to write predicates (left hand of each line) is
fairly intuitive and can be done without prior knowledge of
a predeﬁned structure like that of Robot Control Language
(RCL) commands [37], which imposes a tree representation
structure which is too focused on a particular robotic task.
In other words, we propose to let the users deﬁne which
meaning/predicate representation they want to use. For in-
stance, some words like “on” can be used in the meaning
representation when needed (e.g. “clean table, put mug on
table ; clean the table and put the mug on it”), and can be
discarded when there is no ambiguity and thus not necessary
(e.g. “search recipe internet, recipe tiramisu ; search the
recipe of tiramisu on internet”). Accordingly, one can adapt
the meaning representation to any kind of “series of slots” –
or distributed representations – as soon as they are consistent
with each other. This neural model is not committed to
predicates, and our architecture could be used to learn other
type of representations.

A way to obtain distributed coherent representations would
be to use multimodal, grounded, and embodied representa-
tions. For instance, a robot performing some actions could
use its proprioception representation of the structure of the
action as a teacher for the output of the neural parser.
Conversely, it could use its visual representations of the
properties of an object (e.g. small and pink) as inputs instead
of the words “small” and “pink” of sentence 19.

To frame it

in a more neurobiological framework,

in
Hinaut et al. (2013) [3] we proposed that the connections
from the reservoir to the outputs could model the projections
from a subpart of Broca’s area to the input of the basal
ganglia (i.e.
the striatum). As the basal ganglia receive
projections from nearly all cortical areas, it is a good place
to share multimodal representations. Moreover, the reservoir

neural activity is itself using distributed “mixed-selectivity”
representations [7][38]. Additionally, even if the model is
rather simple – the core part is a random recurrent network
– it is able to learn sentence-meaning associations in ﬁfteen
different
languages. Not many models have this feature
but it is an important (if not compulsory) one for models
aiming at modelling sentence processing and language acqui-
sition [39][25]. Some future experiments could show if this
high ﬂexibility of output, would enable the model to adapt
across time to changing output structures: this could help in
modelling developmental language acquisition. We believe
that such a model could help understand how, not one lan-
guage, but how any language could be acquired, along with
their multimodal, grounded and embodied representations,
with a loosely predeﬁned architecture.

This claim on grounding may seem too strong, that is why
we would like to provide some intuitive speculations of why
it would work. First, the model can be adapted to different
kinds of outputs like collection of“slot-structures” inspired
from predicates. Actually, one could deﬁne these “predicate-
like” structures in such a way that it corresponds to a tree
structure when explored in a ”depth-ﬁrst” way: each non-
terminal node would correspond to the ﬁrst argument/slot
of each predicate. We believe that using such predicate-
like structures allows the users to ﬂexibly deﬁne their own
structured outputs, and adapt them to their cognitive system.
Secondly, the training in the reservoir computing framework
is based on a similar idea as that of Support Vector Ma-
chines: the inputs are projected into a non-linear and high-
dimensional space from which one hopes to ﬁnd a linear
separation to a given problem. In the current paper, we are
using symbolic outputs, but intuitively one can see that such
ESN could learn as well the task by ﬁnding a linear relation
with distributed outputs that are orthogonal with each other.
Additionally, one can assume that it should be fairly the same
for non-orthogonal distributed outputs, as the reservoir (i.e.
the recurrent layer) in itself has correlated unit activations.
Since such multimodal, embodied or grounded outputs have a
particular latent structure, the reservoirs states should be rich
enough to enable a linear mapping to be found. This is even
more true if equivalent multimodal, embodied or grounded
states are provided as input to the reservoir.

ACKNOWLEDGMENT

We thank Ikram Chraibi Kaadoud, Pramod Kaushik, Louis
Devers, and Iris Wieser for their ideas of sentences for the
home corpora. We also greatly thank Francisco Cruz, Paul
Gu´erin, Olatz Pampliega, Miguel L´opez Cui˜na, Thalita Firmo
Drumond, Bhargav Teja Nallapu, Antoaneta Genova, Oc´eane
Plassart, William Schueller, Giulia Fois, Mohammad Ali
Zamani, Chandrakant Bothe, Hwei Geok Ng for their trans-
lations in different languages of the 21 sentences-meaning
pairs. We thank Remya Sankar, Bhargav Teja Nallapu and
Fabien Benureau for their (very) useful feedback.

REFERENCES

[1] F. Cruz, J. Twiefel, S. Magg, C. Weber, and S. Wermter. Interactive re-
inforcement learning through speech guidance in a domestic scenario.
In Proc. of IJCNN, pages 1–8. IEEE, 2015.

[2] X. Hinaut, M. Petit, G. Pointeau, and P. Dominey. Exploring the ac-
quisition and production of grammatical constructions through human-
robot interaction with echo state networks. Frontiers in Neurorobotics,
8, 2014.

[3] X. Hinaut and P. Dominey. Real-time parallel processing of gram-
matical structure in the fronto-striatal system: a recurrent network
simulation study using reservoir computing. PloS one, 8(2):e52946,
2013.

[4] P. Dominey. Complex sensory-motor sequence learning based on
recurrent state representation and reinforcement learning. Biological
cybernetics, 73(3):265–274, 1995.

[5] W. Maass, T. Natschl¨ager, and H. Markram. Real-time computing
without stable states: A new framework for neural computation based
on perturbations. Neural computation, 14(11):2531–2560, 2002.
[6] P. Werbos. Backpropagation through time: what it does and how to

do it. Proceedings of the IEEE, 78(10):1550–1560, 1990.

[7] M. Rigotti, O. Barak, M. Warden, X. Wang, N. Daw, E. Miller, and
S. Fusi. The importance of mixed selectivity in complex cognitive
tasks. Nature, 497(7451):585–590, 2013.

[8] Timothy Brick and Matthias Scheutz.

Incremental natural language
In Human-Robot Interaction (HRI), 2007 2nd
processing for hri.
ACM/IEEE International Conference on, pages 263–270. IEEE, 2007.
[9] David Schlangen and Gabriel Skantze. A general, abstract model
the 12th
of incremental dialogue processing.
Conference of the European Chapter of the Association for Compu-
tational Linguistics, pages 710–718. Association for Computational
Linguistics, 2009.

In Proceedings of

[10] S. Harnad. The symbol grounding problem. Physica D: Nonlinear

Phenomena, 42(1-3):335–346, 1990.

[11] F. van der Velde. Communication, concepts and grounding. Neural

networks, 62:112–117, 2015.

[12] T. Taniguchi, T. Nagai, T. Nakamura, N. Iwahashi, T. Ogata, and
H. Asoh. Symbol emergence in robotics: a survey. Advanced Robotics,
30(11-12):706–728, 2016.

[13] X. Hinaut and S. Wermter. An incremental approach to language
acquisition: Thematic role assignment with echo state networks.
In
Proc. of ICANN 2014, pages 33–40, 2014.

[14] K. Caluwaerts, M. D’Haene, D. Verstraeten, and B. Schrauwen.
Locomotion without a brain: physical reservoir computing in tensegrity
structures. Artiﬁcial life, 19(1):35–66, 2013.

[15] J. Burms, K. Caluwaerts, and J. Dambre. Reward-modulated hebbian
in compliant

plasticity as leverage for partially embodied control
robotics. Frontiers in neurorobotics, 9:9, 2015.

[16] H. Jaeger. The “echo state” approach to analysing and training re-
current neural networks. Bonn, Germany: German National Research
Center for Information Technology GMD Technical Report, 148:34,
2001.

[17] J. Elman. Finding structure in time. Cognitive science, 14(2):179–211,

1990.

[18] R. Miikkulainen. Subsymbolic case-role analysis of sentences with

embedded clauses. Cognitive Science, 20(1):47–73, 1996.

[19] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock`y, and S. Khudanpur.
Extensions of recurrent neural network language model. In Acoustics,
Speech and Signal Processing (ICASSP), 2011 IEEE International
Conference on, pages 5528–5531. IEEE, 2011.

[20] S. Frank. Strong systematicity in sentence processing by an echo state
network. In International Conference on Artiﬁcial Neural Networks,
pages 505–514. Springer, 2006.

[21] M. Tong, A. Bickett, E. Christiansen, and G. Cottrell. Learning
grammatical structure with echo state networks. Neural networks,
20(3):424–432, 2007.

[22] G. Marcus, S. Vijayan, S. Rao, and P. Vishton. Rule learning by

seven-month-old infants. Science, 283(5398):77–80, 1999.

[23] A. Goldberg. Constructions: A construction grammar approach to

argument structure. University of Chicago Press, 1995.

[24] A. Goldberg. Constructions: a new theoretical approach to language.

Trends in cognitive sciences, 7(5):219–224, 2003.

[25] M. Tomasello. Constructing a language: A usage based approach
to language acquisition. Cambridge, MA: Harvard University Press,
2003.

[37] K. Dukes. Semeval-2014 task 6: Supervised semantic parsing of

robotic spatial commands. SemEval 2014, page 45, 2014.

[38] P. Enel, E. Procyk, R. Quilodran, and P. Dominey. Reservoir comput-
ing properties of neural dynamics in prefrontal cortex. PLoS Comput
Biol, 12(6):e1004967, 2016.

[39] F. Chang. Symbolically speaking: A connectionist model of sentence

production. Cognitive science, 26(5):609–651, 2002.

[26] E. Bates and B. MacWhinney. Competition, variation, and language
learning. Mechanisms of language acquisition, pages 157–193, 1987.
[27] P. Dominey, M. Hoen, and T. Inui. A neurolinguistic model of gram-
matical construction processing. Journal of Cognitive Neuroscience,
18(12):2088–2107, 2006.

[28] X. Hinaut, J. Twiefel, and S. Wermter. Recurrent neural network
for syntax learning with ﬂexible predicates for robotic architectures.
In Proc. of the IEEE Conference on Development and Learning and
Epigenetic Robotics (ICDL-EpiRob). IEEE, 2016.

[29] X. Hinaut, J. Twiefel, M. Petit, P. F. Dominey, and S. Wermter. A
recurrent neural network for multiple language acquisition: Starting
In NIPS 2015 Workshop on Cognitive
with english and french.
Computation: Integrating Neural and Symbolic Approaches, 2015.

[30] D. Jurafsky and J. Martin. Speech and Language Processing: An Intro-
duction to Natural Language Processing, Computational Linguistics,
and Speech Recognition. Pearson International, 2nd edition, 2009.

[31] X. Hinaut, J. Twiefel, M. Borghetti Soares, P. Barros, L. Mici, and
S. Wermter. Humanoidly speaking – learning about the world and
language with a humanoid friendly robot. In IJCAI Video competition,
Buenos Aires, Argentina. https://youtu.be/FpYDco3ZgkU, 2015.
[32] J. Twiefel, X. Hinaut, M. Borghetti, E. Strahl, and S. Wermter. Using
Natural Language Feedback in a Neuro-inspired Integrated Multimodal
In Proc. of RO-MAN, New York City, USA,
Robotic Architecture.
2016.

[33] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean.
Distributed representations of words and phrases and their composi-
tionality. In Advances in neural information processing systems, pages
3111–3119, 2013.

[34] M. Lukoˇseviˇcius. A practical guide to applying echo state networks.
In Neural Networks: Tricks of the Trade, pages 659–686. Springer,
2012.

[35] Chomsky N. Aspects of the Theory of Syntax. MIT Press., 1965.
[36] J. Twiefel, X. Hinaut, and S. Wermter. Syntactic reanalysis in language
models for speech recognition. In Proc. of the IEEE Conference on
Development and Learning and Epigenetic Robotics (ICDL-EpiRob).
IEEE, 2017.

