Long Short-Term Memory of Language Models for Predicting Brain
Activation During Listening to Stories Subba Reddy Oota, Frederic
Alexandre, Xavier Hinaut

To cite this version:

Subba Reddy Oota, Frederic Alexandre, Xavier Hinaut. Long Short-Term
Memory of Language Models for Predicting Brain Activation During
Listening to Stories. CogSci 2022 - Cognitive Science Society, Jul 2022,
Toronto, Canada. ￿hal-03697443￿

HAL Id: hal-03697443

https://inria.hal.science/hal-03697443

Submitted on 16 Jun 2022

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

Long Short-Term Memory of Language Models for Predicting Brain
Activation During Listening to Stories

Subba Reddy Oota (subba-reddy.oota@inria.fr) Inria Bordeaux, France

Frederic Alexandre (frederic.alexandre@inria.fr) Inria Bordeaux, France

Xavier Hinaut (xavier.hinaut@inria.fr) Inria Bordeaux, France

Abstract

Several popular sequence-based and pretrained language mod- els have
been found to be successful for text-driven prediction of brain
activations. However, these models still lack long- term memory
plausibility (i.e. how they deal with long-term dependencies and
contextual information) as well as insights on the underlying neural
substrate mechanisms. This paper studies the influence of context
representations of different lan- guage models such as sequence-based
models: Long Short- Term Memory networks (LSTMs) and ELMo, and a
pretrained Transformer language model (Longformer). In particular, we
study how the internal hidden representations align with the brain
activity observed via fMRI when the subjects listen to several narrative
stories. We use brain imaging recordings of subjects listening to
narrative stories to interpret word and se- quence embeddings. We
further investigate how the represen- tations of language models layers
reveal better semantic con- text during listening. Experiments across
all language model representations provide the following cognitive
insights: (i) the representations of LSTM cell states are better aligned
with brain recordings than LSTM hidden states, the cell state ac- tivity
can represent more long-term information, (ii) the rep- resentations of
ELMo and Longformer display a good predic- tive performance across brain
regions for listening stimuli; (iii) Posterior Medial Cortex (PMC),
Temporo-Parieto-Occipital junction (TPOJ), and Dorsal Frontal Lobe (DFL)
have higher correlation versus Early Auditory (EAC) and Auditory Asso-
ciation Cortex (AAC). Keywords: Brain Encoding; Linear Mapping; fMRI;
LSTM; ELMo; Longformer; Transformer;

Introduction In the past decade, artificial neural networks have
witnessed a remarkable insights in the computational neuroscience com-
munity in understanding how the brain performs stimulus per- ception (1)
given various forms of sensory inputs like visual processing in object
recognition tasks (Yamins et al., 2014; Cadieu et al., 2014; Eickenberg,
Gramfort, Varoquaux, & Thirion, 2017), or (ii) by studying higher-level
cognition like language processing (Gauthier & Levy, 2019; Schrimpf et
al., 2021; Schwartz, Toneva, & Wehbe, 2019). This line of work, namely
brain encoding, aims at constructing neural brain ac- tivity given an
input stimulus.

Sentence comprehension is studied using fMRI since a while (Constable et
al., 2004), Some studies have looked into the modelling of language
comprehension: e.g. how sequence-based language models such as
echo-state networks (ESN) (Hinaut & Dominey, 2013) or long short-term
memory networks (LSTM) (Jain & Huth, 2018; Variengien & Hinaut, 2020)
encode syntactic structures and contextual information.

Morevoer, (Jain & Huth, 2018) used LSTMs to get the con- text
representation of sentences (with a next word prediction task) and then
used this representation to predict fMRI data. Some works studied the
LSTM capacity of representing long-term information (Karpathy, Johnson,
& Fei-Fei, 2015) and its ability to model working memory (O’Reilly &
Frank, 2006), there still lacks investigation of the long-term memory
cognitive plausibility of LSTM and its link to fMRI data. In this paper,
we open the black box of LSTM to look at par- ticular LSTM activations:
the cell state and the hidden state. This can give more insights on
longer-term and shorter-term information. Indeed, the cell state
mechanism has been in- troduced in the original LSTM paper (Hochreiter &
Schmid- huber, 1997) in order to keep the error gradient of back-
propagation constant over long-time scales. Thus, its activ- ity can
represent more long-term information than the hid- den state of the
LSTM. We also investigate how the pre- trained bi-directional sequence
embedding language model ELMo (Peters et al., 2018) handles the longer
context and interprets the LSTM layers representations that better
predict brain activity.

Recently, the researchers studied how the representations from
Transformer (Vaswani et al., 2017) based language models such as BERT
(Devlin, Chang, Lee, & Toutanova, 2019) and RoBERTa (Liu et al., 2019)
could directly pre- dict fMRI data. Interestingly, such
Transformer-based neu- ral representations have been found to be very
effective for brain encoding as well (Schrimpf et al., 2021). On the
other hand, (Gauthier & Levy, 2019) fine-tune a pretrained BERT model on
multiple natural language processing tasks to find tasks best correlated
with high decoding performance. In recent works, (Caucheteux, Gramfort,
& King, 2021a; An- tonello, Turek, Vo, & Huth, 2021) interpret the
representa- tions of the Transformer model (GPT-2 (Radford et al.,
2019)) by disentangling the high-dimensional Transformer represen-
tations of language models into four combinatorial classes: lexical,
compositional, syntactic, and semantic representa- tions to explore
which class is highly associated with lan- guage cortical ROIs. However,
these models are unable to handle the long-term dependencies (sequence
length is fixed to 512 words) due to their self-attention operation. To
over- come this limitation, recently, (Beltagy, Peters, & Cohan, 2020)
introduced Longformer making it easy to process doc- uments of thousands
of tokens or longer and combining local

windowed attention with global attention.

In this paper, we uncover insights about the association be- tween fMRI
voxel activations and representations of diverse language models: LSTM,
ELMo, and Longformer. The pre- dictive power of language model specific
representations with brain activation is ascertained by (1) using ridge
regression on such representations and predicting activations and (2)
com- puting popular metrics like 2V2 accuracy and Pearson corre- lation
between actual and predicted activations.

Specifically, we make the following contributions in this paper. (1)
Given a language model pretrained on corpora by handling long-term
dependencies, we propose the problem of finding which of these are the
most predictive of fMRI brain (2) The investigation of long- activity
for listening tasks. term context of language model results reveals that
ELMo and Longformer representations display better correlation during
narrative story listening. (3) We also investigate the inter- nal memory
representations of LSTM (cell state and hidden state) and derive
interesting insights that the cell state repre- sentations yield better
performance than hidden state repre- sentations.

Methodology

Brain Imaging Dataset Narratives-Pieman (Listening to Stories) The
“Narratives” collection aggregates a variety of fMRI datasets collected
while human subjects listened to naturalistic spoken sto- ries. The
Narratives dataset that includes 345 subjects, 891 functional scans, and
27 diverse stories of varying dura- tion totaling ∼4.6 hours of unique
stimuli (∼43,000 words) was proposed in (Nastase et al., 2021). Similar
to earlier works (Caucheteux, Gramfort, & King, 2021b), we analyze data
from 82 subjects listening to the story titled ‘PieMan’ with 259 TRs
(repetition time)1. A TR is the length of time between corresponding
consecutive points in fMRI: here it is 1.5 sec. We list number of voxels
per ROI (Region of Inter- est) in this dataset in Table 1. We use the
multi-modal par- cellation of the human cerebral cortex (Glasser Atlas:
con- sists of 180 ROIs in each hemisphere) to display the brain maps
(Glasser et al., 2016), since the Narratives dataset con- tains
annotations tied to this atlas. The data covers ten brain ROIs, i.e.,
Left hemisphere (L), and Right hemisphere (R) for each of the following:
(i) early auditory cortex (EAC: A1, LBelt, MBelt, PBelt, and R1) which
plays a key role for sound perception since it represents one of the
first cortical processing stations for sounds; (ii) auditory association
cortex (AAC: A4, A5, STSdp, STSda, STSvp, STSva, STGa, and TA2) which is
concerned with the memory and classification of sounds; (iii) posterior
medial cortex (PMC: POS1, POS2, v23ab, d23ab, 31pv, 31pd, 7m) which has
been implicated in tasks as diverse as attention, memory, spatial
navigation, emotion, self-relevance detection, and reward evaluation;
(iv) the temporo parieto occipital junction (TPOJ: TPOJ1, TPOJ2,

1282 TRs (before preprocessing) and 259 TRs (after preprocess-

ing).

TPOJ3, STV, PSL) which is a complex brain territory heav- ily involved
in several high-level neurological functions, such as language,
visuo-spatial recognition, writing, reading, sym- bol processing,
calculation, self-processing, working mem- ory, musical memory, and face
and object recognition; and (v) the dorsal frontal lobe (DFL: L 55b,
SFL, L 44, L 45, IFJA, IFSP) which covers the aspects of pragmatic
process- ing such as discourse management, integration of prosody,
interpretation of nonliteral meanings, inference making, am- biguity
resolution, and error repair. These five brain ROIs (EAC, AAC, TPOJ,
DFL, and PMC) span a cortical hierar- chy supporting language and
narrative comprehension (Huth, De Heer, Griffiths, Theunissen, &
Gallant, 2016; Baldassano et al., 2017).

Table 1: # Voxels in each ROI in the Narratives Dataset. LH - Left
Hemisphere. RH - Right Hemisphere. Pieman has 82 subjects.

PMC ROIs→ EAC LH RH LH RH LH RH LH RH LH RH # Voxels 808 638 1420 1493
1198 1204 847 1188 1061 875

TPOJ

AAC

DFL

Encoding Models

To explore how and where contextual language features are represented in
the brain when listening to stories, we extract internal hidden
representations from two sequence- based models: Random LSTM and LSTM,
ELMo (obtain- ing context-dependent word embeddings), and popular pre-
trained Transformer language model (Longformer) used for describing each
stimulus sentence and use them in an encod- ing model to predict brain
responses. Our main objective is to compare the correlation between each
model dense hidden representations and human cognitive process. In this
paper, we train fMRI encoding models using Ridge regression on stimuli
representations obtained using four models: Random LSTM, LSTM, ELMo, and
Longformer. The main goal of each fMRI encoder model is to predict brain
responses as- sociated with each brain region given stimuli. In all
cases, we train a ridge regression model per subject separately. Fol-
lowing the literature on brain encoding (Caucheteux et al., 2021b;
Toneva, Stretcu, P´oczos, Wehbe, & Mitchell, 2020), we choose to use a
ridge regression model instead of more complex models. For instance,
(Affolter, Egressy, Pascual, & Wattenhofer, 2020) used a neural
network-based model that directly maps fMRI-to-word in a decoding setup.
We plan to explore more such models as part of future work in brain
encoding. Here, our main is objective to investigate the influ- ence of
context representations of different language models such as
sequence-based models: LSTM and ELMo, and pop- ular pretrained
Transformer language model (Longformer).

LSTM

First, we train an LSTM (Hochreiter & Schmidhuber, 1997) network to
predict the probability of the next word as a func- tion of the history
of previous words. The weights of LSTMs are learned using the error
back-propagation through time

(BPTT). To make the association between encoded stimuli from LSTM’s
internal components and fMRI brain activity, we do the folllowing: (i)
At the time step t, we use vector at to represent the internal neurons
of encoded stimuli in LSTM. In this paper, at may be hidden state vector
(ht ) and cell state vector (ct ). (ii) In order to map the stimuli
encoded vector at of LSTM and brain activity at the t-th time step (Yt
), we define a simple linear model, ridge regression, to predict the
brain activity ( ˆYt ) from at , as discussed in the ridge regression
section.

Pretrained text Transformer: Longformer

Longformer (Beltagy et al., 2020) builds on BERT’s lan- guage masking
strategy and supports long document genera- tive sequence-to-sequence
tasks. We use the pretrained Long- former model with a local attention
mechanism, where the default window size is set to 5. To obtain the
stimuli repre- sentation, we use the last layer token representations
where each token dimension is 768.

Linear Probing of Language Models

Here, we do not train Random LSTM, LSTM, ELMo, and Longformer networks
to directly predict brain activities, for two reasons. First, the
dimension of the fMRI voxels varies among different subjects and across
different ROIs. There- fore, it is not convenient to design a universal
neural network architecture for generating outputs of different
dimensions. Second, the goal of this research is not to improve the
perfor- mance of language models in predicting fMRI. We want to explore
linear mappings between particular features of lan- guage models states
and neural activities in the auditory and language brain ROIs. Namely,
we look at (i) the character- istics of hidden state vectors (ht ) and
artificial memory vec- tors (ct ) in both LSTMs and Random LSTMs, and
(ii) local context vectors obtained from performance-optimized deep
neural network models (ELMo and Longformer). Therefore, we avoid any
possible supervision from the fMRI data when training LSTM and Random
LSTM language models.

Ridge Regression

We trained a ridge regression based encoding model to pre- dict the fMRI
brain activity associated with the semantic vec- tor representation
obtained from each language model: ran- dLSTM (hidden state, cell
state), LSTM (hidden state, cell state), GloVe, ELMo, and Longformer.
Each voxel value is predicted using a separate ridge regression model.
Formally, at the time step (t), we encode the stimuli as Xt ∈ RN×D and
brain region voxels Yt ∈ RN×V , where N denotes the number of training
examples, D denotes the dimension of input stim- uli representation, and
V denotes the number of voxels in a particular region. Hyper-parameter
Setting: We used sklearn’s ridge- regression with default parameters,
5-fold cross-validation, Stochastic-Average-Gradient Descent Optimizer,
Hugging- face for Longformer, MSE loss function, and L2-decay

(λ):1.0. We used Word-Piece tokenizer for the Longformer model and
Spacy-tokenizer for the GloVe and ELMo models.

Model Prediction Across Whole Brain To determine the significant voxel
predictions across the whole-brain, we ran the permutation tests where
we shuffled the true responses 5000 times, computed the Pearson correla-
tion scores, and finally obtained the FDR corrected p-values for the
whole brain results using both Longformer and ELMo. We set the
correlation score of voxels to zero if the p-value of the correlation
obtained from the permutation test is above the significance threshold
(p> 0.05, FDR corrected).

Evaluation Metrics We evaluate our models using popular brain encoding
evalua- tion metrics described in the following. Given a subject and a
brain region, let N be the number of samples. Let {Yi}N i=1 and { ˆYi}N
i=1 denote the actual and predicted voxel value vectors for the ith
sample. Thus, Y ∈ RN×V and ˆY ∈ RN×V where V is the number of voxels in
that region. 2V2 Accuracy is computed as follows.

2V2Acc =

I[{cosD(Yi, ˆYi) + cosD(Yj, ˆYj)}

N−1 ∑ i=1

N ∑ j=i+1

1 NC2 < {cosD(Yi, ˆYj) + cosD(Yj, ˆYi)}]

where cosD is the cosine distance function. I[c] is an indicator
function such that I[c] = 1 if c is true, else it is 0. The higher the
2V2 accuracy, the better. Pearson N ∑n PC= 1

as i=1 corr[Yi, ˆYi] where corr is the correlation function.

Correlation

computed

(PC)

is

Comparison to Other Language Models We compare the LSTM model with the
Longformer and several other pretrained language models: Random LSTM,
ELMo, and GloVe. We encoded the number of words same for both ELMo and
Longformer to match the performance. LSTM Training: We experimented with
one layer of LSTM to perform the next word prediction. In our next word
predic- tion, we first split each story in half (of 27 stories); we des-
ignate the first half as the training set and the second half as the
test set. The model is implemented in Keras with Tensor- Flow backend
(Abadi et al., 2016) with cross-entropy as loss, Adam optimizer (Kingma
& Ba, 2014), the number epochs set to 100, the batch size is of 64,
applied dropout with a keep- probability of 0.2, and tried LSTM with
hidden state size is set to 100, the dimensionality of word embeddings
is set to 100. The other hyper-parameters are learning rate (0.01), and
maximum sequence length is set to 5. Random LSTM: We use a random LSTM
model where the LSTM neurons are randomly initialized and kept frozen.
We use the output and cell state vectors at each time step to per- form
fMRI encoding. The configuration details of Random LSTM are the same as
that original LSTM model.

Figure 1: Pearson correlation coefficient (top figure) and 2V2 Accuracy
(bottom figure) between predicted and true responses across different
brain regions using a variety of language models (for Narratives-Pieman
dataset). Results are averaged across all participants. ELMo and
Longformer are the best. Rand Noise stands for a “Random noise vector”.
(hidden state) stands for the “short-term memory of internal state of
the LSTM”.

ELMo: ELMo (Embeddings from Language Models) is a successful NLP
framework developed by the Al- lenNLP (Peters et al., 2018) group.
Unlike earlier embed- dings, ELMo embeddings represent words in a
contextual fashion using a bidirectional LSTM model. We perform the
average of word embeddings at each TR (1.5sec.) to obtain the contextual
representation. GloVe: based word vectors (each word is a 300-dimension
vector) (Pennington, Socher, & Manning, 2014), and the av- erage of word
embeddings results in context vector in each time step.

Results In order to assess the performance of the fMRI encoder mod- els
learned using the representations from a variety of lan- guage models,
we computed the 2V2 accuracy and Pear- son correlation coefficient
between the predicted and true responses across various ROIs for the
listening (Narratives- Pieman) dataset (Fig. 1).

Encoding performance of Language Models From Fig. 1, we observe that the
profiles of performance show low scores in the early auditory cortex
(EAC) and auditory association cortex (AAC); average scores in TPOJ and
DFL; and superior scores in PMC. This aligns with the known lan- guage
hierarchy for spoken language understanding (Huth et al., 2016;
Baldassano et al., 2017; Nastase, Liu, Hillman, Norman, & Hasson, 2020).
Language models ELMo, and

Longformer yield better performance in predicting the brain responses
than the LSTM model across all the ROIs. These Pearson correlation (ρ)
results are comparatively much higher compared to those obtained using
the pretrained GPT2 model in (Caucheteux et al., 2021a) (ρ ranging from
0.02 − 0.06). As shown in Fig. 1, our method obtains more than 3 times
higher correlations (ρ ranging from 0.02 − 0.19)2. The main reason is
that the Longformer is designed to process docu- ments of thousands of
tokens or longer sequences while GPT- 2 models are unable to handle the
long-term dependencies (sequence length is fixed to 512 words). Also,
the narrative dataset consists of longer documents (more than 2000 words
in one story); the traditional transformer models consider the context
up to 512 words, whereas Longformer handles even longer documents.

Further, from Fig. 1, we see that the bilateral posterior me- dial
cortex (PMC) associated with higher language function exhibits a higher
correlation among all the brain ROIs. ROIs, including bilateral TPOJ and
bilateral DFL, yield higher cor- relations with the ELMo and Longformer,
which is in line with the language processing hierarchy in the human
brain. Finally, across all regions, Rand Noise vector and Rand LSTM
models have worse correlation compared to LSTM and other language
models. In summary, different and dis-

2we do not apply on the same number of subjects and/or same amount of
stories than in (Caucheteux et al., 2021a). However, we tested with few
other stories such as Lucy and Slumlord, and our results (higher
correlations) show similar trends.

EAC_LEAC_RAAC_LAAC_RPMC_LPMC_RTPOJ_LTPOJ_RDFL_LDFL_R00.10.2Rand
NoiseGloVeRandLSTM (hidden state)RandLSTM (cell state)LSTM (hidden
state)LSTM (cell state)ELMoLongformerAverage of SubjectsPearson
CorrelationEAC_LEAC_RAAC_LAAC_RPMC_LPMC_RTPOJ_LTPOJ_RDFL_LDFL_R0.50.7Average
of Subjects2V2 AccuracyTable 2: p-values obtained using post hoc
pairwise compar- isons for the three best models (+ LSTM hidden state).

Models compared Longformer vs ELMo LSTM (Cell state vs hid- den state)
Longformer vs LSTM (cell state) ELMo vs LSTM (cell state)

EAC 0.521 0.991

AAC 0.271 0.177

PMC 0.168 0.0357*

TPOJ 0.054 0.0038*

DFL 0.356 0.158

0.048* 0.0008* 0.00002* 0.00003* 0.0015*

0.372

0.003*

0.00004* 0.00006* 0.0049*

tinct language model features seem to be related to the en- coding
performance in listening tasks.

In order to estimate the statistical significance of the per- formance
differences, we performed one-way ANOVA on the mean correlation values
for the subjects across the language models (GloVe, LSTM (cell state),
LSTM (hidden state), ELMo, and Longformer) for the five brain ROIs. The
main effect of the ANOVA test was significant for all the ROIs with p≤
10−2 with confidence 95%. Further, post hoc pairwise comparisons (Ruxton
& Beauchamp, 2008) confirmed the visual observations that on both 2V2
accuracy and Pearson correlation measures, tasks such as ELMo and
Longformer performed significantly better compared to other models, as
shown in Table 2.

LSTM: Effects of Hidden State vs Cell State Vectors

In order to explore how LSTM hidden units learn to en- code the
long-term and short-term memory information and the interaction between
the two types of working memories, we compare the encoding performance
between representa- tions of hidden state and cell state vectors. Fig. 1
show- cases the fMRI encoding performance of both RandLSTM and LSTM
models where the cell state representations (long term-memory vector)
yield better performance than hidden state representations (short-term
memory). This supports the cognitive plausibility of the LSTM cell
architecture. Besides, the performance of GloVe and RandLSTM models have
sig- nificantly equal performance, indicating that semantic context is
missing in these models.

Which ELMo layers perform better encoding?

We investigate how the performance of ELMo changes at dif- ferent layers
(Embedding layer, LSTM layer-1, and LSTM layer-2), as they are provided
in different contexts. The re- sults are shown in Fig. 2. From Fig. 2,
we observe that layer- 2 displays better 2v2 accuracy and Pearson
correlation score compared to other layers. We further observe that the
layer 1 show a sharp increase in performance compared to embed- ding
layer in the context of narrative story listening.

Which Longformer layers perform better encoding?

Given the hierarchical processing of language information across the
Transformer layers, we further examine how these Transformer layers
encode fMRI brain activity using encoder layers of Longformer. We
present the layer-wise encoding performance results across brain ROIs in
Fig. 3. We observe that in all the layers, intermediate layers (6 to 8)
perform the

Figure 2: ELMo layers: Pearson correlation coefficient (top) and 2V2
Accuracy (bottom) between predicted and true re- sponses across
different brain regions using layers of ELMo model. Results are averaged
across all participants.

best for narrative listening, followed by a decrease in perfor- mance.

Figure 3: Longformer layers: Pearson correlation coefficient between
predicted and true responses across different brain regions (different
color lines) using Longformer. Results are averaged across all
participants. Middle layers (6 and 8) show best correlation.

Cognitive Insights

We further analyse in more detail the prediction performance of the
encoder model trained on sub ROIs for the ELMo In the EAC, the sub ROI
pbelt and Longformer in Fig. 4. (parabelt) display higher Pearson
correlation among other sub ROIs, and it is adjacent to the lateral belt
on the exposed surface of the superior temporal gyrus (STG). Also, the
pbelt area represent the next level in the auditory hierarchy and mainly
concerned with memory or decision-making. Simi- larly in the AAC, the
sub ROIs such as A4, A5, stsvp, and stsda yield better correlation, and
these ROIs shares the primary medial and posterior borders with TPOJ
(Trumpp, Kliese,

EAC_LEAC_RAAC_LAAC_RPMC_LPMC_RTPOJ_LTPOJ_RDFL_LDFL_R00.10.2Embedding
Layer (Input layer)Layer-1Layer-2Average of SubjectsPearson
CorrelationEAC_LEAC_RAAC_LAAC_RPMC_LPMC_RTPOJ_LTPOJ_RDFL_LDFL_R0.50.60.70.8Average
of Subjects2V2 Accuracy246810120.050.10.150.2EACAACPMCTPOJDFLLayer
NumberPearson CorrelationFigure 5: BrainMaps: Whole-brain prediction
correlation us- ing representations of Longformer (left) and ELMo
(right) in one sample subject (subject 1) of Narratives-Pieman dataset.

Discussion

(1) We used a ridge regression model instead of more com- plicated
    models for encoding; (2) We experimented with sev- eral language
    models where the pretrained context represen- tations (such as in
    ELMo and Longformer) are better predic- tors of voxel
    activations. (3) In LSTM, the cell state repre- sentations (long
    term memory vector) yield better encoding performance than hidden
    state representations; thus, internal dynamics of LSTMs seem to have
    more cognitively plausible activations than classically studied LSTM
    activations. (4) We used different layers of ELMo and Longformer,
    where higher layers display better correlation for ELMo while
    intermedi- ate layers show superior performance for Longformer.
(2) The control and attention ROIs in the posterior cingulate cor- tex,
    together with the superior frontal language region (sfl in DFL) and
    TPOJ, are part of the language network associated with narrative
    comprehension. (6) The posterior medial cor- tex (PMC),
    temporo-parieto-occipital junction (TPOJ), dorsal frontal lobe (DFL)
    have higher correlation versus early audi- tory and auditory
    association cortex.

Limitations

We believe that more complex models instead of simple ridge regression
models can lead to further exciting insights: (i.e. we may be able to
link more directly internal model mech- anisms to brain activations).
However, to achieve this, one would need more data (more subjects and
longer stories) to train such complex models.

Conclusion

This paper studied the long-term memory plausibility of lan- guage
models for brain encoding. We observe that building individual encoding
models and interpreting the internal rep- resentations among models can
provide a more in-depth un- derstanding of the neural representation of
language infor- mation. Our experiments on the Narrative listening
stories dataset lead to interesting cognitive insights.

Figure 4: Pearson correlation coefficient between predicted and true
responses across different sub ROIs of the Language Network using ELMo
and Longformer. Results are averaged across all participants.

Hoenig, Haarmeier, & Kiefer, 2013). Further, there is evi- dence that
these sub ROIs of the AAC process perceptual and conceptual acoustic
sounds during auditory stories and social interaction tasks (Glasser et
al., 2016). It can be observed that sub ROIs such as Pos1 and Pos2 have
a higher Pearson cor- relation than other sub ROIs of the PMC region.
Both sfl and l55b display a higher correlation among all the sub ROIs
for the DFL ROI. However, all the sub ROIs in the TPOJ yield higher
correlation, as shown in Fig. 4. The control and atten- tion ROIs in the
posterior cingulate cortex (for ex., POS1 in PMC), together with the
superior frontal language region (sfl in DFL) and TPOJ, are part of the
language network associ- ated with narrative comprehension (Nastase et
al., 2020): it is encouraging to see that both ELMo and Longformer also
relate to semantic analysis of the ongoing narrative because they obtain
best performance, showing that capturing longer- term context is
important.

Brain maps for whole brain predictions

The whole-brain prediction Pearson correlation for all the voxels using
ELMo and Longformer is shown in Fig. 5. In the listening task, we
observe from Fig. 5 that Longformer dis- plays higher correlation values
for many voxels than ELMo. From Fig. 5, we see that ROIs such as EAC and
AAC have a lower percentage of voxels with a higher correlation com-
pared to PMC and TPOJ brain ROIs (higher percentage of voxels with
higher correlation).

00.050.10.150.2Pos1Pos2V23abd23ab31pv31pd7m00.050.10.15tpoj1tpoj2tpoj3stvpsl00.050.10.150.2l55bsfll44l45ifjaifsp00.050.10.15A4A5stsdpstsdastsvpstsvastgata200.050.10.150.2A1lbeltmbeltpbeltR1Pearson
CorrelationPearson CorrelationPearson CorrelationPearson
CorrelationPearson
CorrelationPMCTPOJDFLAACEACELMoLongformerLHLHRHRHAcknowledgments

We thank the anonymous reviewers for their very helpful comments on
previous version of this paper.

References Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,
J., . . . others (2016). Tensorflow: a system for large-scale machine
learning. In Osdi.

Affolter, N., Egressy, B., Pascual, D., & Wattenhofer, R. (2020).
Brain2word: Decoding brain activity for language generation. arXiv
preprint arXiv:2009.04765.

Antonello, R., Turek, J., Vo, V., & Huth, A. (2021). Low- dimensional
structure in the space of language represen- arXiv preprint tations is
reflected in brain responses. arXiv:2106.05426.

Baldassano, C., Chen, J., Zadbood, A., Pillow, J. W., Hasson, U., &
Norman, K. A. (2017). Discovering event structure in continuous
narrative perception and memory. Neuron, 95(3), 709–721.

Beltagy, I., Peters, M. E., & Cohan, A.

(2020). Long- former: The long-document transformer. arXiv preprint
arXiv:2004.05150.

Cadieu, C. F., Hong, H., Yamins, D. L., Pinto, N., Ardila, D., Solomon,
E. A., . . . DiCarlo, J. J. (2014). Deep neural net- works rival the
representation of primate it cortex for core visual object recognition.
PLoS Computational Biology, 10(12), e1003963.

Caucheteux, C., Gramfort, A., & King, J.-R. (2021a). Disen- tangling
syntax and semantics in the brain with deep net- In International
conference on machine learning works. (pp. 1336–1348).

Caucheteux, C., Gramfort, A., & King, J.-R. (2021b). Model- based
analysis of brain activity reveals the hierarchy of lan- guage in 305
subjects. arXiv preprint arXiv:2110.06078. Constable, R. T., Pugh, K.
R., Berroya, E., Mencl, W. E., Westerveld, M., Ni, W., & Shankweiler, D.
(2004). Sen- tence complexity and input modality effects in sentence
comprehension: an fmri study. NeuroImage, 22(1), 11–21. Devlin, J.,
Chang, M.-W., Lee, K., & Toutanova, K. (2019). Bert: Pre-training of
deep bidirectional transformers for language understanding. In
Proceedings of the 2019 con- ference of the north american chapter of
the association for computational linguistics: Human language
technologies, volume 1 (long and short papers) (pp. 4171–4186).

Eickenberg, M., Gramfort, A., Varoquaux, G., & Thirion, B. (2017).
Seeing it all: Convolutional network layers map the function of the
human visual system. NeuroImage, 152, 184–194.

Gauthier, J., & Levy, R. (2019). Linking artificial and human neural
representations of language. In Proceedings of the 2019 conference on
empirical methods in natural language processing and the 9th
international joint conference on natural language processing
(emnlp-ijcnlp) (pp. 529–539). Glasser, M. F., Coalson, T. S., Robinson,
E. C., Hacker, C. D., Harwell, J., Yacoub, E., . . . others (2016). A
multi-modal

parcellation of human cerebral cortex. Nature, 536(7615), 171–178.

Hinaut, X., & Dominey, P. F. (2013). Real-time parallel pro- cessing of
grammatical structure in the fronto-striatal sys- tem: A recurrent
network simulation study using reservoir computing. PloS one, 8(2),
e52946.

Hochreiter, S., & Schmidhuber, J. (1997). Long short-term

memory. Neural computation, 9(8), 1735–1780.

Huth, A. G., De Heer, W. A., Griffiths, T. L., Theunissen, F. E., &
Gallant, J. L. (2016). Natural speech reveals the semantic maps that
tile human cerebral cortex. Nature, 532(7600), 453–458.

Jain, S., & Huth, A. G. (2018). Incorporating context into language
encoding models for fmri. In Proceedings of the 32nd international
conference on neural information pro- cessing systems (pp. 6629–6638).

Karpathy, A., Johnson, J., & Fei-Fei, L.

(2015). Visualiz- ing and understanding recurrent networks. arXiv
preprint arXiv:1506.02078.

Kingma, D. P., & Ba, J. (2014). Adam: A method for stochas-

tic optimization. arXiv preprint arXiv:1412.6980.

Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., . . .
Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692. Nastase, S. A., Liu, Y.-F.,
Hillman, H., Norman, K. A., & (2020). Leveraging shared connectivity to
Hasson, U. aggregate heterogeneous datasets into a common response
space. NeuroImage, 217, 116865.

Nastase, S. A., Liu, Y.-F., Hillman, H., Zadbood, A., Hasen- fratz, L.,
Keshavarzian, N., . . . others (2021). Narratives: fmri data for
evaluating models of naturalistic language comprehension. bioRxiv,
2020–12.

O’Reilly, R. C., & Frank, M. J.

(2006). Making working memory work: a computational model of learning in
the prefrontal cortex and basal ganglia. Neural computation, 18(2),
283–328.

Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors
for word representation. In Proceedings of the 2014 conference on
empirical methods in natural lan- guage processing (emnlp)
(pp. 1532–1543).

Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K.,
& Zettlemoyer, L. (2018). Deep contextualized word representations.
arXiv preprint arXiv:1802.05365. Radford, A., Wu, J., Child, R., Luan,
D., Amodei, D., (2019). Language models are unsu-

Sutskever, I., et al. pervised multitask learners. OpenAI blog, 1(8), 9.

Ruxton, G. D., & Beauchamp, G. (2008). Time for some a priori thinking
about post hoc testing. Behavioral ecology, 19(3), 690–693.

Schrimpf, M., Blank, I., Tuckute, G., Kauf, C., Hosseini, E. A.,
Kanwisher, N., . . . Fedorenko, E. (2021). The neural architecture of
language: Integrative reverse-engineering converges on a model for
predictive processing. PNAS, Vol. Inducing brain-relevant bias in
natural language processing models.

Schwartz, D., Toneva, M., & Wehbe, L.

(2019).

Advances in Neural Information Processing Systems, 32, 14123–14133.

Toneva, M., Stretcu, O., P´oczos, B., Wehbe, L., & Mitchell, T. M.
(2020). Modeling task effects on meaning represen- tation in the brain
via zero-shot meg prediction. Advances in Neural Information Processing
Systems, 33.

Trumpp, N. M., Kliese, D., Hoenig, K., Haarmeier, T., & Kiefer, M.
(2013). Losing the sound of concepts: Dam- age to auditory association
cortex impairs the processing of sound-related concepts. Cortex, 49(2),
474–486.

Variengien, A., & Hinaut, X.

(2020). A journey in esn and lstm visualisations on a language task.
arXiv preprint arXiv:2012.01748.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
A. N., . . . Polosukhin, I. (2017). Attention is all In Advances in
neural information processing you need. systems (pp. 5998–6008).

Yamins, D. L., Hong, H., Cadieu, C. F., Solomon, E. A., Seib- ert, D., &
DiCarlo, J. J. (2014). Performance-optimized hi- erarchical models
predict neural responses in higher visual cortex. Proceedings of the
National Academy of Sciences, 111(23), 8619–8624.


