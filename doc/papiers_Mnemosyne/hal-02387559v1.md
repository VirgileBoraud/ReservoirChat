Using Conceptors to Transfer Between Long-Term and
Short-Term Memory
Anthony Strock, Nicolas P. Rougier, Xavier Hinaut

To cite this version:

Anthony Strock, Nicolas P. Rougier, Xavier Hinaut. Using Conceptors to Transfer Between Long-
Term and Short-Term Memory. Artificial Neural Networks and Machine Learning (ICANN) 2019, Sep
2019, Munich, Germany. pp.19-23, ￿10.1007/978-3-030-30493-5\_2￿. ￿hal-02387559￿

HAL Id: hal-02387559

https://inria.hal.science/hal-02387559

Submitted on 29 Nov 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Using conceptors to transfer between
long-term and short-term Memory

Anthony Strock2,1,3,∗

, Nicolas Rougier1,2,3, and Xavier Hinaut1,2,3

1 INRIA Bordeaux Sud-Ouest, Bordeaux, France
2 LaBRI, Université de Bordeaux, CNRS, UMR 5800, Talence, France
3 IMN, Université de Bordeaux, CNRS, UMR 5293, Bordeaux, France
\* Corresponding author: Anthony.Strock@inria.fr

Abstract. We introduce a model of working memory combining short-term
and long-term components. For the long-term component, we used Concep-
tors in order to store constant temporal patterns. For the short-term compo-
nent, we used the Gated-Reservoir model: a reservoir trained to hold a trig-
gered information from an input stream and maintain it in a readout unit. We
combined both components in order to obtain a model in which information
can go from long-term memory to short-term memory and vice-versa.

1 Introduction

Jaeger recently showed how to store and retrieve several temporal patterns using an
extension of reservoirs [1, 2]. The key idea is to project, using Conceptors, the net-
work activity into a lower dimensional space speciﬁc to the patterns. These Con-
ceptors can be considered as a long-term memory (of the temporal patterns). In the
meantime, we have shown [3] how a reservoir, using a gating signal, can faithfully
memorize an information for a short delay (i.e. working memory) from a stream of
random inputs. In this model, the maintenance of information in readout unit(s) is
remarkably precise for rather long periods of time. However, for very long periods of
time, the precision is lost because of a slow drift in the dynamics. In the present work,
we introduce a preliminary attempt to link short-term and long-term memories by
combining these two approaches: (1) a gated reservoir maintaining short-term in-
formation and (2) several conceptors maintaining long-term information.

2 Methods

We consider a reservoir of 1000 neurons that has been trained to solve a gating task
described in [3]. In this task the model receives an input continuously varying over
time (the value) and another input being either 0 or 1 (the trigger). To succeed the
task, the output has to be updated to the value received when the trigger is active but
has to stay still otherwise (similarly to a line attractor). In other words, the trigger acts
as a gate that controls the entry of the value in the memory (the output). The overall
dynamics of the network we consider is described by the following equations:
x[n] = C tanh (cid:161)Wi nu[n] + W (x[n − 1] + ξ) + W f b(y[n − 1])(cid:162) and y[n] = Wout x[n] (1)

2

A. Strock et al.

where u[n], x[n] and y[n] are respectively the input, the reservoir and the output
at time n. W , Wi n, W f b, Wout and C are respectively the recurrent, the input, the
feedback, the output and the conceptor weight matrices and ξ is a uniform white
noise term added to reservoir units. W , Wi n, W f b are uniformly sampled between
−1 and 1. Only W is modiﬁed to have sparsity level equal to 0.5 and a spectral radius
of 0.1. When Wout is computed to solve the gating task, the conceptor C is considered
to be ﬁxed and equal to the identity matrix (C = I ). In normal mode, the conceptor
C is equal to a conceptor Cm that is generated and associated to a constant value
m. In order to compute this conceptor Cm, we impose a trigger (T = 1) as well as
the input value (V = m) at the ﬁrst time step, such that the reservoir has to maintain
this value for 100 time steps. During these 100 time steps, we use the identity matrix
in place of the conceptor. The conceptor Cm is then computed according to Cm =
X X T (cid:161)X X T + I
, where X corresponds to the concatenation of all the 100 reservoir
a
states after the trigger, each row corresponding to a time step, I the identity matrix
and a the aperture. In all the experiments the aperture has been ﬁxed to a = 10. For
the conceptors pre-computed in Figure 1 and 2, the reservoir have been initialised
with its last training state.

(cid:162)−1

3 Results

Transfer between long-term and short-term memory: Figure 1 displays the two
core ideas of our approach: (1) How to transfer short-term to long-term memory
and (2) How to retrieve (in short-term memory) an information stored in long-term
memory. (1) The long-term memory we consider is the conceptor Cm associated to
the value m maintained in short-term memory. We compute Cm using the 100 ﬁrst
time steps after a trigger without conceptors (C = I ), after what we update C with Cm.
On ﬁgure 1B we can see that it doesn’t seem to cause any interference in the short-
term memory. However now the memory lies both in the conceptor C (long-term)
and in the output unit y (short-term). A more extensive and quantitative analysis
could study whether applying Cm would be a way to stabilize the short-term mem-
ory. (2) Now, the long-term memory we consider are only conceptors C D
m associated
to discrete values between -1 and 1 (11 values uniformly spread between -1 and 1).
Similarly as before, after a trigger we compute a new conceptor C D
m using the 100
ﬁrst time steps and without conceptors (C = I ). Then, we search for the closest con-
ceptor C i
m using a distance between
conceptors and we update C with this conceptor. On ﬁgure 1C, we see the following
behavior: after a trigger, the value is correctly updated in short-term memory and
remains stable until C is updated (after 100 time steps) and then the output jumps
to the closest discrete representation of the memory.

m among the conceptors with discrete values C D

Generalizing long-term memory: On ﬁgure 2, we show two main ideas: (1) how
a linear interpolation between two conceptors can allow to generalize to gating of
other values, and (2) a representation of the space in which lies the conceptors and
their link to the memory they encode. (1) Interpolation and extrapolation C of con-
ceptor C0.1 and conceptor C1.0 has been computed as C = λC1.0 + (1 − λ)C0.1 with 31

Using conceptors to transfer between long-term and short-term Memory

3

Fig. 1. Approximation with conceptors or discrete conceptors. Black: Evolution of the short-
term memory (i.e. the readout y). Gray lines: the discrete value considered. Light gray areas:
time when conceptors are computed for the current value stored in short-term memory. A.
Discrete conceptors are appplied B-C. Exact conceptors are computed using 100 time steps
after a trigger while C = I. B. The exact conceptor is applied to the following time steps. C.
The closer conceptor among the discretized conceptor is applied for the following time steps.
Dashed lines represents the memory that should have been kept if not discretized.

λ values uniformly spread between -1 and 2. Even though the interpolated (λ ∈ [0, 1])
conceptors obtained are not exactly equivalent to Cm conceptors obtained in ﬁg-
ure 1, they seem to also correspond to a retrieved long-term memory value to be
maintained. The mapping between λ and the value is non-linearly encoded. For
right-extrapolation (λ ∈ [1, 2]) the conceptor seems to be linked to a noisy version
of a Cm conceptor. A value seems still to be retrieved from long-term memory and
maintained in short-term memory: the output activity is not constant, but its mov-
ing average is constant. For left-extrapolation (λ ∈ [−1, 0]), the conceptor obtained
seems not to encode any more information than the conceptor C0: all the output ac-
tivities collapse to zero. (2) Principal component analysis have been performed using
201 pre-computed conceptors associated to values uniformly spread between -1 and
1. The ﬁrst three components explain already around 85% of the variance. The ﬁrst
component seems to non-linearly encode the absolute value of the memory (ﬁgure
2B) whereas the second component seems to non-linearly encode the memory itself
(ﬁgure 2C). The Cm conceptors form a line but not a straight line (ﬁgure 2E-G), that
might explain why extrapolation doesn’t work as expected.

050010001500200025001.00.50.00.51.0OutputA050010001500200025001.00.50.00.51.0OutputTicks:B05001000150020002500Time1.00.50.00.51.0OutputTicks:C4

A. Strock et al.

Fig. 2. Generalisation of Cm conceptors. Red: two learned conceptors C0.1 and C1.0. Black:
Linear interpolation between conceptor C0.1 and conceptor C1.0. Gray: learned Cm for 201
value of m uniformly spread between −1 and 1. A Evolution of the short-term memory against
time for the different conceptors. B-D Link between principal components of the constant
conceptors and the memory it is encoding. For the interpolated conceptors, the memory is
considered as the mean in the last 1000 time steps. E-G Representation of the conceptors in
the three principal components of the Cm conceptors.

4 Discussion

This preliminary study introduces the basis for establishing a link between long-term
and short-term memory. Future work will concentrate on removing the engineered
steps, namely, the ofﬂine computation and selection of the closest conceptor. This
could be realized using the autoconceptors introduced in [2]. Moreover, this study
raised concerns concerning the best way to interpolate and extrapolate conceptors
since, as we have shown, a mere linear combination might not represent the best
solution.

References

1. Jaeger, H.: Controlling recurrent neural networks by conceptors. arXiv preprint

arXiv:1403.3369 (2014)

2. Jaeger, H.: Using conceptors to manage neural long-term memories for temporal patterns.

Journal of Machine Learning Research 18(13), 1–43 (2017)

3. Strock, A., Hinaut, X., Rougier, N.P.: A robust model of gated working memory. biorXiv

(2019). https://doi.org/10.1101/589564

05001000150020002500Time0.00.51.01.5OutputA42024PC1 (53.79%)101Value maintainedB1.00.50.00.51.0PC2 (22.08%)101Value maintainedC0.50.00.51.0PC3 (8.97%)101Value maintainedD42024PC1101PC2E1.00.50.00.51.0PC20.50.00.51.0PC3F0.50.00.51.0PC32.50.02.5PC1G