De Cambridge Analytica à ChatGPT, comprendre
comment l’IA donne un sens aux mots
Frédéric Alexandre

To cite this version:

Frédéric Alexandre. De Cambridge Analytica à ChatGPT, comprendre comment l’IA donne un sens
aux mots. The Conversation France, 2023. ￿hal-04156230￿

HAL Id: hal-04156230

https://inria.hal.science/hal-04156230

Submitted on 12 Jul 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

De Cambridge Analy0ca à ChatGPT, comprendre comment l’IA donne un sens aux mots 

ALEXANDRE Frédéric, Centre Inria de l’université de Bordeaux, CNRS, Bordeaux INP 

Un des problèmes majeurs que l’IA n’a toujours pas résolus aujourd’hui est celui de l’ancrage 
du symbole (symbol grounding problem), c’est-à-dire comment des symboles (des mots par 
exemple) peuvent être associés à leur signiﬁcaPon. Par exemple, si je dis : « le chat dort sur 
son coussin car il est faPgué », la plupart des êtres humains comprendra sans eﬀort que « il » 
renvoie à « chat » et pas à « coussin ». C’est ce qu’on appelle le raisonnement de bon sens. 
Par contre, ce raisonnement va être beaucoup plus diﬃcile en IA et je vais expliquer 
pourquoi. Mais je vais aussi présenter la technique dite de [plongement 
lexical](h[ps://fr.wikipedia.org/wiki/Plongement\_lexical) qui, si elle ne résout pas tous les 
problèmes, est cependant d’une redoutable eﬃcacité. Il est important de connaitre les 
principes de ce[e technique car c’est celle qui est uPlisée dans la plupart des modèles d’IA 
récents, dont ChatGPT. 

Le problème de l’ancrage du symbole a été idenPﬁé bien avant l’IA, par des philosophes et 
des linguistes, et a été résumé, au début du XXème par trois concepts regroupés dans un 
triangle dit sémioPque, dont les trois sommets renvoient respecPvement au symbole (le 
signiﬁant), l’objet qu’il représente et le concept (ou le signiﬁé), correspondant à l’image 
mentale que nous nous sommes construite, suite à nos expériences avec cet objet. Ce sont 
ces expériences (et en parPculier leurs valeurs émoPonnelles) et les propriétés de cet objet 
qu’elles nous ont permis d’extraire qui nous perme[ent d’avoir ce bon sens pour raisonner à 
son sujet et qui sont singulièrement absentes d’un système d’IA et qui lui donnent ce[e 
limitaPon d’accès à la sémanPque des symboles. 

John Searle a illustré ce problème en 1980 avec son expérience de pensée dite de la chambre 
chinoise. Dans ce[e chambre, un être humain qui ne parle pas le chinois reçoit des quesPons 
écrites en chinois. Il a accès à un ensemble de règles qui lui dit mécaniquement comment 
réagir à chaque caractère (ou combinaison de caractères) possibles, ce qui lui permet d’écrire 
une réponse à la quesPon et de l’envoyer à l’extérieur de la chambre. Ceci permet de 
montrer que si, pour une tâche donnée (un ensemble de quesPons), on est capable d’écrire 
l’ensemble des règles qui régissent son traitement, alors un système mécanique (ou un 
humain qui ne comprend pas le chinois) peut donner l’illusion à l’extérieur qu’il est 
intelligent. Mais même dans ce cas (et on peut imaginer qu’écrire un tel ensemble de règles 
n'est pas simple voire impossible pour de nombreuses tâches), l’agent ne comprendra pas ce 
qu’il fait et n’aura donc pas accès à la signiﬁcaPon des symboles qu’il manipule (ce que fera 
très naturellement toute personne parlant le chinois). Searle en déduisait alors qu’une 
approche purement mécanique, comme celle d’un ordinateur, quelle que soit la complexité 
des règles ou des algorithmes qu’il manipule, ne donnera donc jamais accès à la capacité de 
comprendre le sens de ce que l’on fait. 

Considérant que notre cerveau est tout à la fois capable de manipuler des symboles (des 
mots par exemple) et d’apprendre leur signiﬁcaPon à travers ses interacPons avec le monde, 
Stevan Harnard a repris ces idées en 1990 pour reformuler le problème de l’ancrage du 
symbole, en proposant qu’on pouvait le résoudre à une condiPon : L’interprétaPon 
sémanPque d’un système de symboles doit être faite de façon intrinsèque à ce système, si on 

 
 
 
 
 
veut qu’il ait eﬀecPvement accès au sens des symboles qu’il manipule. Il s’agirait donc 
d’ancrer notre système de symboles dans nos expériences sensorimotrices avec le monde, 
pour apprendre des relaPons symboles-signiﬁcaPons. De ce[e concepPon est née l’approche 
de l’IA incarnée, qui postule qu’un tel système (un robot par exemple) devrait avoir un corps 
pour ressenPr et interagir avec le monde, si il veut pouvoir donner un sens aux symboles 
qu’il manipule. Il reste maintenant à voir comment faire ça en praPque. C’est ce que propose, 
dans le cadre très restreint de l’analyse de textes, le plongement lexical. 

Ce[e technique consiste à remplacer un mot (qui peut être vu comme un symbole abstrait, 
impossible à relier directement à sa signiﬁcaPon) par un vecteur numérique (une liste de 
nombres). Notons que ce passage au numérique fait que ce[e représentaPon peut être 
directement uPlisée par des réseaux de neurones et bénéﬁcier de leurs capacités 
d’apprenPssage. En parPculier, ces réseaux de neurones vont, à parPr de très grands corpus 
de textes, apprendre à plonger un mot dans un espace numérique de grande dimension 
(typiquement 300) où chaque dimension calcule la probabilité d’occurrence de ce mot dans 
certains contextes. En simpliﬁant, on remplace par exemple la représentaPon symbolique du 
mot ‘chat’ par 300 nombres représentant la probabilité de trouver ce mot dans 300 types de 
contextes diﬀérents (texte historique, texte animalier, texte technologique, etc.) ou de co-
occurrence avec d’autres mots (oreilles, moustache ou avion). Même si ce[e approche peut 
sembler très pauvre, elle a pourtant un intérêt majeur en grande dimension : elle code avec 
des valeurs numériques proches des mots dont le sens est proche, ce qui va alors perme[re 
de déﬁnir des noPons de proximité et de distance pour comparer le sens de symboles, ce qui 
est un premier pas vers leur compréhension. Pour donner une intuiPon de la puissance de 
telles techniques (en fait, de la puissance des staPsPques en grande dimension pour des 
phénomènes avec des régularités telles qu’on en voit dans notre monde cogniPf), prenons 
un exemple similaire dont beaucoup ont entendu parler. 

C’est en eﬀet avec une approche similaire que des sociétés comme [Cambridge 
AnalyPca](h[ps://fr.wikipedia.org/wiki/Cambridge\_AnalyPca) ont pu agir sur le déroulement 
d’élecPons en apprenant à associer des préférences électorales (représentaPons 
symboliques) à diﬀérents contextes d’usages numériques (staPsPques subPlisées à parPr de 
pages Facebook d’usagers). Leurs méthodes reposent sur une publicaPon scienPﬁque parue 
en 2014 dans la revue PNAS qui comparait des jugements humains et des jugements issus de 
staPsPques sur des proﬁls Facebook. L’expérimentaPon reportée dans ce[e publicaPon 
demandait à quelques dizaines de milliers de parPcipants de déﬁnir certains de leurs traits 
psychologiques (sont-ils consciencieux, extraverPs, etc.). Ces parPcipants avaient donc des 
éPque[es (dites symboliques) représentant ces traits. On pouvait également les représenter 
par une éPque[e (dite numérique) comptant les ‘Likes’ qu’ils avaient mis sur Facebook sur 
diﬀérents sujets (sports, loisirs, cinéma, cuisine, etc). On pouvait alors, par des staPsPques 
dans cet espace numérique de grande dimension, apprendre à associer certains endroits de 
cet espace à certains traits psychologiques. Ensuite, pour un nouveau sujet, uniquement en 
regardant son proﬁl Facebook, on pouvait voir dans quelle parPe de cet espace il se trouvait 
et donc de quels types de traits psychologiques il est le plus proche. On pouvait également 
comparer ce[e prédicPon à ce que connait de ce sujet ses proches. Le résultat principal de 
ce[e publicaPon est que, si on s’en donne les moyens (dans un espace d’assez grande 
dimension, donc avec assez de ‘Likes’ à récolter, et avec assez d’exemples, ici plus de 70 000 
sujets), le jugement staPsPque peut être plus précis que le jugement humain. Autrement dit, 

 
 
qu’avec 10 Likes, on en sait plus sur vous que votre collègue de bureau ; 70 Likes que vos 
amis ; 275 Likes que votre conjoint. Ce[e publicaPon avait tout d’abord comme but de nous 
alerter sur le fait que, quand on recoupe diﬀérents indicateurs en grand nombre, nous 
sommes très prévisibles et qu’il faut donc faire a[enPon quand on laisse des traces sur les 
réseaux sociaux car de nombreux acteurs sur internet peuvent nous faire des 
recommandaPons ou des publicités ciblées avec une très grande eﬃcacité. L’exploitaPon de 
telles techniques est d’ailleurs la principale source de revenu de ces acteurs. 

Cambridge AnalyPca est allée un cran plus loin en subPlisant les proﬁls Facebook de millions 
d’américains et en apprenant à associer leurs Likes avec leurs préférences électorales pour 
mieux cibler des campagnes électorales. De telles techniques ont également été uPlisées lors 
du vote sur le Brexit, ce qui a conﬁrmé leur eﬃcacité. Notons que c’est uniquement 
l’aspiraPon illégale des proﬁls Facebook qui a été reprochée par la jusPce, ce qui doit 
conPnuer à nous rendre méﬁants quant aux traces que l’on laisse sur Internet. En exploitant 
ce même pouvoir des staPsPques en grand dimension, les techniques de plongement lexical 
uPlisent de grands corpus de textes (que l’on trouve facilement sur Internet (Wikipédia, livres 
numérisés, réseaux sociaux) pour associer des mots avec leur probabilité d’occurrence dans 
diﬀérents contextes (dans diﬀérents types de textes). Comme on l’a vu plus haut, ceci permet 
de considérer une proximité dans cet espace de grande dimension comme une similarité 
sémanPque et donc de calculer avec des mots en prenant en compte leur signiﬁcaPon. Un 
exemple classique qui est rapporté est de prendre le vecteur numérique représentant le mot 
Roi, de lui soustraire le vecteur (de même taille car reportant les probabilité d’occurrence sur 
les mêmes critères) représentant le mot Homme, de lui ajouter le vecteur représentant le 
mot Femme, pour obtenir un vecteur très proche de celui représentant le mot Reine. 
Autrement dit, on a bien réussi à apprendre une relaPon sémanPque de type A est à B ce 
que C est à D. 

Le principe retenu ici (dans ce monde lexical) pour déﬁnir la sémanPque est que deux mots 
proches sont uPlisés dans des mêmes contextes. On parle ici de sémanPque 
distribuPonnelle. C’est ce principe de codage des mots qu’uPle ChatGPT, auquel il ajoute 
d’autres techniques, comme on peut le voir dans le [texte] (« De quoi ChatGPT est-il le nom]. 

On a indiqué plus haut que coder avec des valeurs numériques proches des mots dont le 
sens est proche permet d’aider à leur compréhension. On peut se demander ce qu’il manque 
pour se rapprocher encore de la façon qu’a notre cerveau de comprendre un concept. On 
aborde ce[e quesPon dans le [texte] (ChatGPT est-il intelligent comme un humain ?). 

 
 
 
 
