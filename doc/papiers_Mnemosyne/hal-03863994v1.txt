Délibérer avec l’intelligence artificielle au service de l’intelligence
naturelle. Frédéric Alexandre, Thierry Viéville, Marie-Hélène Comte

To cite this version:

Frédéric Alexandre, Thierry Viéville, Marie-Hélène Comte. Délibérer avec
l’intelligence artificielle ￿hal- au service de l’intelligence
naturelle.. Penser calculer délibérer., Mare & Martin, pp.19, 2022.
03863994￿

HAL Id: hal-03863994

https://inria.hal.science/hal-03863994

Submitted on 21 Nov 2022

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

Titre de l’ouvrage: Penser , calculer , délibérer.

Titre du chapitre : Délibérer avec l’intelligence artificielle au
service de l’intelligence naturelle.

Ceci est le preprint du chapitre publié dans le livre.

Auteurs:

Frédéric Alexandre Chercheur en neurosciences cognitives et en
intelligence artificielle, Inria et Université de Bordeaux Thierry
Viéville Chercheur en neurosciences computationnelles et en sciences de
l’éducation, Inria et Université Côte d’Azur avec la relecture et le
conseil de Marie-Hélène Comte Ingénieure pédagogique du Learning Lab
Inria, coordinatrice de projets

Résumé : La numérisation de la société et le traitement automatique de
l’information, y compris avec des techniques dites d’intelligence
artificielle, induisent des ruptures dans notre façon de penser,
calculer et délibérer. Mais comment fonctionnent ces fonctions
cognitives et comment peuvent-elles être affectées par cette révolution
numérique ? Pour comprendre en profondeur cela, nous allons d’abord
prendre le temps d’expliquer ce que nous comprenons aujourd’hui de notre
intelligence biologique qui pense, ce qui offrira un éclairage crucial
sur ce qui se passe quand on utilise des machines qui calculent, avant
de conclure en quoi cela aide à réfléchir sur comment délibérer. Car
oser comprendre les aspects scientifiques et techniques de la pensée et
du calcul est essentiel pour se donner les moyens de délibérer en toute
conscience avec les outils intellectuels et numériques qui nous sont
donnés.

Mots clés : intelligence artificielle, neuroscience cognitive,

I. Introduction

II. Penser : comment marche le cerveau ? II.I Comportements dirigés par
    les stimuli II.II Comportements dirigés par les buts

III. Calculer : comment coder et traiter mécaniquement l’information ?

III.I Comment passer de nos connaissances à des données ? III.II Que
peut-on demander à une machine de calculer ?

IV. Délibérer: comment mettre l’intelligence artificielle à notre
    service ?

IV.I De la différence entre penser et calculer IV.II Allier pensée et
calcul pour délibérer.

1

2 3 5

7 7 11

14 14 15

I. Introduction

Aujourd’hui, avec la numérisation de la société (allant de
l’instrumentation et de la mise en ligne des entreprises, au
développement d’internet et des réseaux sociaux, et à leur

pénétration dans nos foyers), le stockage et surtout le traitement
massif des données s’impose comme une norme pour le traitement de
l’information. En particulier, les progrès récents dans ce qu’on appelle
communément1 intelligence artificielle ont permis la réalisation de
dispositifs aux résultats impressionnants : reconnaissances d’objets
visuels, traitement de la langue naturelle, outils logiciels d’analyse
de données et d’aide à la décision, y compris sur des sujets critiques
comme le diagnostic médical, les décisions militaires opérationnelles,
ou les arbitrages de justice. Des tâches qui requéraient l’intelligence
humaine sont maintenant dévolues à des algorithmes.

Cette efficacité, basée sur du traitement statistique massif et de
grande efficacité pour des tâches très spécifiques, finit par imposer ce
type de traitement comme la norme d’interaction entre un humain et un
dispositif numérique, surtout avec sa généralisation dans notre
environnement et sa présence dans tous nos objets numériques. Nous
allons questionner cela en explicitant les limites de ces approches et
en montrant des alternatives.

De plus, avec le terme d’intelligence artificielle2, cela laisse à
penser qu’il y a un certain degré de similarité avec notre intelligence
naturelle. Cela peut aller jusqu’à faire un amalgame (penser par exemple
que les machines vont devenir intelligentes au sens biologique du terme
ou même qu’elles traitent déjà l’information comme le font les humains,
voire que les humains devraient traiter l’information comme elles le
font) et nous voulons aider à bien distinguer ce qui relève de la
connaissance de ce qui relève de la croyance.

Pour cela, et pour aider à comprendre comment délibérer avec
l’intelligence artificielle mise au service de l’intelligence naturelle,
entre calculer et penser3, donc, nous allons expliquer en première
partie, avec l’apport des neurosciences et des sciences de la cognition,
que nous avons une vision assez élaborée de ce qui se passe dans notre
cerveau quand nous traitons de l’information et contrôlons notre
comportement. Ainsi, le cerveau n’est pas un organe à traiter
massivement des données de façon aveugle et systématique et fonctionne
bien différemment des algorithmes statistiques en question ici.

Dans une seconde partie, nous rentrerons dans les détails de la façon
dont nous codons les informations quand nous les faisons traiter par des
algorithmes et expliciterons le fonctionnement des différentes formes de
ce qu’on appelle intelligences artificielles (au pluriel, en évoquant
d’autres formes existantes de traitement de l’information, au-delà du
traitement statistique massif de type big-data).

Forts de ce double éclairage, qui va nous permettre de bien comprendre
la différence entre penser (au sens de l’intelligence humaine) et
calculer (sur des valeurs numériques ou symboliques, donc au sens de
l’informatique), nous pourrons dans la troisième partie discuter dans
quelle mesure, avec quels bénéfices et quelles limites, délibérer avec
l’intelligence artificielle au service de l’intelligence naturelle.

II. Penser : comment marche le cerveau ?

1 Ce qu’on appelle intelligence artificielle aujourd’hui est surtout le
traitement statistique ``big data´´ : c’est-à-dire de grands volumes de
données, de grandes variétés, approximatives, et ceci le plus souvent de
manière opaque (traitement par une “boîte noire”). 2 Le terme
“artificiel” signifie à la fois, (i) être le produit de l’activité et de
l’habileté humaine (opposé à naturel), mais aussi (ii) quelque chose de
“factice”, c’est à dire imité, donc, qui ne l’est pas vraiment . Il
serait plus correct de parler d’«intelligence simulée» qui correspond à
la définition reconnue
https://fr.wikipedia.org/wiki/Intelligence_artificielle. 3 On distingue
penser de calculer au sens expliqué ici :
https://interstices.info/calculer-penser

Au fur et à mesure des progrès des neurosciences et de leurs techniques
d’observation, on a pu affiner notre compréhension de ce qui se passe
dans notre cerveau, dans différentes circonstances de la vie. Mesurer et
décrypter les activités de notre cerveau nous renseigne sur ce qui se
passe dans notre vie mentale et nos comportements. Les premières études
ont porté sur la perception et sur l’action (Lettvin et al. 1968)4,
toutes deux reliées à des phénomènes extérieurs facilement objectivables
(Goodale and Humphrey 1998)5. Puis on a pu aussi considérer des
perceptions internes, dites intéroception (Craig 2003)6, correspondant
au traitement des sensations internes relatives au fonctionnement de
notre corps ou relatives à la perception du plaisir et de la douleur. On
peut aussi considérer des actions internes qui correspondent aux
commandes que nous envoyons vers nos organes internes ou à certaines
prises de décision dont les effets ne sont pas immédiatement visibles de
l’extérieur (nous précisons cela plus bas). Ces études ont pu permettre
de définir, dans notre cerveau, certaines structures comme étant
sensorielles (codant et recueillant des informations sensorielles),
motrices (codant et envoyant des commandes motrices) ou sensorimotrices
(codant et représentant des transformations d’informations sensorielles
vers des informations motrices) (M. M. Mesulam 1998)7.

Ensuite, au-delà de l’immédiateté, le cerveau est aussi le siège de
notre mémoire et une activité cérébrale peut refléter l’évocation de
souvenirs passés ou l’exploitation de contingences apprises. Depuis les
travaux fondateurs de E. Kandel (Kandel 2006)8 sur les mécanismes de la
mémoire et les études cliniques de M. Moscovitch et B. Milner sur la
dissociation de systèmes de mémoires (Squire and Zola 1996)9, de
nombreuses connaissances ont été accumulées sur l’architecture et la
dynamique mnésique. On fera en particulier la distinction entre deux
types de mémoires: la mémoire explicite que l’on peut déclarer (« je
sais que ») et la mémoire implicite que l’on sait réaliser (« je sais
faire ») (Squire 1992)10. Ainsi pour le premier type, l’hippocampe nous
permet de mémoriser très rapidement un épisode particulier, tel qu’on
l’a vécu (on parle de mémoire épisodique), alors que le cortex va
mémoriser des relations sémantiques, plus générales et abstraites, et
doit être soumis à une répétition fréquente de contingences pour
mémoriser lentement ces concepts. Pour le second type, deux structures
forment, avec le cortex, ce que l’on appelle la mémoire procédurale. Ce
sont les ganglions de la base, qui vont apprendre les comportements
motivés ou encore le cervelet pour le contrôle moteur.

Sur la base de ces structures sensorielles, motrices et mnésiques, il
est possible de rendre compte d’un certain nombre de comportements et de
leur implantation cérébrale. Les comportements peuvent être dirigés par
des buts établis par l’individu, mais ils peuvent être également dirigés
par des stimuli, en réaction à l’environnement. Nous abordons d’abord
cette situation.

4 Lettvin, J. Y., H. R. Maturana, W. S. McCulloch, and W. H. Pitts.
1968. “What the Frog’s Eye Tells the Frog’s Brain.” Edited by W. C.
Corning and M. Balaban. The Mind: Biological Approaches to Its
Functions, 233–58. 5 Goodale, M. A., and G. K. Humphrey. 1998. “The
Objects of Action and Perception.” Cognition 67 (1–2): 181–207 6 Craig,
A. D. 2003. “Interoception: The Sense of the Physiological Condition of
the Body.” Current Opinion in Neurobiology 13 (4): 500–505.
https://doi.org/10.1016/s0959-4388(03)00090-4. 7 Mesulam, M. M. 1998.
“From Sensation to Cognition.” Brain 121 (6): 1013–52.
https://doi.org/10.1093/brain/121.6.1013. 8 Kandel, Eric. 2006. A La
Recherche de La Mémoire. Odile Jacob. 9 Squire, L. R., and S. M. Zola.
1996. “Structure and Function of Declarative and Nondeclarative Memory
Systems.” Proceedings of the National Academy of Sciences of the United
States of America 93 (24): 13515–22.
https://doi.org/10.1073/pnas.93.24.13515. 10 Squire, L. R. 1992.
“Declarative and Nondeclarative Memory: Multiple Brain Systems
Supporting Learning and Memory.” Journal of Cognitive Neuroscience 4
(3): 232–43.

II.I Comportements dirigés par les stimuli

Dans ce premier cas, on étudie ce qui se produit dans notre cerveau,
suite à la perception d’un stimulus quelconque (un objet par exemple).
Cette perception peut nous inciter à faire une action dirigée vers cet
objet (on parle d’affordance, possibilité d’action suscitée par une
perception, cf. (Cisek 2007)11). Si cette action est réalisée, cela
induira une transformation locale du monde tel qu’on le perçoit (ou
éventuellement tel qu’on l’imagine à travers notre mémoire) et tel qu’on
le ressent (par intéroception). Par exemple, suite à l’action “saisir”,
l’objet auparavant sur la table est maintenant dans notre main droite et
cela nous procure du plaisir car cet objet est comestible. Cette
description nous conduit à préciser deux concepts importants, les
réponses et les renforcements.

Une action, ou plus généralement une réponse (interne ou externe) de
notre organisme, correspond en fait à une transformation locale du
monde, que nous traduisons par une transition de son état sensoriel
initial à son état résultant de la réponse. Il y a donc, pour
représenter nos comportements et leurs effets sur le monde, une dualité
entre réponses et transitions d’états. Cette dualité va être permise par
la dualité de nos mémoires. Si ce que nous représentons (et pouvons
déclencher) dans notre mémoire procédurale correspond à des réponses (en
particulier motrices), ce que nous représentons et apprenons dans notre
cortex en mémoire sémantique pour représenter nos comportements, est
avant tout sensoriel, c’est-à-dire représenté en termes de perceptions
externes ou internes (Bindra 1978)12. C’est en particulier le rôle du
cortex frontal (la partie antérieure de notre cortex) qui va représenter
nos comportements, des plus simples aux plus complexes. Nous savons
comment cela se fait : le but immédiat de l’action motrice est
représenté dans les cortex moteur et prémoteur (Graziano 2006)13, et la
décomposition spatiale et temporelle des tâches se fait plutôt dans le
cortex préfrontal latéral (Fuster 2001)14, tandis que leur motivation et
leur but final sont représentés dans le cortex préfrontal médial (Wise
2008)15.

Ceci nous conduit à parler maintenant de renforcement (récompense ou
punition) que l’organisme peut recevoir suite au comportement
sélectionné en réponse au stimulus. Cela génère des comportements
motivés, ce cadre permettant de décrire de nombreux mécanismes
d’apprentissage d’organismes vivants (Robbins and Everitt 1996)16. Nous
évoquons alors deux composantes essentielles de ces phénomènes, les
émotions et les motivations (Cardinal et al. 2002)17 et leur rôle majeur
dans notre système nerveux (Alexandre 2021)18 : survivre (maintenir la
structure de notre organisme, nous nourrir, nous reproduire, etc.). Pour
cela, il est vital de savoir non seulement détecter, mais aussi prédire
l’arrivée de stimuli biologiquement importants (d’où l’importance
d’avoir un système de

11 Cisek, Paul. 2007. “Cortical Mechanisms of Action Selection: The
Affordance Competition Hypothesis.” Philosophical Transactions of the
Royal Society of London. Series B, Biological Sciences 362 (1485):
1585–99. https://doi.org/10.1098/rstb.2007.2054. 12 Bindra, Dalbir.
1978. “How Adaptive Behavior Is Produced: A Perceptual-Motivational
Alternative to Response Reinforcements.” Behavioral and Brain Sciences 1
(1): 41–52. https://doi.org/10.1017/S0140525X00059380. 13 Graziano,
Michael. 2006. “The Organization of Behavioral Repertoire in Motor
Cortex.” Annual Review of Neuroscience 29 (March): 105–34.
https://doi.org/10.1146/annurev.neuro.29.051605.112924. 14 Fuster,
Joaquı́n M. 2001. “The Prefrontal Cortex—An Update : Time Is of the
Essence.” Neuron 30 (2): 319–33.
https://doi.org/10.1016/s0896-6273(01)00285-9. 15 Wise, Steven P. 2008.
“Forward Frontal Fields: Phylogeny and Fundamental Function.” Trends in
Neurosciences 31 (12): 599–608.
https://doi.org/10.1016/j.tins.2008.08.008. 16 Robbins, T. W., and B. J.
Everitt. 1996. “Neurobehavioural Mechanisms of Reward and Motivation.”
Current Opinion in Neurobiology 6 (2): 228–36. 17 Cardinal, Rudolf N.,
John A. Parkinson, Jeremy Hall, and Barry J. Everitt. 2002. “Emotion and
Motivation: The Role of the Amygdala, Ventral Striatum, and Prefrontal
Cortex.” Neuroscience & Biobehavioral Reviews 26 (3): 321–52.
https://doi.org/10.1016/s0149-7634(02)00007-6. 18 Alexandre, Frédéric..
2021. “A Global Framework for a Systemic View of Brain Modeling.” Brain
Informatics, February. https://doi.org/10.1186/s40708-021-00126-4.

signalisation plaisir/douleur spécifique) et de pouvoir exprimer des
émotions permettant d’anticiper et éventuellement prévenir ou
accompagner la survenue de ces stimuli particuliers, qui pourront
correspondre à des buts à atteindre ou à éviter car annonciateurs
d’effets de l’environnement que l’on nomme parfois punitions et
récompenses. Depuis I. Pavlov, ces circuits ont été bien décrits dans le
cerveau (Ledoux 2000)19.

Un autre mécanisme important pour aider à maintenir la structure de
notre organisme est celui de nos motivations (O’Reilly 2020)20 qui
permettent justement de définir et de se donner les moyens d’atteindre
des buts. Ces buts peuvent être externes (comme de la boisson) et on
parlera alors de motivation extrinsèque, déclenchée par un besoin (la
soif) qui donnera une importance plus forte à l’action de boire plutôt
que d’autres actions qui auraient pu être suscitées par d’autres
stimuli. Ces buts peuvent être internes (comme l’acquisition d’une
compétence ou d’une connaissance) et on parlera de motivation
intrinsèque, pouvant correspondre à la curiosité ou à la pratique
assidue de certaines activités.

Ce cadre expliquant les comportements motivés permet d’expliquer
certains comportements dirigés par les stimuli perçus. Parmi l’ensemble
des réponses possibles à ces stimuli, celles qui, dans le passé, ont
fourni le plus de renforcement sont privilégiées, ce que Thorndike a
appelé la loi de l’effet. Il y a aussi la possibilité de comportements
réflexes, automatiques, sélectionnés par l’évolution parce que
bénéfiques ou automatisés par une pratique intensive (on parlera de
comportements habituels, cf. (Boraud, Leblois, and Rougier 2018)21). Il
y a enfin la nature et l’intensité (on parlera de saillance,
c’est-à-dire du fait que cela attire l’attention) des stimuli eux-mêmes
qui peuvent expliquer qu’on va y répondre en priorité. En résumé, notre
attention sera attirée vers les stimuli qui se détachent des autres
naturellement ou parce qu’ils appellent des comportements habituels, ou
associés par apprentissage à des motivations ou des émotions.

II.II Comportements dirigés par les buts

Nous disposons aussi d’une approche téléologique de l’organisation de
nos comportements, à savoir dirigés par des buts. Dans ce cas, un but
est considéré en premier, fourni par une motivation ou par une consigne
émanant de l’environnement. La sélection du comportement se base sur le
choix d’une action parce qu’elle permet de réaliser ce but. Si aucune
n’est directement accessible, un raisonnement à rebours permet de
définir une série d’actions dont la première peut opérer depuis l’état
présent, et passant par des sous-buts intermédiaires contribuant au but
final . En contraste avec le cas précédent où la réponse est élaborée
simplement en réaction à l’environnement, il est important ici de
disposer d’un modèle interne du monde (qu’on pourrait se représenter
comme un simulateur mental: on est capable de s’imaginer faire une
action et d’en voir les conséquences), nous permettant d’anticiper les
conséquences de nos actions. Dans ce cas, le choix de l’action n’est
plus basé sur une analyse rétrospective de son efficacité passée (selon
la loi de l’effet) mais sur une analyse prospective (selon un modèle
interne du monde) de ce qu’une action pourrait nous apporter si on la
déclenchait (Dolan and Dayan 2013)22.

Si on peut supposer que dans ce cas, les mécanismes seront plus
complexes que pour un comportement réactif simple, il est important de
mentionner que ces mécanismes mentaux internes, en lien avec ce qu’on
appelle imagination, utilisent les mêmes circuits cérébraux

19 Ledoux, Joseph E. 2000. “Emotion Circuits in the Brain.” Annual
Review of Neuroscience 23 (1): 155–84.
https://doi.org/10.1146/annurev.neuro.23.1.155. 20 O’Reilly, Randall C.
2020. “Unraveling the Mysteries of Motivation.” Trends in Cognitive
Sciences 24 (6): 425–34. https://doi.org/10.1016/j.tics.2020.03.001. 21
Boraud, Thomas, Arthur Leblois, and Nicolas P. Rougier. 2018. “A Natural
History of Skills.” Progress in Neurobiology 171 (December): 114–24.
https://doi.org/10.1016/j.pneurobio.2018.08.003. 22 Dolan, Ray J., and
Peter Dayan. 2013. “Goals and Habits in the Brain.” Neuron 80 (2):
312–25. https://doi.org/10.1016/j.neuron.2013.09.007.

(Schacter, Addis, and Buckner 2007)23. En fait, l’hippocampe est
capable non seulement de mémoriser des épisodes vécus, séquences de
perceptions et de réponses émises pour les contrôler en vue d’obtenir un
renforcement, mais aussi de rappeler ces épisodes et les rejouer, c’est
à dire réactiver les régions du cerveau qui avaient répondu lors de cet
épisode, voire de générer des épisodes inédits combinant des épisodes
existants (Pezzulo et al. 2014)24. En fonction de la situation, on
pourra donc rappeler un épisode pertinent ou même plusieurs morceaux
d’épisodes différents et on pourra ainsi s’imaginer faire quelque chose
que l’on n’a encore jamais fait. Ce mécanisme de contrôle de
l’imagination s’appelle la pensée (Pezzulo and Castelfranchi 2009)25.

Dans les deux cas (la pensée ou l’action directe sur le monde), c’est le
cortex frontal qui assure le travail de contrôle (Fuster 2001)26, c’est
à dire de sélection des indices importants à traiter (par l’attention
sélective) et d’organisation dans le temps du comportement (par
l’inhibition de comportements non adaptés et l’activation de règles de
comportement adaptées à la situation, selon la gamme des tâches qu’il
représente). Quand ceux-ci sont mis en œuvre par la pensée, ceci permet
de délibérer, c’est-à-dire d’évoquer plusieurs possibilités et
d’imaginer leurs conséquences potentielles, avant de décider. Cette
capacité est particulièrement importante chez les humains avec le
développement de notre cortex frontopolaire, particulièrement marqué
dans notre espèce (Koechlin et al. 1999)27.

Pour répondre à notre question initiale (que se passe-t-il dans notre
cerveau quand nous pensons et agissons ?), nous avons évoqué ici les
différents types de comportement que nous pouvons réaliser (Balleine and
Dickinson 1998)28: actions réflexes, réponses émotionnelles,
comportements dirigés par un stimulus ou par un but. Dans notre vie
courante, ils vont s’articuler et parfois entrer en compétition pour
définir concrètement notre comportement (Cisek 2012)29. Selon la
compréhension actuelle de ces mécanismes, il semble que nous sommes en
général mûs par un but, que nous cherchons à résoudre (O’Reilly et
al. 2014)30. Pour ce faire, nous essayons d’abord d’utiliser des
stratégies, qui se présentent comme des règles qui appliquent une action
si des conditions sont remplies afin d’obtenir les résultats attendus.
Au-delà de ces règles comportementales éprouvées, nous essayons de nous
adapter et même de faire preuve de créativité si nous détectons que
l’environnement a changé et que nos stratégies ne sont plus adaptées
(Duverne and Koechlin 2017)31.

23 Schacter, Daniel L., Donna Rose Addis, and Randy L. Buckner. 2007.
“Remembering the Past to Imagine the Future: The Prospective Brain.”
Nature Reviews Neuroscience 8 (9): 657–61.
https://doi.org/10.1038/nrn2213. 24 Pezzulo, Giovanni, Matthijs A. A.
Van der Meer, Carien S. Lansink, and Cyriel M. A. Pennartz. 2014.
“Internally Generated Sequences in Learning and Executing Goal-Directed
Behavior.” Trends in Cognitive Sciences 18 (12): 647–57.
https://doi.org/10.1016/j.tics.2014.06.011. 25 Pezzulo, Giovanni, and
Cristiano Castelfranchi. 2009. “Thinking as the Control of Imagination:
A Conceptual Framework for Goal-Directed Systems.” Psychological
Research PRPF 73 (4): 559–77. https://doi.org/10.1007/s00426-009-0237-z.
26 Fuster, Joaquı́n M. 2001. “The Prefrontal Cortex—An Update : Time Is
of the Essence.” Neuron 30 (2): 319–33.
https://doi.org/10.1016/s0896-6273(01)00285-9. 27 Koechlin, E., G.
Basso, P. Pietrini, S. Panzer, and J. Grafman. 1999. “The Role of the
Anterior Prefrontal Cortex in Human Cognition.” Nature 399 (6732):
148–51. https://doi.org/10.1038/20178. 28 Balleine, Bernard W, and
Anthony Dickinson. 1998. “Goal-Directed Instrumental Action: Contingency
and Incentive Learning and Their Cortical Substrates.” Neuropharmacology
37 (4): 407–19. https://doi.org/10.1016/S0028-3908(98)00033-1. 29 Cisek,
Paul. 2012. “Making Decisions through a Distributed Consensus.” Current
Opinion in Neurobiology 22 (6): 927–36.
https://doi.org/10.1016/j.conb.2012.05.007. 30 O’Reilly, Randall C.,
Thomas E. Hazy, Jessica Mollick, Prescott Mackie, and Seth Herd. 2014.
“Goal-Driven Cognition in the Brain: A Computational Framework.”
ArXiv:1404.7591, 2014. http://arxiv.org/abs/1404.7591. 31 Duverne,
Sandrine, and Etienne Koechlin. 2017. “Hierarchical Control of Behaviour
in Human Prefrontal Cortex.” In The Wiley Handbook of Cognitive Control,
207–20. John Wiley & Sons, Ltd.
https://doi.org/10.1002/9781118920497.ch12.

La notion de créativité32 (Guilford 1962)33 a une définition assez
précise au niveau de la cognition, c’est la capacité de produire un
travail à la fois nouveau (c’est-à-dire original, inattendu) et
approprié (c’est-à-dire utile, adapté aux contraintes de la tâche). Cela
s’exprime dans un double processus de pensée divergente (chercher une
nouvelle idée) puis convergente (vérifier qu’elle convient), comme
analysé par exemple par (Dietrich 2004)34. Ces processus sont modélisés
en cognition informatique (Alexandre 2020b)35.

Ces liens entre activité cérébrale et comportement sont en particulier
explorés et confirmés grâce à des expérimentations en imagerie cérébrale
qui permettent d’observer l’activation de réseaux à large échelle dans
notre cerveau, que l’on peut faire correspondre aux grands réseaux
neurocognitifs élaborés à partir des théories de divers domaines des
sciences cognitives (M. Mesulam 2008)36. C’est ainsi que l’on retrouve,
pour les comportements dirigés par les stimuli, des réseaux relatifs au
langage, à la cognition spatiale ou encore à la saillance (qui attire
l’attention, donc). On retrouve par ailleurs des réseaux réalisant des
comportements dirigés par le but, comme les réseaux dédiés à l’attention
et au contrôle cognitif. Toujours dans ce cadre, on observe aussi le
réseau en mode défaut, c’est-à-dire celui qui est activé par défaut,
quand la consigne est de ne rien faire et de ne penser à rien. Dans ce
cas, on voit un accès à nos sensations internes et à notre mémoire
épisodique, pour évoquer des ressentis qui sont en particulier utilisés
dans des phases de créativité (Dietrich 2004)37.

III. Calculer : comment coder et traiter mécaniquement l’information ?

Regardons maintenant38 ce que nous pouvons faire faire à une machine, à
savoir après avoir codé l’information, faire traiter celle-ci par des
algorithmes. Expliquer à chacune et

32 C’est la capacité d’un individu à imaginer ou construire et mettre en
œuvre un concept neuf, un objet nouveau ou à découvrir une solution
originale à un problème. Cette originalité se définit dans un contexte
donné. La démarche commence par la reconnaissance d’un problème. C’est à
partir de là qu’un processus de divergence s’engage. Ce dernier
correspond à une exploration à partir d’éléments connus (la créativité
ex-nihilo n’a jamais été observée) de combinaisons ou de déformations
inédites. Elle se termine, par convergence, dans une nouvelle solution
du problème, par un mécanisme de vérification et d’évaluation de la
nouvelle proposition. 33 Guilford, Joy Paul. 1962. “Creativity: Its
Measurement and Development.” A Source Book for Creative Thinking,
151–67. 34 Dietrich, Arne. 2004. “The Cognitive Neuroscience of
Creativity.” Psychonomic Bulletin & Review 11 (6): 1011–26.
https://doi.org/10.3758/BF03196731. 35 Alexandre, Frédéric. 2020b.
“Creativity Explained by Computational Cognitive Neuroscience.” In .
https://hal.inria.fr/hal-02891491. 36 Mesulam, M. 2008. “Representation,
Inference, and Transcendent Encoding in Neurocognitive Networks of the
Human Brain.” Annals of Neurology 64 (5): 367–78. 37 Dietrich, Arne.
2004. “The Cognitive Neuroscience of Creativity.” Psychonomic Bulletin &
Review 11 (6): 1011–26. https://doi.org/10.3758/BF03196731. 38 Nous
proposons ici une vision synthétique et invitons la lectrice ou le
lecteur à approfondir ces notions en profitant de formations numériques
attestées et librement utilisables: - https://classcode.fr/iai qui
introduit ces notions d’intelligence artificielle symbolique et
numérique de manière très concrète mais sans aucun prérequis technique;
- https://classcode.fr/snt qui fournit une formation citoyenne de base
en informatique qui correspond à ce que les élèves du secondaire
apprennent désormais pour se former sur ces sujets; et pour aller plus
loin: - https://www.elementsofai.fr permet de se former aux éléments
plus techniques de l’intelligence artificielle; - https://tinyl.io/3kpr
permet de se former à l’intelligence artificielle symbolique utilisée,
entre autres, au niveau du web sémantique. Cette section est une
synthèse de ces éléments en vue de répondre à la question posée ici.

chacun ces grandes idées de l’informatique va nous permettre de
comprendre le fonctionnement de ce qu’on appelle intelligence
artificielle.

III.I Comment passer de nos connaissances à des données ?

Nous parlons usuellement de connaissances39 lorsque notre esprit
assimile un contenu objectif. Ce contenu est préalablement traduit en
signes et en idées. C’est donc une possession symbolique des choses, qui
permet l’émergence du sens. Mais comment coder cela dans une machine ?
Autrement dit comment réduire ces connaissances à des données ?
Regardons cela.

Nommer les choses : rien n’existe si il n’est pas nommé. C’est bien le
cas en informatique, à tous les niveaux, et tout particulièrement avec
l’omniprésence d’internet. Chaque “ressource” (on nomme ainsi tous les
objets immatériels qui sont considérés) a un identifiant unique, ces
fameuses constructions telles que :

https://fr.wikipedia.org/wiki/Internationalized_Resource_Identifier

On parle d’« identificateurs de ressources internationalisés » (IRI),
d’aucun parle -improprement- « d’adresse internet ». Avoir un nommage
unique de chaque ressource permet de la distinguer des autres, donc de
pouvoir s’y référer de manière bien définie quand on la manipule. De
plus, quand cela est pertinent, cet identificateur correspond à son
emplacement sur Internet, ce qui fournit les éléments pour y accéder. On
parle alors de « localisateur uniforme de ressource » (URL en anglais).
Cet identificateur a aussi parfois des paramètres40, qui permet de
piloter son fonctionnement ou interagir avec elle : par exemple
l’adresse internet de votre chauffage connecté permet de régler à
distance la température de la maison.

Ces IRI sont un peu plus que des “noms”. Ce sont des locutions d’un
petit langage qui explicite le processus pour interagir avec la
ressource, on parle de protocole, et toute autre information utile pour
y accéder (la machine où elle se trouve et le chemin sur cette machine)
ou la contrôler (par exemple les paramètres pour envoyer un message).

Cette entreprise humaine collective est immense :

-   elle est universelle au sens où tous les alphabets du monde sont
    utilisables pour

former ces identificateurs;

-   tous les pays du monde, même en guerre les uns contre les autres,
    adoptent ces mêmes standards pour encoder, les lettres, puis les
    mots, puis les locutions. Imaginez un groupe qui imaginerait
    réinventer ses propres standards : coupé du reste du monde, il
    n’existerait tout simplement plus.

Nommer ces ressources est indispensable quand elles sont accessibles par
internet, mais c’est aussi le cas pour des informations traitées
localement au sein d’un logiciel. En effet, les entrées et sorties de
ces traitements ne feraient pas de sens si elles n’étaient pas
correctement nommées.

39
https://fr.wikipedia.org/wiki/Connaissance#Définition_de_la_connaissance
40 Essayez vous-mêmes ! Par exemple entrez :

https://fr.wikipedia.org/wiki/Internationalized_Resource_Identifier?action=raw

dans un navigateur avec le paramètre ``action=raw´´ en plus de l’adresse
de la page, vous allez voir un affichage étonnant : vous venez de donner
l’ordre au site web de fournir le contenu de la page, mais sous la forme
qu’utilisent les personnes qui écrivent dans wikipédia pour l’éditer, en
utilisant une syntaxe bien définie
https://www.mediawiki.org/wiki/Help:Formatting.

Interagir avec les objets numériques : cette démarche concerne aussi
les objets connectés qui nous entourent (éléments robotiques d’une
voiture, pacemaker de notre voisin, robot cuisinier qui nous guide pour
faire de bonnes recettes), par exemple, cet identificateur :

http://192.168.1.137/?action=start&temperature=20&time=17:00

pourrait, disons, lancer mon chauffage domestique à partir de mon
smartphone. Ces objets deviennent accessibles et utilisables, car ils
sont nommés. Même si nous n’utilisons pas directement ces
identificateurs, lorsque nous cliquons sur des “menus” d’applications,
en fait, ce sont de tels codes qui sont générés.

Voilà une première forme d’intelligence artificielle qui nous entoure et
qui est déjà là : notre quotidien se remplit d’objets connectés auxquels
nous déléguons plein de petites tâches cognitives et sensori-motrices
élémentaires spécifiques et parfois rudimentaires (faire garer sa
voiture toute seule, soulager sa mémoire d’un rappel de rendez-vous,
transformer notre parole en texte, traduire d’une langue humaine à une
autre) et c’est cela que l’on appelle aujourd’hui couramment
intelligence artificielle. Nous utilisons cela sur nos smartphones, bien
entendu, mais cela existera aussi de plus en plus au sein de beaucoup de
biens manufacturés.

De l’information aux nombres : Une monumentale entreprise de
numérisation s’est appliquée à toutes les informations humaines, tout
est numérique désormais. Pour cela, nous avons standardisé la façon de
coder toutes les lettres des alphabets, attribuant à chaque lettre un
code numérique, ce nombre étant lui-même représenté en “binaire”
c’est-à-dire par une suite de ‘0’ et de ‘1’. Nous avons fait de même
avec les images découpées en points (dits “pixels”) assez petits pour
donner à l’œil humain une impression de continuité, la couleur de chaque
point étant codée par quelques nombres. Il en va de même des sons,
l’intensité sonore étant découpée en une séquence de valeurs numériques,
donc une myriade de ‘0’ et de ‘1’. Et en combinant ces éléments, des
vidéos aux textes illustrés, voilà toutes nos bibliothèques et
médiathèques devenues numériques.

Pourquoi en binaire ? La raison profonde est que si un paramètre peut
prendre deux valeurs possibles (‘0’ ou ‘1’ ou bien ‘oui’ ou ‘non’, peu
importe) alors cela crée un atome d’information. S’il n’y avait qu’une
seule valeur, on ne pourrait rien dire. Ensuite, en combinant plusieurs
éléments binaires (bits) on peut coder n’importe quelle liste
d’éléments, par exemple, avec deux bits on codera les trois couleurs
primaires:

rouge -> ‘00’, vert -> ‘01’, bleu -> ‘10’ et avec plus de bits toute une
palette de couleurs, et au-delà toute information. Pour s’en convaincre
il suffit de jouer au “portrait” (appelé aussi “qui-est-ce”), à deviner
tout ce qui est possible au fil de simples réponses par oui ou non. Une
autre belle propriété du codage binaire est que si vous encodez les
nombres en binaire:

0 -> ‘0000’, 1 -> ‘0001’, 2 -> ‘0010’, 3-> ‘0011’, … alors tous les
calculs numériques usuellement faits en numération décimale fonctionnent
exactement pareil en binaire, avec des règles de calcul tellement plus
simples qu’il est extrêmement efficace de les câbler électriquement,
même s’il faut environ quatre bits pour coder un chiffre.

Le fait que toutes ces informations se codent en binaire a plusieurs
conséquences: + toutes les opérations générales qui ne dépendent pas du
contenu, comme mémoriser, transmettre, compresser, chiffrer (pour les
rendre inaccessibles sans une clé), dupliquer ces données, se font avec
des algorithmes universels, et à très faible coût; + le fait que
dupliquer ces données est une opération à coût quasi nul, bouleverse
l’économie de ces biens informationnels: on achète plus un livre, par
exemple, mais un droit à lire un texte.

Information symbolique et numérique : Il y a deux grandes façons de
coder une information, par exemple un dessin ou bien un air de musique:

-   en approximant numériquement l’image du dessin (avec des pixels) ou
    bien l’enregistrement sonore de cet air de musique (les intensités
    sonores successives);

-   en définissant symboliquement ce dessin, par exemple en spécifiant
    les formes à

dessiner, ou cet air de musique en fournissant la partition de musique à
jouer.

Regardons l’exemple d’un cercle ci-après, qui est codé sur une grille de
pixels, en noircissant chaque pixel qui rencontre le cercle : on observe
que le rendu est peu ressemblant, il faudrait une grille de pixels bien
plus fine, pour que l’approximation numérique soit plus fidèle. On
pourrait aussi coder ce cercle en donnant la position de son centre et
de son rayon, ce qui le définit parfaitement et permettrait de le
tracer. On dit alors que le cercle est défini “symboliquement” par les
éléments symboliques en question.

Dans le premier cas, numérique, l’information est forcément
approximative mais nous pouvons encoder des formes complexes de la même
manière que des formes simples, dans le second cas symbolique, on offre
une description sans approximation, mais limitée. Par exemple, pour
l’air de musique, ce sera sans les nuances de l’interprète.

Information humaine et informatique : L’information, avec sa
numérisation, est devenue une quantité physique qui se mesure,

-   
-   

elle a un “poids” c’est le nombre de bits pour la coder de manière non
redondante (en cherchant à la compresser au maximum de manière
réversible); lorsque nous générons cette information avec un programme
(par exemple un calcul) on définit aussi sa “complexité” c’est-à-dire la
taille du programme dont on a besoin à minima pour la générer et sa
“profondeur” c’est-à-dire le temps de calcul minimal que mettra ce
programme pour la produire.

Ce sont des notions complètement inédites qui font basculer la notion
d’information telle que nous l’entendons dans la vie courante vers une
notion formalisée.

Quand on utilise des techniques de codage de l’information comme par
exemple le codage binaire, cela ne signifie en rien que l’on en capture
le sens. Par exemple, tous les idéogrammes chinois sont bien codés en
binaire au niveau lexical, cela ne signifie pas que toute la subtilité
de la langue a été formalisée. Des outils de représentation de
connaissances comme les ontologies informatiques permettent de
formaliser partiellement le sens de concepts; les résultats sont parfois
très performants, mais c’est aussi en regardant les limites de ces
spécifications que notre intelligence humaine peut aller plus loin dans
l’analyse du sens des choses étudiées.

Un cran plus loin, nous sommes amenés à distinguer langue (humaine) et
langage (formel) au sens de Dowek (Dowek 2019)41. Les deux utilisent des
symboles, mais une langue formelle a volontairement un vocabulaire
contrôlé limité et une grammaire minimale, et

41 Dowek, Gilles. 2019. Langues et langages : Ce dont on ne peut parler,
il faut l’écrire. Le Pommier.
https://www.editions-lepommier.fr/ce-dont-ne-peut-parler-il-faut-lecrire.

s’applique à un champ sémantique précis, avec un objectif opérationnel
précis, différent de ceux d’une communication humaine. Il y a des
langues formelles non informatiques : une partition de musique, par
exemple, s’écrit dans un langage musical formel, pour faire fonctionner
un instrument de musique, tandis que l’interprétation musicale va
dépasser ce qui est codé dans la partition, pour devenir l’expression
d’une langue musicale.

Le langage juridique cherche à être le plus formel possible, et se
heurte ainsi à deux grandes limites : (i) tant qu’il sera écrit en
langue humaine, il pâtira des ambiguïtés de cette langue (à l’inverse
d’une poésie qui en joue et parfois s’en joue); (ii) dans la mesure où
il s’applique à des actions humaines, son objet est impossible à
appréhender de manière exhaustive et hors de portée d’une spécification
complète. Le but du droit est justement de prendre en compte la
singularité même du cas instruit, au-delà d’appliquer une règle.

À l’inverse, “informatiser” est un processus qui vise à réduire une
information à des données. Sur wikipédia, par exemple, il y a un texte
qui va décrire en langue humaine une ressource, mais il y aussi wikidata
qui extrait de ce texte toutes les informations, on parle parfois de
métadonnées, qui peuvent se réduire à une liste de faits. Une personne,
par exemple, pourrait -selon les besoins- être réduite aux éléments de
son état civil (nom, adresse, profession), aux compétences de son
curriculum-vitae et la liste de ses activités socioprofessionnelles, à
ses antécédents judiciaires, à des traits de caractère prédéfinis, aux
éléments du dossier médical, aux relations formelles avec son entourage
(fille-de, proche-de, en-procès-avec, …). Juger (dans tous les sens, on
en discutera à la section IV.II. du terme) une action sur ces éléments a
quelque chose d’équitable et de rigoureux, mais peut sembler
terriblement inhumain : on propose en fin de chapitre des lectures pour
approfondir cet aspect.

C’est donc en premier lieu par la façon de coder les données, les choix
de représentation que nous faisons, ce que nous prenons en compte et ce
que nous négligeons, que nous transformons, en les rendant
artificielles, les connaissances de notre intelligence naturelle.

III.II Que peut-on demander à une machine de calculer ?

Nous voilà avec des données factuelles, que pouvons nous en faire ? Pour
cela il faut maintenant représenter et spécifier le traitement de ces
données. Nous allons regarder maintenant comment traiter les
informations quand elles se présentent sous forme de symboles, par
exemple les trier, tirer des conséquences des faits, etc…

Coder le l’information symbolique pour la traiter : prenons ce fait «
Muhammad est-le-fils-de Mūsā

ﻰﺳوﻣ

نﺑ

دﻣﺣﻣ

» :

Nous énonçons une connaissance minimale de la forme “sujet prédicat
objet”, et il est effectivement important de structurer et de
modulariser au maximum nos informations, plutôt que de les livrer
amalgamées à la machine. De ce fait, on en déduit plusieurs choses : si
il est le fils de Mūsā et non la fille, alors c’est un homme, donc un
humain, donc un animal, puisque tous les humains le sont. Et si on sait
par ailleurs que Mūsā est le fils de Aïcha alors on en déduit par
transitivité que Muhammad est le petit-fils de Aïcha, etc. Cela se code
par des règles de la forme:

?x est-le-fils-de ?y ⟹?x est-du-genre masculin ET ?x est humain ?x
est-le-fils-de ?y ET ?y est-le-fils-de ?z ⟹?x est-le-petit-fils-de ?z

Ce qui est important pour nous ici est que nous pouvons coder des règles
et que l’application de ces règles permet de déduire d’autres faits.
Cela permet de trouver les conséquences d’une hypothèse, d’expliciter
des informations implicites qui découlent des données en entrée, de
vérifier si un ensemble de faits ne génère pas une contradiction,
d’interroger une base de données sur ce qui a été explicitement défini
et tout ce qui en découle.

Cela signifie que nous pouvons mécaniser dans une certaine mesure des
raisonnements logiques, qui se réduisent à des calculs sur des symboles.
On peut alors mener des raisonnements qui dépassent de beaucoup la
capacité cognitive humaine. Il y a de multiples langages et de multiples
outils en informatique pour cela. On démontre même des résultats
mathématiques avec des systèmes de déduction automatiques, ce qui permet
de vérifier qu’il n’y a pas eu d’erreur humaine dans les déductions (il
peut y en avoir au niveau des règles et des faits qui ont été définis).
Les plus modernes et efficaces qui nous concernent sont ceux du Web
sémantique : on parle d’ontologie informatique pour nommer cette
formalisation des connaissances.

Mieux encore, on sait aussi comment mécaniser des calculs “incertains”,
par exemple si on pense qu’“il est relativement possible” que Muhammad
soit le fils de Mūsā, alors il est relativement possible aussi que ce
soit un homme. On quantifie très souvent cette certitude relative par
des probabilités, mais il est plus réaliste de formaliser cela par des
notions plus proches de la cognition humaine, en utilisant les notions
de nécessité et de possibilité.

On connaît aussi les limites intrinsèques à ces mécanismes : certaines
formulations peuvent donner lieu à des calculs qui ne finissent jamais
ou dont le temps de calcul est prohibitif (facilement plus que l’âge de
l’univers, disons); on tombe aussi sur des formules qui ne peuvent être
ni démontrées, ni infirmées : elles sont indécidables. Plus important
encore est le fait qu’il est démontré que pour obtenir des résultats
très sophistiqués (de grande profondeur logique, pour utiliser le terme
exact), il faudra un temps de calcul très long.

Paramétrer le traitement numérique de l’information : la méthodologie
précédente marche bien pour des informations qui se formalisent
aisément. Mais si on doit, par exemple, distinguer un chat d’un chien
dans une image, alors il sera bien difficile de “formaliser” cela avec
des règles symboliques en tenant compte de toutes les variantes
possibles. Il y a alors une toute autre famille de solutions, basées sur
ce que l’on nomme réseaux de neurones artificiels.

Source: Zeiler, Matthew D., and Rob Fergus. Visualizing and
understanding convolutional networks. Computer Vision ECCV 2014.
Springer International Publishing, 2014. 818 833 Prenons une image en
entrée, comme illustré ci-dessus42. Accumulons des petits calculs entre
les pixels voisins, par exemple des moyennes ou des différences, pour ne
considérer que les valeurs les plus importantes du résultat obtenu.
Puis, à la sortie de cette couche de calculs, considérons d’autres
calculs qui combinent les premiers de la même manière, et ainsi de
suite, jusqu’à un calcul dont la sortie sera une valeur entre 0 et 1
pour dire si c’est plutôt un “chat” qu’un “chien”. On parle de “neurone”
pour désigner le petit calcul élémentaire, et de réseau “profond” de
neurone pour expliciter le fait que l’on accumule une pile profonde de
couches de calcul.

42 Reproduit avec autorisation de Zeiler, Matthew D., and Rob Fergus.
“Visualizing and understanding convolutional networks.” Computer
Vision–ECCV 2014. Springer International Publishing, 2014. 818–833

Un neurone combine donc plusieurs entrées dans des proportions
ajustables, ces proportions définissent les paramètres du calcul.

Une telle architecture jouit d’une propriété remarquable : c’est une
transformation qui peut réaliser de manière approximative grâce aux
statistiques, une très grande famille de transformations. Pour cela on
ajuste les paramètres de chaque neurone, à partir d’exemples : on
fournit des images de chiens et de chats, et chaque erreur d’estimation
donne lieu à une correction progressive des paramètres. Après quelques
millions (plutôt que milliers) d’exemples on observe que cet
apprentissage statistique fonctionne : il y a peu d’erreurs parmi les
animaux utilisés pour l’apprentissage, mais aussi parmi des animaux
jamais utilisés mais qui sont statistiquement cohérents avec les
exemples fournis. On dispose de résultats empiriques qui permettent
d’estimer la robustesse d’un tel calcul.

Le résultat n’est pas “miraculeux” : nous avons toutes et tous
l’habitude de la “déraisonnable efficacité” des statistiques, quand -par
exemple- on prédit le résultat d’une élection à partir d’un sondage sur
un petit échantillon d’une population. C’est une situation similaire
ici, on a en quelque sorte fait un sondage sur un échantillon de chiens
et de chats et la prédiction s’applique alors à toute une population.

Ce que ces résultats montrent par exemple en matière de reconnaissance
d’images, c’est que le problème n’est pas si compliqué que cela.
Prenons, par exemple, la transcription de la parole en mots: les sons de
la parole humaine, par exemple, se décomposent en une cinquantaine de
sons élémentaires (on parle de phonèmes, qui correspondent plus ou moins
aux sons des voyelles ou des consonnes) et se combinent pour former
environ 1000 à 10000 mots selon le niveau de langage. Ce sont des
quantités assez raisonnables au regard des millions de calculs que peut
faire une machine; il n’est donc pas surprenant, à condition d’utiliser
assez de données, de pouvoir transformer la parole en texte, bien
entendu sans rien en comprendre, ni faire la moindre analyse
linguistique du contenu

Ces systèmes sont programmés “par les données”. Personne n’explicite les
traitements à faire. On se contente de définir une architecture en
choisissant les nombre de couches, le nombre et le type de calculs
élémentaires par couche, puis de lancer l’ajustement des paramètres en
fournissant avec des méthodologies assez précises les données en très
grand nombre. En ce sens, le système est un mécanisme d’apprentissage
automatique.

Ce sont ces algorithmes qui permettent, à ce qu’on appelle le big data,
d’offrir aujourd’hui des résultats spectaculaires en matière de
reconnaissance visuelle ou sonore, et de prédiction de toutes sortes, y
compris sur des données peu modélisables. Ainsi commence-t-on à prédire
des décisions de justice, comme discuté dans la troisième partie.

Bien entendu un tel système de transcription ne comprend rien, le texte
transcrit ne fait aucun sens pour lui, c’est juste un calcul. De plus,
ces calculs sont des “boîtes noires” : il est assez hasardeux en
regardant l’intérieur des calculs de pouvoir expliciter grâce à quelles
caractéristiques on différencie, par exemple, un chien d’un chat. Par
ailleurs, ils sont d’autant plus efficaces que leur tâche est très
spécifique : distinguer un chien d’un chat sera bien plus efficace qu’un
chien de tous les animaux du zoo, et si le système a appris à distinguer
un chat d’un chien, le travail d’apprentissage est à refaire si on
change d’animal. Pire encore, on sait créer des images “antagonistes”
c’est-à-dire prendre une image de chat et identifier les quelques pixels
qu’il faut modifier, pour le faire prendre pour un chien. Et de façon
étonnante, ces pixels peuvent être très peu nombreux et les humains ne
pas se laisser flouer par ces changements qui ne modifient que la
distribution statistique des pixels et pas leur organisation
structurelle.

Ce sont des algorithmes “universels” déraisonnablement efficaces, mais
difficiles à interpréter, très coûteux en données, spécifiques d’une
tâche cognitive donnée, et très fragiles en matière de performance.

Quelques autres splendides algorithmes : on vient de voir comment faire
faire à la machine des calculs symboliques ou numériques qui auraient
été qualifiés d’intelligents s’ilsavaient été fait par une personne
humaine. Dans ces deux cas, on ne programme pas explicitement des
instructions à faire exécuter par la machine, mais on fournit des
connaissances symboliques ou des données numériques qui vont permettre à
l’algorithme de résoudre le problème posé. Il y a en fait plusieurs
autres façons de produire des calculs sophistiqués. On peut par exemple
programmer “par contraintes” : fournir les données du problèmes, les
contraintes à respecter et le but à obtenir, par exemple, pour planifier
une suite d’actions ou résoudre un problème d’ordonnancement (exemple:
gestion de stocks ou de transport d’objets).

Bien entendu, on peut aussi programmer de manière “impérative43”
c’est-à-dire expliciter manuellement les instructions du calcul. Tous
les langages et outils de programmation usuels, par exemple ceux qui
permettent de s’initier à l’informatique comme Scratch44, disposent de
cette base impérative. Elle permet de programmer des algorithmes parfois
utilisés à de très grandes échelles, par exemples les algorithmes de tri
(manipuler un ensemble de données triées est souvent bien plus efficace
qu’en vrac) ou de calcul d’un plus court chemin (qui peut servir à la
fois à un itinéraire routier ou à trouver comment gagner à un jeu en
trouvant “le chemin” vers une situation dominante, etc…). Dans tous les
cas, pour réaliser l’interface entre tous ces mécanismes, il faut
impérativement savoir programmer de manière impérative pour faire le
lien entre tous ces outils.

IV. Délibérer: comment mettre l’intelligence artificielle à notre
    service ?

IV.I De la différence entre penser et calculer

Dans la première partie consacrée à l’intelligence naturelle, nous avons
d’abord parlé de la nécessité, pour un être vivant, d’interagir avec son
environnement à travers ses perceptions, ses actions et les traces
mnésiques qu’il en conserve. C’est avec ces éléments qu’il peut élaborer
des comportements qui peuvent permettre de répondre aux stimulations de
l’environnement aussi bien que d’obéir à ses buts internes,
principalement relatifs à sa survie. Ce que montrent ensuite les
neurosciences et en particulier au cours de l’évolution, c’est la montée
en gamme des stratégies utilisées pour réaliser des traitements de plus
en plus complexes. Le fait qu’un cerveau ait “plusieurs millions
d’années” d’expériences accumulées45 est probablement la cause première
de ces performances, on montre en effet que pour produire des résultats
sophistiqués, un calcul ou un processus quel qu’il soit doit réaliser un
nombre d’étapes vertigineusement grand. De ce fait, on a pu associer ces
traitements à des phénomènes comme la pensée, la créativité ou même la
conscience, mais

43 Par impératif on entend le fait qu’il permettent de définir des
séquences d’instructions, d’affecter des valeurs à des variables,
d’exécuter des instructions conditionnelles ou des boucles, Ils
disposent aussi d’autres abstractions comme des fonctions, des objets
informatiques, qui “encapsulent” des instructions pour les manipuler
plus efficacement. 44 https://scratch.mit.edu permet à chacune et chacun
d’apprendre en manipulant et en jouant les bases de l’informatique, le
langage https://www.python.org est lui très utilisé au niveau scolaire.
Il est bien établi que pour comprendre comment marchent tous ces
mécanismes, y compris ceux que l’on manipule avec des données, il faut
des bases en informatique ce qui est maintenant le cas pour tous nos
enfants, heureusement. 45 Voir par exemple
https://www.epi.asso.fr/revue/articles/a1302b.htm pour une discussion
grand public sur les différences cerveau / ordinateur.

finalement le principe reste le même: l’être vivant et singulièrement
l’humain, traite les données qu’il perçoit pour les transformer en
informations utilisables pour contrôler son environnement et en
connaissances pour mieux les généraliser et les transmettre à ses
congénères46.

Dans la seconde partie consacrée au traitement automatique de
l’information, nous avons indiqué que la force principale de cette
approche est l’accès aux régularités cachées dans les données par leur
traitement statistique. C’est ici que ces techniques peuvent supplanter
l’humain, par leur capacité de calcul mécanique à grande échelle et à
haute fréquence que nous ne pouvons pas atteindre. Mais c’est aussi leur
limite: c’est un traitement massif de données où le cas particulier ne
peut pas être pris en compte et issu d’un calcul purement mécanique qui
n’a pas accès au sens ou à l’interprétation des concepts manipulés.

Par opposition au traitement automatique de l’information qui, par le
calcul, cherche à extraire les régularités statistiques de
l’environnement, le but pour le vivant est d’en extraire son sens, pour
les dimensions qui concernent sa survie et son intérêt propre: on ne
doit pas lire des milliards de phrase ou voir des millions d’images de
chat pour apprendre à parler ou reconnaître un animal. On essaie
simplement d’en extraire ce qui est suffisant pour notre compréhension
du monde et de ses règles, afin de mieux pouvoir l’utiliser pour notre
bénéfice. On passe ainsi notre temps à (très bien) reconnaître
(interpréter plutôt) des scènes ou des phrases qu’on n’a encore jamais
vues ou entendues, avec un système pouvant “comprendre” car ayant déjà
“vécu” des choses similaires. On utilise des “données” en nombre très
raisonnable, mais dans un espace de représentation interne qui est
extrêmement grand et structuré. La complexité d’un cerveau est de
l’ordre de grandeur de tous les ordinateurs du monde8.

Pour approfondir cette différence entre penser et calculer3 John Searle,
propose l’expérience de pensée, dite de la chambre chinoise47. Dans
cette chambre, un opérateur (humain ou artificiel) pourrait appliquer
des règles syntaxiques pour répondre à des questions écrites en chinois,
sans connaître cette langue. Il pourrait faire illusion pour un
observateur extérieur mais pour autant, il ne saurait pas parler chinois
car il n’aurait pas accès au sens des phrases qu’il construit. C’est
bien ici l’importance de la sémantique et sa dissociation d’une
représentation symbolique qui est questionnée.

IV.II Allier pensée et calcul pour délibérer.

Délibérer48 ? « Examiner, peser tous les éléments d’une question avec
d’autres personnes, ou éventuellement en soi-même, avant de prendre une
décision, pour arriver à une conclusion, en pesant le pour et le contre,
de façon à décider par un débat ». On voit à la fois la dimension
intrinsèquement humaine du concept, mais aussi la dimension collégiale :
il s’agit dans la plupart des cas de délibérer au pluriel de la pensée
de plusieurs cerveaux.

Comment délibérer au mieux aujourd’hui, où nous vivons à l’ère numérique
et au temps des algorithmes49, comme le discutent (Abiteboul and Dowek
2017)50 ? Ce qui a été partagé

46 Cette compréhension est d’ailleurs extrêmement fructueuse pour
concevoir d’autres mécanismes d’intelligence artificielle, bien
différents de ce qui se fait avec ce qu’on appelle le “big data”, comme
mis en lumière ici (Alexandre 2020a), déjà cité. 47 Voir
https://fr.wikipedia.org/wiki/Chambre_chinoise pour une présentation de
cette expérience et https://plato.stanford.edu/entries/chinese-room pour
une discussion complète y compris critique de cette expérience de
pensée, le choix de la langue chinoise n’étant peut-être pas le plus
pertinent compte tenu de la structure de la langue. 48 Cité de
http://atilf.atilf.fr à consulter pour une belle définition très
complète : https://www.cnrtl.fr/definition/deliberer 49 On pourra lire
(Abiteboul and Dowek 2017) le « temps des algorithmes » pour une
réflexion plus poussée sur ces mutations par deux grands collègues en
science informatique. 50 Abiteboul, Serge, and Gilles Dowek. 2017. Le
Temps Des Algorithmes. Le pommier.

précédemment offre un double éclairage : à la fois sur ce qui peut-être
délégué à la machine, avec ses limites, et sur ce qui reste l’apanage de
la pensée humaine.

On comprend aisément que ces algorithmes peuvent servir “d’extension
cognitive” comme les applications de nos smartphones qui complètent
notre mémoire, nous aident à nous orienter géographiquement, ou
transcrivent en texte notre parole. Ce sont des outils qui se mettent
dans la boucle de notre interaction avec le monde, comme lorsque nous
prenons une perche pour rallonger la portée de notre bras. C’est à nous
de ne pas inverser les rôles.

Pourrions nous arriver à une situation où un outil hyper-puissant va
rendre son verdict, c’est-à-dire le résultat de ses calculs sur les
données engrangées et l’humain n’aura plus qu’à s’exécuter ? Pourquoi
pas … mais ce serait notre choix. Imaginons bien plus simplement s’en
remettre au verdict51 d’un tirage à pile ou face. Et il y a des
situations extrêmes (par exemple de survie) où cela reste le dernier
outil pour départager toute autre raison écartée. Dans ce cas c’est bien
l’humain qui décide de s’en remettre à ce mécanisme, ce n’est pas le
tirage au sort qui est responsable. S’en remettre à une “IA” quelle
qu’elle soit, est un choix humain. Nous faire croire que “c’est pas de
notre fait c’est l’IA” est un oxymore.

Cela ne veut pas dire que l’occurrence de ces nouvelles technologies est
sans influence sur notre façon de penser, bien au contraire. Comme l’a
très bien théorisé Gilbert Simondon52, la réalité technique fait partie
intégrante de la réalité humaine et par les outils qu’elle génère
transforme la société humaine dans laquelle elle est conçue et donc
transforme sa culture, et la pensée humaine elle-même.

Ainsi, ce qu’on nomme “intelligence artificielle” ne serait pas liée au
fait que les machines vont devenir “intelligentes comme nous” mais
(au-delà de leur déraisonnable efficacité pour des tâches précises),
induirait le fait que notre vision53 de l’intelligence humaine doit
évoluer. Voyons deux aspects.

L’informatique nous force à réfléchir et à concevoir la meilleure façon
de représenter nos connaissances, mais cela serait vrai même si nous ne
confions pas leur traitement à une machine, mais à notre intelligence
biologique, on parle alors de pensée informatique. On peut aussi penser
qu’une bonne partie du travail est faite quand l’humain a réfléchi à la
manière de présenter le problème à la machine: c’est souvent cet effort
de codage qui permet de présenter le problème sous un angle qui le rend
simple à traiter. Nous voilà donc en train de penser non plus
directement à la solution d’un problème mais à la meilleure façon de la
calculer, manuellement ou algorithmiquement.

De même que nous déléguons à la machine le soin de nous transporter, ou
faire les calculs numériques, en laissant le soin à notre cerveau de
choisir comment faire faire cela, et quoi faire du résultat, nous devons
procéder de même avec ces nouveaux outils. Avant il fallait penser puis
délibérer, maintenant il faut penser deux fois : penser comment
calculer, et délibérer en repensant face au résultat du calcul.

51 Philosophiquement, le hasard est un mécanisme absolument impartial,
sans aucune subjectivité, ne prenant en compte aucun facteur discutable,
au contraire de tout autre algorithme qui est sujet à débat car il est
le résultat de choix. 52 On lira à ce propos l’excellent texte
introductif de Gérard Giraudon
https://www.lemonde.fr/blog/binaire/2020/04/05/informatique-culture-et-technique-le-schisme-de-simo
ndon sur cette pensée philosophique. 53 Si ces “processus” sont juste là
pour “optimiser l’organisation du travail humain”, alors comme cela a
déjà été observé lors de la révolution industrielle cela va juste
appauvrir le travail humain, et nous transformer en machine, inversant
le rôle entre outil et artisan.

Cela impose une chose importante : former chacune et chacun (qui
délibèrent et à propos de qui on délibère) aux fondements du numérique,
donc à l’informatique, pour comprendre ce qui a été fait et vérifier
aussi comment cela a été fait. Voilà un moyen pour que l’intelligence
artificielle nous rende collectivement plus intelligents.

Pour aller plus loin.

Voici quelques références supplémentaires qui traite des liens entre
justice et algorithmes ● Un algorithme capable de prédire les décisions
des juges : vers une robotisation de

la justice ? (Barraud 2017)54
https://www.cairn.info/revue-les-cahiers-de-la-justice-2017-1-page-121.htm

● Comment va se rendre la justice au temps des algorithmes ?

https://www.lemonde.fr/blog/binaire/2019/11/25/comment-va-se-rendre-la-justice-au-t
emps-des-algorithmes/

● La justice prédictive et l’égalité devant la loi.

https://www.lemonde.fr/blog/binaire/2019/11/25/comment-va-se-rendre-la-justice-au-t
emps-des-algorithmes/

● Magie numérique et défis juridiques.

https://www.lemonde.fr/blog/binaire/2021/02/05/magie-numerique-et-defis-juridiques/

Bibliographie.

Frédéric Alexandre : Directeur de Recherche Inria, en sciences du
numérique, responsable

de l’équipe Inria Mnemosyne, hébergée à l’Institut des Maladies
Neurodégénératives sur le NeuroCampus de Bordeaux. Dans son parcours en
Intelligence Artificielle puis en neuroscience cognitive, il cherche à
déchiffrer les mécanismes de notre architecture cognitive, en
considérant en particulier comment nos différentes formes de mémoires
contribuent à nous faire appréhender le monde et à élaborer notre
pensée.

Thierry Viéville : Directeur de Recherche Inria, membre de Mnemosyne et
du laboratoire LINE de l’INSPÉ de Nice en sciences de l’éducation,
ancien chargé de mission Inria pour la médiation scientifique et la
formation des enseignantes et enseignants en science informatique et
fondements du numérique dans le secondaire. Il contribue à modéliser de
manière pluri-disciplinaire l’apprentissage humain ou mécanique.

Marie-Hélène Comte : relectrice et conseillère sur ce chapitre,
ingénieure pédagogique et

documentaliste, co-autrice de formations citoyennes en ligne à la pensée
informatique et d’initiation à l’intelligence artificielle citoyenne.

Références.

Abiteboul, Serge, and Gilles Dowek. 2017. Le Temps Des Algorithmes. Le
pommier. Alexandre, Frédéric. 2020a. “Les relations difficiles entre
l’Intelligence Artificielle et les

Neurosciences.” Interstices, August. https://hal.inria.fr/hal-02925517.
———. 2020b. “Creativity Explained by Computational Cognitive
Neuroscience.” In .

https://hal.inria.fr/hal-02891491.

———. 2021. “A Global Framework for a Systemic View of Brain Modeling.”
Brain Informatics, February. https://doi.org/10.1186/s40708-021-00126-4.

Balleine, Bernard W, and Anthony Dickinson. 1998. “Goal-Directed
Instrumental Action:

Contingency and Incentive Learning and Their Cortical Substrates.”
Neuropharmacology 37 (4): 407–19.
https://doi.org/10.1016/S0028-3908(98)00033-1.

Barraud, Boris. 2017. “Un algorithme capable de prédire les décisions
des juges : vers une

54 Barraud, Boris. 2017. “Un algorithme capable de prédire les décisions
des juges : vers une robotisation de la justice ?” Les Cahiers de la
Justice N° 1 (1): 121–39.

robotisation de la justice ?” Les Cahiers de la Justice N° 1 (1):
121–39. Bindra, Dalbir. 1978. “How Adaptive Behavior Is Produced: A
Perceptual-Motivational

Alternative to Response Reinforcements.” Behavioral and Brain Sciences 1
(1): 41–52. https://doi.org/10.1017/S0140525X00059380.

Boraud, Thomas, Arthur Leblois, and Nicolas P. Rougier. 2018. “A Natural
History of Skills.”

Progress in Neurobiology 171 (December): 114–24.
https://doi.org/10.1016/j.pneurobio.2018.08.003.

Cardinal, Rudolf N., John A. Parkinson, Jeremy Hall, and Barry J.
Everitt. 2002. “Emotion

and Motivation: The Role of the Amygdala, Ventral Striatum, and
Prefrontal Cortex.” Neuroscience & Biobehavioral Reviews 26 (3): 321–52.
https://doi.org/10.1016/s0149-7634(02)00007-6.

Cisek, Paul. 2007. “Cortical Mechanisms of Action Selection: The
Affordance Competition Hypothesis.” Philosophical Transactions of the
Royal Society of London. Series B, Biological Sciences 362 (1485):
1585–99. https://doi.org/10.1098/rstb.2007.2054.

———. 2012. “Making Decisions through a Distributed Consensus.” Current
Opinion in Neurobiology 22 (6): 927–36.
https://doi.org/10.1016/j.conb.2012.05.007. Craig, A. D. 2003.
“Interoception: The Sense of the Physiological Condition of the Body.”

Current Opinion in Neurobiology 13 (4): 500–505.
https://doi.org/10.1016/s0959-4388(03)00090-4.

Dietrich, Arne. 2004. “The Cognitive Neuroscience of Creativity.”
Psychonomic Bulletin &

Review 11 (6): 1011–26. https://doi.org/10.3758/BF03196731.

Dolan, Ray J., and Peter Dayan. 2013. “Goals and Habits in the Brain.”
Neuron 80 (2):

312–25. https://doi.org/10.1016/j.neuron.2013.09.007.

Dowek, Gilles. 2019. Langues et langages : Ce dont on ne peut parler, il
faut l’écrire. Le Pommier.
https://www.editions-lepommier.fr/ce-dont-ne-peut-parler-il-faut-lecrire.

Duverne, Sandrine, and Etienne Koechlin. 2017. “Hierarchical Control of
Behaviour in

Human Prefrontal Cortex.” In The Wiley Handbook of Cognitive Control,
207–20. John Wiley & Sons,
Ltd. https://doi.org/10.1002/9781118920497.ch12. Fuster, Joaquı́n M.
2001. “The Prefrontal Cortex—An Update : Time Is of the Essence.” Neuron
30 (2): 319–33. https://doi.org/10.1016/s0896-6273(01)00285-9.

Goodale, M. A., and G. K. Humphrey. 1998. “The Objects of Action and
Perception.”

Cognition 67 (1–2): 181–207.

Graziano, Michael. 2006. “The Organization of Behavioral Repertoire in
Motor Cortex.”

Annual Review of Neuroscience 29 (March): 105–34.
https://doi.org/10.1146/annurev.neuro.29.051605.112924.

Guilford, Joy Paul. 1962. “Creativity: Its Measurement and Development.”
A Source Book for

Creative Thinking, 151–67.

Kandel, Eric. 2006. A La Recherche de La Mémoire. Odile Jacob. Koechlin,
E., G. Basso, P. Pietrini, S. Panzer, and J. Grafman. 1999. “The Role of
the

Anterior Prefrontal Cortex in Human Cognition.” Nature 399 (6732):
148–51. https://doi.org/10.1038/20178.

Ledoux, Joseph E. 2000. “Emotion Circuits in the Brain.” Annual Review
of Neuroscience 23

(1): 155–84. https://doi.org/10.1146/annurev.neuro.23.1.155.

Lettvin, J. Y., H. R. Maturana, W. S. McCulloch, and W. H. Pitts. 1968.
“What the Frog’s Eye

Tells the Frog’s Brain.” Edited by W. C. Corning and M. Balaban. The
Mind: Biological Approaches to Its Functions, 233–58.

Mesulam, M. 2008. “Representation, Inference, and Transcendent Encoding
in

Neurocognitive Networks of the Human Brain.” Annals of Neurology 64 (5):
367–78.

Mesulam, M. M. 1998. “From Sensation to Cognition.” Brain 121 (6):
1013–52.

https://doi.org/10.1093/brain/121.6.1013.

O’Reilly, Randall C. 2020. “Unraveling the Mysteries of Motivation.”
Trends in Cognitive

Sciences 24 (6): 425–34. https://doi.org/10.1016/j.tics.2020.03.001.

O’Reilly, Randall C., Thomas E. Hazy, Jessica Mollick, Prescott Mackie,
and Seth Herd. 2014. “Goal-Driven Cognition in the Brain: A
Computational Framework.” ArXiv:1404.7591, 2014.
http://arxiv.org/abs/1404.7591.

Pezzulo, Giovanni, and Cristiano Castelfranchi. 2009. “Thinking as the
Control of

Imagination: A Conceptual Framework for Goal-Directed Systems.”
Psychological Research PRPF 73 (4): 559–77.
https://doi.org/10.1007/s00426-009-0237-z. Pezzulo, Giovanni, Matthijs
A. A. Van der Meer, Carien S. Lansink, and Cyriel M. A. Pennartz. 2014.
“Internally Generated Sequences in Learning and Executing Goal-Directed
Behavior.” Trends in Cognitive Sciences 18 (12): 647–57.
https://doi.org/10.1016/j.tics.2014.06.011.

Robbins, T. W., and B. J. Everitt. 1996. “Neurobehavioural Mechanisms
of Reward and

Motivation.” Current Opinion in Neurobiology 6 (2): 228–36.

Schacter, Daniel L., Donna Rose Addis, and Randy L. Buckner. 2007.
“Remembering the

Past to Imagine the Future: The Prospective Brain.” Nature Reviews
Neuroscience 8 (9): 657–61. https://doi.org/10.1038/nrn2213.

Squire, L. R. 1992. “Declarative and Nondeclarative Memory: Multiple
Brain Systems

Supporting Learning and Memory.” Journal of Cognitive Neuroscience 4
(3): 232–43.

Squire, L. R., and S. M. Zola. 1996. “Structure and Function of
Declarative and

Nondeclarative Memory Systems.” Proceedings of the National Academy of
Sciences of the United States of America 93 (24): 13515–22.
https://doi.org/10.1073/pnas.93.24.13515.

Wise, Steven P. 2008. “Forward Frontal Fields: Phylogeny and Fundamental
Function.”

Trends in Neurosciences 31 (12): 599–608.
https://doi.org/10.1016/j.tins.2008.08.008.


