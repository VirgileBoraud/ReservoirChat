Self-Organizing Machine Architecture
Bernard Girau, Benoit Miramond, Nicolas P. Rougier, Andres Upegui

To cite this version:

Bernard Girau, Benoit Miramond, Nicolas P. Rougier, Andres Upegui. Self-Organizing Machine Ar-
chitecture. ERCIM News, 2021, 125, pp.4. ￿hal-03186497￿

HAL Id: hal-03186497

https://inria.hal.science/hal-03186497

Submitted on 31 Mar 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Self-Organizing Machine Architecture 
by Bernard Girau (Université de Lorraine), Benoît Miramond (Université Côte d’Azur), 
Nicolas Rougier (Inria Bordeaux), Andres Upegui (University of Applied Sciences of Western Switzerland). 

[Teaser] 

SOMA is a France-Switzerland collaborative project which aims to develop a computing machine 
with self-organizing properties inspired by the functioning of the brain. 

The SOMA project addresses this challenge by lying at the intersection of four main research fields, 
namely adaptive reconfigurable computing, cellular computing, computational neuroscience, and 
neuromorphic engineering. 

In the framework of SOMA, we designed the SCALP platform, a 3D array of FPGAs and processors 
permitting to prototype and evaluate self-organization mechanisms on physical cellular machines. 

The tremendous increase of transistors integration during the last few years has reached the limits of 
classic Von Neuman architectures. Nonetheless, one major issue is the design and the deployment of 
applications that cannot make an optimal use of the available hardware resources. This limit is even 
more acute when we consider application domains where the system evolves under unknown and 
uncertain conditions such as mobile robotics, IoT, autonomous vehicles or drones. In the end, it is 
impossible to foresee every possible context that the system will face during its lifetime, making thus 
impossible to identify the optimal hardware substrate to be used. Interestingly enough, the biological 
brain has solved this problem using a dedicated architecture and mechanisms that offer both adaptive 
and dynamic computations, namely, self-organization [2]. However, even if neuro-biological systems 
have often been a source of inspiration for computer science, the transcription of self-organization at 
the hardware level is not straightforward and requires a number of challenges to be taken-up. We are 
precisely working on coupling this new computational paradigm with an underlying conventional 
systolic architecture [1]. Using self-organized neural populations onto a cellular machine where local 
routing resources are not separated from computational resources; it ensures natural scalability and 
adaptability as well as a better performance/power consumption trade-off compared to other 
conventional embedded solutions. This new computing framework may indeed represent a viable 
integration of neuromorphic computing into the classical Von Neumann architecture and could endow 
these hardware systems with novel adaptive properties [3]. 

Cortical plasticity and cellular computing in hardware 

This objective lead us to study a desirable property from the brain that encompasses all others: 
cortical plasticity. This term refers to one of the main developmental properties of the brain where 
the organization of its structure (structural plasticity) and the learning of the environment (synaptic 
plasticity) develop simultaneously toward an optimal computing efficiency. Such developmental 
process is only made possible by some key features: focus on relevant information, representation of 
information in a sparse manner, distributed data processing and organization fitting the nature of 

 
 
 
 
 
 
 
data, leading to a better efficiency and robustness. Our goal is to understand and design the first 
artificial blocks that are involved in these principles of plasticity. Hence, transposing plasticity, and its 
underlying blocks, into hardware contribute to define a substrate of computation endowed with self-
organization properties stemming from the learning of incoming data. Neural principles of plasticity 
may not be sufficient to ensure that such a substrate of computation is scalable enough in the 
perspective of future massively parallel devices. Our claim is that the expected properties of such 
alternative computing devices could emerge from a close interaction between cellular computing 
(decentralization and hardware compliant massive parallelism) and neural computation (self-
organization and adaptation). We also claim that neuro-cellular algorithmics and hardware design are 
so tightly related that these two aspects should be studied together. Therefore, we propose to 
combine neural adaptivity and cellular computing efficiency through a neuro-cellular approach of 
synaptic and structural self-organization that defines a fully decentralized control layer for 
neuromorphic reconfigurable hardware. To achieve this goal, the project gathers neuroscientists, 
computer science researchers, hardware architects and micro-electronics designers to explore the 
concepts of a Self-Organizing Machine Architecture: SOMA (see figure 1). This Self-Organization 
property already studied in various fields of computer science, is studied for the very first time in a 
new context with a transverse look from the computational neuroscience discipline to the design of 
reconfigurable microelectronic circuits. The project focuses on the blocks that will pave the way in the 
long term for smart computing substrates, exceeding the limits of current technology. 

Convergence of research fields 

Previous works have already explored the possibility of using neural self-organizing models to control 
task allocation on parallel substrates [1], while adapting neural computational paradigms to cellular 
constraints. Adaptive reconfigurable computing focuses on virtualisation of reconfigurable hardware, 
runtime resource management, dynamic partial reconfiguration, and self-adaptive architectures. 
Cellular approaches of distributed computing result in decentralized models that are particularly well 
adapted to hardware implementations. However, cellular computation still lacks adaptation and 
learning properties. This gap may be filled with the help of computational neuroscience and 
neuromorphic engineering through the definition of models that exhibit properties like unsupervised 
learning, self-adaptation, self-organization, and fault tolerance which are of particular interest for 
efficient computing in embedded and autonomous systems. However, these properties only emerge 
from large fully connected neural maps that result in intensive synaptic communications. 

Our self-organizing models are deployed on the Self-configurable 3-D Cellular multi-FPGA Adaptive 
Platform (SCALP) (figure 2), which has been developed in the framework of the SOMA project. SCALP 
is a multi-FPGA hardware platform permitting to prototype 3D NoC architectures with dynamic 
topologies. A node is composed of a Xilinx Zynq SoC (dual-core ARM Cortex-A9 @866 MHz + Artix-7 
programmable logic with 74,000 cells), 2 Gb DDR3 SDRAM, a 5-port Ethernet switch, and a PLL. The 
inherent cellular scalable architecture of SCALP, coupled with its enhanced interfaces, provides the 
ideal computation platform for implementing cellular neuromorphic architectures by imposing real 
physical connectivity constraints. Also, a real 3D machine architecture (instead of a simulated one) can 
better handle the scalability issues when modelling dynamic bio-inspired 3D neural connectivity. We 
already proposed such models using both dynamical sprouting and pruning of synapses inside a self-
organizing map and a method to migrate neurons between clusters to dynamically reassign neurons 

 
from one task to another. Those methods provide dynamic structural and computational resource 
allocations, inspired from the brain structural and functional plasticity, and are currently being 
deployed onto the SCALP platform. 

Links: 

[L1]: https://anr.fr/Project-ANR-17-CE24-0036 

[L2]: https://gitedu.hesge.ch/soma/scalp\_board/-/wikis/home 

References: 
[1]: Rodriguez, L., Miramond, B., Granado, B., Toward a sparse self-organizing map for 
neuromorphic architectures, ACM Journal on Emerging Technologies in Computing Systems (JETC) 
11 (4), 1-25, 2015 

[2] : Detorakis, G.I. and Rougier, N.P. 2012. A Neural Field Model of the Somatosensory Cortex: 
Formation, Maintenance and Reorganization of Ordered Topographic Maps. PLoS ONE 7, 7, 
e40257. 

[3]: Upegui, A., Thoma, Y., Sanchez, E., Perez-Uribe, A., Moreno, J. M., & Madrenas, J. (2007, August). 
The Perplexus bio-inspired reconfigurable circuit. In Proceedings of the Second NASA/ESA 
Conference on Adaptive Hardware and Systems. 

Please contact: 

Nicolas Rougier, Inria Bordeaux Sud-Ouest, France, Nicolas.Rougier@inria.fr. +33 7 82 50 31 10 

Figures: 

Figure 1. SOMA is a convergence point between past research approaches toward new computation paradigms: adaptive 
reconfigurable architecture, cellular computing, computational neuro-science, and neuromorphic hardware 

 
 
 
Figure 2. The SCALP platform, a set of FPGAs and processors with 3D topology, was designed to evaluate self-organization 
mechanisms on cellular machines. Algorithms based on cellular self-organizing maps are the basis of the self-adaptation 
properties. 

 
 
 
 
