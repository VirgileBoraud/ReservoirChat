What is computational reproducibility? Olivia Guest, Nicolas P. Rougier

To cite this version:

Olivia Guest, Nicolas P. Rougier. What is computational
reproducibility?. IEEE CIS Newsletter on Cognitive and Developmental
Systems, 2016, 13 (1). ￿hal-01358082￿

HAL Id: hal-01358082

https://inria.hal.science/hal-01358082

Submitted on 31 Aug 2016

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

What is computational reproducibility?

Olivia Guest Department of Experimental Psychology University of Oxford,
United Kingdom

Nicolas P. Rougier INRIA Bordeaux Sud-Ouest, Talence, France Institut
des Maladies Neurodégénératives, Université Bordeaux, Centre National de
la Recherche Scientiﬁque, UMR 5293, Bordeaux, France LaBRI, Université
de Bordeaux, Institut Polytechnique de Bordeaux, Centre National de la
Recherche Scientiﬁque, UMR 5800, Talence, France

Computational modelling is

the process by which phenomena found in complex systems are expressed
algorithmically. The creation of such simulations is useful because it
allows us to test whether our understanding is sophisticated enough to
create credible working models In neuroscience and of the phenomena we
are studying. cognitive science especially, computational modelling
comprises more than just capturing a single phenomenon, it also
implements a theory. It gives scientists a method of allowing their
ideas to be executed, i.e., for emergent properties to appear when they
are implemented and run (McClelland, 2009). In this context, a model is
said to be replicable if experiments within it can be carried out
successfully using the original codebase, with the implicit assumption
that such a codebase is available.

However, for models to be evaluated it is mandatory to en- sure they are
reproducible (Topalidou, Leblois, Boraud, & Rougier, 2015). That is,
that they can be recreated based on their speciﬁcation — the details
deemed important enough to be included in the accompanying article
(Hinsen, 2015). Ideally, this should be possible without contacting the
au- thors for advice, and critically, without referring to the orig-
inal code (Cooper & Guest, 2014). If the speciﬁcation is suﬃcient to
successfully recreate the codebase from scratch, then the model is said
to be reproducible. This adds further credence to both the model and its
overarching theoretical framework. If not, and the model cannot be
recreated, then even if the experiments can be carried out successfully
within the original codebase, the model is not reproducible (Crook,
Davison, & Plesser, 2013).

How to share computational research?

Access to the original codebase is not always straightfor- ward. There
have been few substantial changes within schol- arly communication and
research dissemination since 1665, when the ﬁrst academic journals (Le
Journal des Sçavans and

Philosophical Transactions of the Royal Society) were pub- lished.
Dissemination of scientiﬁc discoveries via publishers continues to
consist primarily of static text and ﬁgures. How- ever, most research is
underpinned by, if not wholly com- prised of, code, which is inherently
dynamic.

Given code forms the backbone of modern scientiﬁc re- search, it is
perhaps unusual that its position within this framework is not clear.
For example, it is not straightfor- ward where codebases should be
placed: in a footnote (with code assured to be available upon request),
in supplementary materials, or in an online repository? Even though more
jour- nals are requesting code, as well as raw data, few publisher-
backed repositories exist. It is striking that an overwhelming number of
journals make no provisions for and oﬀer little guidance on hosting
these ﬁles or indeed facilitating access to them.

Is it time for progress?

The open source and open science communities proposed solutions to some
of the aforementioned problems without publishers’ aid nor mediation.
Firstly, a set of new innovative software tools (e.g., the binder
project) make modelling work more accessible. Secondly, some researchers
have taken mat- ters into their own hands and created resources for best
prac- tice (e.g., version control: Blischak, Davenport, & Wilson, 2016;
Eglen et al., 2016; Wilson, 2016). While others lead by example: Ogrean
et al. (2016) published an article with an in- teractive ﬁgure; and the
LIGO Open Science Center released extensive amounts of data and code
(LIGO Open Science Center: Tutorials, 2016). In the same vein, the
ReScience journal encourages the reproduction of modelling work.

Is the scientiﬁc community ready to embrace and facili- tate changes
with respect to: associating articles with origi- nal codebases in a
transparent way and, more broadly, mak- ing sure computational theories
are well-speciﬁed and coher- ently implemented?

2

OLIVIA GUEST

References

Blischak, J. D., Davenport, E. R., & Wilson, G.

(2016, 01). A quick introduction to version control with git and github.
PLoS Comput Biol, 12(1), 1-18. doi: 10.1371/journal.pcbi.1004668

Cooper, R. P., & Guest, O. (2014). Implementations are not
speciﬁcations: Speciﬁcation, replication and experi- mentation in
computational cognitive modeling. Cog- nitive Systems Research, 27, 42 -
49. doi: 10.1016/ j.cogsys.2013.05.001

Crook, S. M., Davison, A. P., & Plesser, H. E. (2013). 20 years of
computational neuroscience. In M. J. Bower (Ed.), (pp. 73–102). New
York, NY: Springer New York. doi: 10.1007/978-1-4614-1424-7_4

Eglen, S., Marwick, B., Halchenko, Y., Hanke, M., Suﬁ, S., Gleeson, P.,
. . . Poline, J.-B. (2016). Towards standard practices for sharing
computer code and programs in neuroscience. bioRxiv. doi: 10.1101/045104

Hinsen, K. (2015). Writing software speciﬁcations. Com- puting in
Science & Engineering, 17(3), 54–61. doi: 10.1109/MCSE.2015.64

LIGO Open Science Center: Tutorials. (2016). https://
losc.ligo.org/tutorials/. (Accessed: 2016-06- 02)

McClelland, J. (2009). The place of modeling in cognitive science.
Topics in Cognitive Science, 1(1), 11–38. doi:
10.1111/j.1756-8765.2008.01003.x

Ogrean, G. A., van Weeren, R. J., Jones, C., Forman, W., Dawson, W. A.,
Golovich, N., . . . Ebeling, H. (2016). Frontier ﬁelds clusters: Deep
chandra observations of the complex merger macs j1149.6+2223. The Astro-
doi: 10.3847/0004 physical Journal, 819(2), 113. -637X/819/2/113

Topalidou, M., Leblois, A., Boraud, T., & Rougier, N. P. (2015). A long
journey into reproducible computa- tional neuroscience. Frontiers in
Computational Neu- roscience, 9(30). doi: 10.3389/fncom.2015.00030
Wilson, G. (2016). Software carpentry: lessons learned [ver- sion 2;
referees: 3 approved]. F1000Research, 3(62). doi:
10.12688/f1000research.3-62.v2


