From Phonemes to Sentence Comprehension: A
Neurocomputational Model of Sentence Processing for
Robots
Xavier Hinaut

To cite this version:

Xavier Hinaut. From Phonemes to Sentence Comprehension: A Neurocomputational Model of Sen-
tence Processing for Robots. SBDM2018 Satellite-Workshop on interfaces between Robotics, Artificial
Intelligence and Neuroscience, May 2018, Paris, France. ￿hal-01964524￿

HAL Id: hal-01964524

https://inria.hal.science/hal-01964524

Submitted on 3 Jan 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

From Phonemes to Sentence Comprehension:
A Neurocomputational Model of Sentence Processing for Robots
X. Hinaut1,2,3

MNEMOSYNE Team

xavier.hinaut@inria.fr

1. Inria Bordeaux Sud-Ouest, Talence, France.
2. LaBRI, UMR 5800, CNRS, Bordeaux INP, Université de Bordeaux, Talence, France.
3. Institut des Maladies Neurodégénératives, UMR 5293, CNRS, Université de Bordeaux, Bordeaux, France.

Abstract
There has been an important progress these last years in 
speech recognition systems. The word recognition error rate 
went down with the arrival of deep learning methods. 
However, if one uses cloud speech API and integrate it inside a 
robotic architecture [3][11], one faces a non negligible 
number of wrong sentence recognition. Thus speech 
recognition can not be considered as solved (because many 
sentences out of their contexts are ambiguous). We believe 
that contextual solutions (i.e. adaptable and trainable on 
different HRI applications) have to be found. In this 
perspective, the way children learn language and how our 
brains process utterances may help us improve how robots 
process language. Getting inspiration from language 
acquisition theories and how the brain processes sentences 
we previously developed a neuro-inspired model of sentence 
processing [2][4]. In this study, we investigate how this model 
can process different levels of abstractions as input: sequence 
of phonemes, seq. of words or grammatical constructions. We 
see that even if the model was only tested on grammatical 
constructions before, it has better performances with words 
and phonemes inputs.

Materials & Methods
Echo State Networks [7]
Update equation of the reservoir (recurrent layer) and 
the readout (output layer):

Matrices Win and W are randomly generated. 

Training of the output weights with ridge regression

Model details and parameters
Number of reservoir units: 500. 
Spectral radius: 1. Input scaling: 0.6. Reservoir weight 
std: 0.1. Leak-rate: 0.06. Regularization param.: 2.5\*10-4
for PHON and WORD, and 5\*10-6 for CONST. Infrequent 
word threshold for INF: 5.

Sentence examples produced by users

Corpus is composed of 190 English sentences. [3][5] 
Word to phoneme conversion is done with CMU dict.

Discussion
This study tried to understand what kind of input information 
is most relevant for learning to parse sentences with simple 
neurocognitive mechanisms (unstructured recurrent 
networks and Hebbian-like learning). Results showed that 
WORD condition performing best in normal conditions, but 
only from a short increase in performance. 
We also explored noisy conditions, where 5% of the words 
were randomly replaced by other words. WORD and PHON 
conditions resisted better to noise than CONST condition.

Given these results, we speculate that the PHON condition 
would give better results than WORD cond. when dealing 
with real speech inputs.

In future work, we will process real speech data in order to: 
(1) test different speech recognizers that will provide 
sequences of phonemes or seq. of words; (2) use the 
recognized phonemes/words to train and test the current 
model, and see which condition PHON/WORD/CONST gives 
the best generalization.

Materials & Methods

Sentence parsing model with different input conditions

Semantic words (SWs)

on the left please put the toy .

SW1

SW2

SW3

left
put
toy
Memory of SWs 

Experimental conditions
[PHON] (phonemes)
1.
[WORD] (words)
2.
[CONST] (words with SW)
3.
Function words (FWs)
and abstract symbols
Out-of-Vocabulary (2. + 3.)
•
repl. of infrequent 
words by “&” [INF]
• no word replacement

Random Noise [NOISE]
• 5% of words changed

the

to

on

and

…

&
(e.g. please)

SW
(e.g. toy)

Input / Output

1
W
S

2
W
S

Meaning: P (A, L)

P: Predicate
A: Agent
L: Location

put (toy, left)
P(A, L)

P

A

L left

put

P

A

L

P

A

L

toy

3
W
S

Fixed connections
Connection

Learnable connections

Inactive connection
Active connection

&: Infrequent FW symbol
SW: Semantic Word symbol

Input
Layer

Reservoir
(Hidden Layer)

Read-out
Layer

Example of one input sentence in different conditions
-
- Output: point (triangle) ; touch (triangle)

Input: “Point the triangle and then touch it”

Seq. of Phonemes [PHON]
Seq. of Words [WORD]
Grammatical Constructions [CONST]
[WORD] + [INF]
[CONST] + [INF]

[PHON] + [NOISE]
[WORD] + [NOISE]
[CONST] + [NOISE]

P OY1 N T DH AH0 T R AY1 AE2 NG G AH0 L AH0 N D DH EH1 N T AH1 CH IH1 T .
it .
point
it .
SW
it .
point
it .
SW

and then touch
and then SW
and & touch
and & SW

the triangle 
the SW
the triangle 
the SW

P OY1 N T DH AH0 T R AY1 AE2 NG G AH0 L P UH1 T DH EH1 N T AH1 CH IH1 T .
it .
point
it .
SW

the triangle 
the SW

then touch
then SW

put
SW

Results
Mean error in percent (with std) for full sentence comprehension

Conditions

Default

18.49 (1.76)

INF

N/A

NOISE

33.11 (0.77) 

18.12 (1.38) 

16.51 (1.26) 

29.73 (0.48) 

21.46 (1.41) 

17.71 (1.49) 

40.53 (0.77) 

PHON

WORD

CONST

Results for 10-fold cross-validation (4-fold for NOISE) averaged over 100 instances. Full sentence comprehension imply 
that all output roles are correctly recognized.

References

Links

Video of Human-Robot Interaction: 

youtu.be/FpYDco3ZgkU

Corpus and code: 

github.com/neuronalX/EchoRob

Acknowledgments
This work was partly supported by the PHC PROCOPE (Campus 
France - DAAD) LingoRob project 37857TF. We thank Johannes 
Twiefel for very interesting discussions.

[1] M. Tomasello, Constructing a language: A usage based approach to language acquisition. 
Cambridge, MA: Harvard University Press, 2003.
[2] P. Dominey, M. Hoen, and T. Inui, “A neurolinguistic model of grammatical construction 
processing,” Journal of Cognitive Neuroscience, vol. 18, no. 12, pp. 2088–2107, 2006. 
[3] X. Hinaut, M. Petit, G. Pointeau, and P. Dominey, “Exploring the acquisition and production of 
grammatical constructions through human-robot interaction with echo state networks,” Front in 
Neurorob, vol. 8, 2014.
[4] X. Hinaut and P. Dominey, “Real-time parallel processing of grammatical structure in the fronto-
striatal system: a recurrent network simulation study using reservoir computing” PloS one, 8, 2, p. 
e52946, 2013.
[5] X. Hinaut, J. Twiefel, M. Petit, P. F. Dominey, and S. Wermter, “A recurrent neural network for 
multiple language acquisition: Starting with english and french,” in CoCo NIPS 2015 Workshop.
[6] A. Goldberg, Constructions: A construction grammar approach to argument structure. University 
of Chicago Press, 1995.
[7] H. Jaeger, “The “echo state” approach to analysing and training recurrent neural networks” Bonn, 
Germany: German National Research Center for Information Technology GMD Technical Report, vol. 
148, p. 34, 2001.
[8] Marcus, G. F. et al. (1999). “Rule learning by seven-month-old infants ”. Science 283, 77–80.
[9] T. Taniguchi, T. Nagai, T. Nakamura, N. Iwahashi, T. Ogata, and H. Asoh, “Symbol emergence in 
robotics: a survey,” Advanced Robotics, vol. 30, no. 11-12, pp. 706–728, 2016. 
[10] J. Bergstra, D. Yamins, and D. D. Cox, “Hyperopt: A python library for optimizing the 
hyperparameters of machine learning algorithms,” in Proceedings of the 12th Python in Science 
Conference, pp. 13–20. 2013.
[11] J. Twiefel, X. Hinaut, M. Borghetti, E. Strahl, and S. Wermter, “Using Natural Language Feedback 
in a Neuro-inspired Integrated Multimodal Robotic Architecture,” in Proc. of RO-MAN, New York City, 
USA, 2016. 
[12] X. Hinaut and J. Twiefel, “Teach your robot your language! trainable neural parser for modelling 
human sentence processing: Examples for 15 languages,” (Submitted). 

