[Re] A Reservoir Computing Model of
Reward-Modulated Motor Learning and Automaticity
Remya Sankar, Nicolas Thou, Nicolas P. Rougier, Arthur Leblois

To cite this version:

Remya Sankar, Nicolas Thou, Nicolas P. Rougier, Arthur Leblois.
[Re] A Reservoir Computing
Model of Reward-Modulated Motor Learning and Automaticity. The ReScience journal, 2021, 7 (11),
￿10.5281/zenodo.5718075￿. ￿hal-03482094￿

HAL Id: hal-03482094

https://inria.hal.science/hal-03482094

Submitted on 15 Dec 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

R E S C I E N C E C

Replication / Computational Neuroscience
[Re] A Reservoir Computing Model of Reward-Modulated
Motor Learning and Automaticity

Remya Sankar1,2,3, ID , Nicolas Thou1, ID , Nicolas P. Rougier1,2,3, ID Arthur Leblois3, ID
1INRIA Bordeaux Sud-Ouest, Bordeaux, France – 2LaBRI, Université de Bordeaux, Institut Polytechnique de Bordeaux, Centre
National de la Recherche Scientiﬁque, UMR 5800, Talence, France – 3Institut des Maladies Neurodégénératives, Université de
Bordeaux, Centre National de la Recherche Scientiﬁque, UMR 5293, Bordeaux, France

Edited by
Benoît Girard ID

Reviewed by
Daniel Schmid ID
Robin Gutzen ID

Received
12 March 2020

Published
22 November 2021

DOI
10.5281/zenodo.5718075

A replication of pyle2019.

1 Introduction

Pyle and Rosenbaum [1] introduce a novel learning algorithm to the reservoir comput-
ing framework, which harnesses the dynamics of a recurrently connected network to
generate time series. Most existing algorithms are built on fully supervised learning
rules (e.g. FORCE [2]), which limits their potential applications, or the more biologically-
realistic reinforcement learning techniques (e.g. RMHL [3]) which unfortunately fail to
converge on complex spatio-temporal signal generation tasks. Pyle and Rosenbaum [1]
use the advantages of these two learning rules, while averting their individual shortcom-
ings, by combining the two algorithms to form the SUPERTREX model. The workings of
this model are aligned to the theory of motor learning involving the basal ganglia, from
rodent and songbird literature [4]. This hypothesises that a cortical pathway works in
tandem with the basal ganglia for motor skill acquisition, wherein the basal ganglia
pathway functions as a tutor, providing guiding signals that would ultimately be consol-
idated in the primary cortical pathway in charge of production of the motor commands
[5]. Here, the basal ganglia pathway, which uses reward-modulated exploration based
learning, akin to the RMHL algorithm, works in parallel with the cortical pathway, mod-
eled using the fully supervised FORCE algorithm. The SUPERTREX model uses both
these pathways in parallel, with the RMHL-based pathway providing the supervisory
signal that the FORCE-based pathway requires.
In this article, we provide a modular and user-friendly Python re-implementation of the
model presented by Pyle and Rosenbaum [1]. We were able to successfully reproduce
the model performance in Python, for two tasks out of the three presented in the original
article. For the third task, we were able to do so with limited robustness. We address this
by introducing some modifications, and discuss how their inclusion vastly improves the
robustness as well as scalability of the model.

Terminology —

• Original scripts: the MATLAB scripts used by the authors to produce the results

presented in [1].

• Python adaptation: our Python adaptation of the original MATLAB scripts.

• Python re-implementation: an improved version of our Python adaptation.

Copyright © 2021 R. Sankar et al., released under a Creative Commons Attribution 4.0 International license.
Correspondence should be addressed to Remya Sankar (Remya.Sankar@inria.fr)
The authors have declared that no competing interests exists.
Code is available at https://github.com/rsankar9/Reimplementation-SUPERTREX/releases/tag/v3.0 – DOI https://doi.org/10.5281/zenodo.4596425.
Open peer review is available at https://github.com/ReScience/submissions/issues/50.

ReScience C 7.1 – Sankar et al. 2021

1

[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

1.1 Framework

Pyle and Rosenbaum [1] proposes a model for sensorimotor learning using the frame-
work of reservoir computing. The model is based on two existing reservoir computing
techniques: FORCE and RMHL.

τ

dx
dt

= −x + Jr + Qz

r = tanh(x) + ϵ

(1)

(2)

FORCE (or first-order reduced and controlled error) is a fully supervised learning rule,
which is widely used within the reservoir computing framework [2]. A recurrently con-
nected reservoir, composed of rate-coded neurons is trained to produce a target time
series by modifying the readout weights between the reservoir and the output layer (Eq
3, 4). The output, in turn, interacts with the reservoir by providing feedback (Eq 1, 2).
FORCE can accurately generate complex dynamical target time-series. However, the
model must have explicit knowledge of the target function, as FORCE requires a fully
supervisory signal of the correct output in order to compute the error during training.

z1 = W1r

τw1

dW1
dt

= −erT P

(3)

(4)

RMHL (or Reward-Modulated Hebbian Learning) is built on the concept of reinforce-
ment learning, and uses only a scalar error signal indicating reward, allowing it to be
applicable in a wider range of scenarios than FORCE [3]. RMHL introduces perturba-
tions in the performance of the model, and uses the information gained from this explo-
ration to find the target (Eq 5, 6). This is akin to dopamine-dependent Hebbian learning
in the basal ganglia. However, RMHL fails to converge to an accurate solution on sev-
eral complex tasks. Moreover, it has been observed in songbirds that while the basal
ganglia provides a tutor signal in the early stages, learning is eventually consolidated in
a parallel cortical pathway, which is primarily responsible for motor activity [5]. RMHL
cannot account for such empirical observations.

z2 = W2r + Ψ(e)η

τw2

dW2
dt

= Φ(ˆe)ˆzrT

(5)

(6)

SUPERTREX (Supervised Learning Trained by Reward Exploration), the model proposed
by the authors, tries to merge the advantages of both of these algorithms by combin-
ing both models. The more wide-ranged applicability of RMHL, owing to its usage of
a one dimensional error signal, is used to train the model, while the superior mainte-
nance ability of FORCE is recruited to consolidate the tutoring of the RMHL pathway.
This could also potentially support the empirical evidence showing the basal ganglia
and cortical pathways working in tandem for motor skill acquisition, discussed above.
Thus, the SUPERTREX model consists of two parallel pathways, one based on RMHL (ex-
ploratory) and one based on FORCE (mastery), each consisting of its own set of weights.
The mastery pathway uses the output of the exploratory pathway as its supervisory sig-
nal (Eq 7, 8, 9).

z = z1 + z2

(7)

2

ReScience C 7.1 – Sankar et al. 2021

[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

τw1

dW1
dt

= (z − z1) rT P

τw2

dW2
dt

= Φ(ˆe)ˆzrT

(8)

(9)

where x denotes the reservoir dynamics, J the recurrent connectivity matrix, Q the feed-
back weights and r the reservoir activity. The output z of the SUPERTREX model is the
combination of the outputs z1 and z2 of the FORCE and RMHL pathways, respectively.
W1 and W2 denote the readout weights of the two pathways, respectively, e denotes the
squared distance between the output and the target trajectory, τ is the corresponding
timescales for learning, η is the exploratory noise, ϵ is a small noise term and P is a
running estimate of the inverse of the correlation matrix of rates. Ψ and Φ are two sub-
linear functions that serve to damp runaway oscillations during learning and control
weight update, respectively. ˆx is a high-pass filtered version of x, which represents the
recent changes in x.

1.2 Task

The authors test the SUPERTREX model on three motor tasks, with increasing difficulty,
and compare its performance to those of FORCE and RMHL. The target of each task is to
learn to produce a given spatio-temporal signal, under different constraints. Task 1 tests
the performance of the model when the target output is known. This task requires the
spatio-temporal signal to be produced directly by the model. Thus, the error signal is a
direct indicator of the change required in the output of the model, i.e. fully supervisory.
Task 2 and Task 3 use the paradigm of exploration by a multi-segmented arm, pivoted
at a point.
Task 2 tests the performance of the model when the target output is unknown, and only
an indirect error signal is provided. The task requires the angles between the arm seg-
ments to be generated by the model, which would in turn produce the trajectory of the
target spatio-temporal signal. In this case, the error signal is not a direct indicator of
the change required in the output of the model. A non-linear inverse transformation
of the trajectory would be required to compute the desired angles between the arm seg-
ments. It is, thus, not a fully supervisory signal, but simply a reinforcement signal. In
Task 3, the movement of the arm segments are penalised variably. This creates the need
to choose one from multiple candidate solutions by optimising the cost of changing the
angles between the arm segments.
The simulation for each task includes a training phase and a testing phase. In the train-
ing phase, for ten periods, the time-series is generated by the model while the weights
are being updated according to the current error feedback. After this, in the testing
phase (lasting five periods), the readout weights are frozen and the time-series is gen-
erated using these frozen weights, without any further feedback-based update. In the
SUPERTREX model, the exploratory pathway is also deactivated. It is also worth noting
that the authors use teacher-forcing in the testing phase, which considerably improves
the model performance by limiting the dependence on the stability of the learned solu-
tion (refer to Section “State information provides stability of learned output” in [1]).

Disclaimer — Pyle and Rosenbaum [1] proceed to test the model under variations of the
above tasks, including disrupted learning and with additional state information. How-
ever, these variations have not been replicated by us. We only test the performance of
the three learning rules on the three tasks, specified above.

ReScience C 7.1 – Sankar et al. 2021

3

[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

2 Comparison with Python Adaptation

In this section, we compare the results presented in the paper [1] with the MATLAB im-
plementation by the authors and our Python adaptation. The original scripts, although
not available online, are readily available on request. We present our adaptation of this
model in the open source framework Python, which has been built based on the paper
and the MATLAB scripts provided by the authors. In contrast to the original scripts, it
is modular and is easily modifiable with external json descriptor files. We compare the
results presented in the paper, with simulations of the MATLAB scripts, provided by the
authors, and also with our adaptation in Python 1.

To validate our Python adaptation, we test the three algorithms on three tasks by simulat-
ing them using both the original scripts and our Python adaptation. For each algorithm-
task combination, we produce ten simulations with arbitrary seeds initialising the ran-
dom generator and one additional simulation using the default seed of MATLAB (equiv-
alent to seed 5489 of the numpy random generator). Except for the default seed, the
ten arbitrary seeds are different for the Python and MATLAB simulations, and for each
algorithm-task combination. Both MATLAB and numpy use the Mersenne Twister pseudo-
random number generator [6]. To evaluate the performance of the algorithm, the au-
thors plot the “distance from target”, i.e. the square root of the low pass filtered version
of the mean squared error, over the progression of the simulation. In order to cate-
gorise the model performance as satisfactory or unsatisfactory, we further compute a
deviation metric by calculating the mean “distance from target” over the testing phase.
If this deviation metric is below the threshold of 0.5 (set by visual inspection), the model
is said to have satisfactorily learnt and produced the target output.

2.1 Task 1

Here, we compare the simulations of the original scripts and our adaptation for Task 1,
using FORCE, RMHL and SUPERTREX, with the results presented in the article. Task 1
is designed to test the performance of these three algorithms when generating a known
target output. The objective of this task is to produce a time-series of 2-D coordinates re-
quired to traverse a target trajectory, in this case, the parameterized curve of a butterfly.
The model is trained to generate an output which closely matches the target function.

The article claims that:

• under the FORCE framework, the target time-series is learned accurately and is

maintained in a stable manner during the testing phase (Figure 1a).

• under the RMHL framework, the target time-series is generated accurately during
the training phase, however is not maintained perfectly during the testing phase
(Figure 1b).

• under the SUPERTREX framework, the target time-series is learned accurately and
is also maintained in a stable manner during testing phase, albeit not as well as
FORCE (Figure 1c).

We validate these observations with the MATLAB scripts provided by the authors as well
as with our Python adaptation. To do so, we run the simulations with the default seed
and repeat it ten times with different (arbitrarily chosen) seeds initialising the random
number generator.
We observe that:

1In Figures 1- 4, the results presented in the paper have been reused in the column titled ”original”.

ReScience C 7.1 – Sankar et al. 2021

4

[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

• under the FORCE framework, the target time-series is learned accurately and is
maintained in a stable manner during the testing phase, as claimed. The mean
deviation over eleven simulations, for both the original scripts (0.003±0.002; n=11)
and the Python adaptation (0.004 ± 0.003; n=11) is much lower than the threshold
of 0.5 (Figure 1a, 2a).

• under the RMHL framework, the target time-series is generated accurately during
the training phase, however is not maintained perfectly during the testing phase,
as claimed. The mean deviation, for both the original scripts (0.168 ± 0.038; n=11)
and the Python adaptation (0.182 ± 0.046; n=11), is higher than that with FORCE
(Figure 1b, 2b).

• under the SUPERTREX framework, the target time-series is learned accurately and
is also maintained in a stable manner during testing phase, albeit not as well as
FORCE, as claimed. The mean deviation, for both the original scripts (0.006±0.003;
n=11) and the Python adaptation (0.006 ± 0.003; n=11), is much better than that for
RMHL, but slightly worse than with FORCE (Figure 1c, 2c).

Both the original scripts and the Python adaptation are able to successfully closely re-
produce the results presented in the paper for Task 1 (Figure 1,2; Table 1, 2).

2.2 Task 2

Here, we compare the simulations of the original scripts and our Python adaptation for
Task 2, using FORCE, RMHL and SUPERTREX, with the results presented in the article.
Task 2 is designed to test the performance of these three algorithms when generating an
unknown target from an indirect error signal. Using the paradigm of a pivoted multi-
segmented arm, the objective of this task is to produce a time-series by generating the
angles between the arm segments. Motor output does not control the position of the
end-effector of the arm, but instead controls the angles of the arm joints, which are non-
linearly related to end-effector position.

The article claims that:

• the FORCE framework cannot be applied to this task, as FORCE requires the ex-
act target to be provided as a supervisory error, which in this case would be the
unknown target angles. Since, we do not have this information beforehand, and
require the model to derive it, the FORCE framework is inapplicable to this task.

• under the RMHL framework, the target time-series is imitated well by the model
during the training phase, however the weights do not converge, and hence, it is
unable to maintain the time-series in a stable manner during the testing phase
(Figure 3a).

• under the SUPERTREX framework, the target time-series is learned accurately
and is also generated in a stable manner, with minor divergences, during testing
phase, owing to the contribution of the pathway based on the FORCE algorithm
(Figure 3b).

We verify these observations with the MATLAB scripts provided by the authors as well
as with our Python adaptation. To do so, we run the simulations with the default seed
of MATLAB and re-simulate it with ten arbitrary seeds initialising the random number
generator. We do not modify any task conditions or model hyper-parameters.
We observe that:

• indeed, the FORCE framework is inapplicable to this task.

ReScience C 7.1 – Sankar et al. 2021

5

[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

MATLAB

Original

Python

E
C
R
O
F

)
t
(
x

)
t
(
y

(a) Results for Task 1 with the FORCE algorithm. The target time-series is learned accurately during the train-
ing phase and is maintained in a stable manner during the testing phase, in both implementations, as pre-
sented in [1].

L
H
M
R

)
t
(
x

)
t
(
y

(b) Results for Task 1 with the RMHL algorithm. The target time-series is learned accurately during the training
phase, though not maintained perfectly during the testing phase, in both implementations, as presented in
[1].

X
E
R
T
R
E
P
U
S

)
t
(
x

)
t
(
y

(c) Results for Task 1 with the SUPERTREX algorithm. The target time-series is learned accurately during the
training phase, and is also maintained in a stable manner during testing phase, albeit not as well as FORCE,
in both implementations, as presented in [1].

Figure 1. Comparison of the performances of the MATLAB scripts (left column) and the Python
adaptation (right column) with the results presented in the original article (center column), for
the three learning algorithms on Task 1 [1]. All simulations shown here use the MATLAB default
(5489) as the seed for the random number generator. In each subfigure, the top row shows the
target trajectory (red) with the trajectory generated by the model (blue) throughout the test phase.
The second row shows the time-series (blue) generated by the model (x and y coordinates, in this
case) along with the target time-series (red). The grey vertical line marks the separation of the
training and testing phase.

ReScience C 7.1 – Sankar et al. 2021

6

1s1s[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

MATLAB

Original

Python

E
C
R
O
F

E
S
M

|
|

W

|
|

(a) Results for Task 1 with the FORCE algorithm. The target time-series is learned accurately during the train-
ing phase and is maintained in a stable manner during the testing phase, as presented in [1].

L
H
M
R

E
S
M

|
|

W

|
|

(b) Results for Task 1 with the RMHL algorithm. The target time-series is learned accurately during the training
phase, though not maintained perfectly during the testing phase, as presented in [1].

X
E
R
T
R
E
P
U
S

E
S
M

|
|

W

|
|

(c) Results for Task 1 with the SUPERTREX algorithm. The target time-series is learned accurately during the
training phase, and is also maintained in a stable manner during testing phase, albeit not as well as FORCE,
as presented in [1].

Figure 2. Comparison of the performances of the MATLAB scripts (left column) and the Python
adaptation (right column) with the results presented in the original article (center column), for the
three learning algorithms on Task 1 [1]. All simulations shown here use the MATLAB default (5489)
as the seed for the random number generator. In each subfigure, the top row shows the target
trajectory (red) with the trajectory generated by the model (blue) throughout the test phase. The
second row shows the error metric (blue) over the simulation (x and y coordinates, in this case),
using the log scale for the y axis. The bottom row shows the progression of the corresponding
weight matrices (SUPERTREX: W1 in purple; W2, in green). The horizontal grey line, in the test
phase, indicates the deviation metric.
ReScience C 7.1 – Sankar et al. 2021

7

10-410-210010−410−210000.20.00.210-410-210010−410−210000.20.00.210-410-210010−410−210000.20.00.21s1s[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

• under the RMHL framework, the target time-series is imitated well by the model
during the training phase, however the weights do not converge, and hence, it is
unable to maintain the time-series in a stable manner during the testing phase.
The mean deviation over eleven simulations, for both the original scripts (0.759 ±
0.284; n=11) and the Python adaptation (0.814 ± 0.288; n=11) is higher than the
threshold of 0.5 (Figure 3a).

• under the SUPERTREX framework, the target time-series is learned accurately and
is also generated in a stable manner, with minor divergences, during testing phase,
owing to the contribution of the pathway based on the FORCE algorithm. The
mean deviation over eleven simulations, for both the original scripts (0.011±0.003;
n=11) and the Python adaptation (0.012 ± 0.005; n=11) is below the threshold of 0.5
and much lower than that with RMHL (Figure 3b).

The MATLAB scripts provided by the authors and the Python adaptation are able to suc-
cessfully closely reproduce the results presented for Task 2 in the paper, with the default
seed as well as with the 10 arbitrary seeds (Figure 3; Table 1, 2).

2.3 Task 3

Here, we compare the performance of the MATLAB scripts and our Python adaptation
on Task 3, for the three algorithms, with the results presented in the article. Task 3 is
an extension of Task 2, designed to test the constraint optimisation ability of these three
algorithms when generating an unknown target from an indirect error signal. Using
the paradigm of a pivoted multi-segmented arm, the objective of this task is to produce
a time-series by generating the angles between the arm segments, while also optimis-
ing the movement cost of each arm segment. Hence, the arm is required to traverse
the butterfly, while carefully choosing the segment to rotate, in order to minimise the
movement cost of its segments. Post the training phase, the readout weights are frozen
and in the SUPERTREX model, the exploratory pathway is deactivated.

The article claims that:

• FORCE canʼt be applied to this task, as explained for Task 2.

• under the RMHL framework, the target time-series is imitated well by the model
during the training phase, however the weights do not converge, and hence, it
poorly maintains the time-series during the testing phase (Figure 4a).

• under the SUPERTREX framework, the performance is much better than RMHL.
The target time-series is learned accurately and is also generated with minor di-
vergences, during testing phase (Figure 4b).

We verify these observations with the MATLAB scripts provided by the authors as well
as with our Python adaptation. To do so, we run the simulations with the default seed
of MATLAB and re-simulate it with ten arbitrary seeds initialising the random number
generator.
We observe that:

• indeed, the FORCE framework is inapplicable to this task, as claimed.

• under the RMHL framework, the target time-series is imitated well by the model
during the training phase, however the weights do not converge, and hence, it
poorly maintains the time-series during the testing phase, as claimed. The mean
deviation over eleven simulations, for both the original scripts (0.850±0.313; n=11)
and the Python adaptation (0.658 ± 0.216; n=11) is higher than the threshold of 0.5.
All eleven simulations with different seeds did not generate the target output in a
satisfactory manner (i.e. deviation > 0.5 for 11/11 seeds) (Figure 4a).

ReScience C 7.1 – Sankar et al. 2021

8

[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

MATLAB

Original

Python

L
H
M
R

1
θ

2
θ

E
S
M

(a) Results for Task 2 with the RMHL algorithm. The target time-series is imitated well by the model during
the training phase (not shown), however, it is unable to maintain the time-series in a stable manner during
the testing phase, in both implementations, as presented in [1].

X
E
R
T
R
E
P
U
S

1
θ

2
θ

E
S
M

(b) Results for Task 2 with the SUPERTREX algorithm. The target time-series is learned accurately during the
training phase, and is also maintained in a stable manner, during the testing phase, in both implementations,
as presented in [1].

Figure 3. Comparison of the performances of original scripts (left column) and Python adaptation
(right column) with the results presented in the original article (center column), for the RMHL and
SUPERTREX, on Task 2 [1]. All simulations shown here use the MATLAB default (5489) as the seed
for the random number generator. In each subfigure, the top row shows the target trajectory (red)
with the trajectory generated by the algorithm (blue) throughout the test phase. The second row
shows the time-series (blue) generated by the model (joint angles (θi), in this case). The bottom
row shows the distance from target metric (blue) over the simulation (x and y coordinates, in this
case), using the log scale for the y axis. The horizontal grey line, in the test phase, indicates the
deviation metric. The grey vertical line marks the separation of the training and testing phase.

ReScience C 7.1 – Sankar et al. 2021

9

10-310-110110−310−110110-310-110110−310−11011s1s[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

MATLAB
Task Model Mean Median

#1

#2

#3

#2ʼ

FORCE
RMHL
ST
RMHL
ST
RMHL
ST
RMHL
ST

0.003
0.168
0.006
0.759
0.011
0.850
0.881
0.846
0.016

0.002
0.165
0.004
0.740
0.010
0.794
0.845
0.807
0.015

Std
0.002
0.038
0.003
0.284
0.003
0.313
0.224
0.299
0.007

Python adaptation

Mean Median
0.004
0.182
0.006
0.814
0.012
0.658
0.837
0.738
0.009

0.003
0.182
0.005
0.799
0.011
0.647
0.827
0.713
0.008

Std
0.003
0.046
0.003
0.288
0.005
0.216
0.241
0.256
0.003

Python re-implementation
Mean Median
0.003
0.201
0.004
0.697
0.010
0.849
0.140
0.839
0.067

Std
0.003
0.053
0.003
0.263
0.004
0.360
0.071
0.310
0.035

0.002
0.203
0.003
0.681
0.009
0.794
0.116
0.794
0.062

Table 1. Deviation metric showing the performance of the original MATLAB scripts, Python adapta-
tion and Python re-implementation on different tasks. Each variant is simulated with the default
seed (5489) and ten additional seeds. The mean, median and standard deviation of the deviation
metric over these eleven simulations are tabulated here. Note that for task #2ʼ, the SUPERTREX
statistics have been computed using only 2 simulations, for the original MATLAB scripts and
Python adaptation. (ST: SUPERTREX; #2ʼ: 3 segment variant of Task 2)

#1

Task Model
FORCE
RMHL
ST
RMHL
ST
RMHL
ST
RMHL
ST

#2ʼ

#2

#3

MATLAB

Satisfactory
11
11
11
0
11
1
5
0
2

Total
11
11
11
11
11
11
11
11
2

Python adaptation
Total
Satisfactory
11
11
11
11
11
11
11
0
11
11
11
2
11
4
11
1
2
2

Python re-implementation
Satisfactory
11
11
11
0
11
0
10
0
11

Total
11
11
11
11
11
11
11
11
11

Table 2. The proportion of model simulations categorised as having satisfactory performance.
Each variant is simulated with the default seed (5489) and ten additional seeds. Number of sat-
isfactory simulations refers to the number of simulations that were below the threshold (0.5) for
the deviation metric. The total number of simulations refer to the number of simulations which
successfully reached completion, without the weights growing exponentially. (ST: SUPERTREX;
2ʼ: 3 segment variant of Task 2)

• under the SUPERTREX framework, the performance is not much better than RMHL,
contrary to the articleʼs claim. The target time-series is not generated in a satisfac-
tory manner, during the testing phase, for more than 50% of the tested simula-
tions (Original scripts: 6/11 and Python adaptation: 7/11). The mean deviation
over eleven simulations, for both the original scripts (0.881 ± 0.224; n=11) and the
Python adaptation (0.837 ± 0.241; n=11) is above the threshold of 0.5 and compa-
rable with that of RMHL (Figure 4b).

The original scripts and the Python adaptation are able to successfully reproduce the
results presented in the paper with the default seed as well as with the 10 arbitrary seeds
for the RMHL algorithm, but not for the SUPERTREX algorithm (Figure 3; Table 1, 2).

3 Modiﬁcation

The Python adaptation is a close adaptation of the original MATLAB scripts provided by
the authors. However, on simulating their performance on the three tasks, we observed
that while, for the first two tasks, the models performed as described in Pyle and Rosen-
baum [1], the performance of the SUPERTREX algorithm on Task 3 was not consistent,
and was dependent on the seed used for the random number generator. On inspecting

ReScience C 7.1 – Sankar et al. 2021

10

[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

MATLAB

Original

Python

L
H
M
R

(a) Results for Task 3 with the RMHL algorithm, using the default seed (5489) for the random number generator.
The target trajectory is imitated well by the model during the training phase (not shown), however, it poorly
maintains the time-series during the testing phase, in both implementations, as presented in [1].

X
E
R
T
R
E
P
U
S

1
θ

2
θ

3
θ

E
S
M

(b) Results for Task 3 with the SUPERTREX algorithm using the default seed (5489) for the random number
generator. The target time-series is learned accurately during the training phase, but is not maintained during
the testing phase, in both implementations, in contrast to the results presented in [1].

X
E
R
T
R
E
P
U
S

(c) Results for Task 3 with the SUPERTREX algorithm using different implementations (MATLAB, left and
Python adaptation, right) and different seeds (295728336, left and 5624282, right) for the random number gen-
erator. The target trajectory is learned accurately during the training phase, and is also maintained in a stable
manner, with slight divergences (Deviation: 295728336: 0.215 ± 0.073; 5624282: 0.190 ± 0.054) , during the
testing phase, in both implementations, similar to the results presented in [1].

Figure 4. Comparison of the performances of original scripts (left column) and Python adaptation
(right column) with the results presented in the original article (center column), for RMHL and
SUPERTREX, on Task 3 [1]. Each subfigure shows the target trajectory (red) with the trajectory
generated by the algorithm (blue) throughout the test phase. In the second subfigure, the the
middle rows show the time-series (blue) generated by the model (joint angles (θi), in this case).
The bottom row shows the distance from target metric (blue) over the simulation (x and y coordi-
nates, in this case), using the log scale for the y axis. The horizontal grey line, in the test phase,
indicates the deviation metric. The grey vertical line marks the separation of the training and
testing phase.
ReScience C 7.1 – Sankar et al. 2021

11

10-110110−11011s1s[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

further, we notice that this was, in some cases, due to the uncontrolled exponential in-
crease in the readout weights.

To look into the robustness of the implementations further, we test the performance of
the RMHL and SUPERTREX algorithms on Task 2 with certain modifications to the task
parameters, specifically, the number of arm segments and the length of the arm seg-
ments. It would be expected for the behaviour to be comparable with the performance
on the original task performance, or undergo a gradual decline. We test Task 2 on the
arm parameters, which were used in Task 3, i.e. by increasing the number of arm seg-
ments from two to three and changing the length of each arm segment. We observe
that RMHL performance is comparable to the original Task 2, wherein the time series
is generated during the training phase, but is not maintained beyond (Original scripts:
0.846 ± 0.299, Python adaptation: 0.738 ± 0.256; n=11). On the other hand, simulations
of the SUPERTREX model, with 2 out of 11 seeds, were able to produce the target out-
put satisfactorily (Original scripts: 0.016 ± 0.007, Python adaptation: 0.009 ± 0.003; n=2)
(Figure 5). However, in simulations with 9 out of 11 seeds, the weights increase exponen-
tially, rendering the simulation unable to progress in a meaningful manner (Table 1, 2).

In order to improve the model performance, make the model more scalable in terms of
task parameters, and also more robust (as seen in Task 3, with respect to reproducibility
with different random seeds), we introduce two minor alterations.

1. We introduce a compensation factor to the update of the readout weights in the
exploratory pathway, inversely proportional to the number of segments. Specifi-
cally, when the number of segments is greater than two, we multiply the weight
update by 0.1/n\_segs for Task 2 and by 0.5/n\_segs for Task 3.

2. The SUPERTREX model transfers the information from the exploratory pathway
to the mastery pathway, only if the error is consistently below a certain threshold.
In the original scripts, this threshold is set at 1.5e-3 for Task 1 and Task 2, while at
1.5e-2 for Task 3. We change the transfer threshold for Task 2 from 1.5e-3 to 1.5e-2.

These slight modifications address the shortcomings we encountered earlier with the
performance of SUPERTREX in Task 2 and 3. Alteration #1, by including a compen-
sation factor for the change in number of arm segments, prevents the weights from
increasing exponentially, and lets the simulation proceed in a meaningful manner. Al-
teration #2, by increasing the error threshold governing the transfer of information to
the mastery pathway, makes the model more tolerant of fluctuations, while continuing
to explore and learn a good solution. Although this does not lead to a critical change
for Task 1 (Original scripts: 0.006 ± 0.003, n=11; Modified Python re-implementation:
0.004 ± 0.003, n=11) and Task 2 (Original scripts: 0.011 ± 0.003, n=11; Modified Python
re-implementation: 0.010±0.004, n=11), this alteration improves the performance of SU-
PERTREX on Task 3 (Original scripts: 0.881±0.224, n=11; Modified Python re-implemen-
tation: 0.140 ± 0.071, n=11). Simulations with 10 out of 11 seeds had satisfactory perfor-
mance (Deviation < 0.5), compared to 6 out of 11 simulations for the original scripts.
Further, it unlocks the potential for the model to be more scalable. We find that with
these alterations, on merely increasing the number of time-steps per training cycle and
with no further fine tuning of hyper-parameters, the model is able to proceed without
an exponential increase in weights over a wider range of task parameters. For instance,
on adding surplus segments with length 0.1 each, the model is able to perform in a sat-
isfactory manner, for most cases, with up to 50 arm segments (Table 3, Figure 6). Better
accuracy can be achieved by further fine tuning of the hyper-parameters.

ReScience C 7.1 – Sankar et al. 2021

12

[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

MATLAB

Python with modification

Simulation breaks in less than 1s

1
θ

2
θ

3
θ

|
|

W

|
|

E
S
M

Figure 5. Robustness of the SUPERTREX model on a Task 2 variant. The performance of the origi-
nal scripts (left column) and modified Python re-implementation (right column) is tested for the
SUPERTREX learning algorithm on a variant of Task 2 with increased number of arm segments
(lengths: 1.8, 1.2, 0.6). The top panel shows the target trajectory (red) with the trajectory gener-
ated by the algorithm (blue) throughout the test phase. The next three rows show the time-series
(blue) generated by the model (joint angles (θi), in this case). The fourth row shows the progres-
sion of the norm of the weight matrix (W1 in purple; W2, in green). The bottom row shows the
distance from target metric (blue) over the simulation, using the log scale for the y axis. The hor-
izontal grey line, in the test phase, indicates the deviation metric. The grey vertical line marks
the separation of the training and testing phase. Using the MATLAB scripts, the readout weights
increase uncontrollably rendering the model unable to learn. The Python re-implementation, us-
ing a compensation factor to harness the weight update, is able to learn and converge to produce
the target time-series.

ReScience C 7.1 – Sankar et al. 2021

13

00.30.00.30.0010.11010−310−11011s1s[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

5 segments

6 segments

7 segments

8 segments

9 segments

10 segments

)
t
(
x

)
t
(
y

E
S
M

)
t
(
x

)
t
(
y

E
S
M

)
t
(
x

)
t
(
y

E
S
M

Figure 6. Scalability of the performance of the modified Python re-implementation using the SU-
PERTREX algorithm on Task 2. The lengths of the arm segments are 1.8, 1.2 and 0.6 for the first
three segments (akin to Task 3) and 0.1 for each additional segment. Here, the simulations for
Task 2 with 5 to 50 segments are shown, all using the default seed 5489 for the random number
generator. (Continued on next page.)

ReScience C 7.1 – Sankar et al. 2021

14

10−310−110110−310−110110−310−110110−310−110110−310−110110−310−1101[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

20 segments

30 segments

40 segments

50 segments

)
t
(
x

)
t
(
y

E
S
M

)
t
(
x

)
t
(
y

E
S
M

Figure 6. (Continued from previous page.) Scalability of the performance of the modified Python
re-implementation using the SUPERTREX algorithm on Task 2. The lengths of the arm segments
are 1.8, 1.2 and 0.6 for the first three segments (akin to Task 3) and 0.1 for each additional segment.
Here, the simulations for Task 2 with 5 to 50 segments are shown, all using the default seed 5489 for
the random number generator. In each subfigure, the top panel shows the produced trajectory,
the middle panels show the evolution of the x and y coordinates of the end-effector of the arm
(blue) throughout the training and test phase, along with the target coordinates (red). The grey
vertical line marks the separation of the training and testing phase. The bottom panel shows the
progression of the distance from target metric (blue) over the simulation, using the log scale for
the y axis. The horizontal grey line, in the test phase, indicates the deviation metric.

ReScience C 7.1 – Sankar et al. 2021

15

10−310−110110−310−110110−310−110110−310−11011s1s[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

Task 2 variant

Deviation metric

No. of segments

Time steps Mean Median Standard Deviation

3
4
5
6
7
8
9
10
15
20
30
40
50

10000
10000
10000
10000
10000
10000
10000
10000
10000
15000
20000
20000
30000

0.057
0.242
0.141
0.181
0.173
0.253
0.331
0.417
0.538
0.366
0.549
0.372
0.375

0.032
0.221
0.080
0.160
0.129
0.230
0.297
0.409
0.512
0.324
0.489
0.313
0.283

0.069
0.136
0.147
0.133
0.130
0.137
0.168
0.151
0.179
0.188
0.236
0.228
0.281

Table 3. Deviation metric showing the performance of the modified Python implementation on
increasing number of segments for Task 2. Each variant is simulated with the default seed (5489)
and ten additional seeds. The mean, median and standard deviation of the deviation metric over
these eleven simulations are tabulated here.

4 Discussion

In this article, we discussed the SUPERTREX model presented by Pyle and Rosenbaum
[1]. We compared the results presented in the paper, both with the results obtained
using the original scripts, and with our modular and user-friendly Python adaptation.
Furthermore, we were able to improve the robustness and scalability of the model with
two minor alterations.
The Python adaptation strives to be a close adaptation of the original scripts in MATLAB
and differs mainly in the method of initialisation of the reservoir connectivity matrix.
This is due to the usage of the function sprandn in the original scripts, whose internal
implementation is not freely available. Figure 7 shows that, on importing initialisation
matrix from MATLAB, the exact same results can be obtained in the Python adaptation,
as well. Most of the details for the implementation of the models are also described in
the paper. Only two necessary details were missing, both concerning the update of the
readout weights of the exploratory pathway in the RMHL and SUPERTREX models: One,
the inclusion of a crucial learning rate of 0.0005, for Tasks 1-3, and two, an additional
compensatory factor of 0.5, for Task 3. There is another discrepancy in the function
psi(x) for Task 3. The scripts provided by the authors use a factor of 0.005, whereas the
article mentions this factor to be 0.025.
The three algorithms (FORCE, RMHL and SUPERTREX) have been tested on three tasks,
presented in Pyle and Rosenbaum [1]. For Task 1 and 2, we verify that the three algo-
rithms function as presented in the paper, and validate that our Python re-implemen-
tation produces comparable results. For Task 3, the SUPERTREX modelʼs behaviour is
also reproducible, although the performance is dependent on the seed used for the ran-
dom number generator. Furthermore, we observed that this implementation is quite
sensitive to changes in task parameters, such as the number of arms. This was due to
the uninhibited increase in the readout weights. We propose the inclusion of a compen-
sation factor for the number of arm segments, which inhibits the growth of the readout
weights, and allows the simulation to proceed in a meaningful manner. This consider-
ably improves the robustness and the scalability of the original model.

ReScience C 7.1 – Sankar et al. 2021

16

[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

We conclude that the results presented in the paper are reproducible for two tasks, using
the original MATLAB scripts provided by the authors, and also, replicable in Python for
all tasks with comparable performance.

MATLAB

Python

)
t
(
x

)
t
(
y

|
|

W

|
|

E
S
M

Figure 7. Similarity between the original scripts and the Python adaptation. The performance of
the original scripts (left column) and the Python adaptation (right column) is tested for the RMHL
learning algorithm on a Task 1. The reservoir connectivity matrix for the Python simulation was
initialised using the MATLAB equivalent. Using this initialisation, the progression of the Python
simulation is identical to that of the MATLAB simulation. The top panel shows the target trajectory
(red) with the trajectory generated by the model (blue) throughout the test phase. The next two
rows show the time-series (blue) generated by the model (x and y coordinates, in this case). The
third row shows the progression of the norm of the weight matrix. The bottom row shows the
distance from target metric (blue) over the simulation, using the log scale for the y axis. The
horizontal grey line, in the test phase, indicates the deviation metric. The grey vertical line marks
the separation of the training and testing phase.

ReScience C 7.1 – Sankar et al. 2021

17

00.20.00.210-310-110110−310−11011s1s[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity

References

1.

2.

R. Pyle and R. Rosenbaum. “A reservoir computing model of reward-modulated motor learning and automatic-
ity.” In: Neural computation 31.7 (2019), pp. 1430–1461.
D. Sussillo and L. F. Abbott. “Generating coherent patterns of activity from chaotic neural networks.” In: Neuron
63.4 (2009), pp. 544–557.

3. G. M. Hoerzer, R. Legenstein, and W. Maass. “Emergence of complex computational structures from chaotic

neural networks through reward-modulated Hebbian learning.” In: Cerebral cortex 24.3 (2014), pp. 677–690.

4. M. S. Brainard and A. J. Doupe. “What songbirds teach us about learning.” In: Nature 417.6886 (2002), pp. 351–

5.

358.
B. P. Ölveczky, T. M. Otchy, J. H. Goldberg, D. Aronov, and M. S. Fee. “Changes in the neural control of a complex
motor sequence during learning.” In: Journal of neurophysiology 106.1 (2011), pp. 386–397.

6. M. Matsumoto and T. Nishimura. “Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random
number generator.” In: ACM transactions on modeling and computer simulation (TOMACS) 8.1 (1998), pp. 3–
30.

ReScience C 7.1 – Sankar et al. 2021

18

