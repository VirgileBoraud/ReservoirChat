Deep Neural Networks and Brain Alignment: Brain
Encoding and Decoding (Survey)
Subba Reddy Oota, Manish Gupta, Raju S. Bapi, Gael Jobard, Frédéric

Alexandre, Xavier Hinaut

To cite this version:

Subba Reddy Oota, Manish Gupta, Raju S. Bapi, Gael Jobard, Frédéric Alexandre, et al.. Deep
Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). 2023. ￿hal-04162064￿

HAL Id: hal-04162064

https://hal.science/hal-04162064

Preprint submitted on 14 Jul 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding
(Survey)

Subba Reddy Oota1,2 , Manish Gupta3,4 , Raju S. Bapi3 , Gael Jobard2
Frederic Alexandre1,2 , Xavier Hinaut1,2
1INRIA, Bordeaux, France, 2University of Bordeaux, France, 3IIIT Hyderabad, India, 4Microsoft,
Hyderabad, India
subba-reddy.oota@inria.fr, gmanish@microsoft.com, raju.bapi@iiit.ac.in, gael.jobard@u-bordeaux.fr,
frederic.alexandre@inria.fr, xavier.hinaut@inria.fr

Abstract
How does the brain represent different modes of
information? Can we design a system that au-
tomatically understands what the user is think-
ing? Such questions can be answered by study-
ing brain recordings like functional magnetic res-
onance imaging (fMRI). As a first step, the neu-
roscience community has contributed several large
cognitive neuroscience datasets related to passive
reading/listening/viewing of concept words, narra-
tives, pictures and movies. Encoding and decod-
ing models using these datasets have also been pro-
posed in the past two decades. These models serve
as additional tools for basic research in cognitive
science and neuroscience. Encoding models aim
at generating fMRI brain representations given a
stimulus automatically. They have several practi-
cal applications in evaluating and diagnosing neu-
rological conditions and thus also help design ther-
apies for brain damage. Decoding models solve
the inverse problem of reconstructing the stim-
uli given the fMRI. They are useful for designing
brain-machine or brain-computer interfaces.
In-
spired by the effectiveness of deep learning mod-
els for natural language processing, computer vi-
sion, and speech, recently several neural encoding
and decoding models have been proposed. In this
survey, we will first discuss popular representations
of language, vision and speech stimuli, and present
a summary of neuroscience datasets. Further, we
will review popular deep learning based encoding
and decoding architectures and note their benefits
and limitations. Finally, we will conclude with a
brief summary and discussion about future trends.
Given the large amount of recently published work
in the ‘computational cognitive neuroscience’ com-
munity, we believe that this survey nicely organizes
the plethora of work and presents it as a coherent
story.

1 Introduction
Neuroscience is the field of science that studies the structure
It
and function of the nervous system of different species.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

43

42

56

55

52

47

48

50

57

54

53

51

49

involves answering interesting questions like the following1.
(1) How learning occurs during adolescence, and how it dif-
fers from the way adults learn and form memories. (2) Which
44
specific cells in the brain (and what connections they form 45
with other cells), have a role in how memories are formed?
46
(3) How animals cancel out irrelevant information arriving
from the senses and focus only on information that matters.
(4) How do humans make decisions? (5) How humans de-
velop speech and learn languages. Neuroscientists study di-
verse topics that help us understand how the brain and ner-
vous system work.
Motivation: The central aim of neuroscience is to unravel
how the brain represents information and processes it to carry
out various tasks (visual, linguistic, auditory, etc.). Deep neu-
ral networks (DNN) offer a computational medium to cap-
ture the unprecedented complexity and richness of brain ac-
tivity. Encoding and decoding stated as computational prob-
lems succinctly encapsulate this puzzle. As the previous sur-
veys systematically explore the brain encoding and decod-
ing studies with respect to only language [Cao et al., 2021;
Karamolegkou et al., 2023],
this survey summarizes the
latest efforts in how DNNs begin to solve these problems
and thereby illuminate the computations that the unreachable
brain accomplishes effortlessly.
Brain encoding and decoding: Two main tasks studied in
cognitive neuroscience are brain encoding and brain decod-
ing, as shown in Figure 1. Encoding is the process of learn-
ing the mapping e from the stimuli S to the neural activation
F . The mapping can be learned using features engineering or
deep learning. On the other hand, decoding constitutes learn-
ing mapping d, which predicts stimuli S back from the brain
activation F . However, in most cases, brain decoding aims
at predicting a stimulus representation R rather than actually
reconstructing S. In both cases, the first step is to learn a se-
mantic representation R of the stimuli S at the train time.
Next, for encoding, a regression function e : R → F is
trained. For decoding, a function d : F → R is trained.
These functions e and d can then be used at test time to pro-
cess new stimuli and brain activations, respectively.
Techniques for recording brain activations: Popular tech-
niques for recording brain activations include single Micro-

60

62

67

68

61

66

69

65

58

59

63

64

70

71

77

74

79

81

76

82

78

72

80

75

73

1https://zuckermaninstitute.columbia.edu/file/5184/download?

token=qzId8vyR

Figure 1: Computational Cognitive Neuroscience of Brain Encoding and Decoding: Datasets & Stimulus Representations

83

84

85

86

87

88

89

90

91

92

93

94

95

96

97

98

99

100

101

102

103

104

105

106

107

108

109

110

111

112

113

Electrode (ME), Micro-Electrode array (MEA), Electro-
Cortico Graphy (ECoG), Positron emission tomography
(PET), functional MRI (fMRI), Magneto-encephalography
(MEG), Electro-encephalography (EEG) and Near-Infrared
Spectroscopy (NIRS). These techniques differ in their spatial
resolution of neural recording and temporal resolution.

fMRIs enable high spatial but low time resolution. Hence,
they are good for examining which parts of the brain handle
critical functions. fMRI takes 1-4 seconds to complete a scan.
This is far lower than the speed at which humans can process
language. On the other hand, both MEG and EEG have high
time but low spatial resolution. They can preserve rich syn-
tactic information [Hale et al., 2018] but cannot be used for
source analysis. fNIRS are a compromise option. Their time
resolution is better than fMRI, and spatial resolution is bet-
ter than EEG. However, this spatial and temporal resolution
balance may not compensate for the loss in both.
Stimulus Representations: Neuroscience datasets contain
stimuli across various modalities: text, visual, audio, video
and other multimodal forms. Representations differ based on
modality. Older methods for text-based stimulus representa-
tion include text corpus co-occurrence counts, topic models,
syntactic, and discourse features.
In recent times, both se-
mantic and experiential attribute models have been explored
for text-based stimuli. Semantic representation models in-
clude distributed word embeddings, sentence representation
models, recurrent neural networks (RNNs), and Transformer-
based language models. Experiential attribute models rep-
resent words in terms of human ratings of their degree of
association with different attributes of experience, typically
on a scale of 0-6 or binary. Older methods for visual stim-

115

114

119

ulus representation used visual field filter bank and Gabor
wavelet pyramid for visual stimuli, but recent methods use
models like ImageNet-pretrained convolutional neural net-
116
works (CNNs) and concept recognition methods. For audio 117
stimuli, phoneme rate and the presence of phonemes have
118
been leveraged, besides deep learning models like Sound-
Net. Finally, for multimodal stimulus representations, re-
120
searchers have used both early fusion and late fusion deep 121
learning methods. In the early fusion methods, information 122
across modalities is combined in the early steps of process-
123
ing. While in late fusion, the combination is performed only 124
at the end. We discuss stimulus representation methods in 125
detail in Sec. 2.
126
Naturalistic Neuroscience Datasets: Several neuroscience
datasets have been proposed across modalities (see Figure 2).
These datasets differ in terms of the following criteria: (1)
Method for recording activations: fMRI, EEG, MEG, etc. (2)
Repetition time (TR), i.e. the sampling rate. (3) Character-
istics of fixation points: location, color, shape. (4) Form of
stimuli presentation: text, video, audio, images, or other mul-
timodality. (5) Task that participant performs during record-
134
ing sessions: question answering, property generation, rating 135
quality, etc. (6) Time given to participants for the task, e.g.,
136
1 minute to list properties. (7) Demography of participants:
males/females, sighted/blind, etc. (8) Number of times the re-
sponse to stimuli was recorded. (9) Natural language associ-
ated with the stimuli. We discuss details of proposed datasets
in Sec. 3.
Brain Encoding: Other than using the standard stimuli repre-
142
sentation architectures, brain encoding literature has focused 143
on studying a few important aspects: (1) Which models lead 144

127

128

129

140

141

137

132

133

131

139

138

130

Stimulus Repr.EncodingDecodingNeuroscience DatasetsStimulus RepresentationsFigure 2: Representative Samples of Naturalistic Brain Dataset: (LEFT) Brain activity recorded when subjects are reading and listening to
the same narrative (Deniz et al. 2019), and (RIGHT) example naturalistic image stimuli from various public repositories: BOLD5000 (Chang
et al. 2019), SSfMRI (Beliy et al., 2019), and VIM-1 (Kay et al., 2008).

145

146

147

148

149

150

151

152

153

154

155

156

157

158

159

160

161

162

163

164

165

(2) How
to better predictive accuracy across modalities?
can we disentangle the contributions of syntax and seman-
tics from language model representations to the alignment
between brain recordings and language models? (3) Why do
some representations lead to better brain predictions? How
are deep learning models and brains aligned in terms of their
information processing pipelines? (4) Does joint encoding
of task and stimulus representations help? We discuss these
details of encoding methods in Sec. 5.
Brain Decoding: Ridge regression is the most popular brain
decoder. Recently, a fully connected layer [Beliy et al., 2019]
or multi-layered perceptrons (MLPs) [Sun et al., 2019] have
also been used. While older methods attempted to decode to
a vector representation using stimuli of a single mode, newer
methods focus on multimodal stimuli decoding [Pereira et
al., 2016; Oota et al., 2022c]. Decoding using Transform-
ers [Gauthier and Levy, 2019; Toneva and Wehbe, 2019;
D´efossez et al., 2022; Tang et al., 2022], and decoding to ac-
tual stimuli (word, passage, image, dialogues) have also been
explored. We discuss details of these decoding methods in
Sec. 6.

Figure 3: Alignment between deep learning systems and human
brains [Toneva et al. 2019].

Computational Cognitive Science (CCS) Research goals:
CCS researchers have primarily focused on two main ar-
167
eas [Doerig et al., 2022] (also, see Figure 3). (1) Improving 168
predictive Accuracy. In this area, the work is around the fol-
169
lowing questions. (a) Compare feature sets: Which feature
set provides the most faithful reflection of the neural repre-
sentational space? (b) Test feature decodability: “Does neu-

171

172

170

166

218

219

220

221

222

227

223

degree of association with different attributes of experience,
typically on a scale of 0-6 [Anderson et al., 2019; Ander-
son et al., 2020; Berezutskaya et al., 2020; Just et al., 2010;
Anderson et al., 2017b] or binary [Handjaras et al., 2016;
Wang et al., 2017].
Visual Stimulus Representations: For visual stimuli, older
methods used visual field filter bank [Thirion et al., 2006;
224
Nishimoto et al., 2011] and Gabor wavelet pyramid [Kay 225
et al., 2008; Naselaris et al., 2009]. Recent methods use
226
models like CNNs [Du et al., 2020; Beliy et al., 2019;
Anderson et al., 2017a; Yamins et al., 2014; Nishida et
al., 2020] and concept recognition models [Anderson et al.,
2020].
Audio Stimuli Representations: For audio stimuli, phoneme
rate and presence of phonemes have been leveraged [Huth et
232
al., 2016]. Recently, authors in [Nishida et al., 2020] used 233
features from an audio deep learning model called SoundNet
234
for audio stimuli representation.
Multimodal Stimulus Representations: To jointly model
the information from multimodal stimuli, recently, various
multimodal representations have been used. These include
processing videos using audio+image representations like
VGG+SoundNet [Nishida et al., 2020] or using image+text
240
combination models like GloVe+VGG and ELMo+VGG 241
in [Wang et al., 2020]. Recently, the usage of multimodal
242
text+vision models like CLIP, LXMERT, and VisualBERT 243
was proposed in [Oota et al., 2022d].
244

238

239

228

231

230

229

237

236

235

3 Naturalistic Neuroscience Datasets

245

256

255

254

We discuss the popular text, visual, audio, video and other
246
multimodal neuroscience datasets that have been proposed 247
in the literature. Table 1 shows a detailed overview of brain 248
recording type, language, stimulus, number of subjects (|S|)
249
and the task across datasets of different modalities. Figure 2 250
shows examples from a few datasets.
251
Text Datasets: These datasets are created by presenting 252
words, sentences, passages or chapters as stimuli. Some of
253
the text datasets include Harry Potter Story [Wehbe et al.,
2014], ZUCO EEG [Hollenstein et al., 2018] and datasets
proposed in [Handjaras et al., 2016; Anderson et al., 2017a;
Anderson et al., 2019; Wehbe et al., 2014]. In [Handjaras et
257
al., 2016], participants were asked to verbally enumerate in 258
one minute the properties (features) that describe the entities
259
the words refer to. There were four groups of participants: 5 260
sighted individuals were presented with a pictorial form of the
261
nouns, 5 sighted individuals with a verbal-visual (i.e., written 262
Italian words) form, 5 sighted individuals with a verbal au-
263
ditory (i.e., spoken Italian words) form, and 5 congenitally 264
blind with a verbal auditory form. Data proposed by [An-
265
derson et al., 2017a] contains 70 Italian words taken from 266
seven taxonomic categories (abstract, attribute, communica-
267
tion, event/action, person/social role, location, object/tool) in 268
the law and music domain. The word list contains concrete
269
as well as abstract words. ZUCO dataset [Hollenstein et al.,
2018] contains sentences for which fMRIs were obtained for
3 tasks: normal reading of movie reviews, normal reading of
Wikipedia sentences and task-specific reading of Wikipedia
273
sentences. For this dataset curation, sentences were presented 274

272

270

271

Figure 4: Language Model

173

174

175

176

177

178

179

180

181

182

183

184

185

186

187

188

189

190

191

192

193

194

195

196

197

198

199

200

201

202

203

204

205

206

207

208

209

210

211

212

213

214

215

216

217

ral data Y contain information about features X?” (c) Build
accurate models of brain data: The aim is to enable simula-
tion of neuroscience experiments. (2) Interpretability. In this
area, the work is around the following questions. (a) Examine
individual features: Which features contribute most to neural
activity? (b) Test correspondences between representational
spaces: “CNNs vs ventral visual stream” or “Two text rep-
resentations”. (c) Interpret feature sets: Do features X, gen-
erated by a known process, accurately describe the space of
neural responses Y? Do voxels respond to a single feature or
exhibit mixed selectivity? (d) How does the mapping relate to
other models or theories of brain function? We discuss some
of these questions in Sections 5 and 6.

Older methods

2 Stimulus Representations
In this section, we discuss types of stimulus representations
that have been proposed in the literature across different
modalities: text, visual, audio, video and other multimodal
stimuli.
Text Stimulus Representations:
for
text-based stimuli representation include text corpus co-
occurrence counts [Mitchell et al., 2008; Pereira et al., 2013;
Huth et al., 2016], topic models [Pereira et al., 2013], syn-
tactic features and discourse features [Wehbe et al., 2014].
In recent times, for text-based stimuli, both semantic mod-
els as well as experiential attribute models have been ex-
plored. Semantic representation models include word em-
bedding methods [Pereira et al., 2018; Wang et al., 2020;
Pereira et al., 2016; Toneva and Wehbe, 2019; Anderson
et al., 2017a; Oota et al., 2018], sentence representation
models (see Figure 4) [Sun et al., 2020; Sun et al., 2019;
Toneva and Wehbe, 2019], RNNs [Jain and Huth, 2018;
Oota et al., 2019] and Transformer methods [Gauthier and
Levy, 2019; Toneva and Wehbe, 2019; Schwartz et al., 2019;
Schrimpf et al., 2021a; Antonello et al., 2021; Oota et
al., 2022b; Aw and Toneva, 2022].
Popular word em-
bedding methods include textual (i.e., Word2Vec, fastText,
and GloVe), linguistic (i.e., dependency), conceptual (i.e.,
RWSGwn and ConceptNet), contextual (i.e., ELMo). Pop-
ular sentence embedding models include average, max, con-
cat of avg and max, SIF, fairseq, skip, GenSen, InferSent,
ELMo, BERT, RoBERTa, USE, QuickThoughts and GPT-
2. Transformer-based methods include pretrained BERT with
various NLU tasks, finetuned BERT, Transformer-XL, GPT-
2, BART, BigBird, LED, and LongT5. Experiential attribute
models represent words in terms of human ratings of their

Dataset
Harry Potter

Authors
[Wehbe et al., 2014]

t
x
e
T

ZuCo
240 Sentences with Con-
tent Words
BCCWJ-EEG
Subset Moth Radio Hour

Vim-1
Generic Object Decoder

l
a
u
s
i
V

BOLD5000
Algonauts

NSD
THINGS

The Moth Radio Hour

Narratives
Natural Stories
The Little Prince
MEG-MASC
BBC’s Doctor Who

[Handjaras et al., 2016]
[Anderson et al., 2017a]
[Hollenstein et al., 2018]
[Anderson et al., 2019]

[Oseki and Asahara, 2020]
[Deniz et al., 2019]
[Thirion et al., 2006]

[Kay et al., 2008]
[Horikawa and Kamitani,
2017]
[Chang et al., 2019]
[Cichy et al., 2019]

[Allen et al., 2022]
[Hebart et al., 2022]

[Handjaras et al., 2016]
[Huth et al., 2016]
[Brennan and Hale, 2019]

[Anderson et al., 2020]
[Nastase et al., 2021]
[Zhang et al., 2020]
[Li et al., 2021]
[Gwilliams et al., 2022]
[Seeliger et al., 2019]

Table 1: Naturalistic Neuroscience Datasets

Stimulus

Lang.
English Reading Chapter 9 of Harry Potter and the Sorcerer’s Stone

Type
fMRI/
MEG
fMRI
fMRI
EEG
fMRI English Reading 240 active voice sentences describing everyday situations

Italian Verbal, pictorial or auditory presentation of 40 concrete nouns, four times
Italian
English Reading 1107 sentences with 21,629 words from movie reviews

Reading 70 concrete and abstract nouns from law/music, five times

fMRI
fMRI

-
-

Japanese Reading 20 newspaper articles for ∼30-40 minutes

EEG
fMRI English Reading 11 stories
fMRI

-

Viewing rotating wedges (8 times), expanding/contracting rings (8
times), rotating 36 Gabor filters (4 times), grid (36 times)
Viewing sequences of 1870 natural photos
Viewing 1,200 images from 150 object categories; 50 images from 50
object categories; imagery 10 times
Viewing 5254 images depicting real-world scenes
Viewing 92 silhouette object images and 118 images of objects on natural
background
Viewing 73000 natural scenes
Viewing 31188 natural images

-
-

-
-

fMRI
fMRI/
MEG
fMRI
fMRI/
MEG
fMRI
fMRI English Listening eleven 10-minute stories
EEG

English Listening Chapter one of Alice’s Adventures in Wonderland (2,129

words in 84 sentences) as read by Kristen McQuillan

fMRI English Listening one of 20 scenario names, 5 times
fMRI English Listening 27 diverse naturalistic spoken stories. 891 functional scans
fMRI English Listening Moth-Radio-Hour naturalistic spoken stories.
fMRI English Listening audiobook for about 100 minutes.
MEG English Listening two hours of naturalistic stories. 208 MEG sensors
fMRI English Viewing spatiotemporal visual and auditory videos (30 episodes). 120.8
whole-brain volumes (∼23 h) of single-presentation data, and 1.2 vol-
umes (11 min) of repeated narrative short episodes. 22 repetitions

Italian Verbal, pictorial or auditory presentation of 40 concrete nouns, 4 times

|S| Task
9

Story understanding

Imagine a situation with noun

20 Property Generation
7
12 Rate movie quality
14 Passive reading

40 Passive reading
9
9

Passive reading and Listening
Passive viewing

2
5

Passive viewing
Repetition detection

4
Passive viewing
15 Passive viewing

8
8

Passive viewing
Passive viewing

20 Property Generation
7
Passive Listening
33 Question answering

Imagine personal experiencs

26
345 Passive Listening
19 Passive Listening
112 Passive Listening
27 Passive Listening
1

Passive viewing

Japanese Ads

[Nishida et al., 2020]

fMRI

Japanese Viewing 368 web and 2452 TV Japanese ad movies (15-30s). 7200 train

52 Passive viewing

Pippi Langkous

[Berezutskaya et al., 2020] ECoG Swedish/

Dutch

and 1200 test fMRIs for web; fMRIs from 420 ads.
Viewing 30 s excerpts of a feature film (in total, 6.5 min long), edited
together for a coherent story

37 Passive viewing

Algonauts
Natual Short Clips
Natual Short Clips
60 Concrete Nouns

[Cichy et al., 2021]
[Huth et al., 2022]
[Lahner et al., 2023]
[Mitchell et al., 2008]
[Sudre et al., 2012]

fMRI English Viewing 1000 short video clips (3 sec each)
fMRI English Watching natural short movie clips
fMRI English Watching 1102 natural short video clips
fMRI English Viewing 60 different word-picture pairs from 12 categories, 6 times each
MEG English Reading 60 concrete nouns along with line drawings. 20 questions per

10 Passive viewing
Passive viewing
5
10 Passive viewing
9
Passive viewing
9 Question answering

Pereira

[Pereira et al., 2018]

fMRI English Viewing 180 Words with Picture, Sentences, word clouds; reading 96

16 Passive viewing and reading

[Zinszer et al., 2018]

fNIRS English 8 concrete nouns (audiovisual word and picture stimuli): bunny, bear,

24 Passive viewing and listening

kitty, dog, mouth, foot, hand, and nose; 12 times repeated.

Neuromod

[Cao et al., 2021]
[Boyle et al., 2020]

fNIRS Chinese Viewing and listening 50 concrete nouns from 10 semantic categories.
fMRI English Watching TV series (Friends, Movie10)

7
6

Passive viewing and listening
Passive viewing and listening

text passages; 72 passages. 3 times repeated.

noun lead to 1200 examples.

o
i
d
u
A

o
e
d
i
V

l
a
d
o
m

i
t
l
u
M

r
e
h
t
O

275

276

277

278

279

280

281

282

283

284

285

286

287

288

289

290

291

292

293

to the subjects in a naturalistic reading scenario. A complete
sentence is presented on the screen. Subjects read each sen-
tence at their own speed, i.e., the reader determines for how
long each word is fixated and which word to fixate next.

Visual Datasets: Older visual datasets were based on binary
visual patterns [Thirion et al., 2006]. Recent datasets con-
tain natural images. Examples include Vim-1 [Kay et al.,
2008], BOLD5000 [Chang et al., 2019], Algonauts [Cichy
et al., 2019], NSD [Allen et al., 2022], Things-data[Hebart et
al., 2022], and the dataset proposed in [Horikawa and Kami-
tani, 2017]. BOLD5000 includes ∼20 hours of MRI scans
per each of the four participants. 4,916 unique images were
used as stimuli from 3 image sources. Algonauts contains two
sets of training data, each consisting of an image set and brain
activity in RDM format (for fMRI and MEG). Training set 1
has 92 silhouette object images, and training set 2 has 118
object images with natural backgrounds. Testing data con-
sists of 78 images of objects on natural backgrounds. Most
of the visual datasets involve passive viewing, but the dataset

in [Horikawa and Kamitani, 2017] involved the participant
doing the one-back repetition detection task.

294

295

298

296

297

Audio Datasets: Most of the proposed audio datasets are
in English [Huth et al., 2016; Brennan and Hale, 2019;
Anderson et al., 2020; Nastase et al., 2021], while there is
one [Handjaras et al., 2016] on Italian. The participants were
involved in a variety of tasks while their brain activations
were measured: Property generation [Handjaras et al., 2016],
passive listening [Huth et al., 2016; Nastase et al., 2021],
302
question answering [Brennan and Hale, 2019] and imagining 303
themselves personally experiencing common scenarios [An-
304
derson et al., 2020]. In the last one, participants underwent
fMRI as they reimagined the scenarios (e.g., resting, reading,
writing, bathing, etc.) when prompted by standardized cues.
Narratives [Nastase et al., 2021] used 17 different stories as
stimuli. Across subjects, it is 6.4 days worth of recordings.

309

306

308

305

307

301

300

299

Video Datasets: Recently, video neuroscience datasets have
also been proposed. These include BBC’s Doctor Who [Seel-
iger et al., 2019], Japanese Ads [Nishida et al., 2020], Pippi

310

311

312

Figure 5: Evaluation Metrics for Brain Encoding and Decoding. (LEFT) Pearson Correlation, (MIDDLE) 2V2 Accuracy [Toneva et al.
2020], and (RIGHT) Pairwise Accuracy.

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

Langkous [Anderson et al., 2020] and Algonauts [Cichy et
al., 2021]. Japanese Ads data contains data for two sets of
movies were provided by NTT DATA Corp: web and TV ads.
There are also four types of cognitive labels associated with
the movie datasets: scene descriptions, impression ratings,
ad effectiveness indices, and ad preference votes. Algonauts
2021 contains fMRIs from 10 human subjects that watched
over 1,000 short (3 sec) video clips.
Other Multimodal Datasets: Finally, beyond the video
datasets, datasets have also been proposed with other kinds
of multimodality. These datasets are audiovisual ([Zinszer
et al., 2018; Cao et al., 2021]), words associated with line
drawings [Mitchell et al., 2008; Sudre et al., 2012], pictures
along with sentences and word clouds [Pereira et al., 2018].
These datasets have been collected using a variety of meth-
ods like fMRIs [Mitchell et al., 2008; Pereira et al., 2018],
MEG [Sudre et al., 2012] and fNIRS [Zinszer et al., 2018;
Cao et al., 2021]. Specifically, in [Sudre et al., 2012], sub-
jects were asked to perform a QA task, while their brain ac-
tivity was recorded using MEG. Subjects were first presented
with a question (e.g., “Is it manmade?”), followed by 60 con-
crete nouns, along with their line drawings, in a random or-
der. For all other datasets, subjects performed passive view-
ing and/or listening.

4 Evaluation Metrics
Two metrics are popularly used to evaluate brain encoding
models: 2V2 accuracy [Toneva et al., 2020; Oota et al.,
2022b] and Pearson Correlation [Jain and Huth, 2018], as
shown in Figure 5.

They are defined as follows. Given a subject and a brain
region, let N be the number of samples. Let {Yi}N
i=1 and
{ ˆYi}N
i=1 denote the actual and predicted voxel value vectors
for the ith sample. Thus, Y ∈ RN ×V and ˆY ∈ RN ×V
where V is the number of voxels in that region. 2V2 Accu-
(cid:80)N −1
j=i+1 I[{cosD(Yi, ˆYi) +
racy is computed as
i=1
cosD(Yj, ˆYj)} < {cosD(Yi, ˆYj) + cosD(Yj, ˆYi)}] where
cosD is the cosine distance function. I[c] is an indicator func-
tion such that I[c] = 1 if c is true, else it is 0. The higher
the 2V2 accuracy, the better. Pearson Correlation is com-
i=1 corr[Yi, ˆYi] where corr is the correla-
puted as PC= 1
N

1
NC2

(cid:80)N

(cid:80)n

tion function.

353

354

355

356

358

357

359

360

361

Brain decoding methods are evaluated using popular met-
rics like pairwise and rank accuracy [Pereira et al., 2018;
Oota et al., 2022c]. Other metrics used for brain decod-
ing evaluation include R2 score, mean squared error, and us-
ing Representational Similarity Matrix [Cichy et al., 2019;
Cichy et al., 2021].
Pairwise Accuracy To measure the pairwise accuracy, the
first step is to predict all the test stimulus vector representa-
tions using a trained decoder model. Let S = [S0, S1,· · · ,Sn],
ˆS = [ ˆS0, ˆS1,· · · , ˆSn] denote the “true” (stimuli-derived) and 363
predicted stimulus representations for n test instances resp.
364
Given a pair (i, j) such that 0 ≤ i, j ≤ n, score is 1 365
if corr(Si, ˆSi) + corr(Sj, ˆSj) > corr(Si, ˆSj) + corr(Sj, ˆSi),
else 0. Here, corr denotes the Pearson correlation. Fi-
nal pairwise matching accuracy per participant is the aver-
age of scores across all pairs of test instances. For com-
puting rank accuracy, we first compare each decoded vector
370
to all the “true” stimuli-derived semantic vectors and ranked 371
them by their correlation. The classification performance re-
372
flects the rank r of the stimuli-derived vector for the correct
word/picture/stimuli: 1 −
value for each participant is the average rank accuracy across
all instances.

#instances−1 . The final accuracy 374

r−1

375

369

373

367

366

376

368

362

377

5 Brain Encoding
Encoding is the learning of the mapping from the stimulus
378
domain to the neural activation. The quest in brain encoding 379
is for “reverse engineering” the algorithms that the brain uses
380
for sensation, perception, and higher-level cognition. Recent
breakthroughs in applied NLP enable reverse engineering the
language function of the brain. Similarly, pioneering results
have been obtained for reverse engineering the function of
ventral visual stream in object recognition founded on the ad-
vances and remarkable success of deep CNNs. The overall
schema of building a brain encoder is shown in Figure 6.

383

386

381

387

385

382

384

Initial studies on brain encoding focused on smaller data
sets and single modality of brain responses. Early mod-
els used word representations [Hollenstein et al., 2019].
390
Rich contextual representations derived from RNNs such 391
as LSTMs resulted in superior encoding models [Jain and 392
Huth, 2018; Oota et al., 2019] of narratives. The recent
393

389

388

Figure 6: Schema for Brain Encoding

394

395

396

397

398

399

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

efforts are aimed at utilizing the internal representations
extracted from transformer-based language models such as
ELMo, BERT, GPT-2, etc for learning encoding models of
brain activation [Jat et al., 2020; Caucheteux et al., 2021;
Antonello et al., 2021]. High-grain details such as lexical,
compositional, syntactic, and semantic representations of nar-
ratives are factorized from transformer-based models and uti-
lized for training encoding models. The resulting models are
better able to disentangle the corresponding brain responses
in fMRI [Caucheteux et al., 2021]. Finally, is has been found
that the models that integrate task and stimulus representa-
tions have significantly higher prediction performance than
models that do not account for the task semantics [Toneva et
al., 2020; Schrimpf et al., 2021a].

Similarly, in vision, early models focused on indepen-
dent models of visual processing (object classification) us-
ing CNNs [Yamins et al., 2014]. Recent efforts in visual en-
coding models focus on using richer visual representations
derived from a variety of computer vision tasks [Wang et
al., 2019]. Instead of feed-forward deep CNN models, us-
ing shallow recurrence enabled better capture of temporal dy-
namics in the visual encoding models [Kubilius et al., 2019;
Schrimpf et al., 2020].

Table 2 summarizes various encoding models proposed in
the literature related to textual, audio, visual, and multimodal
stimuli. Figure 7 classifies the encoding literature along var-
ious stimulus domains such as vision, auditory, multimodal,
and language and the corresponding tasks in each domain.
Linguistic Encoding: A number of previous works have in-
vestigated the alignment between pretrained language mod-
els and brain recordings of people comprehending language.
Huth et al. [2016] have been able to identify brain ROIs (Re-
gions of Interest) that respond to words that have a similar
meaning and have thus built a “semantic atlas” of how the
human brain organizes language. Many studies have shown
accurate results in mapping the brain activity using neural
[Ander-
distributed word embeddings for linguistic stimuli
son et al., 2017a; Pereira et al., 2018; Oota et al., 2018;
Nishida and Nishimoto, 2018; Sun et al., 2019]. Unlike ear-

433

434

435

436

lier models where each word is represented as an indepen-
dent vector in an embedding space, [Jain and Huth, 2018]
built encoding models using rich contextual representations
derived from an LSTM language model in a story listen-
ing task. With these contextual representations, they demon-
strated dissociation in brain activation – auditory cortex (AC)
and Broca’s area in shorter context whereas left Temporo-
Parietal junction (TPJ) in longer context. [Hollenstein et al.,
2019] presents the first multimodal framework for evaluat-
ing six types of word embedding (Word2Vec, WordNet2Vec,
GloVe, FastText, ELMo, and BERT) on 15 datasets, includ-
ing eye-tracking, EEG and fMRI signals recorded during lan-
guage processing. With the recent advances in contextual rep-
445
resentations in NLP, few studies incorporated them in relating 446
sentence embeddings with brain activity patterns [Sun et al.,
447
2020; Gauthier and Levy, 2019; Jat et al., 2020].

443

438

439

442

441

437

440

448

444

449

More recently, researchers have begun to study the align-
ment of language regions of the brain with the layers of lan-
450
guage models and found that the best alignment was achieved 451
in the middle layers of these models [Jain and Huth, 2018;
452
Toneva and Wehbe, 2019]. Schrimpf et al. [2021a] examined 453
the relationship between 43 diverse state-of-the-art language
454
models. They also studied the behavioral signatures of human 455
language processing in the form of self-paced reading times,
456
and a range of linguistic functions assessed via standard engi-
457
neering tasks from NLP. They found that Transformer-based 458
models perform better than RNNs or word-level embedding 459
models. Larger-capacity models perform better than smaller
460
models. Models initialized with random weights (prior to 461
training) perform surprisingly similarly in neural predictiv-
462
ity as compared to final trained models, suggesting that net-
work architecture contributes as much or more than expe-
rience dependent learning to a model’s match to the brain.
465
Antonello et al. [2021] proposed a “language representation 466
embedding space” and demonstrated the effectiveness of the
467
features from this embedding in predicting fMRI responses
to linguistic stimuli.
Disentangling the Syntax and Semantics: The represen-
470
tations of transformer models like BERT, GPT-2 have been 471

463

468

464

469

Table 2: Summary of Representative Brain Encoding Studies

Stimuli Authors

[Jain and Huth, 2018]
[Toneva and Wehbe, 2019]
[Toneva et al., 2020]
[Schrimpf et al., 2021b]

Lang.

Stimulus Representations

Dataset
Type
fMRI
fMRI/ MEG English ELMo, BERT, Transformer-XL
MEG
fMRI/ECoG English 43 language models (e.g. GloVe, ELMo, BERT,

English LSTM

English BERT

[Gauthier and Levy, 2019]

fMRI

English BERT, fine-tuned NLP tasks (Sentiment, Natural

GPT-2, XLNET)

[Deniz et al., 2019]
[Jain et al., 2020]
[Caucheteux et al., 2021]
[Antonello et al., 2021]

t
x
e
T

[Reddy and Wehbe, 2021]
[Goldstein et al., 2022]

[Oota et al., 2022b]
[Oota et al., 2022a]
[Merlin and Toneva, 2022]

fMRI
fMRI
fMRI
fMRI

fMRI
fMRI

fMRI
fMRI
fMRI

language inference), Scrambling language model

English GloVe
English LSTM
English GPT-2, Basic syntax features
English GloVe, BERT, GPT-2, Machine Translation, POS

tasks

English Constituency, Basic syntax features and BERT
English GloVe, GPT-2 next word, pre-onset, post-onset

word surprise
English BERT and GLUE tasks
English ESN, LSTM, ELMo, Longformer
English BERT, Next word prediction, multi-word semantics,

scrambling model

[Toneva et al., 2022]
[Aw and Toneva, 2022]

fMRI / MEG English ELMo, BERT, Context Residuals
fMRI

English BART, Longformer, Long-T5, BigBird, and corre-

6
9
9
20

7

9
6

|S| Dataset

Subset Moth Radio Hour
Story understanding
Question-Answering
Neural architecture of language

Imagine a situation with the noun

Ridge

Subset Moth Radio Hour
Subset Moth Radio Hour

345 Narratives

6 Moth Radio Hour

8
8

82
82
8

8
8

Harry Potter
ECoG

Pereira & Narratives
Narratives
Harry Potter

Harry Potter
Passive reading

19, 12 Zhang

82

Narratives

8 MEG-MASC
12
12
5
4
7
4
4
4

Reading Sentences
Pereira
Pereira
BOLD 5000
Algonauts
BOLD 5000
BOLD 5000
BOLD 5000

6 Moth Radio Hour

sponding Booksum models as well
Node Count

English,
Chi-
nese
English Constituency, Dependency trees, Basic syntax fea-

tures and BERT

English Basic syntax features, GloVe and BERT
English BERT-Large, GPT-2 XL
English BERT-Large, GPT-2 XL
English BERT-Large, GPT-2 XL, Text Perturbations

21 downstream vision tasks
CNN models AlexNet, ResNet, DenseNet
21 downstream vision tasks
CNN models AlexNet
CNN models AlexNet

VQ-VAE)

English 5 basic and 25 deep learning based speech models
(Tera, CPC, APC, Wav2Vec2.0, HuBERT, DistilHu-
BERT, Data2Vec

English Wav2Vec2.0 and SUPERB tasks
English Merlo Reseve
English 985D Semantic Vector

English Wav2Vec2.0
English APC, AST, Wav2Vec2.0, and HuBERT
English 19 Speech Models (e.g. DeepSpeech, Wav2Vec2.0,

345 Narratives

7 Moth Radio Hour
Passive listening
19

Narratives
Neuromod

82
5
5 Moth Radio Hour & Short Movie

Ridge
Ridge
Ridge

Clips

English CLIP, VisualBERT, LXMERT, CNNs and BERT
English BriVL
English BridgeTower

5, 82 Periera & Narratives

5
Pereira & Short Movie Clips
5 Moth Radio Hour & Short Movie

Ridge
Ridge
Ridge

Clips

[Zhang et al., 2022b]

fMRI

[Oota et al., 2023a]

[Oota et al., 2023b]
[Tuckute et al., 2023]
[Kauf et al., 2023]
[Singh et al., 2023]
[Wang et al., 2019]
[Kubilius et al., 2019]
[Dwivedi et al., 2021]
[Khosla and Wehbe, 2022]
[Conwell et al., 2023]
[Millet et al., 2022]
[Vaidya et al., 2022]
[Tuckute et al., 2022]

[Oota et al., 2023c]

[Oota et al., 2023d]
[Dong and Toneva, 2023]
[Popham et al., 2021]

[Oota et al., 2022d]
[Lu et al., 2022]
[Tang et al., 2023]

fMRI

MEG
fMRI
fMRI
fMRI
fMRI
fMRI
fMRI
fMRI
fMRI
fMRI
fMRI
fMRI

fMRI

fMRI
fMRI
fMRI

fMRI
fMRI
fMRI

l
a
u
s
i
V

o
i
d
u
A

l
a
d
o
M

i
t
l
u
M

Model

Ridge
Ridge
Ridge
Ridge

Ridge
Ridge
Ridge
Ridge

Ridge

Ridge
Ridge
Ridge

Ridge
Ridge

Ridge

Ridge

Ridge
Ridge
Ridge
Ridge
Ridge
Ridge
Ridge
Ridge
Ridge
Ridge
Ridge
Ridge

Ridge

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

shown to linearly map onto brain activity during language
comprehension. Several studies have attempted to disentan-
gle the contributions of different types of information from
word representations to the alignment between brain record-
ings and language models. Wang et al. [2020] proposed
a two-channel variational autoencoder model to dissociate
sentences into semantic and syntactic representations and
separately associate them with brain imaging data to find
feature-correlated brain regions. To separate each syntac-
tic feature, Zhang et al. [2022a] proposed a feature elim-
ination method, called Mean Vector Null space Projection.
Compared with word representations, word syntactic features
(parts-of-speech, named entities, semantic roles, dependen-
cies) seem to be distributed across brain networks instead of
a local brain region. In the previous two studies, we do not
know whether all or any of these representations effectively

491

490

drive the linear mapping between language models (LMs) and 488
the brain. Toneva et al. [2022] presented an approach to dis-
489
entangle supra-word meaning from lexical meaning in lan-
guage models and showed that supra-word meaning is pre-
dictive of fMRI recordings in two language regions (anterior
and posterior temporal lobes). Caucheteux et al. [2021] pro-
posed a taxonomy to factorize the high-dimensional activa-
tions of language models into four combinatorial classes: lex-
ical, compositional, syntactic, and semantic representations.
They found that (1) Compositional representations recruit a
more widespread cortical network than lexical ones, and en-
compass the bilateral temporal, parietal and prefrontal cor-
tices. (2) Contrary to previous claims, syntax and semantics
are not associated with separated modules, but, instead, ap-
pear to share a common and distributed neural substrate.

497

495

496

498

493

492

494

499

502

501

500

While previous works studied syntactic processing as cap-

503

Vision tasks
and brains

Vision

Alignment between vi-
sion models and brain

Auditory

Brain Encoding

Multimodal

Language

Visual properties
in vision mod-
els and brains

Speech tasks
and brains

Alignment between
pretrained speech
models and brain

Language & Au-
ditory and Vision

Incorporating Lan-
guage into Vision

Alignment between
pretrained language
models and brain

NLP tasks in
language mod-
els and brains

Disentangling
the Syntax
and Semantics

Linguistic proper-
ties in language
models and brains

[Wang et al., 2019;
Dwivedi et al., 2021]

[Kubilius et al., 2019;
Conwell et al., 2023]

[Khosla and Wehbe, 2022]

[Tuckute et al., 2022;
Oota et al., 2023d]

[Vaidya et al., 2022; Millet
et al., 2022; Tuckute et al.,
2022; Oota et al., 2023c]

[Lu et al., 2022;
Dong and Toneva, 2023]

[Oota et al., 2022d;
Popham et al., 2021;
Tang et al., 2023;
Wang et al., 2022]

[Huth et al., 2016; Anderson et
al., 2017a; Jain and Huth, 2018;
Toneva and Wehbe, 2019;
Deniz et al., 2019; Schrimpf
et al., 2021a; Caucheteux
and King, 2020; Goldstein
et al., 2022; Antonello et al.,
2021; Oota et al., 2022a]

[Gauthier and Levy, 2019;
Schwartz et al., 2019; Toneva
et al., 2020; Oota et al., 2022b]

[Wang et al., 2020; Zhang et al.,
2022a; Caucheteux et al., 2021;
Toneva et al., 2022; Reddy
and Wehbe, 2021; Toneva et
al., 2021; Oota et al., 2023a]

[Kumar et al., 2022;
Aw and Toneva, 2022;
Merlin and Toneva, 2022;
Oota et al., 2022e;
Tuckute et al., 2023;
Kauf et al., 2023]

Figure 7: Brain Encoding Survey Tree

504

505

506

tured through complexity measures (syntactic surprisal, node
count, word length, and word frequency), very few have stud-
ied the syntactic representations themselves. Studying syn-

tactic representations using fMRI is difficult because:
(1)
representing syntactic structure in an embedding space is a
non-trivial computational problem, and (2) the fMRI signal

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

is noisy. To overcome these limitations, Reddy et al. [2021]
proposed syntactic structure embeddings that encode the syn-
tactic information inherent in natural text that subjects read
in the scanner. The results reveal that syntactic structure-
based features explain additional variance in the brain activity
of various parts of the language system, even after control-
ling for complexity metrics that capture the processing load.
Toneva et al. [2021] further examined whether the represen-
tations obtained from a language model align with different
language processing regions in a similar or different way.
Linguistic properties in LMs and brains: Understanding
the reasons behind the observed similarities between lan-
guage comprehension in LMs and brains can lead to more
insights into both systems. Several works [Schwartz et al.,
2019; Kumar et al., 2022; Aw and Toneva, 2022; Merlin and
Toneva, 2022; Oota et al., 2022b] have found that using a
fine-tuned BERT leads to improved brain predictions. How-
ever, it is not clear what type of information in the fine-tuned
BERT model led to the improvement. It is unclear whether
and how the two systems align in their information processing
pipeline. Aw and Toneva [2022] used four pre-trained large
language models (BART, Longformer Encoder Decoder, Big-
Bird, and LongT5) and also trained them to improve their
narrative understanding, using the method detailed in Fig-
ure 8. However, it is not understood whether prediction of
the next word is necessary for the observed brain alignment
or simply sufficient, and whether there are other shared mech-
anisms or information that is similarly important. Merlin and
Toneva [2022] proposed two perturbations to pretrained lan-
guage models that, when used together, can control for the ef-
fects of next word prediction and word-level semantics on the
alignment with brain recordings. Specifically, they find that
improvements in alignment with brain recordings in two lan-
guage processing regions–Inferior Frontal Gyrus (IFG) and
Angular Gyrus (AG)–are due to next word prediction and
word-level semantics. However, what linguistic information
actually underlies the observed alignment between brains and
language models is not clear. Recently, Oota et al. [2022e]
tested the effect of a range of linguistic properties (surface,
syntactic and semantic) and found that the elimination of each
linguistic property results in a significant decrease in brain
alignment across all layers of BERT.
Visual Encoding: CNNs are currently the best class of mod-
els of the neural mechanisms of visual processing [Du et al.,
2020; Beliy et al., 2019; Oota et al., 2019; Nishida et al.,
2020]. How can we push these deeper CNN models to cap-
ture brain processing even more stringently? Continued ar-
chitectural optimization on ImageNet alone no longer seems
like a viable option. Kubilius et al. [2019] proposed a shal-
low recurrent anatomical network CORnet that follows neu-
roanatomy more closely than standard CNNs, and achieved
the state-of-the-art results on the Brain-score benchmark. It
has four computational areas, conceptualized as analogous to
the ventral visual areas V1, V2, V4, and IT, and a linear cate-
gory decoder that maps from the population of neurons in the
model’s last visual area to its behavioral choices.

Despite the effectiveness of CNNs, it is difficult to draw
specific inferences about neural information processing us-
ing CNN- derived representations from a generic object-

571

588

576

572

574

575

573

577

589

classification CNN. Hence, Wang et al. [2019] built encoding 569
models with individual feature spaces obtained from 21 com-
570
puter vision tasks. One of the main findings is that features
from 3D tasks, compared to those from 2D tasks, predict a
distinct part of visual cortex.
Auditory Encoding: Speech stimuli have mostly been rep-
resented using encodings of text transcriptions [Huth et al.,
2016] or using basic features like phoneme rate, the sum of
squared FFT coefficients [Pandey et al., 2022], etc. Text
transcription-based methods ignore the raw audio-sensory in-
578
formation completely. The basic speech feature engineering 579
method misses the benefits of transfer learning from rigor-
580
ously pretrained speech DL models.

581
Recently, several researchers have used popular deep 582
583

learning models such as APC [Chung et al., 2020],
Wav2Vec2.0 [Baevski et al., 2020], HuBERT [Hsu et al.,
584
2021], and Data2Vec [Baevski et al., 2022] for encoding 585
speech stimuli. Millet et al. [2022] used a self-supervised 586
learning model Wav2Vec2.0 to learn latent representations
587
of the speech waveform similar to those of the human brain.
They find that the functional hierarchy of its transformer lay-
ers aligns with the cortical hierarchy of speech in the brain,
590
and reveals the whole-brain organisation of speech processing 591
with an unprecedented clarity. This means that the first trans-
592
former layers map onto the low-level auditory cortices (A1 593
and A2), the deeper layers (orange and red) map onto brain 594
regions associated with higher-level processes (e.g. STS and 595
IFG). Vaidya et al. [2022] present the first systematic study 596
to bridge the gap between recent four self-supervised speech 597
representation methods (APC, Wav2Vec, Wav2Vec2.0, and 598
HuBERT) and computational models of the human auditory 599
system. Similar to [Millet et al., 2022], they find that self-
600
supervised speech models are the best models of auditory ar-
eas. Lower layers best modeled low-level areas, and upper-
middle layers were most predictive of phonetic and semantic
areas, while layer representations follow the accepted hier-
604
archy of speech processing. Tuckute et al. [2022] analyzed 605
19 different speech models and find that some audio models
606
derived in engineering contexts (model applications ranged 607
from speech recognition and speech enhancement to audio 608
captioning and audio source separation) produce poor predic-
609
tions of auditory cortical responses, many task-optimized au-
dio speech deep learning models outpredict a standard spec-
trotemporal model of the auditory cortex and exhibit hierar-
chical layer-region correspondence with auditory cortex.
Multimodal Brain Encoding: Multimodal stimuli can be
614
best encoded using recently proposed deep learning based 615
multimodal models. Oota et al. [2022d] experimented with 616
multimodal models like Contrastive Language-Image Pre-
617
training (CLIP), Learning Cross-Modality Encoder Repre-
618
sentations from Transformers (LXMERT), and VisualBERT 619
and found VisualBERT to the best. Similarly, Wang et
620
al. [2022] find that multimdoal models like CLIP better pre-
dict neural responses in visual cortex, since image captions
622
typically contain the most semantically relevant information 623
in an image for humans. [Dong and Toneva, 2023] present a
624
systematic approach to probe multi-modal video Transformer
model by leveraging neuroscientific evidence of multimodal
information processing in the brain. The authors find that in-

610

603

611

602

601

612

613

627

626

625

621

Figure 8: Comparison of brain recordings with language models trained on web corpora (LEFT) and language models trained on book stories
(RIGHT) [Aw and Toneva, 2022].

628

629

630

631

632

633

634

635

636

637

638

639

640

641

642

643

644

645

646

647

648

649

650

651

652

653

termediate layers of a multimodal video transformer are bet-
ter at predicting multimodal brain activity than other layers,
indicating that the intermediate layers encode the most brain-
related properties of the video stimuli. Recently, [Tang et al.,
2023] investigated a multimodal Transformer as the encoder
architecture to extract the aligned concept representations for
narrative stories and movies to model fMRI responses to nat-
uralistic stories and movies, respectively. Since language and
vision rely on similar concept representations, the authors
perform a cross-modal experiment in which how well the lan-
guage encoding models can predict movie-fMRI responses
from narrative story features (story → movie) and how well
the vision encoding models can predict narrative story-fMRI
responses from movie features (movie → story). Overall, the
authors find that cross-modality performance was higher for
features extracted from multimodal transformers than for lin-
early aligned features extracted from unimodal transformers.

6 Brain Decoding
Decoding is the learning of the mapping from neural activa-
tions back to the stimulus domain. Figure 9 depicts the typical
workflow for building an image/language decoder.
Decoder Architectures: In most cases, the stimulus repre-
sentation is decoded using typical ridge regression models
trained on each voxel and its 26 neighbors in 3D to pre-
dict each dimension of the stimulus representation. Also,
decoding is usually performed using the most informative

656

654

655

voxels [Pereira et al., 2018].
In some cases, a fully con-
nected layer [Beliy et al., 2019] or a multi-layered percep-
tron [Sun et al., 2019] has been used.
In some studies,
when decoding is modeled as multi-class classification, Gaus-
657
sian Na¨ıve Bayes [Singh et al., 2007; Just et al., 2010] and 658
SVMs [Thirion et al., 2006] have also been used for decod-
659
ing. Figure 10 summarizes the literature related to various
decoding solutions proposed in vision, auditory, and language
domains.
Decoding task settings: The most common setting is to per-
form decoding to a vector representation using a stimuli of
664
a single mode (visual, text or audio). Initial brain decoding 665
experiments studied the recovery of simple concrete nouns
666
and verbs from fMRI brain activity [Nishimoto et al., 2011]
667
where the subject watches either a picture or a word. Sun 668
et al. [2019] used several sentence representation models to 669
associate brain activities with sentence stimulus, and found 670
InferSent to perform the best. More work has focused on de-
671
coding the text passages instead of individual words [Wehbe
et al., 2014].

661

662

660

663

673

672

Some studies have focused on multimodal stimuli based 674
675

decoding where the goal is still to decode the text represen-
tation vector. For example, Pereira et al. [2018] trained the
676
decoder on imaging data of individual concepts, and showed 677
that it can decode semantic vector representations from imag-
678
ing data of sentences about a wide variety of both concrete
and abstract topics from two separate datasets. Further, Oota

679

680

Figure 9: Schema for Brain Decoding. LEFT: Image decoder [Smith et al. 2011], RIGHT: Language Decoder [Wang et al. 2019]

Table 3: Summary of Representative Brain Decoding Studies

Stimuli Authors

Lang.

Stimulus Representations

|S| Dataset

[Pereira et al., 2018]
[Wang et al., 2020]
[Oota et al., 2022c]
[Tang et al., 2022]

[Beliy et al., 2019]

Dataset
Type
fMRI
fMRI
fMRI
fMRI

fMRI

English
English
English
English

[Takagi and Nishimoto, 2022]
[Ozcelik and VanRullen, 2023]
[Chen et al., 2023b]
[D´efossez et al., 2022]

fMRI
fMRI
fMRI
MEG,EEG English

t
x
e
T

l
a
u
s
i
V

o
i
d
u
A

Word2Vec, GloVe, BERT
BERT, RoBERTa
GloVe, BERT, RoBERTa
GPT, fine-tuned GPT on Reddit comments and au-
tobiographical stories
End-to-End Encoder-Decoder, Decoder-Encoder,
AlexNet
Latent Diffusion Model, CLIP
VDVAE, Latent Diffusion Model
Latent Diffusion Model, CLIP
MEL Spectrogram, Wav2Vec2.0

17 Pereira
Pereira
6
17 Pereira
7 Moth Radio Hour

5 Generic Object Decoding, ViM-1

4 NSD
7 NSD
3 HCP fMRI-Video-Dataset

169 MEG-MASC

Model

Ridge
Ridge
Ridge
Ridge

Ridge

Ridge
Ridge,
CLIP

[Gwilliams et al., 2022]

MEG

English

Phonemes

7 MEG-MASC

Vision

Video reconstruction

Image reconstruction

[Nishimoto et al., 2011;
Chen et al., 2023b]

[Naselaris et al., 2009; Beliy et
al., 2019; Takagi and Nishimoto,
2022; Ozcelik and VanRullen,
2023; Chen et al., 2023a]

Brain Decoding

Auditory

Speech re-
construction

[Anumanchipalli et al., 2019;
D ´efossez et al., 2022]

Language

Reconstructing
continuous language

Decoding
word/sentence vector

Figure 10: Brain Decoding Survey Tree

[Affolter et al., 2020;
Tang et al., 2022]

[Pereira et al., 2018; Sun
et al., 2019; Gauthier and
Levy, 2019; Abdou et al.,
2021; Oota et al., 2022c]

681

682

683

684

685

686

et al. [2022c] propose two novel brain decoding setups: (1)
multi-view decoding (MVD) and (2) cross-view decoding
(CVD). In MVD, the goal is to build an MV decoder that
can take brain recordings for any view as input and predict
the concept. In CVD, the goal is to train a model which takes
brain recordings for one view as input and decodes a seman-

tic vector representation of another view. Specifically, they 687
study practically useful CVD tasks like image captioning, im-
688
age tagging, keyword extraction, and sentence formation.

689

To understand application of Transformer models for de-
690
coding better, Gauthier et al. [2019] fine-tuned a pre-trained 691
BERT on a variety of NLU tasks, asking which lead to im-
692

693

694

695

696

697

698

699

700

701

702

703

704

705

706

707

708

709

710

711

712

713

714

715

716

717

718

719

720

721

722

723

724

725

726

727

728

729

730

731

732

733

734

735

736

737

738

739

740

741

742

743

744

745

746

747

provements in brain-decoding performance. They find that
tasks which produce syntax-light representations yield signif-
icant improvements in brain decoding performance. Toneva
et al. [2019] study how representations of various Trans-
former models differ across layer depth, context length, and
attention type.

Some studies have attempted to reconstruct words [Affolter
et al., 2020], continuous language [Tang et al., 2022], im-
ages [Du et al., 2020; Beliy et al., 2019; Fang et al., 2020;
Lin et al., 2022], speech [D´efossez et al., 2022] or question-
answer speech dialogues [Moses et al., 2019] rather than just
predicting a semantic vector representation. Lastly, some
studies have focused on reconstructing personal imagined ex-
periences [Berezutskaya et al., 2020] or application-based
decoding like using brain activity scanned during a picture-
based mechanical engineering task to predict individuals’
physics/engineering exam results [Cetron et al., 2019] and
reflecting whether current thoughts are detailed, correspond
to the past or future, are verbal or in images [Smallwood and
Schooler, 2015]. Table 3 aggregates the brain decoding liter-
ature along different stimulus domains such as textual, visual,
and audio.

7 Conclusion, Limitations, and Future Trends

Conclusion In this paper, we surveyed important datasets,
stimulus representations, brain encoding and brain decoding
methods across different modalities. A glimpse of how deep
learning solutions throw light on putative brain computations
is given.
Limitations Naturalistic datasets of passive reading/listening
offer ecologically realistic settings for investigating brain
function. However, the lack of a task (as in a controlled
psycholinguistic experiment) that probes the participant’s un-
derstanding of the narrative limits the inferences that can be
made on what the participant’s brain is actually engaged in
while passively following the stimuli. This becomes even
more important when multi-lingual, multiscriptal participants
process stimuli in L2 language or script – it is unclear if the
brain activity reflects the processing of L2 or active suppres-
sion L1 while focusing on L2 [Malik-Moraleda et al., 2022].
Future Trends Some of the future areas of work in this field
are as follows: (1) While there is work on the text, under-
standing the similarity in information processing between vi-
sual/speech/multimodal models versus natural brain systems
(2) Decoding to actual multimodal
remains an open area.
stimuli seems feasible thanks to recent advances in generation
using deep learning models. (3) Deeper understanding of the
degree to which damage to different parts of the human brain
could lead to the degradation of cognitive skills. (4) How can
we train artificial neural networks in novel self-supervised
ways such that they compose word meanings or comprehend
images and speech like a human brain? (5) How can we lever-
age improved neuroscience understanding to suggest changes
in proposed artificial neural network architectures to make
them more robust and accurate? We hope that this survey
motivates research along the above directions.

References
[Abdou et al., 2021] Mostafa Abdou, Ana Valeria Gonz´alez, Mariya Toneva, Daniel
Hershcovich, and Anders Søgaard. Does injecting linguistic structure into lan-
arXiv preprint
guage models lead to better alignment with brain recordings?
arXiv:2101.12608, 2021.

[Affolter et al., 2020] Nicolas Affolter, Beni Egressy, Damian Pascual, and Roger
Wattenhofer. Brain2word: Decoding brain activity for language generation. arXiv
preprint arXiv:2009.04765, 2020.

[Allen et al., 2022] Emily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove,
Jacob S Prince, Logan T Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian
Charest, et al. A massive 7t fmri dataset to bridge cognitive neuroscience and artifi-
cial intelligence. Nature neuroscience, 25(1):116–126, 2022.

[Anderson et al., 2017a] Andrew J Anderson, Douwe Kiela, Stephen Clark, and Mas-
simo Poesio. Visually grounded and textual semantic models differentially decode
brain activity associated with concrete and abstract nouns. TACL, 5:17–30, 2017.

[Anderson et al., 2017b] Andrew James Anderson, Jeffrey R Binder, Leonardo Fer-
nandino, Colin J Humphries, Lisa L Conant, Mario Aguilar, Xixi Wang, Donias
Doko, and Rajeev DS Raizada. Predicting neural activity patterns associated with
sentences using a neurobiologically motivated model of semantic representation.
Cerebral Cortex, 27(9):4379–4395, 2017.

[Anderson et al., 2019] Andrew James Anderson, Jeffrey R Binder, Leonardo Fer-
nandino, Colin J Humphries, Lisa L Conant, Rajeev DS Raizada, Feng Lin, and
Edmund C Lalor. An integrated neural decoder of linguistic and experiential mean-
ing. Journal of Neuroscience, 39(45):8969–8987, 2019.

[Anderson et al., 2020] Andrew James Anderson, Kelsey McDermott, Brian Rooks,
Kathi L Heffner, David Dodell-Feder, and Feng V Lin. Decoding individual identity
from brain activity elicited in imagining common experiences. Nature communica-
tions, 11(1):1–14, 2020.

[Antonello et al., 2021] Richard Antonello, Javier S Turek, Vy Vo, and Alexander
Huth. Low-dimensional structure in the space of language representations is re-
flected in brain responses. NeurIPS, 34:8332–8344, 2021.

[Anumanchipalli et al., 2019] Gopala K Anumanchipalli, Josh Chartier, and Edward F
Chang. Speech synthesis from neural decoding of spoken sentences. Nature,
568(7753):493–498, 2019.

[Aw and Toneva, 2022] Khai Loong Aw and Mariya Toneva.

models for deeper understanding improves brain alignment.
arXiv:2212.10898, 2022.

Training language
arXiv preprint

[Baevski et al., 2020] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and
Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech
representations. NeurIPS, 33:12449–12460, 2020.

[Baevski et al., 2022] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao
Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning
in speech, vision and language. In ICML, pages 1298–1312. PMLR, 2022.

[Beliy et al., 2019] Roman Beliy, Guy Gaziv, Assaf Hoogi, Francesca Strappini, Tal
Golan, and Michal Irani. From voxels to pixels and back: Self-supervision in
natural-image reconstruction from fmri. arXiv preprint arXiv:1907.02431, 2019.

[Berezutskaya et al., 2020] Julia Berezutskaya, Zachary V Freudenburg, Luca Ambro-
gioni, Umut G¨uc¸l¨u, Marcel AJ van Gerven, and Nick F Ramsey. Cortical network
responses map onto data-driven features that capture visual semantics of movie frag-
ments. Scientific reports, 10(1):1–21, 2020.

[Boyle et al., 2020] Julie A Boyle, Basile Pinsard, A Boukhdhir, S Belleville, S Bram-
batti, J Chen, J Cohen-Adad, A Cyr, A Fuente, P Rainville, et al. The courtois project
on neuronal modelling: 2020 data release. In OHBM, 2020.

[Brennan and Hale, 2019] Jonathan R Brennan and John T Hale. Hierarchical struc-
ture guides rapid linguistic predictions during naturalistic listening. PloS one,
14(1):e0207741, 2019.

[Cao et al., 2021] Lu Cao, Dandan Huang, Yue Zhang, Xiaowei Jiang, and Yanan
Chen. Brain decoding using fnirs. In AAAI, volume 35, pages 12602–12611, 2021.

[Caucheteux and King, 2020] Charlotte Caucheteux and Jean-R´emi King. Language
processing in brains and deep neural networks: computational convergence and its
limits. BioRxiv, 2020.

[Caucheteux et al., 2021] Charlotte Caucheteux, Alexandre Gramfort, and Jean-Remi
King. Disentangling syntax and semantics in the brain with deep networks. In ICML,
pages 1336–1348. PMLR, 2021.

[Cetron et al., 2019] Joshua S Cetron, Andrew C Connolly, Solomon G Diamond,
Vicki V May, and James V Haxby. Decoding individual differences in stem learning
from functional mri data. Nature communications, 10(1):1–10, 2019.

[Chang et al., 2019] Nadine Chang, John A Pyles, Austin Marcus, Abhinav Gupta,
Michael J Tarr, and Elissa M Aminoff. Bold5000, a public fmri dataset while view-
ing 5000 visual images. Scientific data, 6(1):1–18, 2019.

748

749
750
751
752

753
754
755

756
757
758
759

760
761
762

763
764
765
766
767

768
769
770
771

772
773
774
775

776
777
778

779
780
781

782
783
784

785
786
787

788
789
790

791
792
793

794
795
796
797

798
799
800

801
802
803

804
805

806
807
808

809
810
811

812
813
814

815
816
817

818
819
820

821
822
823

824
825

826
827
828
829
830

831
832
833
834

835
836
837
838

839
840
841

842
843
844
845

846
847
848
849

850
851
852

853
854
855

856
857
858

859
860

861
862

863
864
865
866

867
868
869
870

871
872
873

874
875
876
877

878
879
880
881
882

883
884
885

886
887
888

889
890
891

892
893
894
895

896
897
898

899
900

901
902

903
904
905

906
907
908

909
910
911

912
913
914

915
916
917
918

919
920
921

922
923
924

[Chen et al., 2023a] Xuhang Chen, Baiying Lei, Chi-Man Pun, and Shuqiang Wang.
Brain diffuser: An end-to-end brain image to brain network pipeline. arXiv preprint
arXiv:2303.06410, 2023.

[Horikawa and Kamitani, 2017] Tomoyasu Horikawa and Yukiyasu Kamitani. Generic
decoding of seen and imagined objects using hierarchical visual features. Nature
communications, 8(1):1–15, 2017.

[Chen et al., 2023b] Zijiao Chen, Jiaxin Qing, and Juan Helen Zhou. Cinematic
mindscapes: High-quality video reconstruction from brain activity. arXiv preprint
arXiv:2305.11675, 2023.

[Chung et al., 2020] Yu-An Chung, Hao Tang, and James Glass. Vector-quantized au-

toregressive predictive coding. Interspeech, pages 3760–3764, 2020.

[Cichy et al., 2019] Radoslaw Martin Cichy, Gemma Roig, Alex Andonian, Kshitij
Dwivedi, Benjamin Lahner, Alex Lascelles, Yalda Mohsenzadeh, Kandan Ramakr-
ishnan, and Aude Oliva. The algonauts project: A platform for communication
between the sciences of biological and artificial intelligence. arXiv e-prints, pages
arXiv–1905, 2019.

[Cichy et al., 2021] Radoslaw Martin Cichy, Kshitij Dwivedi, Benjamin Lahner, Alex
Lascelles, Polina Iamshchinina, M Graumann, A Andonian, NAR Murty, K Kay,
Gemma Roig, et al. The algonauts project 2021 challenge: How the human brain
makes sense of a world in motion. arXiv preprint arXiv:2104.13714, 2021.

[Conwell et al., 2023] Colin Conwell, Jacob S. Prince, Kendrick N. Kay, George A.
Alvarez, and Talia Konkle. What can 1.8 billion regressions tell us about the pres-
bioRxiv,
sures shaping high-level visual representation in brains and machines?
2023.

[Hsu et al., 2021] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal
Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-
supervised speech representation learning by masked prediction of hidden units.
TASLP, 29:3451–3460, 2021.

[Huth et al., 2016] Alexander G Huth, Wendy A De Heer, Thomas L Griffiths,
Fr´ed´eric E Theunissen, and Jack L Gallant. Natural speech reveals the semantic
maps that tile human cerebral cortex. Nature, 532(7600):453–458, 2016.

[Huth et al., 2022] Alexander G Huth, Shinji Nishimoto, An T Vu, Dupre la Tour T,

and Gallant JL. Gallant lab natural short clips 3t fmri data. G-Node, 2022.

[Jain and Huth, 2018] Shailee Jain and Alexander G Huth. Incorporating context into

language encoding models for fmri. In NIPS, pages 6629–6638, 2018.

[Jain et al., 2020] Shailee Jain, Vy Vo, Shivangi Mahto, Amanda LeBel, Javier S
Interpretable multi-timescale models for predicting

Turek, and Alexander Huth.
fmri responses to continuous natural speech. NeurIPS, 33:13738–13749, 2020.

[Jat et al., 2020] S Jat, H Tang, P Talukdar, and T Mitchel. Relating simple sentence
representations in deep neural networks and the brain. In ACL, pages 5137–5154,
2020.

[D´efossez et al., 2022] Alexandre D´efossez, Charlotte Caucheteux, J´er´emy Rapin, Ori
Kabeli, and Jean-R´emi King. Decoding speech from non-invasive brain recordings.
arXiv preprint arXiv:2208.12266, 2022.

[Just et al., 2010] Marcel Adam Just, Vladimir L Cherkassky, Sandesh Aryal, and
Tom M Mitchell. A neurosemantic theory of concrete noun representation based
on the underlying brain codes. PloS one, 5(1):e8622, 2010.

[Deniz et al., 2019] Fatma Deniz, Anwar O Nunez-Elizalde, Alexander G Huth, and
Jack L Gallant. The representation of semantic information across human cerebral
cortex during listening versus reading is invariant to stimulus modality. Journal of
Neuroscience, 39(39):7722–7736, 2019.

[Doerig et al., 2022] Adrien Doerig, Rowan Sommers, Katja Seeliger, Blake Richards,
Jenann Ismael, Grace Lindsay, Konrad Kording, Talia Konkle, Marcel AJ Van Ger-
ven, Nikolaus Kriegeskorte, et al. The neuroconnectionist research programme.
arXiv preprint arXiv:2209.03718, 2022.

[Dong and Toneva, 2023] Dota Tianai Dong and Mariya Toneva. Interpreting multi-
modal video transformers using brain recordings. In ICLR 2023 Workshop on Mul-
timodal Representation Learning: Perks and Pitfalls, 2023.

[Du et al., 2020] Changde Du, Changying Du, Lijie Huang, and Huiguang He. Con-
ditional generative neural decoding with structured cnn feature prediction. In AAAI,
pages 2629–2636, 2020.

[Dwivedi et al., 2021] Kshitij Dwivedi, Michael F Bonner, Radoslaw Martin Cichy,
and Gemma Roig. Unveiling functions of the visual cortex using task-specific deep
neural networks. PLoS computational biology, 17(8):e1009267, 2021.

[Fang et al., 2020] Tao Fang, Yu Qi, and Gang Pan. Reconstructing perceptive images

from brain activity by shape-semantic gan. NeurIPS, 33:13038–13048, 2020.

[Gauthier and Levy, 2019] Jon Gauthier and Roger Levy. Linking artificial and human

neural representations of language. arXiv preprint arXiv:1910.01244, 2019.

[Goldstein et al., 2022] Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain,
Amy Price, Bobbi Aubrey, Samuel A Nastase, Amir Feder, Dotan Emanuel, Alon
Cohen, et al. Shared computational principles for language processing in humans
and deep language models. Nature neuroscience, 25(3):369–380, 2022.

[Gwilliams et al., 2022] Laura Gwilliams, Graham Flick, Alec Marantz, Liina Pylkka-
nen, David Poeppel, and Jean-Remi King. Meg-masc: a high-quality magneto-
encephalography dataset for evaluating natural speech processing. arXiv preprint
arXiv:2208.11488, 2022.

[Hale et al., 2018] John Hale, Chris Dyer, Adhiguna Kuncoro, and Jonathan Brennan.
Finding syntax in human encephalography with beam search. In ACL, pages 2727–
2736, 2018.

[Handjaras et al., 2016] Giacomo Handjaras, Emiliano Ricciardi, Andrea Leo,
Alessandro Lenci, Luca Cecchetti, Mirco Cosottini, and Giovanna Marotta. How
concepts are encoded in the human brain: a modality independent, category-based
cortical organization of semantic knowledge. Neuroimage, 135:232–242, 2016.

[Hebart et al., 2022] Martin N Hebart, Oliver Contier, Lina Teichmann, Adam Rock-
ter, Charles Y Zheng, Alexis Kidder, Anna Corriveau, Maryam Vaziri-Pashkam, and
Chris I Baker. Things-data: A multimodal collection of large-scale datasets for in-
vestigating object representations in brain and behavior. bioRxiv, pages 2022–07,
2022.

[Hollenstein et al., 2018] Nora Hollenstein, Jonathan Rotsztejn, Marius Troendle, An-
dreas Pedroni, Ce Zhang, and Nicolas Langer. Zuco, a simultaneous eeg and eye-
tracking resource for natural sentence reading. Scientific data, 5(1):1–13, 2018.

[Hollenstein et al., 2019] Nora Hollenstein, Antonio de la Torre, Nicolas Langer, and
Ce Zhang. Cognival: A framework for cognitive word embedding evaluation. In
CoNLL, pages 538–549, 2019.

[Karamolegkou et al., 2023] Antonia Karamolegkou, Mostafa Abdou, and Anders
arXiv preprint

Søgaard. Mapping brains with language models: A survey.
arXiv:2306.05126, 2023.

[Kauf et al., 2023] Carina Kauf, Greta Tuckute, Roger Levy, Jacob Andreas, and
Evelina Fedorenko. Lexical semantic content, not syntactic structure, is the main
contributor to ann-brain similarity of fmri responses in the language network.
bioRxiv, pages 2023–05, 2023.

[Kay et al., 2008] Kendrick N Kay, Thomas Naselaris, Ryan J Prenger, and Jack L Gal-
lant. Identifying natural images from human brain activity. Nature, 452(7185):352–
355, 2008.

[Khosla and Wehbe, 2022] Meenakshi Khosla and Leila Wehbe. High-level visual ar-
eas act like domain-general filters with strong selectivity and functional specializa-
tion. bioRxiv, 2022.

[Kubilius et al., 2019] Jonas Kubilius, Martin Schrimpf, Kohitij Kar, Rishi Rajaling-
925
ham, Ha Hong, Najib Majaj, Elias Issa, Pouya Bashivan, Jonathan Prescott-Roy,
926
Kailyn Schmidt, et al. Brain-like object recognition with high-performing shallow 927
recurrent anns. NIPS, 32:12805–12816, 2019.
928

[Kumar et al., 2022] Sreejan Kumar, Theodore R Sumers, Takateru Yamakoshi, Ariel
Goldstein, Uri Hasson, Kenneth A Norman, Thomas L Griffiths, Robert D Hawkins,
and Samuel A Nastase. Reconstructing the cascade of language processing in
the brain using the internal computations of a transformer-based language model.
BioRxiv, pages 2022–06, 2022.

[Lahner et al., 2023] Benjamin Lahner, Kshitij Dwivedi, Polina Iamshchinina, Monika
Graumann, Alex Lascelles, Gemma Roig, Alessandro Thomas Gifford, Bowen Pan,
SouYoung Jin, N Apurva Ratan Murty, et al. Bold moments: modeling short visual
events through a video fmri dataset and metadata. bioRxiv, pages 2023–03, 2023.

[Li et al., 2021] Jixing Li, Shohini Bhattasali, Shulin Zhang, Berta Franzluebbers,
Wen-Ming Luh, R Nathan Spreng, Jonathan R Brennan, Yiming Yang, Christophe
Pallier, and John Hale. Le petit prince: A multilingual fmri corpus using ecological
stimuli. Biorxiv, pages 2021–10, 2021.

[Lin et al., 2022] Sikun Lin, Thomas Christopher Sprague, and Ambuj Singh. Mind
reader: Reconstructing complex images from brain activities. In NeurIPS, 2022.

[Lu et al., 2022] Haoyu Lu, Qiongyi Zhou, Nanyi Fei, Zhiwu Lu, Mingyu Ding,
Jingyuan Wen, Changde Du, Xin Zhao, Hao Sun, Huiguang He, et al. Multi-
modal foundation models are better simulators of the human brain. arXiv preprint
arXiv:2208.08263, 2022.

[Malik-Moraleda et al., 2022] Saima Malik-Moraleda, Dima Ayyash, Jeanne Gall´ee,
Josef Affourtit, Malte Hoffmann, Zachary Mineroff, Olessia Jouravlev, and Evelina
Fedorenko. An investigation across 45 languages and 12 language families reveals
a universal language network. Nature Neuroscience, 25(8):1014–1019, 2022.

[Merlin and Toneva, 2022] Gabriele Merlin and Mariya Toneva. Language models
and brain alignment: beyond word-level semantics and prediction. arXiv preprint
arXiv:2212.00596, 2022.

[Millet et al., 2022] Juliette Millet, Charlotte Caucheteux, Pierre Orhan, Yves
Boubenec, Alexandre Gramfort, Ewan Dunbar, Christophe Pallier, and Jean-Remi
King. Toward a realistic model of speech processing in the brain with self-supervised
learning. arXiv:2206.01685, 2022.

929
930
931
932
933

934
935
936
937

938
939
940
941

942
943

944
945
946
947

948
949
950
951

952
953
954

955
956
957
958

959
960
961

962
963
964

965
966
967

968
969
970
971

972
973
974

975
976
977

978
979
980

981
982
983

984
985
986

987
988
989
990

991
992
993
994

995
996

997
998
999

1000
1001
1002

1003
1004
1005

1006
1007
1008

1009
1010

1011
1012
1013

1014
1015

1016
1017
1018

1019
1020
1021

1022
1023
1024

1025
1026
1027

[Mitchell et al., 2008] Tom M Mitchell, Svetlana V Shinkareva, Andrew Carlson, Kai-
Min Chang, Vicente L Malave, and Robert A Mason. Predicting human brain activ-
ity associated with the meanings of nouns. Science, 320(5880):1191–1195, 2008.

[Moses et al., 2019] David A Moses, Matthew K Leonard, Joseph G Makin, and Ed-
ward F Chang. Real-time decoding of question-and-answer speech dialogue using
human cortical activity. Nature communications, 10(1):1–14, 2019.

[Naselaris et al., 2009] Thomas Naselaris, Ryan J Prenger, Kendrick N Kay, Michael
Oliver, and Jack L Gallant. Bayesian reconstruction of natural images from human
brain activity. Neuron, 63(6):902–915, 2009.

[Nastase et al., 2021] Samuel A Nastase, Yun-Fei Liu, Hanna Hillman, Asieh Zad-
bood, Liat Hasenfratz, Neggin Keshavarzian, Janice Chen, Christopher J Honey,
Yaara Yeshurun, Mor Regev, et al. Narratives: fmri data for evaluating models of
naturalistic language comprehension. bioRxiv, pages 2020–12, 2021.

[Nishida and Nishimoto, 2018] Satoshi Nishida and Shinji Nishimoto. Decoding nat-
uralistic experiences from human brain activity via distributed representations of
words. Neuroimage, 180:232–242, 2018.

[Nishida et al., 2020] Satoshi Nishida, Yusuke Nakano, Antoine Blanc, Naoya Maeda,
Masataka Kado, and Shinji Nishimoto. Brain-mediated transfer learning of convo-
lutional neural networks. In AAAI, pages 5281–5288, 2020.

[Nishimoto et al., 2011] Shinji Nishimoto, An T Vu, Thomas Naselaris, Yuval Ben-
jamini, Bin Yu, and Jack L Gallant. Reconstructing visual experiences from brain
activity evoked by natural movies. Current biology, 21(19):1641–1646, 2011.

[Pereira et al., 2018] Francisco Pereira, Bin Lou, Brianna Pritchett, Samuel Ritter, 1028
Samuel J Gershman, Nancy Kanwisher, Matthew Botvinick, and Evelina Fedorenko. 1029
Toward a universal decoder of linguistic meaning from brain activation. Nature com- 1030
munications, 9(1):1–13, 2018.
1031

[Popham et al., 2021] Sara F Popham, Alexander G Huth, Natalia Y Bilenko, Fatma 1032
Deniz, James S Gao, Anwar O Nunez-Elizalde, and Jack L Gallant. Visual and 1033
linguistic semantic representations are aligned at the border of human visual cortex. 1034
Nature neuroscience, 24(11):1628–1636, 2021.
1035

[Reddy and Wehbe, 2021] Aniketh Janardhan Reddy and Leila Wehbe. Can fmri re- 1036
veal the representation of syntactic structure in the brain? NeurIPS, 34:9843–9856, 1037
2021.
1038

[Schrimpf et al., 2020] Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J Majaj, 1039
Rishi Rajalingham, Elias B Issa, Kohitij Kar, Pouya Bashivan, Jonathan Prescott- 1040
Roy, Franziska Geiger, et al. Brain-score: Which artificial neural network for object 1041
recognition is most brain-like? BioRxiv, page 407007, 2020.
1042

[Schrimpf et al., 2021a] Martin Schrimpf, Idan Blank, Greta Tuckute, Carina Kauf, 1043
Eghbal A Hosseini, Nancy Kanwisher, Joshua Tenenbaum, and Evelina Fedorenko. 1044
The neural architecture of language: Integrative reverse-engineering converges on a 1045
model for predictive processing. PNAS, Vol:To appear, 2021.
1046

[Schrimpf et al., 2021b] Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina 1047
Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina 1048
Fedorenko. The neural architecture of language: Integrative modeling converges on 1049
predictive processing. PNAS, 118(45), 2021.
1050

[Oota et al., 2018] Subba Reddy Oota, Naresh Manwani, and Raju S Bapi. fMRI Se-
mantic Category Decoding Using Linguistic Encoding of Word Embeddings.
In
ICONIP, pages 3–15. Springer, 2018.

[Schwartz et al., 2019] Dan Schwartz, Mariya Toneva, and Leila Wehbe.

Inducing 1051
brain-relevant bias in natural language processing models. NIPS, 32:14123–14133, 1052
2019.
1053

[Oota et al., 2019] Subba Reddy Oota, Vijay Rowtula, Manish Gupta, and Raju S Bapi.
Stepencog: A convolutional lstm autoencoder for near-perfect fmri encoding.
In
IJCNN, pages 1–8. IEEE, 2019.

[Seeliger et al., 2019] K Seeliger, RP Sommers, Umut G¨uc¸l¨u, Sander E Bosch, and 1054
MAJ Van Gerven. A large single-participant fmri dataset for probing brain responses 1055
to naturalistic stimuli in space and time. bioRxiv, page 687681, 2019.
1056

[Oota et al., 2022a] Subba Reddy Oota, Frederic Alexandre, and Xavier Hinaut. Long-
term plausibility of language models and neural dynamics during narrative listening.
In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 44,
2022.

[Oota et al., 2022b] Subba Reddy Oota, Jashn Arora, Veeral Agarwal, Mounika
Marreddy, Manish Gupta, and Bapi Raju Surampudi. Neural language taskonomy:
arXiv preprint
Which nlp tasks are the most predictive of fmri brain activity?
arXiv:2205.01404, 2022.

[Oota et al., 2022c] Subba Reddy Oota, Jashn Arora, Manish Gupta, and Raju S Bapi.
Multi-view and cross-view brain decoding. In COLING, pages 105–115, 2022.

[Oota et al., 2022d] Subba Reddy Oota, Jashn Arora, Vijay Rowtula, Manish Gupta,
In COLING, pages 116–133,

and Raju S Bapi. Visio-linguistic brain encoding.
2022.

[Oota et al., 2022e] Subba Reddy Oota, Manish Gupta, and Mariya Toneva.

Joint
processing of linguistic properties in brains and language models. arXiv preprint
arXiv:2212.08094, 2022.

[Oota et al., 2023a] Subba Reddy Oota, Mounika Marreddy, Manish Gupta, and
Bapi Raju Surampud. Syntactic structure processing in the brain while listening.
arXiv preprint arXiv:2302.08589, 2023.

[Oota et al., 2023b] Subba Reddy Oota, Trouvain Nathan, Frederic Alexandre, and
Xavier Hinaut. Meg encoding using word context semantics in listening stories.
In Interspeech, 2023.

[Oota et al., 2023c] Subba Reddy Oota, Khushbu Pahwa, Mounika Marreddy, Manish
Gupta, and Raju Surampudi Bapi. Neural architecture of speech. In ICASSP, 2023.

[Oota et al., 2023d] Subba Reddy Oota, Agarwal Veeral, Marreddy Mounika, Gupta
Manish, and Raju Surampudi Bapi. Speech taskonomy: Which speech tasks are the
most predictive of fmri brain activity? In 24th INTERSPEECH Conference, 2023.

[Oseki and Asahara, 2020] Yohei Oseki and M Asahara. Design of bccwj-eeg: Bal-
anced corpus with human electroencephalography. In LREC, pages 189–194, 2020.

[Ozcelik and VanRullen, 2023] Furkan Ozcelik and Rufin VanRullen. Brain-diffuser:
Natural scene reconstruction from fmri signals using generative latent diffusion.
arXiv preprint arXiv:2303.05334, 2023.

[Pandey et al., 2022] Pankaj Pandey, Gulshan Sharma, Krishna P Miyapuram, Ra-
manathan Subramanian, and Derek Lomas. Music identification using brain re-
sponses to initial snippets. In ICASSP, pages 1246–1250, 2022.

[Pereira et al., 2013] Francisco Pereira, Matthew Botvinick, and Greg Detre. Using
wikipedia to learn semantic feature representations of concrete concepts in neu-
roimaging experiments. Artificial intelligence, 194:240–252, 2013.

[Singh et al., 2007] Vishwajeet Singh, Krishna P. Miyapuram, and Raju S. Bapi. De- 1057
In 1058
1059

tection of cognitive states from fmri data using machine learning techniques.
Manuela M. Veloso, editor, IJCAI, pages 587–592, 2007.

[Singh et al., 2023] Chandan Singh, Aliyah R Hsu, Richard Antonello, Shailee Jain, 1060
Alexander G Huth, Bin Yu, and Jianfeng Gao. Explaining black box text modules in 1061
natural language with language models. arXiv preprint arXiv:2305.09863, 2023.
1062

[Smallwood and Schooler, 2015] Jonathan Smallwood and Jonathan W Schooler. The 1063
science of mind wandering: empirically navigating the stream of consciousness. 1064
Annual review of psychology, 66:487–518, 2015.
1065

[Sudre et al., 2012] Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila Wehbe, 1066
Alona Fyshe, Riitta Salmelin, and Tom Mitchell. Tracking neural coding of percep- 1067
tual and semantic features of concrete nouns. NeuroImage, 62(1):451–463, 2012.
1068

[Sun et al., 2019] Jingyuan Sun, Shaonan Wang, Jiajun Zhang, and Chengqing Zong. 1069
In AAAI, 1070
1071

Towards sentence-level brain decoding with distributed representations.
pages 7047–7054, 2019.

[Sun et al., 2020] Jingyuan Sun, Shaonan Wang, Jiajun Zhang, and Chengqing Zong. 1072
IEEE 1073
1074

Neural encoding and decoding with distributed sentence representations.
TNNLS, 32(2):589–603, 2020.

[Takagi and Nishimoto, 2022] Yu Takagi and Shinji Nishimoto. High-resolution im- 1075
age reconstruction with latent diffusion models from human brain activity. bioRxiv, 1076
pages 2022–11, 2022.
1077

[Tang et al., 2022] Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander G Huth. 1078
Semantic reconstruction of continuous language from non-invasive brain recordings. 1079
bioRxiv, pages 2022–09, 2022.
1080

[Tang et al., 2023] Jerry Tang, Meng Du, Vy A Vo, Vasudev Lal, and Alexander G 1081
Huth. Brain encoding models based on multimodal transformers can transfer across 1082
language and vision. arXiv preprint arXiv:2305.12248, 2023.
1083

[Thirion et al., 2006] Bertrand Thirion, Edouard Duchesnay, Edward Hubbard, Jessica 1084
Dubois, Jean-Baptiste Poline, Denis Lebihan, and Stanislas Dehaene. Inverse retino- 1085
topy: inferring the visual content of images from brain activation patterns. Neuroim- 1086
age, 33(4):1104–1116, 2006.
1087

[Toneva and Wehbe, 2019] Mariya Toneva and Leila Wehbe. Interpreting and improv- 1088
ing natural-language processing (in machines) with natural language-processing (in 1089
the brain). arXiv preprint arXiv:1905.11833, 2019.
1090

[Toneva et al., 2020] Mariya Toneva, Otilia Stretcu, Barnab´as P´oczos, Leila Wehbe, 1091
and Tom M Mitchell. Modeling task effects on meaning representation in the brain 1092
via zero-shot meg prediction. NIPS, 33, 2020.
1093

[Toneva et al., 2021] Mariya Toneva, Jennifer Williams, Anand B, Christoph Dann, 1094
1095

and Leila Wehbe. Same cause; different effects in the brain. In CLeaR, 2021.

[Pereira et al., 2016] Francisco Pereira, Bin Lou, Brianna Pritchett, Nancy Kanwisher,
Matthew Botvinick, and Ev Fedorenko. Decoding of generic mental representations
from functional mri data using word embeddings. bioRxiv, page 057216, 2016.

[Toneva et al., 2022] Mariya Toneva, Tom M Mitchell, and Leila Wehbe. Combining 1096
computational controls with natural text reveals aspects of meaning composition. 1097
Nature Computational Science, 2(11):745–757, 2022.
1098

1099
1100
1101

1102
1103
1104
1105

1106
1107
1108

1109
1110
1111

1112
1113
1114

1115
1116
1117

1118
1119
1120

1121
1122
1123
1124

1125
1126
1127
1128

1129
1130
1131

1132
1133
1134

1135
1136
1137
1138
1139

1140
1141
1142
1143

[Tuckute et al., 2022] Greta Tuckute, Jenelle Feather, Dana Boebinger, and Josh H
McDermott. Many but not all deep neural network audio models capture brain re-
sponses and exhibit hierarchical region correspondence. bioRxiv, 2022.

[Tuckute et al., 2023] Greta Tuckute, Aalok Sathe, Shashank Srikant, Maya Taliaferro,
Mingye Wang, Martin Schrimpf, Kendrick Kay, and Evelina Fedorenko. Driving
and suppressing the human language network using large language models. bioRxiv,
2023.

[Vaidya et al., 2022] Aditya R Vaidya, Shailee Jain, and Alexander G Huth. Self-
supervised models of audio effectively explain human cortical responses to speech.
arXiv preprint arXiv:2205.14252, 2022.

[Wang et al., 2017] Jing Wang, Vladimir L Cherkassky, and M Adam Just. Predicting
the brain activation pattern associated with the propositional content of a sentence:
Modeling neural representations of events and states. HBM, 10:4865–4881, 2017.

[Wang et al., 2019] Aria Wang, Michael Tarr, and Leila Wehbe. Neural taskonomy:
Inferring the similarity of task-derived representations from brain activity. NeurIPS,
32:15501–15511, 2019.

[Wang et al., 2020] Shaonan Wang, Jiajun Zhang, Haiyan Wang, Nan Lin, and
Chengqing Zong. Fine-grained neural decoding with distributed word representa-
tions. Information Sciences, 507:256–272, 2020.

[Wang et al., 2022] Aria Yuan Wang, Kendrick Kay, Thomas Naselaris, Michael J Tarr,
and Leila Wehbe. Incorporating natural language into vision models improves pre-
diction and understanding of higher visual cortex. BioRxiv, pages 2022–09, 2022.

[Wehbe et al., 2014] Leila Wehbe, Brian Murphy, Partha Talukdar, Alona Fyshe, Aa-
ditya Ramdas, and Tom Mitchell. Simultaneously uncovering the patterns of brain
regions involved in different story reading subprocesses. PloS one, 9(11):e112575,
2014.

[Yamins et al., 2014] Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A
Solomon, Darren Seibert, and James J DiCarlo. Performance-optimized hierarchical
models predict neural responses in higher visual cortex. PNAS, 111(23):8619–8624,
2014.

[Zhang et al., 2020] Yizhen Zhang, Kuan Han, Robert Worth, and Zhongming Liu.
Connecting concepts in the brain by mapping cortical representations of semantic
relations. Nature communications, 11(1):1–13, 2020.

[Zhang et al., 2022a] Xiaohan Zhang, Shaonan Wang, Nan Lin, Jiajun Zhang, and
Chengqing Zong. Probing word syntactic representations in the brain by a feature
elimination method. AAAI, 2022.

[Zhang et al., 2022b] Xiaohan Zhang, Shaonan Wang, Nan Lin, and Chengqing Zong.
Is the brain mechanism for hierarchical structure building universal across lan-
In Proceedings of the 2022 Con-
guages? an fmri study of chinese and english.
ference on Empirical Methods in Natural Language Processing, pages 7852–7861,
2022.

[Zinszer et al., 2018] Benjamin D Zinszer, Laurie Bayet, Lauren L Emberson, Ra-
jeev DS Raizada, and Richard N Aslin. Decoding semantic representations
from functional near-infrared spectroscopy signals. Neurophotonics, 5(1):011003–
011003, 2018.

