Recurrent Neural Network for Syntax Learning with
Flexible Predicates for Robotic Architectures
Xavier Hinaut, Johannes Twiefel, Stefan Wermter

To cite this version:

Xavier Hinaut, Johannes Twiefel, Stefan Wermter. Recurrent Neural Network for Syntax Learning
with Flexible Predicates for Robotic Architectures. The Sixth Joint IEEE International Conference
Developmental Learning and Epigenetic Robotics (ICDL-EPIROB), Sep 2016, Cergy, France.
￿hal-
01417697￿

HAL Id: hal-01417697

https://inria.hal.science/hal-01417697

Submitted on 15 Dec 2016

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Recurrent Neural Network for Syntax Learning with Flexible
Predicates for Robotic Architectures

Xavier Hinaut1,2, Johannes Twiefel1, and Stefan Wermter1

Abstract— We present a Recurrent Neural Network (RNN),
namely an Echo State Network (ESN), that performs sentence
comprehension and can be used for Human-Robot Interaction
(HRI). The RNN is trained to map sentence structures to
meanings (i.e. predicates). We have previously shown that this
ESN is able to generalize to unknown sentence structures.
Moreover, it is able to learn English, French or both at the same
time. The are two novelties presented here: (1) the encapsulation
of this RNN in a ROS module enables one to use it in a
robotic architecture like the Nao humanoid robot, and (2)
the ﬂexibility of the predicates it can learn to produce (e.g.
extracting adjectives) enables one to use the model to explore
language acquisition in a developmental approach.

I. INTRODUCTION

How do children learn language? In particular, how do
they associate the structure of a sentence to its meaning?
This question is linked to the more general issue: how does
the brain associate sequences of symbols to internal symbolic
or sub-symbolic representations? We propose a framework to
understand how language is acquired based on a simple and
generic neural architecture (Echo State Networks) [1] which
is not hand-crafted for a particular task, but on the contrary
can be used for a broad range of applications.

First, we present the general ESN architecture. Then, we
present the reservoir sentence processing model and provide
some examples of its ﬂexibility. Finally, we show how to use
it as a ROS module.

II. METHODS & RESULTS

A. Echo State Networks

The language module is based on an ESN [1] with leaky
neurons: inputs are projected to a random recurrent layer
and a linear output layer (called “read-out”) is modiﬁed by
learning (which can happen online). The units of the recur-
rent neural network have a leak rate (α) hyper-parameter
which corresponds to the inverse of a time constant. These
equations deﬁne the update of the ESN:

x(t + 1) = (1 − α)x(t) + αf (W inu(t + 1) + W x(t)) (1)

y(t) = W outx(t)

(2)

\*This work was supported by a Marie Curie Intra European Fellowship
within the 7th European Community Framework Programme: EchoRob
project (PIEF-GA-2013-627156).

1X. Hinaut, J. Twiefel, and S. Wermter are with Knowledge Technology
Group, Department Informatik, Universit¨at Hamburg, 22527 Hamburg,
Germany {surname}@informatik.uni-hamburg.de

2X. Hinaut is with Inria Bordeaux Sud-Ouest, Talence, France; LaBRI,
UMR 5800, CNRS, Bordeaux INP, Universit´e de Bordeaux, Talence, France;
and Institut des Maladies Neurod´eg´en´eratives, UMR 5293, CNRS, Univer-
sit´e de Bordeaux, Bordeaux, France xavier.hinaut@inria.fr

with x(t), u(t) and y(t) the reservoir (i.e. hidden) state,
the input and the read-out (i.e. output) states respectively
at time t, α the leak rate, W , W in and W out the reservoir,
the input and the output matrices respectively and f the tanh
activation function. After the collection of all reservoir states
the following equation deﬁnes how the read-out (i.e. output)
weights are trained:

W out = Y d[1; X]+

(3)

with Y d the concatenation of the desired outputs, X the
concatenation of the reservoir states (over all time steps
for all train sentences) and M + the Moore-Penrose pseudo-
inverse of matrix M . Hyper-parameters that can be used for
this task are the following: spectral radius: 1, input scaling:
0.75, leak rate: 0.17, number of reservoir units: 100.

Sentences are converted to a sentence structure by replacing
Fig. 1.
semantic words by a SW marker. The ESN is given the sentence structure
word by word. Each word activates a different input unit. During training,
the connections to the readout layer are modiﬁed to learn the mapping
between the sentence structure and the arguments of the predicates. When
a sentence is tested, the most active units are bound with the SW kept in
the SWs memory to form the resulting predicate. (Adapt. from [2].)

B. Reservoir Sentence Processing Model

The reservoir sentence processing model has been adapted
from previous experiments on a neuro-inspired model for
sentence comprehension using ESN [3] and its application
to HRI [2]. The model learns the mapping of the semantic
words (SW; e.g. nouns, verbs) of a sentence onto the different
slots (the thematic roles: e.g. action, location) of a basic
event structure (e.g. action(object, location)). As depicted
in Fig. 1, the system processes a sentence as input and
generates corresponding predicates. Before being fed to the

theontoandSW…Sentence:PALput (toy, left)P(A, L)Semantic words (SWs)Recurrent NeuralNetworkSW1SW2SW3P: PredicateA: AgentL: LocationMeaning: P (A, L)Read-outLayerInputLayeron the left put the toy .SW1SW2SW3putlefttoySWs MemoryFunction words(FWs)PALPALleftputtoyInactive connectionActive connectionLearnable connectionsFixed connectionsConnectionSW: Semantic word item(e.g. toy)thisESN, sentences are transformed into a sentence structure (or
grammatical construction): semantic words (SW), i.e. nouns,
verbs and adjectives that have to be assigned a thematic
role, are replaced by the SW item. The processing of the
grammatical construction is sequential (one word at a time)
and the ﬁnal estimation of the thematic roles for each SW
are read-out at the end of the sentence.

By processing grammatical constructions and not sen-
tences per se, the model is able to bind a virtually unlimited
number of sentences to these sentence structures. Based only
on a small training corpus, this enables the model to process
future sentences with currently unknown semantic words.
One major advantage of this neural network language module
is that no parsing grammar has to be deﬁned a priori: the
system learns only from the examples given in the training
corpus. Here are some input/output transformations that the
language model performs:

• “Please grasp the cup” → grasp(cup)
• “The box is on the right of the cup” → right(box, cup)
• “Right of the cup is the box” → right(box, cup)
As shown, the system can robustly transform different
types of sentences, and even though the last two sentences are
quite different (the order of words is different), the system
can learn to provide an identical predicate representation.
Recently, we have explored the ﬂexibility of the system:
we found that it can handle various kinds of representations
at the same time. For instance, it allows to use nouns as
main elements of a predicate, and use its arguments to ﬁll
in adjectives:

• “Find the big red object”

→ ﬁnd(object), object(big, red)

C. Usage of the ROS Module

The proposed model was encapsulated in a ROS module.
It is coded in Python language and uses rospy library. When
running the program, the reservoir model is trained using the
given text ﬁle and the ROS service is initialized. The training
text ﬁle contains sentences and corresponding predicates and
can be edited easily. Here is one line of a training text ﬁle
as an example:

• find box, box blue; find the blue box
This predicate representation enables one to easily integrate
this model into a robotic architecture. If predicates represent
multiple motor actions that have to be performed in a particu-
lar order, the predicates have to be speciﬁed in chronological
order:

• show banana, point box; first show me
the banana and then point to the box

• show banana, pointing box; before
pointing to the box show me the
banana

Once initialized, a request could be sent

to the ROS
service: it accepts a sentence (text string) as input and returns
an array of predicates in real time. With an ESN of 100 units,
the training of 200 sentences takes about one second on a
laptop computer. Testing a sentence is of the order of 10 ms.

III. DISCUSSION
Previous experiments have shown that the system can be
used in a real-world robot scenario (e.g. with the iCub [2] and
the Nao [4][5] humanoid robots). In Hinaut et al. [6], it has
been shown that the model can learn to process sentences
with out-of-vocabulary words. Moreover, we demonstrated
it can learn sentences both in French and English
that
at the same time. To illustrate how the robot interaction
works, a video can be seen at youtu.be/FpYDco3ZgkU
[4][5]. The source code is available at github.com/
neuronalX/EchoRob.

This ROS module could be employed to process various
hypotheses generated by a speech recognition system (like
in [5]),
then returning the retrieved predicates for each
hypothesis, thus, enabling a semantic analyser or world sim-
ulator to choose the predicates with the highest likelihood.
Preliminary work has shown that the model could be trained
fully incrementally [7], we plan to add this feature to the
ROS module in further work.

The objectives of this model are to improve HRI and
provide models of language acquisition. From the HRI point
of view, the aim of using this neural network-based model
is (1) to gain adaptability because the system is trained
on corpus examples (no need to predeﬁne a parser for
each language), (2) to be able to process natural language
sentences instead of stereotypical sentences (i.e. “put cup
left”), and (3) to be able to generalize to unknown sentence
structures (not in the training data set). Moreover, this model
is quite ﬂexible when changing the output predicate repre-
sentations, as we have shown here. From the computational
neuroscience and developmental robotics point of view, the
aim of this architecture is to model and test hypotheses about
child learning processes of language acquisition [8].

ACKNOWLEDGMENT

We thank Iris Wieser for fruitful discussions and ideas,

and Fabien Benureau for his very useful feedback.

REFERENCES

[1] H. Jaeger. The “echo state” approach to analysing and training recurrent
neural networks. Bonn, Germany: German National Research Center
for Information Technology GMD Technical Report, 148:34, 2001.
[2] X. Hinaut, M. Petit, G. Pointeau, and P. F. Dominey. Exploring
the acquisition and production of grammatical constructions through
human-robot interaction with echo state networks. Front. in Neurorob.,
8, 2014.

[3] X. Hinaut and P.F. Dominey. Real-time parallel processing of grammati-
cal structure in the fronto-striatal system: a recurrent network simulation
study using reservoir computing. PloS one, 8(2):e52946, 2013.

[4] X. Hinaut, J. Twiefel, M. Borghetti Soares, P. Barros, L. Mici, and
In Video competition, IJCAI,

S. Wermter. Humanoidly speaking ...
Buenos Aires, Argentina, 2015.

[5] J. Twiefel, X. Hinaut, M. Borghetti, E. Strahl, and S. Wermter. Using
Natural Language Feedback in a Neuro-inspired Integrated Multimodal
Robotic Architecture. In Proc. of RO-MAN, New York City, USA, 2016.
[6] X. Hinaut, J. Twiefel, M. Petit, P. Dominey, and S. Wermter. A recurrent
neural network for multiple language acquisition: Starting with english
and french. In Proc. of CoCo@NIPS workshop, 2015.

[7] X. Hinaut and S. Wermter. An incremental approach to language
acquisition: Thematic role assignment with echo state networks.
In
Proc. of ICANN 2014, pages 33–40, 2014.

[8] M. Tomasello. Constructing a language: A usage based approach to
language acquisition. Cambridge, MA: Harvard University Press, 2003.

