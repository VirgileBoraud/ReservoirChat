Using event-based metric for event-based neural network
weight adjustment
Bruno Cessac, Rodrigo Salas, Thierry Viéville

To cite this version:

Bruno Cessac, Rodrigo Salas, Thierry Viéville. Using event-based metric for event-based neural net-
work weight adjustment. 20th European Symposium on Artificial Neural Networks, Computational
Intelligence and Machine Learning, Apr 2012, Bruges, Belgium. 18 pp. ￿hal-00755345￿

HAL Id: hal-00755345

https://inria.hal.science/hal-00755345

Submitted on 3 Dec 2012

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Using event-based metric
for event-based neural network weight
adjustment

Bruno Cessac2 and Rodrigo Salas3 and Thierry Vi´eville1 ∗

(1) Inria Cortex, France http://cortex.loria.fr
(2) Inria Neuromathcomp, France
(3) Universidad de Valparaiso. Departamento de Ingenieria Biomedica. Chile

Abstract. The problem of adjusting the parameters of an event-based
network model is addressed here at the programmatic level. Considering
temporal processing, the goal is to adjust the network units weights so
that the outcoming events correspond to what is desired. The present
work proposes, in the deterministic and discrete case, a way to adapt
usual alignment metrics in order to derive suitable adjustment rules. At
the numerical level, the stability and unbiasness of the method is veriﬁed.

(Extended version of the ESANN’12 accepted publication)

1 Introduction

Studying the computational power of neural networks with event-based activity
(e.g.: [1, 2]) is a well-addressed topic, see [3, 4] for a recent review about spik-
ing network computation, while [5] provides a detailed discussion on temporal
aspects of such computations. See [6] for further details on the related modeling
choices. In order to contribute to this general topic, we develop here a frame-
work allowing us to eﬀectively adjust the network parameters in order to tune
the outcoming events.

∈ {

∈ {

0, T

{· · ·

(i.e., 0

· · · }
{

Vn[t]
0, N

Position of the problem We consider an input/output dynamical system with
N units, governed by a recurrent function, V =
being the output
state variable value of the units of output index n
n < N ) at
time t
. Some output units may be “hidden”, i.e. not observed. Here
{
W stands for the network parameters or “weights”, to be tuned. The exact
WVW must be well-
form of V is not relevant at this stage, but the gradient
deﬁned in order to adjust W. One track is to consider regular forms of V. For
a spiking neuron network this means that we have to consider either Hodgkin-
Huxley equations, or some suitable reduction like the FitzHugh-Nagumo model
or the SRM model [2]. Another track, is to “mollify” V, i.e., deﬁnes it as the
limit of a series of regular functions, as experimented in [7].
We deﬁne an event Zn[t] def= H(Vn[t]
, where H is the Heaviside
0, 1
}
−
function, as the fact that the output value is higher than a threshold θ. The

∈ {

θ)

∇

≤

∗Partially supported by the ANR KEOpS project and CORTINA associated team.

goal is thus to adjust the output events Z of the deterministic discrete-time
dynamical system V, with respect to a reference output events ¯Z. The key point
is to deal with the fact that the notion of event is intrinsically “discontinuous”.

Considering alignment metric We deﬁne the distance between two ﬁnite event-
trains ¯Z, Z as the minimum cost of transforming one event-train into another.
See [8, 9] for a general introduction. Following [5], we consider a generalized
alignment metric: non-stationary cost (e.g., recent events may count more than
older ones) and non-linear shift (e.g., neglecting tiny delays), as described in
Fig. 1. Two kinds of operations are deﬁned for an alignment metric.
(i) Event insertion/deletion, the cost of each operation being set to γ±¯t at time
¯t, e.g., γ±¯t = 1, while non-stationary diﬀerent insertion/deletion costs may be
deﬁned.
(ii) Event shift, the cost to shift from one event in ¯Z at time ¯t to one event Z
at time t, being an increasing positive function of the non-stationary normalized
t)/τ ), for a given time-constant τ (e.g. φ¯t((¯t
shift delay φ¯t((¯t
/τ ),
|
while non-stationary non-linear diﬀerent forward/backward shift-cost may be
deﬁned, since φ() is parameterized by ¯t.

t)/τ ) =

¯t
−
|

−

−

t

From the upper to the lower event train is
shown, using from top to bottom an insertion,
a rightward shift, a leftward shift and a deletion
respectively

Fig. 1: An example of minimal alignment (borrowed from [8]).

Obviously the distance is zero (no editing operation) if and only if both
trains are equal, is always bounded by the number of events in both event-trains
(i.e. the cost of deleting/inserting all events), thus also by twice the number
of samples in the discretized case. For small τ , the distance approaches the
number of non-coincident events, since instead of shifting events it is cheaper
to insert/delete non-coincident events, while when τ
0, γ±t = 1 we obtain
the coincidence (or Hamming) distance equal to the number of non-coincident
events. Given two time sequences with the same number of events, there is
always a τ high enough for the distance to correspond to the weighted sum of
time diﬀerences between both train events, as used in, e.g., [4]. More generally,
for high τ , the distance basically equals the diﬀerence in event number (rate
distance) [8].
When considering event-trains with more than one unit, our approach consists to
sum the distances for each unit alignment, i.e., consider each unit independently,
avoiding the related estimation to suﬀer from NP-completeness [9].

→

2 Deﬁning indexed alignment divergence

−

±

Since we want to tune the Z events in order to approximate the ¯Z events, let us
introduce an alignment indexation as follow: δ(t) = ¯t
t if the two events Z[t] = 1
and ¯Z[¯t] = 1 are aligned by a shift, δ(t) =
0 to code for an insertion/deletion,
while δ(t) = 0 otherwise. In words, we not only compute the distance but make
explicit the alignment operations (shif t, deletion, insertion) allowing to “edit”
Z in order to obtain ¯Z. The δ code function is used to explicitly match both
trains.
The distance d¯k,k between the ﬁrst ¯k events in ¯Z and the ﬁrst k events Z and
the related δ indexing are iteratively deﬁned by induction, after [8] but now
generalized (see [7] for a detailed derivation). We write tk, k > 0 the k-th
value such that Z[tk] = 1, with a similar notation for ¯t¯k, ¯k > 0. On one hand,
d¯k,0 =
, since the distance between any event-train and the empty
event-train corresponds to the cost of deleting all events, while δ(t¯k) =
0 in
tl corresponds to inserting all events, with
this case. Similarly, d0,k =
δ(t¯k) = +0. On the other hand:
P

l 105 and complex dynamics the method may fail
ﬁnding the exact solution with the standard parameters. For small length epoch,
as expected, there is always an exact solution, in fact there is one, even if the

∈

∈

raster is not generated from the same model [11].
0 and using several values of
These tests have been done for various values of τ
[0.01, 0.1]. The key point is that we can obtain good numerical results
margin ν
“even if” proﬁles are ﬁnally very sharp, using the proposed continuation method,
consisting of numerically drive υ
0. We have experimented using the conju-
gate gradient algorithm of the GSL (http://www.gnu.org/s/gsl) library, but
have also checked that this is not a critical choice. Robustness has been checked
for γ±t = 1, φt(s) =
and several for generalized metric also. Further numerical
results are provided as supplementary material of this submission, while the code
is available in the open-source EnaS library (http://enas.gforge.inria.fr).

s
|
|

→

≥

5 Conclusion

The key point, here, is the non-learnability of even-based networks [12], since
it is proved that this problem is NP-complete, when considering the estimation
of both weights in the general case, except for exact simulation [11]. We show
that we can “elude” this caveat and propose an alternate eﬃcient estimation
mechanism, inspired by alignment metrics used in spike train analysis [9], thus
providing a complement of other estimation approaches [4], beyond usual convo-
lution metric [9, 5]. At last, the proposed molliﬁcation is a series of convolution
metric, but that converges towards the expected alignment metric.

References

[1] F. Rieke, D. Warland, Rob de Ruyter van Steveninck, and William Bialek. Spikes, Ex-

ploring the Neural Code. The M.I.T. Press, 1996.

[2] W. Gerstner and W. Kistler. Spiking Neuron Models. Cambridge University Press, 2002.

[3] H. Paugam-Moisy and S.M. Bohte. Handbook of Natural Computing, chapter Computing

with Spiking Neuron Networks. Springer Verlag, 2009.

[4] Benjamin Schrauwen. Towards Applicable Spiking Neural Networks. PhD thesis, Univer-

siteit Gent, Belgium, 2007.

[5] Bruno Cessac, H. Paugam-Moisy, and Thierry Vi´eville. Overview of facts and issues about

neural coding by spikes. J. Physiol. Paris, 104:5–18, 2010.

[6] T. Vi´eville, B. Cessac, and R. Salas.

weight adjustment
http://www.loria.fr/ vthierry/results/vieville-etal-2012b.pdf, INRIA, 2011.

(extended version).

Using event-based metric for network
for ESANN’12
Supplementary material

[7] T. Vi´eville and L. Bougrain.

A general algorithm to estimate

networks weights
http://www.loria.fr/ vthierry/results/vieville-etal-2012a.pdf, INRIA, 2011.

Supplementary material

(extended version).

recurrent
for CAP’12

[8] J.D. Victor. Spike train metrics. Current Opinion in Neurobiology, 15(5):585–592, 2005.

[9] C. Houghton and Victor. Measuring representational distances - the spike-train metrics

approach. MIT Press,, 2011.

[10] J. Dauwels, F. Vialatte, T. Rutkowski, and A. Cichocki. Measuring neural synchrony by
message passing. In Advances in Neural Information Processing Systems (NIPS), 2007.

[11] H. Rostro-Gonzalez, Bruno Cessac, Juan-Carlos Vasquez, and Thierry Vi´eville. Back-
engineering of spiking neural networks parameters. Research report, INRIA, 2010.
in
preparation.

[12] Jiˇr´ı ˇS´ıma and Jiˇr´ı Sgall. On the nonlearnability of a single spiking neuron. Neural

Computation, 17(12):2635–2647, 2005.

[13] S. Chemla, F. Chavane, T. Vieville, and P. Kornprobst. Biophysical cortical column
model for optical signal analysis. In W. R. Holmes, R. Jung, and F. Skinner, editors,
Sixteenth Annual Computational Neuroscience Meeting (CNS), volume 8, Suppl 2 of
BMC Neuroscience, July 2007.

[14] Thierry Vi´eville, S. Chemla, and P. Kornprobst. How do high-level speciﬁcations of the

brain relate to variational approaches? J. Physiol. Paris, 101, 2007.

[15] N. Rougier. Dynamic neural ﬁeld with local inhibition. Biological Cybernetics, 94(3):169–

179, 2006.

[16] Bruno Cessac. A discrete time neural network model with spiking neurons. rigorous results

on the spontaneous dynamics. J. Math. Biol., 56(3):311–345, 2008.

[17] Bruno Cessac and T. Vi´eville. On dynamics of integrate-and-ﬁre neural networks with

adaptive conductances. Frontiers in neuroscience, 2(2), July 2008.

[18] R. Brette, M. Rudolph, T. Carnevale, M. Hines, D. Beeman, J. M. Bower, M. Diesmann,
A. Morrison, P. H. Goodman, F. C. Harris Jr., M. Zirpe, T. Natschl¨ager, D. Pecevski,
G. Bard Ermentrout, M. Djurfeldt, A. Lansner, O. Rochel, Thierry Vi´eville, E. Muller,
A. P. Davison, S. El Boustani, and Alain Destexhe. Simulation of networks of spik-
ing neurons: a review of tools and strategies. Journal of Computational Neuroscience,
23(3):349–398, 2007.

[19] R. Guyonneau, R. vanRullen, and S.J. Thorpe. Neurons tune to the earliest spikes through

stdp. Neural Computation, 17:859–879, 2005.

[20] A. Delorme, L. Perrinet, and S. Thorpe. Network of integrate-and-ﬁre neurons using rank
order coding b: spike timing dependant plasticity and emergence of orientation selectivity.
Neurocomputing, 38:539–545, 2001.

[21] P. Baudot. Nature is the code: high temporal precision and low noise in V1. PhD thesis,

Univ. Paris 6, 2007.

[22] Richard H. Masland and Paul R. Martin. The unsolved mystery of vision. Curr Biol.,

17(15):R577–82, Aug 2007.

[23] K. Koch, J. McLean, M. Berry II, P. Sterling, V. Balasubramanian, and M.A. Freed. Eﬃ-
ciency of information transmission by retinal ganglion cells. Current Biology, 14(17):1523–
1530, 2004.

[24] C. VanVreeswijk. What is the neural code? 23 Problems in System neuroscience. van

Hemmen, J.L. and Sejnowski, T.Jr. (eds), Oxford University Press, 2004.

[25] Jean A. Dieudonne. Treatise on Analysis Volume II. Academic Press, 1976.

[26] R. Benedetti and J-J. Risler. Real algebraic and semi-algebraic sets. Hermann, Paris,

1990.

[27] W. Maass. On the relevance of time in neural computation and learning. Theoretical
Computer Science, 261:157–178, 2001. (extended version of ’97, in LNAI 1316:364-384).

[28] P. Dayan and L.F. Abbott. Theoretical Neuroscience : Computational and Mathematical

Modeling of Neural Systems. MIT Press, 2001.

[29] Anton Maximilian Sch¨afer and Hans Georg Zimmermann. Recurrent neural networks are
universal approximators. Lecture Notes in Computer Science, 4131:632–640, 2006.

[30] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal

approximators. Neural Networks, 2:359–366, 1989.

[31] W. Maass. Fast sigmoidal networks via spiking neurons. Neural Computation, 9:279–304,

1997.

[32] W. Maass and T. Natschlager. Networks of spiking neurons can emulate arbitrary hopﬁeld

nets in temporal coding. Neural Systems, 8(4):355–372, 1997.

[33] Wolfgang Maass and Christopher M. Bishop, editors. Pulsed Neural Networks. MIT

Press, 2003.

[34] E.M. Izhikevich. Polychronization: Computation with spikes. Neural Computation,

18:245–282, 2006.

[35] H´el`ene Paugam-Moisy, R´egis Martinez, and Samy Bengio. Delay learning and polychro-

nization for reservoir computing. Neurocomputing, 71:1143–1158, 2008.

A On modeling choices regarding deterministic discrete-

time models.

At the microscopic biological level, units are punctual models of neurons, events
are action potentials, weights stand for synaptic strength, and the dynamical
system is the neural network under consideration. At the mesoscopic biological
level, units are cortical columns (see, e.g., [13] for a discussion on the concept),
events are synchronization, rhythms, or sudden activity change, weights are
related to the average connection strength, and the dynamical system is a cortical
map (see e.g. [14] for a discussion on the concept), including dynamical neural
ﬁelds (see, e.g., [15]).

Considering discretized temporal mapping.

Considering that time is discretized is a twofold issue. On one hand, it corre-
sponds to the fact that not all continuous time sequences correspond to event
trains, since they are constrained by the network dynamics, yielding global time
constraints such as the fact that inter-event intervals are bounded by a refrac-
tory period r and the fact that event times are deﬁned up to some absolute
precision δt (see [5] for a detailed discussion), this being true in both biological
and electronical implementations. The maximal amount of information, for one
unit, is thus bounded during a ﬁnite period [0, T ] as stated in [5]:

T
r

log2

T
δt

bits.

(cid:19)

(cid:18)
In a biological context, the order of magnitude is of 1K bits/second for a neuron,
in coherence with biological observations [1]. On the other hand, at a pragmatic
level, time discretized network models are rather easy to study theoretically
[16, 17], trivial to simulate (contrary to continuous time models, see [18] for
discussion), and correspond without bias to what happens in a computer. We
thus focus on discrete time and are going to brieﬂy point out, from step to step,
to which extent the present development can be applied to continuous time
frameworks.

Considering a deterministic framework.

We also consider here a deterministic framework, i.e. work in a context where
not the “average” input/output response, but an exact or approximate speciﬁc
in a context where “each spike may
input/output response is targeted (i.e.

matter” [19, 20], which seems to be the case, e.g., in the biological visual system
working in natural scenario [21, 22, 23]). More precisely, from [5], we propose the
following pragmatic view of the network result coding scheme (i.e. the “neural
code” in a biological context [24, 1]): two results correspond approximately to
the same code if their distance with respect to a given metric or pseudo-metric1
is small. For instance, considering a rank coding scheme (i.e. in a context where
the relative temporal sorting of the events matter, but not their exact temporal
values [20]) the related pseudo-metric is discrete and easy to state: network
output are equivalent if the ranks of event trains match, and not-equivalent
otherwise. Contrary to this binary choice, our proposal is to introduce a richer
structure: The proposed modeling view is not only to consider a weak notion
of network coding where two codes can only be either equal or diﬀerent, but a
more general notion where two codes are similar up to some quantiﬁed distance.
This seems to correspond to a more realistic view of, for instance, the still
mysterious “neural code” (see [5] for a discussion), and at the estimation level
allows variational optimization mechanisms to be used.

B Variational formulation of the alignment metric

Let us derive and explain the choice of (2). We proceed in three steps.

Coincidence metric molliﬁcation. We start with the simple case where τ = 0.
In that case we have no shift but only insertion/deletion. We count γ+
if we
t
have to insert a spike (Zn[t] = 0 and ¯Zn[t] = 1) and count γ−t
if we have to
delete a spike (Zn[t] = 1 and ¯Zn[t] = 0). The total counts writes:

(3)

(4)

d0

¯Z, V

τ =0

(cid:0)

(cid:1)(cid:12)
(cid:12)

def=
=

writing:

γ±t

def=




P
P
γ+
t
γ−t
0

nt γ±t |
nt γ±t H

¯Zn[t]
(1

Zn[t]
|
2 ¯Zn[t]) (Vn[t]

−
−

θ)

,

−

(cid:1)

(cid:0)

¯Zn[t] = 1, δ[t] = 0
¯Zn[t] = 0, δ[t] = 0
δ[t]

= 0.

,
,
,
¯Zn[t]
|

This comes from the fact that

is equal to 1 if an only
= Zn[t], while the second form derives from the fact, obvious to derive,

Zn[t]

| ∈ {

0, 1



−

if ¯Zn[t]

}

1A metric, corresponding to the intuitive notion of “distance” d(R, R′) between two results
R and R′, is a real function, positive (d(R, R′) ≥ 0), symmetric (d(R, R′) = d(R′, R)),
deﬁnite (d(R, R′) = 0 ⇔ R = R′))), thus semi-deﬁnite (d(R, R) = 0), and subadditive
(d(R, R′) ≤ d(R, R′′) + d(R′′, R′), given any third result R′′).
If such a function is positive, deﬁnite, subadditive, but not symmetric, it is called a divergence.
If such a function is positive, symmetric, subadditive, but only semi-deﬁnite while not deﬁnite,
it is called a pseudo-distance. Thanks to the subadditivity, a pseudo-distance induces a distance
on the quotient space of the equivalence classes of elements at a zero distance to of each-others.
On the reverse, an equivalence relation ≡, corresponds to a discrete pseudo-metric (d(R, R′) ∈
{0, 1}) such that R ≡ R′ ⇔ d(R, R′) = 0. It is the coarser pseudo-metric compatible with the
related equivalence classes, while ﬁner distances quantify to which extents two results diﬀer
(see, e.g., [25] for a text book reference).

6
6
that:

¯Zn[t]
|

Zn[t]
|

−

=

¯Zn[t]
|

−

H (Vn[t]

θ)

|

−

= H

(1

−

2 ¯Zn[t]) (Vn[t]

θ)

.

−

(cid:0)

(cid:1)

If γ±t = 1 the distance is the Hamming distance, counting the number of

non-coincidences.

This distance is discontinuous, with a jump each time Vn[t] crosses the thresh-
old θ. We now replace the Heaviside function by a molliﬁcation, Hυ in (3). The
exact choice of Hυ is detailed in section C. The corresponding molliﬁed metric
dυ( ¯Z, V), is now explicitly varying with V, not only the value of Z, allowing to
tune the state value in order to it drive towards the correct side of the threshold.
now increases the value of Vn[t] if
θ)
−
¯Zn[t] = 1 and decreases the value of Vn[t] if ¯Zn[t] = 0 and, thus drive the the
state value towards the correct threshold side.

2 ¯Zn[t]) (Vn[t]

The term Hυ

(1

−

(cid:0)

(cid:1)

The interest of using the second form of (3), is that it has a symmetric eﬀect
for values below and above the threshold, which is not the case with the former
form.

Variational form of the alignment metric. We now consider an alignment metric
with its corresponding indexing δ.

As for the distance calculation or the divergence indexation, we can construct
the variational form by double induction, on the n-th value such that Zn[tn] = 1
and the ¯n-th value such that ¯Zn[¯t¯n] = 1. This is due to the fact that, in a
minimal path, each event can be either deleted or shifted once to coincide with
an event in the other event-train. Also, an event can be inserted only at a time
that matches the occurrence of an event in the other event-train.

Furthermore, a minimal path cannot include an insertion of an event that
is later deleted or shifted, or a deletion of an event that is later inserted or
shifted, or a shift in both direction, since the cost of such path can be reduced
by eliminating some steps. Individual events cannot intersect, since uncrossing
them reduces the amount of shifting. This means that t + δ[t] is an increasing
function, i.e.,:

t′ < t, ¯Zn[t′ + δ[t′]] = ¯Zn[t + δ[t]] = 1
∀

t′ + δ[t′] < t + δ[t]

⇒
⇔ −

1 < δ[t]
−
t
−

δ[t′
t′

]

.

(5)

, i.e., tk, k > 0 the k-th value such that Z[tk] = 1, with

We write

t1, t2,
a similar notation for ¯t¯k, ¯k > 0.
We start with d¯k,0 =

· · · }

{

due to the fact that the distance between
any event-train and the empty event-train corresponds to the cost of deleting all
tk corresponds to
events, while δ(t¯k) =
0 in this case. Similarly, d0,k =
inserting all events, with δ(t¯k) = +0 in this case.

lt

X

¯Z[t′]=1

(cid:20)

µt

1 +

δ[t′]

−
t′ −

δ[t]
t

.

(cid:21)

The minimization is to be initialized with δ[t] = 0, i.e. considering only dele-
tion/insertion and no shift.

A step further, in order to solve the ambiguity between indexing functions,
from the last time to the previous time, we must choose a function γ±t
that
decreases in the past, while its value must bounded by the higher shift cost, i.e.
φt < γ±t , for the maximal acceptable value of δ[t].

Molliﬁcation of the alignment metric. Let us ﬁnally proposes a molliﬁcation
derived from the molliﬁcation proposed for the coincidence metric and taking
into account the shifts deﬁned by the alignment metric. Here we consider that
the δ indexation is ﬁxed, and propose as criterion:

¯Z, Z
0,T

dυ
υd
(cid:0)
{

(cid:1)
{

0,T

=
nt υd
{
= γ±t Hυ
P
+ φt

,
{
(1
Zn[t] + (1

−

(cid:0)

(cid:16)

2 ¯Zn[t + δ[t]]) (Vn[t]
θ)
−
δ[t]
Zn[t]) Hυ
τ Hυ(θ
(cid:1)

−

−

(cid:16)

−

Vn[t])ν=0

.

(cid:17)(cid:17)

Obviously, the term related to γ±t enjoys the same properties as for the coinci-
dence metric and we simply re-use what has been developed in this case.
The term related to φt has a diﬀerent behavior depending on Zn[t]]:

- If Zn[t] = 1, i.e., when there are both a shift source and a shift target at time
t, there is no adjustment on Vn[t] since reducing one shift delay may increase
the other one, with an unpredictable eﬀect.
- If Zn[t] = 0, i.e., when there is only a shift source at this time t, then the value
of Vn[t] is increased/decreased depending on the sign of δ[t] in order to reduce
the absolute value of the delay:
δ[t] > 0
δ[t] < 0

better shift t in the future
better shift t in the past

t < ¯t
¯t < t

Vn[t]
Vn[t]

in accordance with the signs of variation in the criterion. Furthermore, the vari-
ation of Vn[t] is bounded in order the adjustment not to generate an additional

↓
↑

2Interesting enough is the fact we can apparently easily deﬁne a continuous form of this

variational deﬁnition writing:

d (¯z, z) = minδ

Rt γ(t)±

h|¯zn(t + δ(t)) − zn(t)| + φt “

δ(t)
τ ”i + µ(t)(1 + δ′(t))

with δ(t) = 0 as initial value, writing z(t) =
and also deﬁne the related molliﬁcation, as proposed here in the discrete case.

k ), D being the Dirac distribution,

P

tn
k

D(t − tn

spurious event. This is the reason why we consider Hυ(θ
maintain θ > Vn[t].

−

Vn[t])ν=0 in order to

This last part if the speciﬁcation is clearly a heuristic that has been ad-
justed when experimenting at the numerical level, in order to observe the proper
behavior.

Conclusion. We have made explicit the diﬀerent elements allowing us to pro-
pose (2) as a reasonable criterion. Given δ, the weights are adjusted in order to
optimize the adequacy between the desired and expected event trains. At the
implementation level, a relaxation minimization scheme is proposed:

1. Given a network event dynamic Z calculates δ to estimate the alignment

distance;

2. For this indexing δ optimize the weights to tune Vn[t] in order to reduce

the alignment distance;

3. As soon as the event train is changed, repeat step 1 in order to re-estimate

δ, stopping when a local minimum is found.

C Molliﬁcation of the Heaviside function

Let us specify how to mollify the event generation function ρ() = H(u
θ). The
Heaviside function is deﬁned here with H(0) = 0. In other words, we focus on
the fact an event is deﬁned by a state value above a given threshold θ. The
generalization to other semi-algebraic conditions is straightforward, since they
always can be stated as a combination of Heaviside functions (see [26] for a
treatise on the subject).

−

In words, we precisely need to replace H(u) by a regular function that can

inﬂuence the estimation,
- either if the condition is incorrect,
- or if the condition is correct, but close to be incorrect, i.e. at margin ν of the
correctness boundary, while
- we better require the function to have no inﬂuence if the condition is correct
and beyond this boundary.

A suitable function that ﬁts with this requirement is the Hυ (u) non-linear

regular proﬁle described in Fig. 3, and is deﬁned as:

Hυ (u) def= H(u + √υ ν) exp

,

(6)

υ
u + √υ ν

−

(cid:18)
) = 1. Here ν is a margin, i.e. it
thus with Hυ (u) = 0, u
allows one to maintain the state at a non-inﬁnitesimal distance to the threshold.

√υ ν and Hυ (+

≤ −

∞

(cid:19)

It is easy to verify that this function is regular including in 0.
It is also important to notice that the function is convex on ]

i.e., when Hυ (u) < e−

2 (drawn in magenta in Fig. 3).

, υ
2 −

− ∞

√υ ν[,

−

→

∞

Furthermore, limυ

√υ ν,
−
Hυ(u)
|

√υ ν + a[, a > 0, thus in [0, +
< (1

0 Hυ (u) = H(u), assuming that H(0) = 0. This conver-
→
[. This comes
[
gence is uniform in
−
R −
√υ), thus uniformly bounded when
H(u)
e−
from the fact that
−
|
υ
0. This would not have been the case without a margin ν > 0. More pre-
cisely, the reader can verify, that this is the denominator with an additive term
υd ν, d < 1, say d = 1/2, which guaranties the convergence and the uniformity of
√υ ν, however, it is not possible to have
this convergence in [0, +
convergence uniformity, which seems to be intrinsically related to the disconti-
nuity of H. As a consquence, at the numerical level values must not remain in
this neighborhood, to avoid “spurious jumps” when υ
Hυ

|2 is bounded for the L2 norm (which is not the case
= 2 log(2), so that

−
for the L1 norm), since

A step further,

[. Around

Hυ(u))2du

(H(u)

H
|

→

∞

−

0.

+

∞
−∞

−

υ=1,ν=0

Hυ

H
|

−
Since Hυ(u) = 0, u

|2 is ﬁnite, which is also an important fact for numerical algorithms.

√υ ν, while still regular, the inﬂuence of obviously
correct events is avoided, allowing the criterion to better tune more critical
events.

≤ −

R

(cid:12)
(cid:12)
(cid:12)

More precisely :

H ′υ (u) = Hυ (u)

υ

(u + √υ ν)2 = H

while Hυ (u) < H(u) for u > 0.

u + √υ ν

(cid:0)

(cid:1)

υ

(u + √υ ν)2 + O

1
u

,

(cid:19)

(cid:18)

The derivation is not obvious since formally:

H ′υ (u) = b(u + √υ ν) + Hυ (u)

υ
(u+√υ ν)2

, D() being the Dirac distribution. However, for all

with b(u) def= D (u) exp
−
compact domain bounded function φ(), we obtain:
φ(u)

(cid:0)
(cid:1)
b(u) φ(u) =

exp

υ
u

thus b() = 0 in the distribution sense.

(cid:2)
A step further, there is a closed-form for the n-th derivative, i.e.

R

(cid:0)

(cid:1)

υ
u

−

u=0 = 0
(cid:3)

H (n)
υ

(u) = Hυ (u)
= Hυ (u)

n
1
k=0 (
−
υn

1)k µk,n
−
(u+√υ ν)2 n + O
P

υn−k
(u+√υ ν)2 n−k

1
u2 n−1

for positive integers, iteratively deﬁned as:

µk,n =

1 + (2 n

−

−

k + 1) µk

1,n

−

1

−

1
µk,n
0




≤
straightforward to derive from a piece of computer algebra.



Morally, this molliﬁcation corresponds to a convolution Hυ = υ

H with
respect to some suitable convolution kernel (without, up to our best knowledge,
a closed form expression).

∗

All together, the choice of the molliﬁcation kernel is not trivial. Better choices

do exists, but they have to enjoy at least the properties made explicit here.

,

,

(cid:0)

(cid:1)
k = 0
0 < k < n
k
n

D Numerical veriﬁcation

In order to estimate the variational method performance in terms of precision,
we consider the simplest possible linear feed-forward network:

Vn[t] =

Wnm Im[t] + βn[t], Zn[t] = H(Vn[t]

m
X

θ)

−

where the the input Im[t] is a known deterministic signal and the additive noise
βn[t] is drawn from a zero-mean random Gaussian distribution of standard de-
viation β. Using this model, we perform a master-slave validation test. The
weights of the master are drawn from a Gaussian distribution of standard devi-
ation σ and the additive noise of magnitude β, is added to the master output
before presented to the estimation. The estimation is done on a slave noiseless
linear feed-forward network of the same number of output units N and input
units M , considering C trials of T time steps, which initial weights are similarly
randomly drawn. Here we use N = 4, M = 2, T = 1000, C = 2 with θ = 0.5.
And typically consider σ = 1, β = 0.25 unless speciﬁed. Default alignment
metric parameters are υ = 1, µ = 0.1 and τ = 0 unless their values are explored.
Typical results are illustrated in Fig. 4, where it is numerically shown that
the estimation using the proposed alignment variational estimation has the same
performance as the “ideal” least-square estimation on the state values for small
data errors. This is not a trivial result since the alignment variational estimation
mechanism can not observed the master state values but only the related events.
Furthermore, we approximate here a non-regular discontinuous criterion by the
molliﬁcation mechanism and there were no guaranty that it should work. Here it
does for small and reasonable errors (up to 30% of noisy events in this particular
case). Fig. 4 also shows that the dispersion for this numerical test is small,
yielding a robust result.

A step further, we numerically verify in Fig. 5 that the molliﬁcation param-
eters values have only a marginal inﬂuence on the ﬁnal estimation, in this case.
The use of a margin yields better results, as hypothesized in the speciﬁcation,
while the molliﬁcation introduces a small additional bias reduced when the mol-
liﬁed approximation converges towards the true alignment metric, these eﬀects
being of the seconder order. We also verify in Fig. 5 that these results are stable
for diﬀerent values of time-shift cost.

E Discussion.

Let us propose a short discussion about general links between the present de-
velopment and what is known regarding the computational power of artiﬁcial
or biologically plausible neural networks with event-based (i.e., spiking) activity
[1, 27, 28, 2].

Fig. 4: Estimation error bounds : Alignment error obtained without any es-
timation (in gray), with a least-square estimation on the state values (in red),
and using the proposed alignment variational estimation (in green), for diﬀerent
additive noise β in abscissa. The ordinate corresponds to the average alignment
error form 0 for an exact estimation to 0.5 for a totally random result. The align-
ment molliﬁcation estimation is bounded by the non-estimation case from above
an ideal least-square estimation where all state values (and not only the events)
could be known from below. Error dispersion : Representation of the numerical
test dispersion for the alignment metric estimation. Error-bars correspond to
one standard-deviation.

±

Fig. 5: Molliﬁcation parameters error dependency : Marginal inﬂuence on the
ﬁnal estimation of the molliﬁcation parameters υ in abscissa, for three mar-
gin parameter values µ =
respectively.
in
The ordinate corresponds to the average alignment error. Time-shift cost error
dependency : Comparison between the alignment error obtained without any es-
timation (in gray) with the proposed alignment variational estimation (in green),
using the same conventions as for Fig. 4, showing the stability of the estimation
not only for an event-count metric.

green, magenta, cyan
}

0, 0.1, 0.2

{

{

}

The key problem of calculability: non-learnability.

It is known that recurrent neuron networks with frequency rates are universal
approximators [29], as multilayer feed-forward networks are [30]. This means
that neuron networks are able to simulate dynamical systems, not only to ap-
proximate measurable functions on a compact domain, as originally stated (see,
e.g., [29] for a detailed introduction on these notions).

Spiking neuron networks have been proved to be also universal approximators
[27] and that, theoretically, spiking neurons can perform very powerful computa-
tions with precise event timings. Spiking neurons are at least as computationally
powerful as the sigmoidal neurons traditionally used in artiﬁcial neuron networks
[31, 32]. This result has been shown using a spike-response model (see [33] for
a review) and considering piece-wise linear approximations of the membrane
potential proﬁles. In this context, analog inputs and outputs are encoded by
temporal latencies of event ﬁrings.
It has been shown that any feed-forward
(multi-layer) or recurrent analog neuronal network (e.g. Hopﬁeld network) can
be simulated arbitrarily closely by an insigniﬁcantly larger network of spiking
neurons. The assertion holds even in the presence of noise [31, 32]. Such theo-
retical results highly motivate the use of spiking neuron networks for modelling
and simulation purpose.

The key point, however, is the non-learnability of spiking neurons [12], since
it is proved that this problem is NP-complete, when considering the estimation
of both weights and delays. Here we show that we can “elude” this caveat and
propose an alternate eﬃcient estimation, inspired by biological models.

We also have to notice, that the same restriction apply not only to simulation
but, as far as this model is biologically plausible, also holds at the biological level.
It is thus an issue to wonder if, in biological neuron networks, delays are really
estimated during learning processes, or if a weaker form of weight adaptation,
as developed here, is considered.

As far as this contribution is concerned, we consider a weak notion of bio-
logical plausibility: A simulation is biologically plausible if it veriﬁes an explicit
set of constraints observed in biology. More precisely, we have taken time con-
straints, shared by all dynamics, further called “general time constraints”. The
time constraints are based on biological temporal limits and appear to be very
precious quantitative elements, both for estimating the coding capacity of a
system and for improving simulations.

Considering such learnability constraints, i.e., how can artiﬁcial or biological

systems by-pass such computational barrier ?

By-passing the non-learnability barrier.

As pointed out previously, the weights estimation is proved to be NP-complete.
This means that in order to “learn” the proper parameters we have to “try
all possible combinations of delays”. This is intuitively due to the fact that
each delay has no “smooth” eﬀect on the dynamics but may change the whole
dynamics in an unpredictable way.

This is the way proposed to elude this NP-complete problem by consider-
ing another estimation problem. Here we do not estimate one delay (for each
synapse) but consider connection weights at several delays and then estimate
a balancing of their relative contribution. This means that we consider a weak
delay estimation problem.

The alternative approach is to estimate delayed weights, i.e. a quantitative
weight value W ′′ojd or W ′ijd at each delay d
. Obviously, the case where
1, D
}
there is a weight Wij with a corresponding delay dij
is a particular case
of considering several delayed weights, since we can write Wijd = Wij δ(d
dij),
δ() being the Kronecker symbol in this case. In other words, with our weaker
model, we are still able to estimate a neuron network with adjustable synaptic
delays.

0, D

∈ {

∈ {

−

}

We thus do not restrain the neuron network model by changing the problem,
but enlarge it. In fact, the present estimation provides a smooth approximation
of the previous NP-complete problem.

The key idea of ﬁnding an approximate solution to the previous NP-complete
problem, is instantiated at the implementation level, using molliﬁed metrics, and
the interest of such approach has been stated considering numerical examples.

The main limit of the present approach: polychronization.

A spiking network can polychronize, i.e., exhibit reproducible time-locked but
not synchronous ﬁring patterns within 1 millisecond precision. Polychronization
can be viewd as a generalization of the notions of synchronization and synﬁre
chains. Due to the interplay between the delays and a form synaptic plastic-
ity, the spiking neurons spontaneously self-organize into groups and generate
patterns of stereotypical polychronous activity.

In [34], it has been shown that the number of co-existing polychronous groups
far exceeds the number of neurons in the network, resulting in an unprecedented
memory capacity of the system. The author speculates on the signiﬁcance of
polychrony to the theory of neuronal group selection and cognitive neural com-
putations.

In [35], the network processing and the resulting performance is explained by
the concept of polychronization, The model emphasizes that polychronization
can be used as a tool for exploiting the computational power of synaptic delays
and for monitoring the topology and activity of a spiking neuron network.

Taking such complex aspects of the neural code into account cannot be per-
formed by any available metrics. New metrics, taking long term interactions
into account have to be developed and this is a challenging issue. This is not
the case here and a challenging perspective of the present development.

