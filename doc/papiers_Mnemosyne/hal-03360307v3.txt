Ontology as neuronal-space manifold: Towards symbolic and numerical
artificial embedding Chloé Mercier, Hugo Chateau-Laurent, Frédéric
Alexandre, Thierry Viéville

To cite this version:

Chloé Mercier, Hugo Chateau-Laurent, Frédéric Alexandre, Thierry
Viéville. Ontology as neuronal- space manifold: Towards symbolic and
numerical artificial embedding. KRHCAI 2021 Workshop on Knowledge
Representation for Hybrid & Compositional AI @ KR2021, Nov 2021, Hanoi,
Vietnam. ￿hal-03360307v3￿

HAL Id: hal-03360307

https://inria.hal.science/hal-03360307v3

Submitted on 8 Nov 2021

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International
License

Ontology as neuronal-space manifold: Towards symbolic and numerical
artiﬁcial embedding ∗

Chlo´e Mercier1 , Hugo Chateau-Laurent1 , Fr´ed´eric Alexandre1 ,
Thierry Vi´eville1 1Mnemosyne Team, Inria Bordeaux, LaBRI and IMN
ﬁrstname.lastname@inria.fr

Abstract

Some human cognitive tasks may involve tightly interleaved logical and
numerical computations. On the one hand, on- tologies allow us to
describe symbolic structured knowledge and perform logical inference,
providing a rather natural rep- resentation of human reasoning as
modeled in cognitive psy- chology. On the other hand, spiking neural
networks are a biologically plausible implementation of processing in
brain circuits, yet they process numeric vectors rather than sym- bolic
data. Unifying these symbolic and sub-symbolic ap- proaches is still a
wide and open question, and the Semantic Pointer Architecture (SPA)
based on the Vector Symbolic Ar- chitecture (VSA) provides a way to
manipulate symbols em- bedded as numeric vectors that carry semantic
information. In this paper, as a step towards ﬁlling the
symbolic/numerical gap, we propose to map an ontology onto a SPA-based
archi- tecture with a preliminary partial implementation into spik- ing
neural networks. More speciﬁcally, we focus on ontol- ogy standards used
in the semantic web such as Resource Description Framework [Schema]
(RDF[S]) and the Web On- tology Language (OWL). We provide a detailed
implementa- tion example in the case of speciﬁc RDFS entailments based
on predicate chaining. To that end, we used the neural sim- ulator Nengo
with two associative memories in interaction, the ﬁrst one storing
assertions and the second one storing entailment rules. Reporting
interesting formal results, our embedding enjoys intrinsic properties
allowing semantic rea- soning through distributed numerical computing.
This orig- inal preliminary work thus combines symbolic and numeri- cal
approaches for cognitive modeling, which might be use- ful to model some
complex human tasks such as ill-deﬁned problem-solving, involving
neuronal knowledge manipula- tion.

Keywords: Ontology, Resource Description Framework, Vector Symbolic
Architecture, Semantic Pointer Architec- ture, Neural Engineering
Framework, Neurosymbolism.

1

Introduction

Artiﬁcial Intelligence (AI) has recently made signiﬁcant strides, both
in numerical approaches and symbolic ap- proaches; the former is based
on numerical computation and includes, among others, neural networks or
Bayesian infer- ence, while the latter manipulates knowledge bases such
as

∗Supported by mnemosyne/en/aide.

Inria, AEx AIDE https://team.inria.fr/

used in expert systems or the semantic web. Accordingly, large AI
systems with impressive capabilities have been pro- posed. Nonetheless,
the AI ﬁeld is still lacking uniﬁed sys- tems integrating both
approaches (Sun and Alexandre 2013), although the last decade has seen
the emergence of a new “neurosymbolic” wave aiming to make neural
networks per- form logical reasoning (Garcez and Lamb 2020).

It is generally considered that human cognitive capabil- ities are not
within the reach of either purely symbolic or purely numerical artiﬁcial
intelligence because of the limi- tations of each approach. In hybrid
systems, both symbolic and numerical components are involved (Lallement,
Hilario, and Alexandre 1995), but their contributions are usually kept
separate in distinct aspects of the system. This is the case for example
with ontology-based deep learning (e.g., (Phan et al. 2017; Petrucci,
Ghidini, and Rospocher 2016; Hohenecker and Lukasiewicz 2020)) or
“black-box” co- operation between deep-networks and ontology reasoners
(e.g., (Ayadi et al. 2019; Jim´enez, Elizalde, and Raj 2018)). However,
geometric mapping of ontologies onto Euclidean spaces or manifolds
(e.g., (Eidoon, Yazdani, and Oroum- chian 2008; Tous and Delgado 2006;
Xiao, Huang, and Zhu 2015)) allows to perform reasoning at both a
symbolic and to a limited extent numerical level, while the link be-
tween neural network representation and manifolds is well established
and understood (e.g., (Chui and Mhaskar 2018; Zhu et al. 2017)).

In some cases, there is a need for a full integration of both paradigms,
as allowed by a uniﬁed system, because the underlying task corresponds
to a tight interaction between the properties of the paradigms, which
cannot be achieved with separate modules. This is the case with the
topic we consider in our group, that is human learning. In a recent
development (Mercier et al. 2021), we propose to formal- ize a
problem-solving task, where children have to manip- ulate and assemble
objects to answer instructions given by the teacher (Romero, David, and
Lille 2019). It has been re- viewed in the previous reference that the
underlying cogni- tive functions are as diverse as sensorimotor skills,
exploita- tion of explicit and sometimes partial knowledge, hypothesis
generation and creativity.

In the case of such tasks with so tightly interleaved sym- bolic and
numerical components, some authors propose that a purely numerical,
neuronal approach could implement the

needed uniﬁed system, with an obvious reference to the brain as an
example of a neuronal system manipulating sym- bolic information with
units generally considered as numer- ical systems, especially to
implement high level cognitive functions (see, e.g., (Pulverm¨uller
2013) for a general re- view). Yet it is a wide and still open issue to
establish the numerical primitives required for modeling symbolic rep-
resentation and manipulation (see (Alexandre 2019) for a general
discussion). In the last few years, models using nu- merical mechanisms
to process symbolic information such as logical computation (Shi et
al. 2020) or reasoning (Riegel et al. 2020) have ﬂourished.

There are numerous approaches to vectorize graph data models, such as
translational distance models or random walk based methods (see (Wang,
Qiu, and Wang 2021) for a recent review in link with approximate
statistical reasoning, or (Sajjad, Docherty, and Tyshetskiy 2019)
regarding repre- sentation learning from data) including taking into
account dynamic knowledge such as (Sauerwald and Zanetti 2019),
including approaches combining vector representations and inference
rules such as (Guo et al. 2016). There are also many real valued ﬁrst
order logic (e.g., probabilistic) ap- proaches integrated in or
interfaced with machine learning algorithms (see (Garcez and Lamb 2020)
for a general re- view and, e.g., (Cohen, Yang, and Mazaitis 2017) for
an ex- ample). Our approach is complementary in the sense that we focus
on both a biologically plausible neuronal frame- work, and a symbolic
formalism based on triples (as deﬁned in 2.4) that seems appropriate to
model human reasoning in psychology (McClelland and Rogers 2003).

Speciﬁc questions may arise when trying to integrate for example
relations between ontology speciﬁcations and par- tial knowledge
(Tettamanzi, Zucker, and Gandon 2017), al- lowing one to consider not
only true or false knowledge, but relative degree of truth, as taken
into account here. Most of machine learning algorithms represent such
“partial truth” with only probability. However, the human “level of
truth” seems to be different and related to other notions such as
possibility and necessity, related to a given modality, that is a
context, a given time, and so on, which is also considered as
representative to what is modeled in educational science and philosophy
(see (Smith 1994) while (Rusawuk 2018) proposes a discussion).

As a possible entry to a uniﬁed approach, Vector Sym- bolic
Architectures (VSA) were introduced as a way to ma- nipulate symbolic
information represented as numeric vec- tors (see e.g. (Levy and Gayler
2008) for an introduction). VSAs have been proven helpful to model
high-level cog- nition and account for multiple biological features
(Gayler 2003; Eliasmith 2013). More speciﬁcally, the Semantic Pointer
Architecture (Eliasmith 2013) instantiates so-called semantic pointers
(i.e. vectors that carry semantic informa- tion) and their manipulation
in networks of spiking neurons. This approach makes a signiﬁcant step
towards the uniﬁca- tion of symbolic and sub-symbolic processing in that
it pro- vides a way to translate the former into the latter. Conse-
quently, complex knowledge representation in the form of compositional
structures that are traditionally restricted to symbolic approaches can
now be distilled in numerical and

even neural systems (Crawford, Gingerich, and Eliasmith 2016).

On the other hand, considering knowledge representation and reasoning,
the capabilities of Semantic Web modeling languages, such as RDFS
(Resource Description Framework Schema) and OWL (Web Ontology Language)
(see, e.g. (Allemang, Hendler, and Gandon 2020) for a recent didactic
reference) is a rather accessible and very powerful way of solving
modeling problem and manipulate high-level data representation. To what
extent could such mechanism be bi- ologically plausible ? In order to
contribute to this issue, we show here that suitable design choices
allow us to make explicit how to implement a RDFS1 speciﬁcation using
the Semantic Pointer Architecture. We illustrate this on a simple
example and also discuss to which extent OWL speciﬁcation could beneﬁt
from the same method.

2 Basic design choices

2.1 From symbols to numbers In order to represent symbolic information,
we use RDFS to structure knowledge representation. It is based on the
RDF data model, which represent knowledge as triples, as made explicit
in section 2.4. More precisely, the universe of dis- course is made of
resources, referenced by some universal resource identiﬁer (IRI), i.e. a
ﬁxed lexical token. To struc- ture this universe of discourse, we
consider:

(i) individuals that refer to real-world concrete or abstract

objects, or

(ii) literals to characterize individuals using data attributes, i.e.,
     numerical values, character strings, or any structured information
     such as dates

(iii) concepts and roles (namely classes and properties) that allow to
      structure the knowledge about individuals.

Before going further, we can point out that the present deﬁnition
follows the RDF/RDFS framework, with the fol- lowing variants:

• we conﬂate name with both IRI and blank node, since on the one hand
blank node can be eliminated,2 and on the other hand because we only
process the information locally at this stage, thus avoiding considering
all is- sues regarding distributed information between different
sources;

• we do not consider (i) semantic web speciﬁc literal (e.g.,
rdf:XMLLiteral), or (ii) utility and annotation or other human-targeted
properties (e.g., rdfs:seeAlso) at this stage;

• we will introduce both containers, i.e., ordered or un- ordered
sequences, and collections, i.e., chained lists, later in these
speciﬁcations, but in a somehow different form, adapted to the numerical
representation and obvi- ous to map on RDF representations;

1According to the https://www.w3.org/TR/rdf-schema speciﬁ-

cation.

2Using a standard process related to skolemisation.

• we do not consider all XSD data-types, but will introduce a precise
notion of numerical values and will detail how to represent structured
data in our framework.

At the numerical level, each resource is implemented as a randomly drawn
ﬁxed unit d-dimensional vector, x ∈ Rd, and the key idea is to study to
what extent symbolic rea- soning on resources may correspond to
algebraic operations implemented via numerical computations (that we
make ex- plicit in 3.1., based on a framework introduced in (Eliasmith
2013)). Typically d (cid:39) 100 · · · 1000 and we expect to manip-
ulate k (cid:39) 100 · · · 10000 resources.

A similarity measure is now introduced in order to seman-

tically compare two vectors.

2.2 Semantic similarity

Classically, the cosine similarity (i.e., normalized dot prod- uct,
denoted ·) is used to compute the semantic similarity between two unit
vectors:

x · y def= x(cid:62)y where x(cid:62) denotes the transpose of x.

The key property is that, provided that the space dimen- sion d is large
enough, two randomly chosen different vec- tors will be approximately
orthogonal. More precisely,

x · y ∼ N (0, O(1/d)), i.e., follows a centered normal distribution
(Schlegel, Neu- bert, and Protzel 2020), while by construction x · x =
1.

Based on this, most VSA approaches consider that 2 vec- tors x and y are
semantically equivalent when this similarity τ equals to 1, but with
different ways to interpret the result: - Closed-world reasoning:
Anything, that cannot be stated as true is false, thus τ ∈ {0, 1}, often
obtained by projection and rectiﬁcation (to avoid negative values). This
is the most common interpretation in VSAs. - Open-world reasoning:
Anything might be true unless it can be proven false, prompting a need
to characterize un- known statements; furthermore the notion of negation
is ei- ther not deﬁned (as in the RDFS model yielding monotonic
reasoning only), or deﬁned at a higher level (as in OWL and more
generally in description logics) that we also consider as a perspective
of this work. Here we enrich the notion of being either false or true,
by a numeric representation of partial knowledge, as illustrated in Fig
1. The true value corresponds to 1 (fully possible and fully necessary),
the false value to -1 (neither possible nor necessary, i.e., im-
possible) and the unknown value to 0, which corresponds to a fully
possible but absolutely not necessary value. This representation has
been designed to be compatible with the ternary Kleene logic, beside
being also coherent with respect to the possibility theory3 (not
developed here, please refer to

3To make the link explicit, given necessity ν and possibility π, with ν
≤ π by construction, while ν > 0 ⇒ π = 1 and π < 1 ⇒ ν = 0, we have the
one to one correspondence with our representation using τ ∈ [−1, 1]:

τ def= ν + π − 1 with

(cid:26) π = 1 + H(−τ ) τ ν = H(τ ) τ,

(Denœux, Dubois, and Prade 2020) for a general introduc- tion). This
deterministic representation of partial knowledge can be generalized in
order to also include a probabilistic representation (using a 2D
representation), although we will not develop this aspect any further.

Figure 1: Representation of partial truth τ ∈ [−1, 1], in link with
necessity and possibility.

2.3 Classes and approximate Boolean properties

The ﬁrst kind of concept to structure the knowledge is the hi-
erarchical notion of class, which is equivalent to the notion of Boolean
property, deﬁning the class of all individuals en- joying (or not) this
property, and deﬁning the property that an individual belongs (or not)
to a given class.

In RDFS this translates into, given a scoring τ as intro-

duced previously: x rdf:type c (cid:46) τ

We assume in the following that classes and individuals are in disjoint
semantic sets. Although this is not necessar- ily the case in RDFS, this
assumption is compatible with description logics (i.e. the OWL-DL level
of speciﬁcation) we target to use in future developments, and ensures to
avoid self-reference paradoxes.

At the numeric level, they both correspond to unit vectors,

with the following interpretation: - For a given vector x encoding an
individual, the vector c = x corresponds to the singleton class C = {x}.
- For a given class vector c the individual vector x = c can be
interpreted as a “prototype” for this class.

At a geometric level, this can be interpreted as covari-
ant/contravariant duality, and the similarity between a class covariant
vector c and an individual contravariant vector x may be interpreted as
the fact this individual approximately belongs, or not, to the class.

2.4 Statements: facts and rules

In RDF, the knowledge about the universe of discourse the form subject
is structured into statements of predicate object, called triples, where
subject and object are two resources linked by a relationship (property)
explicitized by the predicate. We introduce weighted triples of the
form:

subject predicate object (cid:46) τ with a value τ as discussed
previously, which generalizes usual RDF statements, introducing a
scoring value (refer to (Tettamanzi, Zucker, and Gandon 2017) for an
introduction and a recent literature review). This is easily stated in
the RDF language itself, using reiﬁcation, while in our case, it is an
intrinsic feature of our design. More precisely, when considering RDFS
where only true (but not false) assertions can be stated, we have τ ∈
[0, 1], while when generalizing to the OWL language where negation can
be stated we will use

τ ∈ [−1, 1], this setting being compatible with both levels of
speciﬁcation.

It is worth noting that such statements are of three kinds,

although this is not explicit in the model

entity data-property literal (cid:46) τ deﬁnes attributes allowing to
specify some entity attributes; entity object-property entity (cid:46) τ

deﬁnes relations between entities;

concept predicate resource (cid:46) τ deﬁnes meta-properties about
classes or properties.

3 Ontology numerical mapping

3.1 A Semantic Pointer Triplestore Let us suppose we need to represent
the following weighted statements:

subject1 predicate1 object1 (cid:46) τ1 subject2 predicate2 object2
(cid:46) τ2 subject2 predicate3 object3 (cid:46) τ3 (note that the same
subject subject2 is used in the two

last statements).

The ﬁrst step towards a Semantic Pointer representation is to encode
each resource by a randomly sampled vector on the unit hypersphere of
our d-dimensional vector space: let us denote si, pi, oi the respective
vector representations of the resources subjecti, predicatei, objecti.

Next, we may store each of these statements into an asso- ciative memory
(Stewart, Tang, and Eliasmith 2011), similar to a hash table, where each
“key” would be a resource and the corresponding value would express the
statements for which this resource is a subject:4

s1 → τ1 B(o1, p1)

s2 → τ2 B(o2, p2) + τ3 B(o3, p3)

for which we need to introduce the following operations: 1. A scalar
multiplication a = τ b that scales a vector b by

a factor of τ and preserves its direction.

2.  A vector superposition (e.g. element-wise addition) a = b+c that
    results in a vector a with a·b = 1 and a·c = 1, assuming b and c are
    orthonormal.

3.  A binding operation a = B(b, c) that outputs a vector a

that is not collinear with either b or c. Let us discuss in detail this
last ingredient. The Seman- tic Pointer Architecture (SPA) developed by
Eliasmith et al(Eliasmith 2013) implements such operations. This cog-
nitive architecture builds upon a particular case of Vector Symbolic
Architecture (VSA) and the Neural Engineering Framework (NEF) (Eliasmith
and Anderson 2002). The NEF provides a set of principles for
representing vectors into biologically plausible networks of neurons,
and imple- menting the desired transformations through synaptic con-
nections (see (Eliasmith and Anderson 2002) for technical details). This
framework (including both NEF and SPA) is already implemented into a
simulator called Nengo (Beko- lay et al. 2014).

4In RDF/RDFS, this kind of database is known as a triplestore,

as it stores statements also referred to as triples.

Several choices are available for the binding operation. The most
classical one is the circular convolution a = b (cid:126) c, which is
used in Holographic Reduced Represen- tations (HRR) (Plate 1995). This
operation is commutative, associative and distributive. In order to
retrieve an element vector b from the resulting vector a, the circular
convolution can be used with the approximate convolutive inverse of the
other element (c−1): b ≈ a (cid:126) c−1.

However, commutativity and associativity could lead to serious
misunderstandings. Let us illustrate this point with an example:

“Luigi eats this Pizza” and “this Pizza has a topping of

Mozzarella”, encoded as:

Luigi → eats (cid:126) Pizza (1) Pizza → hasTopping (cid:126) Mozzarella
(2) By injecting the Pizza subject from the statement (2) into the Pizza
object in the statement (1), we can infer the new statement:

Luigi → eats (cid:126) (Pizza (cid:126) (hasTopping (cid:126)
Mozzarella) (3) Under this form, (3) may be interpretated as “Luigi eats
this Pizza which has a topping of Mozzarella”, but the right member can
be rewritten as:

(3r) = eats (cid:126) Pizza (cid:126) hasTopping (cid:126) Mozzarella
(associa-

tivity)

= hasTopping (cid:126) Mozzarella (cid:126) eats (cid:126) Pizza
(commutativ-

ity)

interpreted as “Luigi has a topping of Mozzarella which eats a Pizza”.
Note that circular references are easily man- aged with this setup,
thanks to the binding mechanism gen- erating always an almost orthogonal
combined vector.

Instead, we will use another binding operation offered by the
Vector-derived Transformation Binding (VTB) algebra, described by
Gosmann and Eliasmith in (Gosmann and Elia- smith 2019), which is also
implemented in Nengo and is neither commutative nor associative, but
distributive and bi- linear.

3.2 Using the VTB algebra

The binding operation

With d the dimensionality of the vector space, which the VTB requires to
be square, and d(cid:48)2 = d, the binding opera- tion is deﬁned,
following (Gosmann and Eliasmith 2019): B(x, y) def= By x

where By is block-diagonal matrix deﬁned as 



B(cid:48) 0 y 0 B(cid:48) y … … 0 0

0 . . . 0 . . . … . . . . . . B(cid:48) y

   

By

def=

   

where

B(cid:48)

y = d

1 4



  

y1 yd(cid:48)+1 … yd−d(cid:48)+1

y2 yd(cid:48)+2 … yd−d(cid:48)+2

. . . . . . . . . . . .



  

yd(cid:48) y2d(cid:48) … yd

With these notations, the ﬁrst relation above becomes s1 → τ1 Bp1 o1
This design choice enjoys the following interesting proper- ties: - The
relation is not commutative in x and y, in the general case, as
required. - The relation is bilinear in x and y (this is obvious for the
former, and easily veriﬁed on the matrix form for the latter). - The
relation is approximately left and right invertible as discussed now.

The right unbinding operation

In order to retrieve an element vector from the bound vec- tor, we need
to bind it to the inverse of the other element vector. Unlike HRR and
the circular convolution, the VTB algebra does not provide two-side
inverses: there is no left inverse for VTB.

The right approximate inverse y∼ must enjoy the follow-

ing property:

∀x, B(B(x, y), y∼)) = By∼ By x = x. The key point here is that since the
coefﬁcients of y are cho- sen randomly and independently identically
distributed, the matrix By is almost orthogonal, thus: y (cid:39) By∼,

B(cid:62)

which corresponds to simply permuting the elements of y.

identity vector iB such that

Consequently,

the right BiB = I, writes explicitly: (cid:26)

[iB]i =

4

d− 1 0

if i = (k − 1) d(cid:48) + k, 0 < k ≤ d(cid:48) otherwise.

where [iB]i stands for the i-th coordinate of the vector. We get iB by
“unfolding” the identity matrix Id(cid:48) line by line and
concatenating it d(cid:48) times (and adjusting the resulting vector by
a d− 1

4 factor).

This allows us to check if a property is a predicate for a given
resource and, if so, retrieve the corresponding object (thus performing
right unbinding):

x → z = B(y, p) = Bp y (cid:62) z = Bp

(cid:62) Bp y = y and y belongs to if and only if Bp the vocabulary.
Speciﬁcally, “y belongs to the vocabulary” means that there exists a
vector v in the vocabulary such that v · y (cid:39) 1. Otherwise the
result is simply undeﬁned, leading to the conclusion that the predicate
is undeﬁned for the given resource.

The left unbinding operation

A step further, we may also want to check if two resources are linked to
each other by a predicate and, if so, retrieve the corresponding
property (referred to as left unbinding). In order to do that, we ﬁrst
need to “ﬂip” the order of the terms involved in the binding. This is
achievable by performing the following operation:

B(a, b) = B↔ B(b, a)

considering the matrix B↔ deﬁned as:

[B↔]ij

def=

 

1

if j = 1 +

(cid:23)

(cid:22) i − 1 d(cid:48)

-   d(cid:48)[(i − 1) mod d(cid:48)]



0 Thanks to this, we obtain:

otherwise.

x → z = B(y, p) = Bp y (cid:62) B↔ z = By

(cid:62) By p = p and p belongs

if and only if By to the vocabulary.

4 Relationship and membership composition

4.1 Hierarchical representations RDF/RDFS provides two preset properties
to account for hi- erarchical representations between individuals and
classes: • x rdf:type c, expresses that the individual x belongs

to the class c.

• c rdfs:subClassOf c’ expresses that the class c is a subclass of the
class c’, or in other words, that the concept c is included in the class
c’. This means that all individuals that belong to c automatically
belong to c’. Consequently, some entailment rules 5 can be used to dis-

close implicit memberships:

-   inheritance (referred to as rdfs9 by the W3C):

x rdf:type c ∧ c rdfs:subClassOf c’ =⇒ x rdf:type c’

-   transitivity of rdfs:subClassOf (rdfs11):

c rdfs:subClassOf c’ ∧ c’ rdfs:subClassOf c’’ =⇒ c rdfs:subClassOf c’’

The Semantic Pointer representation that we proposed earlier, using the
VTB algebra, allows us to implement such rules.

For example, the inheritance rule can be expressed as fol-

lows: x → Btype c and c → BsubClassOf c(cid:48) implies

x → Btype c + Btype BsubClassOf c(cid:48) + Btype c(cid:48) A sufﬁcient
condition for this implication to be true would be Btype BsubClassOf =
Btype, which would mean subClassOf = iB; that may not be desirable as it
would lead to many wrongly inherited memberships.

Instead, we prefer using a second associative memory to encode
entailment rules such as Btype BsubClassOf → Btype, as schematized in
Fig.3. However, we cannot store matrices in the associative memory, only
vectors. But we can observe that both of these matrices are
block-diagonal, and of the form

Bz =

(cid:34) Bz

(cid:48)

0 0 Bz 0

0 0 0 Bz

(cid:48)

(cid:35)

,

(cid:48)

thus correspond to a given vector z. We thus may simply store the
vectors z into the associative memory. In this par- ticular case, we
would therefore store the association:

type (cid:11) subClassOf → type where we introduce a vector composition
operator (cid:11) deﬁned

5A recapitulative table of RDFS entailment rules as deﬁned by the W3C
can be found at https://www.w3.org/TR/2004/REC-rdf- mt-20040210

as: z = a (cid:11) b such that: Bz

(cid:48) = d 1

4 Ba

(cid:48) Bb

(cid:48),

using tensorial notations:

(x p y) i.e., x = Bp y

so that Bz = Ba Bb. Similarly, we

can express

rdfs:subClassOf c’:

the

transitivity of c

c → BsubClassOf c(cid:48)

c(cid:48) → BsubClassOf c(cid:48)(cid:48)

and

implies

c → BsubClassOf c(cid:48) + BsubClassOf BsubClassOf c(cid:48)(cid:48) +
BsubClassOf c(cid:48)(cid:48)

As a consequence, the inference rule will be stored as:

subClassOf (cid:11) subClassOf → subClassOf

Then, by iterating on such inference rule, we easily obtain the
transitive closure of properties inferred from rdf:type and
rdfs:subClassOf statements. At the distributed implementation level,
this means using a loop between the second associative memory encoding
inference rules and the ﬁrst associative memory encoding the input
statements, as schematized in Fig.3, on the last page. This general
princi- ple allows the system to compute the closure of the reason- ing
rules. i.e., ﬁxed point iteration, adding any new state- ment derived
from the application of the rules, until a ﬁxed point is reached.

Let us now discuss, to which extents we can generalize

the present formalism, to any property.

4.2 Relational representations The previous mechanism allows us to
express any entailment of the form:

(x p y) ∧ (y p’ z) =⇒ (x p’’ z)

by storing the rule

p (cid:11) p’ → p” thus implemented by an operator product, for some
predi- cate p, p’, p”. In other words, we may express any entail- ment
based on predicate chaining.

A step further, we need to implement property restric- tions, i.e. at
the RDFS level, property range rdfs:range and domain rdfs:domain, e.g.,
inference of the form:

(x p y) ∧ (p rdfs:domain c) =⇒ (x rdf:type c)

and

(x p y) ∧ (p rdfs:range c) =⇒ (y rdf:type c)

and subproperty hierarchy of the form:

(x p y) ∧ (p rdfs:subPropertyOf q) =⇒ (x q y).

for the rdfs2, rdfs3 and rdfs7 entailment rules, respectively. These
rules belong to Description Logic Programming (DLP), roughly speaking at
the intersection of OWL and Logic Programming with Horn clauses (Grosof
et al. 2003), allowing to implement the complete RDFS language as tar-
geted here and beyond an interesting part of OWL axioms (Levesque 1986).

In order to derive such generalization, we need non trivial

algebra. Let us rewrite, for a given triple:

x = po x = op

s B p y, p (cid:39) so s B yp, p (cid:39) os

p B x y, y (cid:39) sp p B y x, y (cid:39) ps

o B x p, o B p x,

these notations being well deﬁned because, as stated in the previous
section, since (i) the operator is bi-linear, (ii) the B↔ operator
allows to swap property and subject, (iii) the operator is approximately
invertible, while we also can de- ﬁne:

x = p x = o

pB x, y (cid:39) s pB y, y (cid:39) p

sB p, p (cid:39) s sB y, p (cid:39) o

oB x, oB p, introducing projection (e.g., x = p sB p simply states that
the subject x has a property p for some unknown object value, and so
on). Explicitly deriving these expressions is a per- spective of the
present preliminary work.

If we are able to explicitly compute the previous forms of the B which
exist, thanks to the stated algebraic properties, it would allow us to
translate, at least the three previous in- ference rules at the
numerical level. However, in this case, at the implement level, we do
not set a single value to accumu- late all possible derived results.
Given this precision, since:

x = p we may implement

sB p, p = po

s B domain c

while x = po

x = p

s B domain c s B type c, for any x and any c, we derive:

sB po

sB po p

s B domain = po

s B type

and similarly:

s B type while for the last inference rule, we obtain:

s B range = po

oB po p

s B po po using the same kind of algebra.

s B subPropertyOf = po

s B,

Since our design choice leads to a multi-linear relation be- tween the
three statement elements, by construction we can also easily deﬁne
reiﬁcation, i.e., deﬁne the statement s it- self, via a relation of the
form:

s = spoB x p y explicitized,

to be

which is rdf:subject, rdf:predicate, or rdf:object from the calculated
statement, by further algebraic combinations.

and then obtain the

4.3 Other intrinsic OWL properties Beyond our objective of integrating
RDFS axioms, it ap- pears that the properties of the chosen algebraic
VTB struc- ture allows us to directly implement some of the OWL
features. We already mentioned in 4.2 the possibility of inferring any
predicate chaining, allowing to implement OWL2 property chains
(owl:ObjectPropertyChain, i.e., property deﬁned by appending two other
properties) which generalizes this mechanism. Further OWL features
include: • −c the complement of a class c (owl:complementOf) • c1 + c2
the union of two classes c1 and c2, this

deﬁnition may be extended to any number of classes (owl:unionOf)

• the inverse p∼ of a property p (owl:inverseOf) • if a property p is
encoded as a vector p such that the ma- trix Bp is symmetric, we can
directly conclude that this property is symmetric. Indeed, the transpose
of a sym- metric matrix is equal to itself; since the matrices are con-
sidered orthogonal, the transpose is also the approximate inverse,
therefore the approximate inverse of a symmetric matrix is equal to
itself (owl:SymmetricProperty) Further investigating how other intrinsic
OWL properties

can be integrated is a perspective of the present work.

5

Introducing data and data structure

5.1 Representing literals A step further, we not only deﬁne
relationships between en- tities (i.e., “ObjectProperty”) but also
relate an entity to a literal (i.e., “DataProperty”), i.e., a
quantitative or qualita- tive value.

Regarding numerical values, it appears that we can easily

deﬁne multidimensional numerical values of the form:

v = ek1

2 + · · ·

1 + ek2 where ei is a identiﬁer representing the ki-th component and eki
i stands for the i-th iterate of the binding operator (Komer et
al. 2019). This allows to deﬁne integer values, and even more.

If the binding is performed using a convolution operator, it is easily
shown that this generalizes to complex numbers.6 In our case, we propose
an alternative and consider only “physical” numerical values r, i.e.,
bounded in [min, max] and with a ﬁnite precision ε, which is practice,
the case for any physical value. We must thus write:

r def= k

ε

max − min + min, k ∈ {0 · · · (cid:98) max − min

ε

(cid:99)}

considering that two values v1, v2 with |v1 − v2| < ε are
indistinguishable. Such strong speciﬁcation is particularly useful for
numerical calculus (normalized estimations, spu- rious value detection,
estimation precision threshold, . . . ) and in our case, it allows to
consider only the related pos- itive integer value k.

Regarding string literals or qualitative values (e.g,. Boolean “true” or
“false” value), at this stage we simply pro- pose to deﬁne one identiﬁer
(i.e. a string literal and a random unit vector) for each value.

5.2 Data container In RFD and in any knowledge speciﬁcation, we need to
de- ﬁne data containers (i.e., rdfs:Container), mainly ei- ther
unordered (i.e., rdf:Bag or “set”) or ordered (i.e., rdf:Seq or “list”).

More precisely, RDF/RDFS provides several classes to

represent collections: (i) rdf:Container, which includes the following
sub-

classes:

6In a nutshell, because it can be related to a numerical exponen-

(i.i) rdf:Seq, which is an ordered container, (i.ii) rdf:Bag, which is
an unordered container, (i.iii) rdf:Alt, which contains a set of
“alternatives”, the ﬁrst element of which being the default, and the
other elements constituting an unordered collection

(ii) rdf:List completed by:

• the properties rdf:first and rdf:rest • the instance rdf:nil
corresponding to the empty list representation, ordered sets (rdf:List
or In our rdf:Seq) could all be represented as lists in the sense of
RDF, in a recursive manner.7

Furthermore, unordered sets of value are obviously repre-

sented by simple addition, i.e.:

bag = x1 + x2 + · · · allowing to easily implement membership property,
element addition and deletion.

A step ahead, “Alternative” containers (i.e., rdf:Alt in the RDFS sense)
are simply implemented by a combination of the two previous
representations, where rdf:first points to an addition. Similarly, we
could deﬁne vectors like indexed containers like in (Eliasmith 2013).

6 Effective neuronal implementation: an

illustrative example

In order to illustrate these developments and validate the fact the
proposed formalism is effectively compatible with a biologically
inspired approach, we have considered a very small example of ontology
(schematized in Fig. 2) inspired by the so-called “pizza” tutorial
ontology 8 (Horridge 2011). To that end, we used the Nengo simulator
(Bekolay et al. 2014) considering two associative memories as
schematized in Fig. 3. 9

The Nengo platform provides an effective implementation of the Neural
Engineering Framework (NEF) and the Se- mantic Pointer Architecture
(SPA). It enables building large- scale bio-inspired models of spiking
neurons by connecting together reusable modular components (see (Bekolay
et al. 2014) for more details). Therefore, instead of directly ma-
nipulating the neurons, we can implement our architecture

7This is done by storing the following associations:

l →B(rdf:List, rdf:type) + B(l1, rdf:first) + B(l(cid:48), rdf:rest)

l(cid:48) →B(rdf:List, rdf:type) + B(l2, rdf:first) +
B(l(cid:48)(cid:48), rdf:rest)

l(n−1) →B(rdf:List, rdf:type)

-   B(ln, rdf:first)
-   B(rdf:nil, rdf:rest)

…

8https://github.com/owlcs/pizza-ontology 9The code is openly shared at
https://gitlab.inria.fr/line/

tiation via a Fourier transform (Komer et al. 2019).

aide-group/onto2spa

using only those modular neural networks and the available connecting
operations. Among other features, it encom- passes binding and unbinding
operations deﬁned within the VTB algebra, as well as associative
memories which allow us to store and recall patterns organized into SPA
vocabu- laries. Theoretical details underlying the implementation of
such associative memories are available in (Stewart, Tang, and Eliasmith
2011).

To illustrate the use of these features with an example, we created a
Nengo vocabulary containing all the resources of our ontology encoded as
vectors, and we stored asserted memberships and relationships between
these resources into a ﬁrst associative memory (AM1). In a second
associa- tive memory (AM2), we stored the RDFS entailment rule referred
to in our paper as the class inheritance (rdfs9 de- scribed in 4.1).
Such an architecture could store more rules based on predicate chaining
but, for the sake of simplicity, we restricted our example to this one
only.

The idea is to query the knowledge base (stored into AM1) against the
entailment rules (stored into AM2) ac- cording to some speciﬁc cues: our
network can therefore be seen as a question answering system, where the
question to answer is raised by these cues. For instance, in our
example, the system receives a subject cue (thisPizza) and a predicate
cue (type), which induce the question “what is the type of thisPizza”. A
third cue indicates which rule to use: in this case, the class
inheritance entailment (in practice, instead of a third cue, the network
could keep a buffer of several rules to test successively). In order to
visualize the differ- ent steps of the data processing, we plotted the
similarities of the module outputs at several points of the
architecture, compared against the symbols in our vocabulary (Fig 4).

Figure 2: An example of a very simple ontology with two indi- viduals,
black arrows correspond to factual statements input in the data base and
red arrows to inferred statements. Rectangular boxes stand for
individuals, round boxes for classes and arrows are la- beled by
properties.

7 Discussion and conclusion What has been presented here is a proof of
concept of map- ping between RDFS (with some OWL extensions) ontology
speciﬁcation and a biologically plausible numerical imple- mentation
based on VSA, with a preliminary partial numer- ical experimentation.
This mainly aims at opening new re- search perspectives on the
uniﬁcation of symbolic and nu- merical approaches for cognitive modeling
and Artiﬁcial In- telligence.

Here the key point is to demonstrate that any symbolic knowledge, as
possibly expressed in languages such as RDFS and OWL, can be also
represented and manipulated with a numerical neuronal formalism of
associative mem- ory. Regarding the genericity of this demonstration, it
is worth mentioning that (Mandler 2011) proposes that human (neuronal)
memory can represent three classes of structures (in short,
associations, sequences and relational structures), the three of them
being discussed here. This is especially interesting for our team, since
we study human learning us- ing this kind of symbolic approaches
(Mercier et al. 2021). Therefore the biological plausibility of ontology
represen- tation and reasoning is a key issue. By plausibility, we in-
deed do not claim that this is directly coded “as is” in the brain, but
that what corresponds to symbolic processing in the brain may be
represented by such processing, exactly as discussed by (Eliasmith
2013). Beyond that, this kind of rep- resentation allows us to inject
prior knowledge into learning systems, and opens new perspectives on
symbol emergence. Of course, several alternative implementations of on-
tologies in VSA are possible. While we chose a non- commutative,
non-associative algebra to differentiate the functions (subject,
predicate, object) of the resources in a statement, we could also
achieve that in a commutative and associative algebra such as HRR, thus
using a slightly differ- ent representation similar to what has been
done in (Craw- ford, Gingerich, and Eliasmith 2016) to represent the
Word- net database. 10 However, this formalism is quite heavy and we
would miss some of the interesting properties induced by binding
directly the predicate to the object.

This preliminary study is to be completed at different lev- els. At a
technical level, the tensorial generalization in sec- tion 4.2 has been
stated at an abstract level, and has yet to be effectively implemented
in order to target all RDFS mechanisms. The fact that we rely on
Description Logic Programming (DLP) restrains the possibility to take
into ac- count the whole decidable part OWL2 speciﬁcation and we aim to
surpass this limit by further studying the intrinsic al- gebraic
properties of our representation. We will also, as in human cognitive
processes, consider “approximate” rea-

10In that case, the “work-around” consists in adding new vectors
predicate and object accounting for the functions in the triple, as well
as an index ti (represented by a vector ti) to each statement and store
the triples as:

s1 → τ1 t1 (cid:126) (p1 (cid:126) predicate + o1 (cid:126) object)

s1 → τ2 t2 (cid:126) (p2 (cid:126) predicate + o2 (cid:126) object) + τ3
t3 (cid:126) (p3 (cid:126) predicate + o3 (cid:126) object)

Figure 3: A proposition of architecture to implement our ontology and
inference rules in a neuronal system, using the modules provided by the
Nengo simulator. A subject cue (thisPizza) and a predicate cue (type)
prompt the question “what is the type of thisPizza”; a third cue
indicates the rule to use (class inheritance entailment). The network
retrieves information in each associative memory and combines them to
infer that thisPizza is not only a MargheritaPizza but also, through
class inheritance, a Pizza. (Data is in square boxes and their
processing in round boxes. Rectangular purple boxes account for
associative retrievals. The circled numbers are markers locating which
signals are plotted on Fig. 4.)

Figure 4: The numerical simulation results. Each signal is visualized
through its cosine similarity against the vocabulary. The cues guiding
the reasoning (under the form of square inputs) are plotted on the left,
and processing steps are on the right (refer to Fig. 3 for the location
of each signal, marked by the circled numbers). We can observe the
correctness of the inferred object and predicate (respectively PIZZA and
TYPE, plotted on 5 and 6) bound together to characterize the subject
THIS PIZZA.

AM1 (knowledgebase)type ⊘
subClassOfthisPizzatypeB(margherita,type)margheritatypemargheritaB(pizza,subClassOf)pizzaB(pizza,
type)B(margherita, type)
+B(B(pizza,subClassOf),type)SUBJECTCUEPREDICATECUEUnbindingUnbindingINFERREDSTATEMENTWORKINGMEMORYBindingRULE
CUEASSOCIATIVEMEMORIESB(B(pizza,subClassOf),type)BindingSuperposition312654AM1
(knowledgebase)AM2(entailment rules)soning about undecidable facts, by
better considering the se- mantic offered by the possibility/necessity
score used here. Finally, we made explicit how literal representation is
easily possible, but a lot of work is still to be done to have this part
fully operational.

We are also aware that, in our representation, the ontology and
inference rules are “hard-coded” within the associative memories; the
long-term perspective would be to learn those associations. As new
observations come along, we wish to store the newly observed
relationships between individuals. A step further, logical inference as
we proposed is a way to learn new associations, both memberships and
relationships, within the ontology itself: for now, the newly inferred
state- ments are only kept into the working memory, but we could store
them for future use. Last but not least, we also wish to learn the
inference rules themselves, by spotting some patterns that are repeated
within the ontology. We presume that this vector formalism will
facilitate learning, as it is the representation used by most learning
algorithms, although a concrete implementation for this feature is still
an open question. In order to remember inferred relationships, the naive
approach would be to change the connections in the associative memory or
even add new ensembles by hand. However, the neuroscience and machine
learning ﬁelds pro- vide more sophisticated and biologically plausible
solutions to this problem. For example, (Voelker, Crawford, and Elia-
smith 2014) used a combination of unsupervised and super- vised learning
to learn new key-value associations in spiking neurons online.

Acknowledgments. Margarida Romero is highly thanked for powerful ideas
regarding the use of this formalism to model human learning which is at
the origin of this technical work. We also had a great pleasure
discussing with Terrence Stewart whom we deeply thank for his insightful
feedback.

References

Alexandre, F. 2019. De quelles fac¸ons l’intelligence artiﬁ- cielle se
sert-elle des neurosciences ? The Conversation. Allemang, D.; Hendler,
J.; and Gandon, F. 2020. Semantic Web for the Working Ontologist. ACM.

Ayadi, A.; Samet, A.; de Beuvron, F. d. B.; and Zanni-Merk, C. 2019.
Ontology population with deep learning-based NLP: a case study on the
Biomolecular Network Ontology. Procedia Computer Science 159:572–581.

Bekolay, T.; Bergstra, J.; Hunsberger, E.; DeWolf, T.; Stew- art, T. C.;
Rasmussen, D.; Choo, X.; Voelker, A. R.; and Eliasmith, C. 2014. Nengo:
a Python tool for building large- scale functional brain models. Front.
Neuroinform. 7.

Chui, C. K., and Mhaskar, H. N. 2018. Deep Nets for Local Manifold
Learning. Front. Appl. Math. Stat. 4. Publisher: Frontiers.

2017. Cohen, W. W.; Yang, F.; and Mazaitis, K. R. TensorLog: Deep
      Learning Meets Probabilistic DBs. arXiv:1707.05390 [cs]. arXiv:
      1707.05390.

Crawford, E.; Gingerich, M.; and Eliasmith, C. 2016. Bi- ologically
Plausible, Human-Scale Knowledge Representa- tion. Cogn Sci
40(4):782–821. Denœux, T.; Dubois, D.; and Prade, H. 2020. Represen-
tations of Uncertainty in AI: Probability and Possibility. In Marquis,
P.; Papini, O.; and Prade, H., eds., A Guided Tour of Artiﬁcial
Intelligence Research: Volume I: Knowledge Representation, Reasoning and
Learning. Cham: Springer International Publishing. 69–117. Eidoon, Z.;
Yazdani, N.; and Oroumchian, F. 2008. On- In Macdonald, C.; tology
Matching Using Vector Space. Ounis, I.; Plachouras, V.; Ruthven, I.; and
White, R. W., eds., Advances in Information Retrieval, Lecture Notes in
Computer Science, 472–481. Berlin, Heidelberg: Springer. Eliasmith, C.,
and Anderson, C. H. 2002. Neural Engineer- ing:Computation,
Representation, and Dynamics in Neuro- biological Systems. A Bradford
Book. The MIT Press. Pub- lisher: The MIT Press. Eliasmith, C. 2013. How
to Build a Brain: A Neural Ar- chitecture for Biological Cognition. OUP
USA. Google- Books-ID: BK0YRJPmuzgC. Garcez, A. d., and Lamb, L. C.
2020. Neurosymbolic AI: The 3rd Wave. arXiv:2012.05876 [cs]. arXiv:
2012.05876. Gayler, R. 2003. Vector Symbolic Architectures answer
Jackendoff’s challenges for cognitive neuroscience. In Fron- tiers in
Artiﬁcial Intelligence and Applications. Journal Ab- breviation:
ICCS/ASCS International Conference on Cog- nitive Science Publication
Title: ICCS/ASCS International Conference on Cognitive Science. Gosmann,
J., and Eliasmith, C. 2019. Vector-Derived Trans- formation Binding: An
Improved Binding Operation for Deep Symbol-Like Processing in Neural
Networks. Neural Computation 31(5):849–869. Publisher: MIT Press.
Grosof, B. N.; Horrocks, I.; Volz, R.; and Decker, S. 2003. Description
logic programs: combining logic programs with description logic. In
Proceedings of the 12th international conference on World Wide Web, WWW
’03, 48–57. New York, NY, USA: Association for Computing Machinery. Guo,
S.; Wang, Q.; Wang, L.; Wang, B.; and Guo, L. 2016. Jointly Embedding
Knowledge Graphs and Logical Rules. In Proceedings of the 2016
Conference on Empirical Meth- ods in Natural Language Processing,
192–202. Austin, Texas: Association for Computational Linguistics.
Hohenecker, P., and Lukasiewicz, T. Reasoning with Deep Neural Networks.
1808.07980. Horridge, M. 2011. A Practical Guide To Building OWL
Ontologies Using Prot´eg´e 4 and CO-ODE Tools Edition 1.3. Jim´enez, A.;
Elizalde, B.; and Raj, B. 2018. Sound event classiﬁcation using
ontology-based neural networks. In Pro- ceedings of the Annual
Conference on Neural Information Processing Systems, 9. Komer, B.;
Stewart, T. C.; Voelker, A. R.; and Eliasmith, C. 2019. A neural
representation of continuous space using fractional binding. In 41st
Annual Meeting of the Cognitive

2020. Ontology jair 68. arXiv:

Sauerwald, T., and Zanetti, L. 2019. Random Walks on Dynamic Graphs:
Mixing Times, HittingTimes, and Re- turn Probabilities. arXiv:1903.01342
[cs, math]. arXiv: 1903.01342. Schlegel, K.; Neubert, P.; and Protzel,
P. 2020. A compar- ison of Vector Symbolic Architectures.
arXiv:2001.11797 [cs]. arXiv: 2001.11797. Shi, S.; Chen, H.; Ma, W.;
Mao, J.; Zhang, M.; and Zhang, In Proceedings of Y. 2020. Neural Logic
Reasoning. the 29th ACM International Conference on Information &
Knowledge Management, CIKM ’20, 1365–1374. New York, NY, USA:
Association for Computing Machinery. Smith, L. 1994. The development of
modal understanding: Piaget’s possibility and necessity. New Ideas in
Psychology 12(1):73–87. Stewart, T. C.; Tang, Y.; and Eliasmith, C.
2011. A biolog- ically realistic cleanup memory: Autoassociation in
spiking neurons. Cognitive Systems Research 12(2):84–92. Sun, R., and
Alexandre, F. 2013. Connectionist-Symbolic Integration: From Uniﬁed to
Hybrid Approaches. Taylor & Francis Group, 3rd edition edition.
Tettamanzi, A.; Zucker, C. F.; and Gandon, F. 2017. Possi- bilistic
testing of OWL axioms against RDF data. Interna- tional Journal of
Approximate Reasoning. Tous, R., and Delgado, J. 2006. A Vector Space
Model for Semantic Similarity Calculation and OWL Ontology In Bressan,
S.; K¨ung, J.; and Wagner, R., Alignment. eds., Database and Expert
Systems Applications, Lecture Notes in Computer Science, 307–316.
Berlin, Heidelberg: Springer. Voelker, A.; Crawford, E.; and Eliasmith,
C. 2014. Learning large-scale heteroassociative memories in spiking
neurons. Wang, M.; Qiu, L.; and Wang, X. 2021. A Survey on Knowledge
Graph Embeddings for Link Prediction. Sym- metry 13(3):485. Number: 3
Publisher: Multidisciplinary Digital Publishing Institute. Xiao, H.;
Huang, M.; and Zhu, X. 2015. From one point to a manifold: Knowledge
graph embedding for precise link prediction. arXiv preprint
arXiv:1512.04792. Zhu, W.; Qiu, Q.; Huang, J.; Calderbank, R.; Sapiro,
G.; and Daubechies, I. 2017. LDMNet: Low Dimensional Mani- fold
Regularized Neural Networks. arXiv:1711.06246 [cs]. arXiv: 1711.06246
version: 1.

Science Society, 6. Montreal, Canada: Cognitive Science Society.
Lallement, Y.; Hilario, M.; and Alexandre, F. 1995. Neu- rosymbolic
Integration: Cognitive Grounds and Computa- tional Strategies. In
Proceedings World Conference on the Fundamentals of Artiﬁcial
Intelligence, 12. Levesque, H. J. 1986. Knowledge Representation and
Rea- soning. Annu. Rev. Comput. Sci. 1(1):255–287. Publisher: Annual
Reviews. Levy, S. D., and Gayler, R. 2008. Vector Symbolic Ar-
chitectures: A New Building Material for Artiﬁcial General Intelligence.
In Frontiers in Artiﬁcial Intelligence and Ap- plications, 6. Mandler,
G. 2011. From Association to Organization. Curr Dir Psychol Sci
20(4):232–235. Publisher: SAGE Publica- tions Inc. McClelland, J. L.,
and Rogers, T. T. 2003. The parallel distributed processing approach to
semantic cognition. Nat Rev Neurosci 4(4):310–322. Mercier, C.; Roux,
L.; Romero, M.; Alexandre, F.; and Vi´eville, T. 2021. Formalizing
Problem Solving in Com- In 2021 putational Thinking : an Ontology
approach. IEEE International Conference on Development and Learn- ing
(ICDL), 1–8. Petrucci, G.; Ghidini, C.; and Rospocher, M. 2016. Ontol-
ogy Learning in the Deep. In Blomqvist, E.; Ciancarini, P.; Poggi, F.;
and Vitali, F., eds., Knowledge Engineering and Knowledge Management,
Lecture Notes in Computer Sci- ence, 480–495. Cham: Springer
International Publishing. Phan, N.; Dou, D.; Wang, H.; Kil, D.; and
Piniewski, B. 2017. Ontology-based deep learning for human behavior
prediction with explanations in health social networks. In- formation
Sciences 384:298–313. Plate, T. 1995. Holographic reduced
representations. IEEE Trans. Neural Netw. 6(3):623–641. Pulverm¨uller,
F. 2013. How neurons make meaning: brain mechanisms for embodied and
abstract-symbolic semantics. Trends in Cognitive Sciences 17(9):458–470.
Riegel, R.; Gray, A.; Luus, F.; Khan, N.; Makondo, N.; Akhalwaya, I. Y.;
Qian, H.; Fagin, R.; Barahona, F.; Sharma, U.; Ikbal, S.; Karanam, H.;
Neelam, S.; Likhyani, A.; and Srivastava, S. 2020. Logical Neural
Networks. arXiv:2006.13155 [cs]. arXiv: 2006.13155. Romero, M.; David,
D.; and Lille, B. 2019. CreaCube, a Playful Activity with Modular
Robotics. In Gentile, M.; Allegra, M.; and S¨obke, H., eds., Games and
Learning Al- liance, volume 11385. Cham: Springer International Pub-
lishing. 397–405. Series Title: Lecture Notes in Computer Science.
Rusawuk, A. L. 2018. Possibility and Necessity: An Intro- duction to
Modality. Sajjad, H. P.; Docherty, A.; and Tyshetskiy, Y. 2019. Ef-
ﬁcient Representation Learning Using Random Walks for Dynamic Graphs.
arXiv: 1901.01346.

arXiv:1901.01346 [cs, stat].


