A VTA GABAergic computational model of dissociated
reward prediction error computation in classical
conditioning
Pramod Kaushik, J√©r√©mie Naud√©, Surampudi Bapi Raju, Fr√©d√©ric Alexandre

To cite this version:

Pramod Kaushik, J√©r√©mie Naud√©, Surampudi Bapi Raju, Fr√©d√©ric Alexandre. A VTA GABAergic
computational model of dissociated reward prediction error computation in classical conditioning. Neu-
robiology of Learning and Memory, 2022, 193 (107653), Ôøø10.1101/2020.02.06.936997Ôøø. Ôøøhal-03066295Ôøø

HAL Id: hal-03066295

https://hal.science/hal-03066295

Submitted on 1 Jul 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L‚Äôarchive ouverte pluridisciplinaire HAL, est
destin√©e au d√©p√¥t et √† la diffusion de documents
scientifiques de niveau recherche, publi√©s ou non,
√©manant des √©tablissements d‚Äôenseignement et de
recherche fran√ßais ou √©trangers, des laboratoires
publics ou priv√©s.

\*For correspondence:
frederic.alexandre@inria.fr (FA)

A VTA GABAergic computational
model of dissociated reward
prediction error computation in
classical conditioning

Pramod Kaushik1,3, J√©r√©mie Naud√©2, Surampudi Bapi Raju1, Fr√©d√©ric
Alexandre3,4,5\*

1International Institute of Information Technology, Hyderabad, India; 2Institut de
G√©nomique Fonctionnelle, Universit√© Montpellier, Center National de la Recherche
ScientiÔ¨Åque, Institut National de la Sant√© et de la Recherche M√©dicale, Montpellier,
France; 3Inria Bordeaux Sud-Ouest, Talence, France; 4LaBRI, University of Bordeaux,
Bordeaux INP, CNRS, UMR 5800, Talence, France; 5Institute of Neurodegenerative
Diseases, University of Bordeaux, CNRS, UMR 5293, Bordeaux, France

Abstract Classical Conditioning is a fundamental learning mechanism where the Ventral
Striatum is generally thought to be the source of inhibition to Ventral Tegmental Area (VTA)
Dopamine neurons when a reward is expected. However, recent evidences point to a new
candidate in VTA GABA encoding expectation for computing the reward prediction error in the VTA.
In this system-level computational model, the VTA GABA signal is hypothesised to be a combination
of magnitude and timing computed in the Peduncolopontine and Ventral Striatum respectively.
This dissociation enables the model to explain recent results wherein Ventral Striatum lesions
aÔ¨Äected the temporal expectation of the reward but the magnitude of the reward was intact. This
model also exhibits other features in classical conditioning namely, progressively decreasing Ô¨Åring
for early rewards closer to the actual reward, twin peaks of VTA dopamine during training and
cancellation of US dopamine after training.

Introduction

The phasic Ô¨Åring activity of midbrain dopamine neurons is believed to encode a reward prediction
error, which can guide learning and serve as an incentive signal. In his famous experiment, Pavlov
observed that if food follows the ring of a bell, a dog comes to salivate after the bell is rung.
This process is called classical (or pavlovian) conditioning: an unconditioned response (salivation)
originally associated with an Unconditioned Stimulus (US, the food) becomes conditionally elicited
by a Conditioned Stimulus (CS, the bell ring). Schultz and collaborators examined the activity of
midbrain dopamine neurons in primates, during a classical conditioning task similar to Pavlov‚Äôs.
They observed that originally, midbrain dopamine neurons responded with a burst of spikes to
unexpected primary rewards (juice/water dripped in the mouth of thirsty primates), i.e. an US.
After the primates learned the association between a tone CS and the reward US, dopamine cells
responded to the reward-predicting CS. Moreover, these neurons stopped responding to US whose

1 of 17

arrival was expected, being predicted by the CS. When cued rewards failed to be delivered, a large
percentage of dopamine neurons revealed a brief pause with respect to their background Ô¨Åring at
the moment of expected reward.

This predictive behavior of VTA Dopamine neurons was linked to the Temporal DiÔ¨Äerence Learning
algorithm in Reinforcement Learning (Sutton and Barto, 1998). The TD algorithm predicts a future
reward that occurs at a speciÔ¨Åc state ahead of time after a given number of trials. This prediction is
arrived through the computation of a reward prediction error (RPE) that enables learning the value
of each state. It is represented by the equation:

ùõøùë° = ùëüùë° + ùõæ ÃÑùëâùë°+1 ‚àí ÃÑùëâùë°

(1)

where ùëüùë°

is the reward (return) on time step ùë°. Let ÃÑùëâùë°

be the correct prediction that is equal to the
discounted sum of all future rewards. The discounting is done by powers of factor of ùõæ such that
reward at distant time step is less important as compared to recent rewards.

The TD algorithm is powerful in its heuristic value, as it links the activation of dopamine cells
with classical conditioning. However, some properties of TD models are inconsistent with the
electrophysiological data, and it lacks the biological realism needed to explain the mechanisms
through which RPE-like activity emerges in dopamine cells. Most inconsistencies are concerned with
the way time is represented in the TD model. Whereas the TD error signal travels back in time from
US to CS when learning progresses, it has been reported in (Schultz et al., 1997) that US-related
activation of VTA slowly decreases while the CS-related one increases. Ramping dopamine activity
during the CS-US interval has been proposed to reÔ¨Çect back-propagating error signals averaged
over trials (Niv et al., 2005) but this ramping can be observed in individual trials and is actually
related to reward uncertainty (Fiorillo et al., 2003)(Fiorillo et al., 2005). More importantly, when
US is delivered earlier than predicted, VTA dopaminergic neurons are activated at the actual time
of the US but not at the usual time of reward (Hollerman and Schultz, 1998), contrarily to what is
predicted by TD. Critically, when striatum is lesioned, experiments (Takahashi et al., 2016) show
that VTA dopaminergic neurons signal a RPE when reward magnitude changes, but not when time
of the reward is modiÔ¨Åed. This suggests that learning about reward timing is computationally
and anatomically separated from learning about reward magnitude, i.e. a completely diÔ¨Äerent
implementation of reinforcement learning than is usually considered (Joel et al., 2002).

Here we sought a system-level account of how the CS-US interval duration and the value of
the reward are separately learned, and how these two features are combined to give rise to
dopaminergic phasic activity. In particular, we considered two components of the meso-limbic
loops that have been overlooked in previous models: VTA GABA cells and neurons from the
Pedunculopontine Nucleus (PPN). We propose that VTA GABA neurons provide inhibitory drive onto
VTA DA neurons (Eshel et al., 2015) , display persistent Ô¨Åring during the CS-US interval (Cohen et al.,
2012) and are necessary to compute the RPE in DA cells (Eshel et al., 2015). However, here we only
include positive reward predictions and exclude omissions as VTA GABA neurons have been found to
be too weak for reducing baseline dopamine Ô¨Åring (Eshel et al., 2015) and we hypothesize that they
are being handled by the Ventral Pallidum and the Rostromedial Tegmental Nucleus (RMTg) pathway.
Neurons from the PPN project to both VTA DA and GABA neurons and have been found necessary
for appetitive conditioning (Yau et al., 2016). PPN neurons are classically believed to signal the
delivery of actual reward (Vitay and Hamker, 2014). However, during conditioning, two types of
response are recorded from neurons from the PPN : neurons responding to the actual reward
magnitude, and a persistent neuronal Ô¨Åring during the CS-US interval, reÔ¨Çecting the prediction or
expected reward magnitude (Okada and Kobayashi, 2009)(Okada and Kobayashi, 2013). Here we
show that a system-level computational model can account for these yet-unexplained physiological
and lesion data. We propose that the Ventral Striatum learns the reward timing and the Amygdala
the reward magnitude, which is then transferred to the PPN. Expected reward timing and magnitude
are subsequently combined at the level of VTA GABA cells to compute the expectation term needed

2 of 17

by VTA DA neurons to generate a prediction error. We furthermore provide testable predictions for
future experiments, on the role of PPN and VTA GABA in classical conditioning.

Results

We built a system-level network in which reward prediction error (RPE) emerges during learning,
from the interaction of the neruronal ensembles repersenting RPE computation. To illustrate how
RPE is computed in the network, we subjected it to in silico experimental scenarios similar to a
conditioning task. This analogue of classical conditioning consists in the repeated pairing of a CS
with the US, separated by a Ô¨Åxed interval duration. The in silico experiments below examine how the
value (magnitude) of the US reward and the duration of the CS-US interval are learned separately,
and combined by VTA GABA cells. In turn, VTA GABA neurons provide the expectation term used to
cancel the excitation of dopamine cells by the US, at the time of the expected reward (Cohen et al.,
2012)(Eshel et al., 2015). Finally, we show how this new model provides a better description of
experimental data that have been left unexplained yet, i.e. how dopamine cells respond to rewards
delivered earlier than expected (Fiorillo et al., 2008), and how value and timing are dissociated by
VS lesions (Takahashi et al., 2016).

Model Architecture
The system-level computational model attempts to explain how the dopamine reward prediction
error is computed in appetitive conditioning in the VTA through understanding the roles of VTA GABA
and Peduncolopontine (PPN) neurons. The model is shown in Figure 1. The model focuses on the
computation inside the VTA carried out by two populations viz., the VTA DA and VTA GABA neurons.
Lateral Hypothalamus (LH) projects to PPN RD and VS and these in turn project to VTA DA and
VTA GABA respectively. When reward is delivered, it is reported to Ô¨Åre the Lateral Hypothalamus
(LH) and activates the LH ‚Üí PPN RD (Reward delivery) ‚Üí VTA Dopamine pathway resulting in
US dopamine Ô¨Åring prior to any sort of learning (Semba and Fibiger, 1992; Lokwan et al., 1999).
Basolateral Amygdala (BLA) learns the magnitude of the US through the projections from VTA
DA to BLA, which signal a reward prediction error that modulates synaptic plasticity. A pathway
from LH to BLA learns that BLA Ô¨Åring for CS has the same amplitude as the US-induced Ô¨Åring
(Sah et al., 2003). The immediate Ô¨Åring in response to the CS occurs through the BLA recognizing
the cue encoded by the Infero-temporal cortex (IT), with BLA activating the VTA DA through the
BLA‚Üí CE ‚Üí PPN RD ‚Üí VTA DA pathway. The CS has been assumed to be encoded through the IT
in this model though there could be other candidates and this connectivity is not central to the
functioning of the model. The Central Nucleus of the Amygala (CE) has excitatory projections on
PPN FT(Fixation Target) (Okada and Kobayashi, 2013) (Kobayashi and Okada, 2007) which displays
persistent activity unless strongly inhibited. The projections from PPN to VTA are functional in
nature in the model with empirical evidences lacking so far. The LH also projects to the VS, which
is also modulated by VTA DA neurons. Finally, excitatory projections from PPN FT and inhibitory
projections from VS neurons to VTA GABA enable the Ô¨Ånal reward cancellation of VTA DA observed
in electro-physiological experiments.

Control Scenario
In the following, we show how a reward prediction error emerges in VTA Dopamine cells with
learning, as a consequence of network dynamics and plasticity.

‚Ä¢ Initial Trial

The arrival of an unexpected reward induces a Ô¨Åring in the LH neurons. This LH Ô¨Åring
subsequently activates VTA Dopamine neurons, through the PPN RD neurons (Figure 2 B top
panel Trial1, Figure 2 C top panel Trial 1). Hence, the model reproduces that VTA Dopamine
neurons Ô¨Åre upon the delivery of an unexpected reward. No activity in BLA and Ventral

3 of 17

Figure 1. Model diagram illustrating the neuronal structures and their connections involved in Reward
Prediction Error (RPE) computation. Pointed arrows represent excitatory connections, while rounded arrows
represent inhibitory projections. Dashed lines represent learnable connections, while solid lines represent Ô¨Åxed
connections in the model. The red color indicates the VTA population and the yellow color indicates the PPN
population.The blue and green color indicates the source of magnitude and temporal expectation respectively.
Ventral Tegmental Area has Dopamine (DA) and GABA populations which play a central role in computing RPE.
This RPE is transmitted to Basolateral Amydala (BLA) and the Ventral Striatum (VS) where the modulatory
connections undergo change depending on this signal. Both the VTA structures receive inputs from distinct
Peduncolopontine (PPN) neural populations. The PPN Reward Delivery (RD) neurons deliver reward to VTA DA
neurons from the Lateral Hypothalamus (LH) which receives the unconditioned stimulus (US). The PPN Fixation
target (FT) neurons receive their projections from the Central Nucleus (CE) of the Amygdala and these PPN
neurons along with Ventral Striatum (VS) project to VTA GABA forming the inhibitory signal that cancels the VTA
DA upon reward delivery. The BLA is regarded to learn the association between unconditioned stimulus (US)
and the conditioned stimulus (CS) and produces the anticipatory Ô¨Åring in VTA DA through the BLA->CE->PPN
RD->VTA DA pathway. The VS is posited to learn the timing of the interval and it has inhibitory projections on
VTA GABA.

Striatum (VS) and VTA GABA (Figure 2 B bottom panel Trial1) is observed at this stage of
learning. Indeed, Basolateral Amygdala (BLA) has not yet learned to associate the magnitude
of the CS with the US, and the Ventral Striatum (VS) the timing of the CS-US interval duration.
Hence, there is no expectation at the arrival of the CS.

‚Ä¢ Partial Conditioning

The synaptic weights between IT and BLA are updated after each rewarding trial. Consequently,
after a few trials (7 in our simulations), the BLA starts responding to the CS stimulus. This
progressive learning in the BLA generates Ô¨Åring in VTA DA (Figure 2 B top panel) through
PPN RD (Figure 2 C top panel) in response to the arrival of the CS, corresponding to a partial
prediction of reward. BLA activity also generates a partial expectation through tonic Ô¨Åring
in PPN FT (Figure 2 C bottom panel Trial 7). Hence, a partial cancellation of VTA dopamine
neurons happens at this stage. At this stage, the time interval has been completely learned,
contrary to the learning of the US magnitude.
This corresponds to the activity of the Ventral Striatum reaching its minimum at the exact
moment of reward. Hence, the VS does not exert any inhibition at the end of the interval,
which results in the inhibition of VTA DA neurons at the expected time of the US. However,
as the US magnitude is not fully learned yet, the activity of PPN FT to VTA GABA pathway
only results in partial cancellation of VTA DA activity upon US delivery. This is consistent
with experimental results on Partial conditioning (Pan et al., 2005), and designed as a partial
expectation in the TD framework. (Figure 2 B bottom panel Trial 7). This partial expectation
consists in a twin peak of VTA Ô¨Åring, at the respective times of the CS and the US.

‚Ä¢ Complete Reward Cancellation

The Ô¨Ånal state of the circuit, after 16 CS-US pairings, is given in Figure 2 A. The magnitude of
expectation originates from the CS Ô¨Åring in the Central Amygdala (CE) and is maintained in the
PPN FT through a self-sustaining mechanism (Okada and Kobayashi, 2013) (Kobayashi and

4 of 17

Okada, 2007). At the end of learning, the BLA neurons have reached an asymptote in their
Ô¨Åring, which encodes for reward magnitude. Hence, PPN FT neurons display maximum tonic
activity after the presentation of the CS. In parallel, as in partial conditioning, the presentation
of the CS, which Ô¨Åres the IT and thereby the Orbitofrontal Cortex (OFC) (Carmichael and Price,
1995), activates the VS that encodes the interval timing. It acts similar to a negative integrator
and progressively lowers the inhibition that VS exerts on PPN FT, to reach zero inhibition at
the expected time of the reward. Both signals are combined by VTA GABA,the activity of which
peaks at the time of the reward (Figure 2 B bottom panel Trial 16), cancelling VTA dopamine,
which no longer shows Ô¨Åring at the time of the US when reward arrives through the LH (Figure
2 B top panel Trial 16).
The magnitude of expectation originates from the CS Ô¨Åring in the Central Amygdala (CE)
and maintained in the PPN FT through a self sustaining mechanism (Okada and Kobayashi,
2013) (Kobayashi and Okada, 2007). The GABA Ô¨Åring in the VTA is reÔ¨Çective of this (Yau et al.,
2016) and the PPN FT integrates the magnitude from the Central Amygdala (CE) and timing
information from the VS to achieve the ramping signal that encodes both time and magnitude
of the reward delivery.

Variability in Magnitude and Time

1. Variability in Timing

When a reward is delivered earlier than expected (i.e. with a shorter delay than the CS-US
interval that has been learned), US Ô¨Åring is observed in VTA dopamine neurons. More precisely,
in this case of earlier-than-expected US reward, VTA dopamine neurons Ô¨Åre less than the
initial (before learning) Ô¨Åring observed at US delivery. This is consistent with the US being
expected, albeit not at this precise timing. An interpretation would be that partial expectations
are generated during the CS-US interval, hence the reward prediction decreases with time
until the expected timing of US delivery.
In our model, we observed that the earlier the
reward was delivered, the higher was the VTA DA Ô¨Åring (Figure 3 B). VTA DA Ô¨Åring in Figure 3
A (middle left panel) shows a reward delivered before the half-way-point (at the 100ùë°‚Ñé time
step) evoking a dopamine Ô¨Åring, but less than the Ô¨Åring initially induced by an unpredicted
reward. By contrast, Figure 3 B (middle left panel) shows an early reward delivered after the
half-way-mark (at the 300ùë°‚Ñé time step), which induces lesser Ô¨Åring in VTA DA cells. The model
is consistent with physiological data in primates, where earlier-than-expected rewards evokes
progressively less Ô¨Åring as the reward delivery time increases (Fiorillo et al., 2008). Our model
provides a mechanistic explanation for this data.

2. Variability in Magnitude

VTA Dopamine Ô¨Åring reÔ¨Çects the diÔ¨Äerence between the actual reward and the expected
reward. For instance, VTA Dopamine neurons Ô¨Åre on US arrival if a reward is larger than
expected (Figure 4 C middle panel). This corresponds to a positive reward prediction error,
in accordance with physiological data. The subtractive nature of inhibition (Eshel et al.,
2015) encodes the diÔ¨Äerence of magnitude between the actual magnitude of reward and the
expected magnitude of reward ( Figure 4C middle panel).

Dissociation of time and magnitude prediction errors following VS Lesions

1. VS lesion aÔ¨Äects time prediction error

When an experimental lesion is made in the VS from rats (Takahashi et al., 2016) , earlier-than-
expected US does not trigger Ô¨Åring in VTA Dopamine cells. Accordingly, our model reproduces
this lack of timing prediction error, as the virtual lesion of the VS in the model abolishes the
VTA Dopamine response to earlier-than-expected US. Moreover, VTA GABA cells, instead of
displaying a ramping signal, provide a constant tonic inhibition, similar to PPN FT, throughout
the duration of the trial (Figure 4 A middle right panel). Indeed, the model posits that lack
of dopamine Ô¨Åring for the VS-lesioned scenario compared to the control scenario is due

5 of 17

to the higher inhibition from the VTA GABA neurons (Figure 4 A middle right panel). The
VS inhibitory signal acts as a "when" signal providing VTA GABA neurons with information
enabling computation of reward expectation at a given point in time. Due to this information
being lost because of lesion, VTA GABA neurons do not have a ramping signal that peaks at
the right moment. Instead, they have a constant inhibition throughout the interval.

2. VS lesion does not alter magnitude prediction error

The rewards with higher magnitude induce Ô¨Åring in VTA DA cells even if the VS is lesioned. A
reward that is double the magnitude triggers the same eÔ¨Äect on US VTA Dopamine as in the
control scenario (Figure 4 B middle left panel).
Since the VS encodes only temporal information, its lesion does not aÔ¨Äect the magnitude
encoding of the stimuli. This results in VTA DA Ô¨Åring showing the same subtractive eÔ¨Äect as in
the control scenario(Figure 4 B middle left panel). This is due to the VTA GABA encoding the
magnitude from the BLA through the PPN FT neurons. A lack of VTA GABA ramping does not
prevent the VTA DA from having the magnitude encoded within its population, and provides
the same RPE at usual reward time. This behavior of VTA DA neurons in the case of a VS lesion
with larger magnitude is also in accordance with experimental results (Takahashi et al., 2016).

Figure 2. (A) The Ô¨Ågure represents the Ô¨Åring of the neuronal populations after training following the same color conventions
as the model architecture diagram. IT and LH indicate the stimulus and the reward respectively. PPN RD represent the phasic
signals received from BLA and the LH at stimulus onset and reward delivery. VTA DA neurons show Ô¨Åring only at the arrival of
the CS and VTA GABA neurons have a ramping expectation signal that fully cancels the VTA DA signal at reward delivery. BLA
and VS have encoded magnitude and timing of the reward respectively. (B) This Ô¨Ågure portrays the evolution of the VTA and
PPN sub-populations throughout the duration of trials. VTA DA shows Ô¨Åring only at the reward delivery during initial trials while
VTA GABA shows no Ô¨Åring due to absence of any expectation at this stage. At Trial 7, VTA DA shows twin peaks reÔ¨Çecting both a
partial encoding at stimulus arrival and partial cancellation of reward signal. VTA GABA on the other hand has a ramping nature
at the precise time of the arrival of the reward indicating encoding of timing and partial encoding of expectation. The Ô¨Ånal trial
diagram shows the complete cancellation of the VTA DA signal at reward delivery and a complete encoding of stimulus at the CS
arrival. Correspondingly, VTA GABA has a larger expectation signal that ramps that has completely encoded the magnitude of
the reward signal. (C) shows the evolution of the PPN sub-populations during the sequence of trials with PPN RD signalling
reward delivery from LH to VTA DA during the initial trials and PPN FT not showing any sign of expectation at the same time. In
the middle of training, PPN RD reÔ¨Çects a partial Ô¨Åring for stimulus onset and the same reward signal Ô¨Åring as during the initial
trials. PPN FT also reÔ¨Çects this partial Ô¨Åring with a tonic nature of Ô¨Åring that passes onto VTA GABA. During the Ô¨Ånal trial stages,
PPN RD Ô¨Åring peaks for its stimulus arrival Ô¨Åring while a bigger tonic Ô¨Åring signal is observed for PPN FT. (D) portrays the
evolution of VTA DA CS and US signals and that of VTA GABA. Both VTA GABA and VTA DA CS show increased Ô¨Åring across the
trials while VTA DA US progressively reduces ultimately to the background Ô¨Åring rate of VTA DA. All the Ô¨Ågures are averaged for
10 runs.

Discussion

Calculation of RPE in the model
The model aims to provide an alternative view of how RPE is calculated in the brain. Looking at the
model with a functional lens, the VTA GABA signal of the model looks very much like the expectation
signal in Temporal DiÔ¨Äerence (TD) Learning but diÔ¨Äers from TD in how this Ô¨Ånal expectation is

6 of 17

ABCDFigure 3. These Ô¨Ågures indicate the early Ô¨Åring scenarios on VTA and PPN sub-populations. LH indicates the reward delivered
earlier than usual and IT the stimulus. (A) denotes the arrival of the reward at 100ùë°‚Ñé time step after training where VTA DA shows
some Ô¨Åring compared to no Ô¨Åring after training when reward is delivered at the usual time. (B) This Ô¨Åring for an earlier reward is
still larger than a later arrival of early reward at 300 time steps where VTA DA barely shows any Ô¨Åring. (C) indicates the
progressively later "early" rewards Ô¨Åre less as early reward delivery times get closer to usual reward arrival times in accordance
with data on (Fiorillo et al., 2008)

Figure 4. These Ô¨Ågures indicate how timing information is lost but magnitude information of the reward is maintained on VTA
sub-populations. LH indicates the reward delivered earlier than usual and IT the stimulus. During the VS lesion scenario, VTA
GABA is no longer able to integrate the timing information coming from VS and just reÔ¨Çects the projection from PPN FT neurons.
Hence, early rewards are treated as any other reward that comes at the usual time, reÔ¨Çecting no Ô¨Åring (left panel). But when the
reward is of a higher magnitude (middle panel), it is able to indicate the prediction error just as in the control scenario (right
panel) where VS is not lesioned and VTA GABA integrates both magnitude and timing.

calculated. Unlike TD, the expectation signal is a resultant computation derived from calculation of
time and calculation of magnitude. The calculation of time involves learning a single parameter,
the slope, that learns an interval between two points (here the CS and the US) and calculation
of magnitude involves learning the amplitude of the US (encoded as persistent Ô¨Åring). The Ô¨Ånal
expectation calculated in VTA GABA of the model is the interaction between a slope like signal of
time and the persistent amplitude of magnitude giving rise to a TD like expectation signal.

Role of Timing in conditioning
The importance of CS-US interval timing is one of the key postulates of the proposed theory
underlying the model. In this model, the interval timing mechanism uses the dopamine signal
since the onset of the CS to learn the underlying temporal distribution to predict the arrival of the
reward, however it needn‚Äôt be the case. Learning the interval time separating the CS and the reward
could happen without dopamine with a phenomenon called sensory-preconditioning (Sadacca
et al., 2016). Our model predicts that since time and magnitude are separate signals in the brain,
the learning of time precedes the learning of magnitude for reward prediction error to take place.
Interval timing learning in animals has been observed to happen in very few trials and sometimes
even one. The model posits that interval timing learning is an integral part of reward prediction
error computation in appetitive learning and the learning of timing happens before the magnitude
of the stimulus so as to construct an inhibitory signal that has a ramping nature before it ramps to
its minimum amplitude.

The timing mechanism used in the model is very simplistic, reÔ¨Çecting the learning of a single
parameter to get the Ô¨Ånal slope of inhibition. Here, the ability of the model to learn the timing
arises from learning the slope of the interval timing function using a geometric learning rule (refer

7 of 17

ABCABCto Ventral Striatum and OFC section in Methods for more details) and this learning rule is robust
for diÔ¨Äerent intervals (Rivest and Bengio, 2011). The slope is modelled as a negative ramp in
the striatum acting as disinhibition on the VTA GABA neurons. Emmons et al. (2017) report such
ramping activities in the striatum for the control of temporal processing. It is possible that more
complex timing mechanisms are incorporated in the striatum to accommodate stochasticity in the
temporal interval.

This is consistent with the original Temporal DiÔ¨Äerence Reinforcement Learning (TDRL) represen-
tation of dopamine where the cue is tracked since the onset of its presentation, state by state until
reward is delivered as in the complete serial compound (CSC) stimulus representation (Schultz et al.,
1997). Here too, the state is tracked at each time point using the timing signal. However, unlike the
TD model , which is model-free and tracks value across states, this model learns a separate value
signal for state and reward and state representation are computed separately unlike TD-Learning.
This dissociation enables the model to learn changes in magnitude independent of the state and
learn the state independent of the magnitude.

Dual pathway model
The dual pathway models of classical conditioning posit that the mechanism with which the CS Ô¨Åring
occurs at the onset of a trained cue is dissociated from the expectation that inhibits the reward
signal at the time of the reward. Our model too, largely follows the same pattern but with some
deviations (O‚Äôreilly et al., 2007). This model replicates the observation in Fiorillo et al. (2008) about
early rewards delivered at diÔ¨Äerent time points exhibiting diÔ¨Äerent Ô¨Åring. The interpretation of
this model is that, this dopamine error is indicating a mismatch in state rather than a mismatch in
magnitude. Since magnitude is a separate signal and does not require updating, the early reward
Ô¨Åring is indicative of a violation of a belief of the state the animal is in. This prediction error is
predicted to happen between the striatal inhibition and VTA GABA. This inhibition is also predicted
to play a role in the lesion experiments done by Takahashi et al. (2016) where VS lesions hamper
the ability of the animal in tracking state. This lesion inturn has an eÔ¨Äect on VTA GABA which is no
longer inhibited and only serves the magnitude component of the reward prediction error. Hence,
early reward prediction errors no longer happen, because the animal does not recognize state
prediction errors and expects the reward to happen all the time. It was also found in the study that
VTA non dopamine neurons have a higher Ô¨Åring after the VS lesions consistent with our model and
our model predicts, VTA GABA to have higher Ô¨Åring when VS is lesioned. This model hypothesis a
continuous ramping signal that is active throughout the duration of the CS and the US, peaking at
the time of the US and speculates that this inhibition signal and the CS Ô¨Åring could have the same
source.

Redundancy in RPE signals
Though the model architecture shows a circuit-wise strict delineation between expected magnitude
and expected timing, there could be a degree of mixing between these signals.
Indeed, on a
general note, we relied on the assumption that each structure constitutes a module that computes
a variable (though not necessarily a term of the classical RPE equation). However, the way RPE
is computed in dopamine cells seems more mixed (Tian et al., 2016). The multiple areas that
send inputs to dopamine neurons do not encode ‚Äúpure‚Äù variables (reward, expectation etc. . . )
needed to compute to the RPE, but rather a mixture of these variables (and/or mixed with a RPE
signal) suggesting some redundancy in RPE computation (Hunt and Hayden, 2017). At face value,
this would suggest that instead of VS purely encoding expected timing and PPN purely encoding
expected magnitude, these circuits would encode heterogeneous signals of both reward and timing
expectations. However, more neurons in the LH seem to encode reward and not expectations (as in
our model), while striatal cells encode more expectations and less reward (as in our model) (Tian
et al., 2016). Hence, the functional separation between timing and magnitude might not be as
stringent as in our model, but there would still exist a strong bias in VS and PPN signals towards

8 of 17

encoding timing and magnitude, respectively.

Heterogeneity of PPN
Though much of PPN‚Äôs anatomical and chemical characteristics are unknown, studies have shown
functional diÔ¨Äerences between populations inside PPN. SpeciÔ¨Åc populations within PPN exist which
Ô¨Åre phasically for rewards and others which have sustained Ô¨Åring from reward prediction to delivery
of reward, and the activation sustaining till the delivery of the reward even if the arrival of the
reward is delayed (Okada and Kobayashi, 2009). Moreover these neurons seem to encode amount
of reward Ô¨Åring higher for rewards with larger magnitude (Okada and Kobayashi, 2009) (Hong
and Hikosaka, 2014) portraying a graded Ô¨Åring signal capable of diÔ¨Äerentiating reward amounts
consistent with this models characteristics. Studies have also pointed out a growing role for PPN
projections to VTA non-dopamine neurons as necessary for appetitive pavlovian conditioning (Yau
et al., 2016) and activation of PPN glutamate neurons to be reinforcing (Yoo et al., 2017). The
hypothesis of this model is that a subset of PPN neurons (PPN FT) convey reward magnitude
information to the VTA GABA neurons and any optogenetic silencing of these neurons can interfere
with the computation of reward prediction error, though isolating these neurons could prove
diÔ¨Écult owing the structures heterogeneity.

VTA GABA theory of computing RPE in classical conditioning
Previous studies have implicated the striatum (Usuda et al., 1998) as the source of the inhibitory
signal cancelling the dopamine. But, recent projection-speciÔ¨Åc activation by optogenetic studies
among others have shown that the inhibition from striatum has weak to no inhibitory eÔ¨Äects on
DA neurons when stimulating direct striatal inputs on DA neurons in the VTA (KeiÔ¨Çin and Janak,
2015) (Bocklisch et al., 2013) (Chuhma et al., 2011)(Xia et al., 2011)(Klein-Fl√ºgge et al., 2011). There
have been a number of recent results, suggesting an alternate pathway within VTA which might
be responsible for this inhibitory signal. Moreover, optogenetic studies done on VTA (Cohen et al.,
2012)(Eshel et al., 2016) have pointed out not only does the VTA GABA neurons exert enough
inhibition to cancel VTA DA neurons but the inhibition is also subtractive in nature and hence
suitable for computation of reward prediction error (Eshel et al., 2015). This model hypothesises
that the ramping nature of expectation which encodes both magnitude of the stimulus and time of
arrival is encoded in the VTA GABA which acts as the site of integration between these two diÔ¨Äerent
dimensions of the reward. One possible explanation of this distributed nature of reward prediction
error could be that this is what allows for rapid recomputation of values (either of time or magnitude)
and allows the animal to exhibit and sometimes fast, adaptive behavior. Parallels could be drawn
with the literature of reinforcement learning that the animals are not purely engaged in model-free
reinforcement learning and that the dopamine signal itself could be not just performing reward
prediction errors and diÔ¨Äerences in timing could elicit an error from the dopamine system for state
prediction errors. For example, dopamine Ô¨Åring for early reward delivery could be interpreted as
a state prediction error where the animals has to reevaluate the time of the reward rather than
the magnitude of the reward. The precise interpretation of the dopamine prediction error could
be handled by the upstream areas to determine what computations are to be done to reÔ¨Çect a
changed scenario. Here we did not consider a role for VTA GABA in reward omission nor in the
extinction of conditioning. Indeed, VTA GABA Ô¨Åring does not seem to code for the absence of reward
(Cohen et al., 2012). Instead, there may be an asymmetrical computation for omissions compared
to reward, with the former likely driven by the Lateral Habenula, thus forming a separate circuit
(Baker et al., 2016).

This model examines the role of VTA GABA in computing the reward prediction error along with
a few other substrates based on some of the results provided by Takahashi et al. (2016). This paper
adopts a semi-markov approach to explain the Ô¨Åndings while the model given here attempts to
provide a system level model of how the underlying neuronal substrates might act. There are a few
other behaviors that is observed in the model. The authors note that removing VS does not remove

9 of 17

expectation and the animal in eÔ¨Äect expects reward all the time, this could be the VTA GABA signal
we observe in the model when VS is lesioned. VTA GABA loses its ramping functionality and has a
Ô¨Çat tonic Ô¨Åring pattern carrying on its earlier peak expectation thoughout the duration of the trial,
The authors also note that "non-dopaminergic" neurons show signiÔ¨Åcantly higher baseline Ô¨Åring
rate when VS is lesion. Our model hypothesises that it is indeed the VTA GABA neurons that are
now exhibiting a higher Ô¨Çat expectation due to the VS being lesioned. Thus, the model proposes
that it is the VS input to VTA GABA that gives its expectation signal the temporal speciÔ¨Åcity that the
authors mention in their paper.

Methods and Materials

Evaluation of the model
The paradigm used to evaluate the model is a simple CS-US associative learning task and also
considers how the expectation cancels out the dopamine peak at the time of the reward. The trial
duration is 500 time steps with each time step corresponding to 1ms. The stimulus is presented
at the 10ùë°‚Ñé time step and is kept switched on till the arrival of the reward at the 400ùë°‚Ñé time step
(400ms). The reward and the stimulus have by default a magnitude of 1. The number of trials for
the entire conditioning to happen was set at 14 trials (i.e. trials required for the learning algorithm
to converge).

Model Description
Computational principles
The system-level model is composed of mean-Ô¨Åeld description of neuronal populations representing
distinct, interconnected brain structures. Population dynamics is described by its average Ô¨Åring
frequency across time ùëà (ùë°), which is taken as the positive part of a membrane potential ùëâ (ùë°),
represented by the following equations:

ùúè.

ùëëùëâ (ùë°)
ùëëùë°

= (‚àíùëâ (ùë°) + ùëîùëíùë•ùëê(ùë°) ‚àí ùëîùëñùëõ‚Ñé(ùë°) + ùêµ + ùúÇ(ùë°))

ùëà (ùë°) = (ùëâ (ùë°))+

(2)

(3)

Here ùúè is the time constant of the cell, ùêµ is the baseline Ô¨Åring rate and ùúÇ(ùë°) is the additive noise
term chosen randomly at each time step from a uniform distribution between ‚àí0.01 and 0.01.
The incoming aÔ¨Äerent synaptic currents ùëîùëíùë•ùëê
represent the weighted sum of excitatory
and ùëîùëñùëõ‚Ñé
and inhibitory Ô¨Åring rates, respectively, the weight representing the synaptic weights between the
populations.

Some of the neuronal populations extract a short-term phasic activity from their incoming
inputs, by removing out the tonic component of the input. This is done by the following equations:

ùúè.

ùëëùë•(ùë°)
ùëëùë°

= (‚àíùë•(ùë°) + ùë•(ùë°))

ùúôùúè,ùëò(ùë•(ùë°)) = (ùë•(ùë°) ‚àí ùëò.ùë•(ùë°))+

(4)

(5)

Here ùë•(ùë°) integrates the incoming input ùë•(ùë°) with a time constant ùúè and thus represents the tonic
component of the input, while ùúôùúè,ùëò(ùë•(ùë°)) represents the positive part of the diÔ¨Äerence between
ùë•(ùë°) and ùë•(ùë°). Hence, The constant ùëò controls how much of the original input is kept, a ùëò value
of 0 indicates the entire synaptic input is to be preserved and a ùëò value of 1 outputs the phasic
component only, i.e. the entire tonic component has been entirely removed.

10 of 17

A Bound function is used when the Ô¨Åring of a population is described with an upper and a lower

limit in certain populations.

ùúì(ùë•) =

0
x
1

‚éß
‚é™
‚é®
‚é™
‚é©

if x < 0
if 0 < x < 1
if x > 1

(6)

A threshold function is also used in some populations and it outputs 1 when the input exceeds

a threshold Œì, 0 otherwise:

ŒîŒì(ùë•) =

{

0
1

if x < Œì
otherwise

(7)

The learning rules deÔ¨Åned in the model are based on the Hebbian learning rule and a DA
modulated learning rule in the case of BLA like the multiplicative three factor learning rule. The
evolution over time of the weight ùë§(ùë°) of a synapse between the neuronal population ùëùùëüùëí (presynaptic
neurons) and the neuronal population ùëùùëúùë†ùë° (postsynaptic neurons) is governed by:

ùëëùë§(ùë°)
ùëëùë°

= (ùõº.ùëàùëùùëüùëí(ùë°).ùëàùëùùëúùë†ùë°(ùë°))

(8)

where ùë§ is the weight term, ùõº the learning rate, ùëàùëùùëüùëí(ùë°) and ùëàùëùùëüùëí(ùë°) are indicating the Ô¨Åring rates of
the presynaptic and postsynaptic neuronal populations, respectively.

Population deÔ¨Ånitions
Representations of inputs
The sensory inputs of the CS and the reward input of the US are encoded by the inferotemporal
cortex (IT) and the lateral hypothalamus (LH), respectively, simply as square wave signals:

ùëà (ùë°) = ùêº(ùë°)+

(9)

where ùêº(ùë°) is an external input resulting either from a stimulus or from a reward.

Basolateral Amygdala
The BLA receives inputs about the CS from the IT, the US from the LH, as well as VTA DA output. This
allows the BLA to learn to associate the CS with the US, thus providing a magnitude expectation.
The equation below is the same equation as in Equation 2 without the inhibitory component and
with the presence of a tonic to phasic conversion.

ùúè.

ùëëùëâ (ùë°)
ùëëùë°

= (‚àíùëâ (ùë°) + ùúôùúèùëíùë•ùëê,ùëò(ùëîùëíùë•ùëê(ùë°)) + ùúÇ(ùë°))

(10)

ùëà (ùë°) = (ùëâ (ùë°))+

with ùúè = 10ms, ùúèùëíùë•ùëê = 10ms, ùëò= 1.
The CS is learned by updating the synaptic weights between IT and BLA and the learning rule is
given by:

ùëëùë§(ùë°)
ùëëùë°

= ùê∑.ùõº.ùëàùëùùëüùëí(ùë°).(ùëàùëöùëéùëî ‚àí ùëàùëùùëúùë†ùë°(ùë°))+

(11)

Here ùê∑ indicates the presence of the US corresponding to the dopaminergic neuronal modulation
from the VTA, ùõº is the learning rate equal to 0.003, ùëàùëöùëéùëî
and ùëàùëùùëúùë†ùë°
are the Ô¨Åring rates of presynaptic and postsynpatic neurons, respectively.

is the magnitude of LH Ô¨Åring, ùëàùëùùëüùëí

11 of 17

Central Amygdala
The CE is the output nuclei of the amygdala and it projects to both the PPN nuclei, relaying
information from the BLA. The CE projects to the PPN RD neurons that convey US and CS Ô¨Åring to
the VTA dopamine neurons and PPN FT neurons that convey reward expectation.

The equations for the membrane potential and the Ô¨Åring rate are the same as Equation 10 and

Equation 3, respectively, with ùúè = 20ms, ùúèùëíùë•ùëê = 5ms, ùëò = 1.

Peduncolopontine nucleus
The PPN has two distinct populations in this model for reward and expectation. The PPN is a
heterogeneous structure both in terms of neuronal populations and of responses during classical
conditioning (Okada and Kobayashi, 2009). Hence, we modeled two distinct subpopulations reÔ¨Çect-
ing the two major classes of responses found experimentally: FT (Fixation Target) population, which
activates brieÔ¨Çy upon CS or US presentation, and RD (Reward Delivery) population, which display
sustained activity during the CS-US interval.

PPN RD
The PPN Reward Delivery neurons signal the occurrence of the CS and the US from the CE and the
LH, respectively. It also contains a sub-population of inhibitory neurons that inhibits the PPN FT
neurons.

The equations for the membrane potential and the Ô¨Åring rate are the same as Equation 10 and

Equation 3 respectively, with ùúè = 5ms, ùúèùëíùë•ùëê = 5ms, ùëò = 1.

PPN FT
The PPN FT neurons encode the magnitude expectation delivered to the VTA GABA neurons. The
PPN FT neurons receive information from the CE and are inhibited by the PPN RD neurons. They
serve to maintain a constant magnitude that is conveyed to the VTA GABA neurons for Ô¨Ånal reward
prediction error computation. The equations for the membrane potential and the Ô¨Åring rate are
the same as Equation 2 without baseline Ô¨Åring and Equation 3 respectively with ùúè = 5ms.

Ventral Striatum and OFC
It has long been thought that the Ventral Striatum (VS) is responsible for the reward prediction term
in RPE calculation. In our model, the VS encodes the duration of the CS-US interval only. The VS is
composed of inhibitory cells, and signals the timing of expected reward to VTA GABA cells through
a decrease in activity. The OFC (Orbitofrontal Cortex) relays the presence of the US from the IT
to the VS. Then, a simpliÔ¨Åed timing model comprising a negative integrator similar to the timing
algorithm in Rivest and Bengio (2011) signals the interval duration through a slowly decreasing
activity until the expected timing of the US. To do so, the integrator here has an amplitude of 1 at
the beginning of the trial and after weight updating, decreases its Ô¨Åring to 0 at the precise time of
reward delivery. In this framework, learning the CS-US interval consists in adjusting the slope of the
slowly-decreasing activity.

Mechanism of timing
The timing mechanism in the VS transforms a tonic excitatory input from the OFC (which is the
same as the signal from IT, cf Ô¨Åg 2.A) and transforms it into a decreasing sustained activity, whose
slope depends upon the weights between OFC and VS.

ùúè.

ùëëùëâ (ùë°)
ùëëùë°

= (ùëîùëíùë•ùëê(ùë°)) ‚àí ùëâ (ùë°).ùúôùúèùëöùëúùëë(ùëîùëöùëúùëë(ùë°))) + ùúÇ(ùë°))

(12)

12 of 17

ùëà (ùë°) = (ŒîŒì(ùëîùëíùë•ùëê(ùë°) ‚àí ùúôùúèùëöùëúùëë(ùëîùëöùëúùëë(ùë°))) ‚àí ùúì(ùëâ (ùë°))+

(13)

with ùúè = 1ms, ùúèùëöùëúùëë = 5ms and ùëîùëöùëúùëë(ùë°) signals reward delivery from VTA dopamine to VS or LH to

VS with Œì threshold function reaching 1 for any value greater than 0. ùúì() is a bounded function.

Figure 5. The slope is decreased at every iteration until it exceeds the duration (the red line) enabling exact correction of the
weight encoding the duration to be found (the black line). The colors indicate the progressive iterations

As described in Ô¨Ågure 5, weight is updated after each iteration according to the following rule:

ùëëùë§(ùë°)
ùëëùë°

= ùê∑.(‚àíùõº.ùë§ + ŒîŒì(ùëà (ùë°)).ùë§.(ùëà (ùë°)‚àï(1 ‚àí ùëà (ùë°)))

(14)

where ùê∑ indicates the presence of dopamine ùõº is the learning rate equal to 0.4. The Ô¨Årst term
decreases the weight based on ùõº and the weight keeps decreasing until the bound is reached when
ŒîŒì(ùëà (ùë°)) becomes greater than 0 at the time of the reward. The correcting update is the second
term of the weight updating and the slope is increased with a weight increase encoding the duration
of the interval. As a result of this learning process, the weight between OFC and VS will result in VS
predicting the occurrence of US and disinhibiting VTA GABA accordingly.

It should be noted that the model postulates that the learning of time happens before the

learning of value of the stimulus, i.e., its magnitude.

VTA
The VTA comprises two major neuronal populations, dopaminergic (DA) and gabaergic (GABA),
glutamatergic cells representing less than 3 percent of VTA cells. VTA GABA neurons locally inhibit
VTA DA neurons and participate in the computation of reward prediction error in VTA DA cells (Cohen
et al., 2012). More precisely, VTA GABA neurons display a sustained, slowly-increasing ramping
activity during the CS-US interval (Eshel et al., 2015) but only signiÔ¨Åcantly aÔ¨Äect phasic DA activity
(i.e. the RPE) rather tonic DA activity during the interval. We thus modeled the two populations
from the VTA as follows.

VTA Dopamine
The VTA dopaminergic (DA) neurons receive excitatory inputs from the PPN RD population, which
conveys actual reward and reward prediction from the amygdala; and inhibitory inputs from the
VTA GABA cells, which signal reward expectation. The diÔ¨Äerence between these excitatory and
inhibitory inputs constitutes the reward prediction error (RPE) (Sutton and Barto, 1998) (Glimcher,
2011). VTA DA neurons broadcast this RPE to the system. During learning, VTA DA neurons initially
Ô¨Åre upon US reward delivery. This US activity progressively gets canceled by VTA GABA signaling the
reward expectation, and at the same time, phasic Ô¨Åring upon CS arrival develops with learning.

ùúè.

ùëëùëâ (ùë°)
ùëëùë°

= (‚àíùëâ (ùë°) + ùúôùúèùëíùë•ùëê,ùëò(ùëîùëíùë•ùëê(ùë°)) ‚àí ùúôùúèùëñùëõ‚Ñé,ùëò(ùëîùëñùëõ‚Ñé(ùë°)) + ùúÇ(ùë°))

13 of 17

With ùúè = 5ms, ùúèùëíùë•ùëê = 5ms, ùëò = 1 and ùêµ is the baseline Ô¨Åring rate of the VTA Dopamine equal to 0.2

ùëà (ùë°) = (ùëâ (ùë°) + ùêµ)+

(15)

VTA GABA
VTA GABA neurons combine inputs from the VS, which encodes the expected time of reward, and
from the PPN, which signals expected reward magnitude. VTA GABA neurons thus encode reward
expectation and inhibit VTA DA neurons. The equation for membrane potential is the same as in
Equation 2 without baseline Ô¨Åring and population dynamics follows Equation 3 with ùúè = 20ms.

This model is implemented in Python, and uses the DANA library for neuronal computation
(Rougier and Fix, 2012). Description of all the other model parameters is detailed in Table 1. The
model can be accessed in the following link : https://github.com/palladiun/Pavlovian-Conditioning-
VTA-GABA

Architectural parameters

Meaning

Value

Parameter

US input\_size

CS input\_size

VTA Dopamine\_size

VTA GABA\_size

BLA\_size

CE\_size

OFC\_size

PPN RD\_size

PPN FT\_size

size of input vectors from LH

size of input vectors from IT

number of neurons in VTA Dopamine

number of neurons in VTA GABA

number of neurons in BLA

number of neurons in CE

number of neurons in OFC

number of neurons in PPN RD

number of neurons in PPN FT

1

4

10

5

1

1

1

4

4

4

PPN Magnitude\_size

number of neurons in PPN Magnitude

BLA\_CE

LH\_PPN\_RD

LH\_BLA

IT\_OFC

CE\_PPN\_RD

CE\_PPN\_Mag

PPN\_RD\_PPN\_Mag

PPN\_RD\_VTA\_Dop

PPN\_Mag\_PPN\_Rel

VS\_PPN\_Rel

PPN\_Rel\_VTA\_GABA

VTA\_Dopamine\_BLA

VTA\_Dopamine\_BLA

Equation parameters

constant weights from BLA to CE

0.15

constant weights from LH to PPN\_RD

constant weights from LH to BLA

constant weights from IT to OFC

constant weights from CE to PPN\_RD

constant weights from CE to PPN\_Mag

constant weights from PPN\_RD to PPN\_Mag

constant weights from PPN\_RD to VTA\_Dop

constant weights from PPN\_Mag to PPN\_Rel

constant weights from VS to PPN\_Rel

constant weights from PPN\_Rel to VTA\_GABA

constant weights from VTA\_Dopamine to BLA

constant weights from VTA\_Dopamine to VS

1.2

1

.25

2

.3

0.8

1

0.2

1

0.25

1

1

VTA\_GABA\_VTA\_Dopamine

constant weights between VTA\_GABA and VTA\_Dopamine

0.2

OFC\_VS

IT\_BLA

initial weights between OFC and VS

initial weights between OFC and VS

0.006

0.01

Table 1. Table describing network architecture and parameters used in activation and learning rules.

14 of 17

Acknowledgments
We would like to acknowledge the following grants which have been a major support to this
research.

‚Ä¢ Indo-French CEFIPRA Grant for the project Basal Ganglia at Large (No. DST-INRIA 2013-02/Basal

Ganglia dated 13-09-2014)

‚Ä¢ Internships programme at INRIA, 6 month Internship with Team Mnemosyne at INRIA Bor-

deaux - Sud-Ouest

We also thank Maxime Carrere for helpful discussions.

References
Baker PM, Jhou T, Li B, Matsumoto M, Mizumori SJ, Stephenson-Jones M, Vicentic A. The lateral habenula
circuitry: reward processing and cognitive control. Journal of Neuroscience. 2016; 36(45):11482‚Äì11488.

Bocklisch C, Pascoli V, Wong JC, House DR, Yvon C, De Roo M, Tan KR, L√ºscher C. Cocaine disinhibits dopamine
neurons by potentiation of GABA transmission in the ventral tegmental area. Science. 2013; 341(6153):1521‚Äì
1525.

Carmichael S, Price JL. Sensory and premotor connections of the orbital and medial prefrontal cortex of

macaque monkeys. Journal of Comparative Neurology. 1995; 363(4):642‚Äì664.

Chuhma N, Tanaka KF, Hen R, Rayport S. Functional connectome of the striatal medium spiny neuron. Journal

of Neuroscience. 2011; 31(4):1183‚Äì1192.

Cohen JY, Haesler S, Vong L, Lowell BB, Uchida N. Neuron-type speciÔ¨Åc signals for reward and punishment in

the ventral tegmental area. nature. 2012; 482(7383):85.

Emmons EB, De Corte BJ, Kim Y, Parker KL, Matell MS, Narayanan NS. Rodent Medial Frontal Control of Temporal
Processing in the Dorsomedial Striatum. The Journal of Neuroscience: The OÔ¨Écial Journal of the Society for
Neuroscience. 2017 Sep; 37(36):8718‚Äì8733.

Eshel N, Bukwich M, Rao V, Hemmelder V, Tian J, Uchida N. Arithmetic and local circuitry underlying dopamine

prediction errors. Nature. 2015; 525(7568):243.

Eshel N, Tian J, Bukwich M, Uchida N. Dopamine neurons share common response function for reward

prediction error. Nature neuroscience. 2016; 19(3):479.

Fiorillo CD, Newsome WT, Schultz W. The temporal precision of reward prediction in dopamine neurons. Nature

neuroscience. 2008; 11(8):966‚Äì973.

Fiorillo CD, Tobler PN, Schultz W. Discrete coding of reward probability and uncertainty by dopamine neurons.

Science. 2003; 299(5614):1898‚Äì1902.

Fiorillo CD, Tobler PN, Schultz W. Evidence that the delay-period activity of dopamine neurons corresponds to
reward uncertainty rather than backpropagating TD errors. Behavioral and brain Functions. 2005; 1(1):7.

Glimcher PW. Understanding dopamine and reinforcement learning: the dopamine reward prediction error

hypothesis. Proceedings of the National Academy of Sciences. 2011; 108(Supplement 3):15647‚Äì15654.

Hollerman JR, Schultz W. Dopamine neurons report an error in the temporal prediction of reward during

learning. Nature neuroscience. 1998; 1(4):304.

Hong S, Hikosaka O. Pedunculopontine tegmental nucleus neurons provide reward, sensorimotor, and alerting

signals to midbrain dopamine neurons. Neuroscience. 2014; 282:139‚Äì155.

Hunt LT, Hayden BY. A distributed, hierarchical and recurrent framework for reward-based choice. Nature

Reviews Neuroscience. 2017; 18(3):172‚Äì182.

Janak PH, Tye KM. From circuits to behaviour in the amygdala. Nature. 2015; 517(7534):284.

Joel D, Niv Y, Ruppin E. Actor‚Äìcritic models of the basal ganglia: New anatomical and computational perspectives.

Neural networks. 2002; 15(4-6):535‚Äì547.

15 of 17

KeiÔ¨Çin R, Janak PH. Dopamine prediction errors in reward learning and addiction: from theory to neural circuitry.

Neuron. 2015; 88(2):247‚Äì263.

Klein-Fl√ºgge MC, Hunt LT, Bach DR, Dolan RJ, Behrens TE. Dissociable reward and timing signals in human

midbrain and ventral striatum. Neuron. 2011; 72(4):654‚Äì664.

Kobayashi Y, Okada KI. Reward prediction error computation in the pedunculopontine tegmental nucleus

neurons. Annals of the New York Academy of Sciences. 2007; 1104(1):310‚Äì323.

Lokwan S, Overton P, Berry M, Clark D. Stimulation of the pedunculopontine tegmental nucleus in the rat

produces burst Ô¨Åring in A9 dopaminergic neurons. Neuroscience. 1999; 92(1):245‚Äì254.

Niv Y, DuÔ¨Ä MO, Dayan P. Dopamine, uncertainty and TD learning. Behavioral and brain Functions. 2005; 1(1):6.

Okada KI, Kobayashi Y. Characterization of oculomotor and visual activities in the primate pedunculopontine
tegmental nucleus during visually guided saccade tasks. European Journal of Neuroscience. 2009; 30(11):2211‚Äì
2223.

Okada Ki, Kobayashi Y. Reward prediction-related increases and decreases in tonic neuronal activity of the

pedunculopontine tegmental nucleus. Frontiers in integrative neuroscience. 2013; 7.

O‚Äôreilly RC, Frank MJ, Hazy TE, Watz B. PVLV: the primary value and learned value Pavlovian learning algorithm.

Behavioral neuroscience. 2007; 121(1):31.

Pan WX, Schmidt R, Wickens JR, Hyland BI. Dopamine cells respond to predicted events during classical
conditioning: evidence for eligibility traces in the reward-learning network. Journal of Neuroscience. 2005;
25(26):6235‚Äì6242.

Pavlov IP. Conditional reÔ¨Çexes: An investigation of the physiological activity of the cerebral cortex. H. Milford;

1927.

Rivest F, Bengio Y. Adaptive drift-diÔ¨Äusion process to learn time intervals. arXiv preprint arXiv:11032382. 2011; .

Rougier NP, Fix J. DANA: distributed numerical and adaptive modelling framework. Network: Computation in

Neural Systems. 2012; 23(4):237‚Äì253.

Sadacca BF, Jones JL, Schoenbaum G. Midbrain dopamine neurons compute inferred and cached value

prediction errors in a common framework. Elife. 2016; 5:e13665.

Sah P, Faber EL, De Armentia ML, Power J. The amygdaloid complex: anatomy and physiology. Physiological

reviews. 2003; 83(3):803‚Äì834.

Schultz W, Dayan P, Montague PR. A neural substrate of prediction and reward. Science. 1997; 275(5306):1593‚Äì

1599.

Semba K, Fibiger HC. AÔ¨Äerent connections of the laterodorsal and the pedunculopontine tegmental nuclei
Journal of Comparative

in the rat: a retro-and antero-grade transport and immunohistochemical study.
Neurology. 1992; 323(3):387‚Äì410.

Sutton RS, Barto AG. Reinforcement learning: An introduction, vol. 1. MIT press Cambridge; 1998.

Takahashi YK, Langdon A J, Niv Y, Schoenbaum G. Temporal speciÔ¨Åcity of reward prediction errors signaled by

putative dopamine neurons in rat VTA depends on ventral striatum. Neuron. 2016; 91(1):182‚Äì193.

Tian J, Huang R, Cohen JY, Osakada F, Kobak D, Machens CK, Callaway EM, Uchida N, Watabe-Uchida M.
Distributed and mixed information in monosynaptic inputs to dopamine neurons. Neuron. 2016; 91(6):1374‚Äì
1389.

Usuda I, Tanaka K, Chiba T. EÔ¨Äerent projections of the nucleus accumbens in the rat with special reference to

subdivision of the nucleus: biotinylated dextran amine study. Brain research. 1998; 797(1):73‚Äì93.

Vitay J, Hamker FH. Timing and expectation of reward: a neuro-computational model of the aÔ¨Äerents to the

ventral tegmental area. Frontiers in neurorobotics. 2014; 8.

Xia Y, Driscoll JR, Wilbrecht L, Margolis EB, Fields HL, Hjelmstad GO. Nucleus accumbens medium spiny
neurons target non-dopaminergic neurons in the ventral tegmental area. Journal of Neuroscience. 2011;
31(21):7811‚Äì7816.

16 of 17

Yau HJ, Wang DV, Tsou JH, Chuang YF, Chen BT, Deisseroth K, Ikemoto S, Bonci A. Pontomesencephalic tegmental
aÔ¨Äerents to VTA non-dopamine neurons are necessary for appetitive Pavlovian learning. Cell reports. 2016;
16(10):2699‚Äì2710.

Yoo JH, Zell V, Wu J, Punta C, Ramajayam N, Shen X, Faget L, Lilascharoen V, Lim BK, Hnasko TS. Activation of

pedunculopontine glutamate neurons is reinforcing. Journal of Neuroscience. 2017; 37(1):38‚Äì46.

17 of 17

