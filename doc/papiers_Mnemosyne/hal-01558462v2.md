Syntactic Reanalysis in Language Models for Speech
Recognition
Johannes Twiefel, Xavier Hinaut, Stefan Wermter

To cite this version:

Johannes Twiefel, Xavier Hinaut, Stefan Wermter. Syntactic Reanalysis in Language Models for
Speech Recognition. 2017 Joint IEEE International Conference on Development and Learning and
Epigenetic Robotics (ICDL-EpiRob), Sep 2017, Lisbon, Portugal. ￿hal-01558462v2￿

HAL Id: hal-01558462

https://inria.hal.science/hal-01558462v2

Submitted on 7 Jul 2017

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Syntactic Reanalysis in Language Models for
Speech Recognition

Johannes Twiefel∗, Xavier Hinaut∗†‡§ and Stefan Wermter∗
∗Knowledge Technology Group, Department of Computer Science, University of Hamburg
Vogt-K¨olln-Straße 30, 22527 Hamburg, Germany {twiefel, hinaut, wermter}@informatik.uni-hamburg.de
†Inria Bordeaux Sud-Ouest, Talence, France xavier.hinaut@inria.fr
‡LaBRI, UMR 5800, CNRS, Bordeaux INP, Universit´e de Bordeaux, Talence, France
§Institut des Maladies Neurod´eg´en´eratives, UMR 5293, CNRS, Universit´e de Bordeaux, Bordeaux, France

Abstract—State-of-the-art speech recognition systems steadily
increase their performance using different variants of deep neural
networks and postprocess the results by employing N-gram
statistical models trained on a large amount of data coming from
the general-purpose domain. While achieving an excellent per-
formance regarding Word Error Rate (17.343% on our Human-
Robot Interaction data set), state-of-the-art systems generate
hypotheses that are grammatically incorrect in 57.316% of the
cases. Moreover, if employed in a restricted domain (e.g. Human-
Robot Interaction), around 50% of the hypotheses contain out-of-
domain words. The latter are confused with similarly pronounced
in-domain words and cannot be interpreted by a domain-speciﬁc
inference system.

The state-of-the-art speech recognition systems lack a mech-
anism that addresses the syntactic correctness of hypotheses.
We propose a system that can detect and repair grammatically
incorrect or infrequent sentence forms. It is inspired by a
computational neuroscience model that we developed previously.
The current system is still a proof-of-concept version of a future
neurobiologically more plausible neural network model. Hence,
the resulting system postprocesses sentence hypotheses of state-of-
the-art speech recognition systems, producing in-domain words
in 100% of the cases, syntactically and grammatically correct
hypotheses in 90.319% of the cases. Moreover, it reduces the
Word Error Rate to 11.038%.

I. INTRODUCTION

State-of-the-art automatic speech recognition (ASR) sys-
tems like Google’s Search by Voice [1], [2], [3] or Baidu’s
Deep Speech 2 [4] are based on deep neural networks (DNN),
which require huge amounts of data and processing power.
Statistical higher order n-gram models are also trained on large
amounts of text data from the general-purpose domain to per-
form language modeling or postprocessing of the hypotheses.
Speech is an important modality in Human-Robot Inter-
action (HRI) and used to communicate with the agent. The
mentioned state-of-the-art ASR systems achieve impressive
performance on benchmark data sets regarding ASR metrics
like Word Error Rate (WER). For HRI, the requirement for
an ASR hypothesis is not only that
it has a low WER,
but also that it can be understood and interpreted by the
robot. This interpretation can be performed using thematic role
labeling systems [5], [6]. If the syntax is violated or out-of-
domain words occur, the ASR hypotheses could possibly not
be interpreted. Often, HRI consists of a speciﬁc application
scenario, where the robot is inside a restricted domain and does

not need to knowledge about the world outside of the domain.
For example, a kitchen robot needs to understand utterances
like ‘cook me a meal’
related to a kitchen scenario,
but is not expected to understand sentences like ‘score a
goal’ which belong to a football domain.

As we showed already in previous work, domain knowledge
helps in outperforming those models in ASR benchmark tasks
[7]. If the sentences to be possibly uttered by the users are
known in advance or could be generated by a given grammar,
the system achieved better performance regarding the Word
Error Rate (WER). Additionally, the hypotheses produced by
our system possess a grammatically and syntactically correct
form. This correctness should assist the interpretation of those
hypotheses by thematic role labeling systems [5]. If the con-
crete sentences for a restricted domain or a grammar are not
available, we also use a statistical n-gram model to generate
the hypotheses, like Google and Baidu did. N-gram models are
only able to help with correcting acoustic recognition errors in
the local context of a word and not in the global context (the
whole sentence). As statistical models rely on probabilities,
their beneﬁt is to be able to generate combinations of words
which are not covered by the training data. This leads to
the dilemma that word combinations can be generated which
violate the grammar and syntax of the given language.

In this work, we propose a method which combines the
advantages of a grammar with the ones of a statistical n-
gram model to reduce the risk of generating incorrect hy-
potheses and is also able to generalize to non-existent word
combinations in the training data. The model
is inspired
by the model of Hinaut and Dominey [5][8] which exploits
the “constructions” (templates of sentences) hypothesis about
children language acquisition [9].

II. RELATED WORK

In this section, we shortly present the mechanisms of state-
of-the-art speech recognition systems and our previous work
on phonemic postprocessing to improve the performance of
ASR systems in a restricted domain.

A. State-of-the-Art Speech Recognition

State-of-the-art speech recognition systems like Google’s
Search by Voice [1], [2], [3] or Baidu’s Deep Speech 2

[4] employ a deep Long Short-Term Memory (LSTM) [10]
together with Connectionist Temporal Classiﬁcation (CTC)
[11]. LSTMs are able to learn long-range dependencies inside
a sequence, compared to N-gram Hidden Markov Models
(HMM) which are only able to model
the local context.
Frames of audio data are taken as input and trained to
produce e.g. phones, letters or words. The problem of the
different timescales between acoustic frames and labels is
solved using CTC, which introduces a blank label to ﬁll the
gaps and performs an alignment between the two sequences.
The outputs are postprocessed by a 5-gram statistical language
model both for Google’s and Baidu’s ASR.

B. Phonemic Postprocessing

Our preliminary work [7] suggests that domain knowledge
helps in improving the results of DNN-based speech recogni-
tion by postprocessing them using this knowledge. Traditional
speech recognition commonly used before the success of Deep
Learning [12] in general consists of an acoustic model, which
generates phonemes from acoustic data and a language model
that generates words based on a grammar or a statistical n-
gram model. The phoneme representation of these words are
then scored against the phoneme sequences generated by the
acoustic model to produce a probabilistic word sequence hy-
pothesis. As described in section II-A, state-of-the-art ASR can
also generate an intermediate representation like phonemes.
However, the acoustic model and the language model can
also both be included in a large DNN and phonemes are not
necessary in this case as words are being generated directly.
The acoustic models used for traditional speech recognition
are based on Mel Frequency Cepstral Coefﬁcients (MFCCs)
[13] which are used to extract human speech from the audio
signal. The acoustic model is trained on MFCC features de-
rived from speech and the corresponding phoneme sequences.
A phoneme is the smallest meaning distinguishing unit
to express a word. Compared to text, a character would
be the smallest meaning distinguishing unit. Phonemes can
be uttered using different phones or speech sounds, which
makes phonemes a superclass of phones. Comparing this to
characters again, a character can be expressed using different
fonts. By this deﬁnition, a phoneme is speaker-independent,
making it a suitable intermediate representation that can be
used for scoring. We hypothesize that a phoneme is also
spoken the same way independently from its domain, meaning
phonemes are spoken the same way in a kitchen, football or
Human-Robot Interaction context, which would make acoustic
modeling domain-independent, and acoustic models could be
transferred from one domain to another.

Another hypothesis is that language models are domain-
dependent, as the training data for a model should follow the
same distribution as the data of the environment it is used
in, and this is only true if a general-purpose model is used
in domains which are a subset of a general-purpose domain.
If a general-purpose language model is used only inside a
speciﬁc domain that does not follow the same distribution as

the general-purpose domain, the language model is not the
optimal one for this domain.

For this reason, we proposed a uniﬁed ASR system that
consists of a large and well-trained DNN-based domain-
independent acoustic model combined with a domain-speciﬁc
language model. Due to the nature of DNNs to require large
amounts of data and the lack of this data in small domains,
we recommended to use traditional language modeling like
statistical n-grams models and grammars.

One of the approaches are based on the traditional open-
source ASR system Sphinx-4 [14], which uses HMMs for
language and acoustic modeling and a Viterbi decoder to ﬁnd
the best word sequence hypothesis. As the acoustic model
relies on MFCCs and is trained on a limited amount of
labeled acoustic data compared to the massive amount of data
companies like Google are able to generate and process, the
acoustic model is the weakness of the Sphinx-4 system. The
scoring is performed on the phoneme level, which offers the
possibility to remove the acoustic model from Sphinx and feed
in a phoneme sequence directly. Instead of training our own
domain-independent acoustic model, we employ the massive
acoustic models of e.g. Google by delegating the acoustic
processing to Google’s Search by Voice. The new uniﬁed
system is called DOCKS [7] and supports language models
in the form of grammars (DOCKS Grammar) or statistical
bigram models (DOCKS Bigram).

Google’s hypothesis for the reference text ‘addressed
mail’ could be something similar to ‘a dressed male’
which is completely incorrect on the word level. On the
phoneme level both grapheme sequences can be represented
as ‘AH D R EH S T M EY L’. We employ the trainable
grapheme-to-phoneme converter SequiturG2P [15] and train it
on CMUdict 0.7a 1 to be able to generate a phoneme sequence
for any grapheme sequence coming from Google’s ASR. These
phoneme sequences are then fed to our postprocessing system.
We proved that this principle works better than using the given
acoustic models of Sphinx-4 [7].

Another approach contained in the DOCKS system is called
DOCKS Sentencelist. If a list of all possible sentences that can
be uttered in a restricted domain is known beforehand, this
restricted but robust approach can be used. The approach is
based on the Levenshtein distance [16], which is a standard
method to calculate a distance score between two sequences
a and b, with i and j being the recursively processed indices
of the sequences:

La,b(i, j) =





max(i, j)



min

if min(i, j) = 0,

La,b(i − 1, j) + 1
La,b(i, j − 1) + 1
La,b(i − 1, j − 1) + 1(ai(cid:54)=bj )



otherwise.

(1)
We convert the 10 best hypotheses from Google’s ASR to
phoneme sequences and do the same for the list of expectable

1http://www.speech.cs.cmu.edu/cgi-bin/cmudict

sentences. Then, a normalized Levenshtein distance is cal-
culated over all 10 best phoneme sequences (H) against all
phoneme sequences of the sentence list (S):

λ = argmin Lhk,sl (|hk|, |sl|)

(2)

where L is the Levenshtein distance. The conﬁdence value
was computed as

γA = max(0, 1 −

Lhk,sl (|hk|, |sl|)
|sl|

)

(3)

with hk ∈ H (set of the 10 best hypotheses) and sl ∈ S (set of
reference sentences) both in phonemic representation. As this
approach is the most restricted one, it performed best (WER
around 0%) if all spoken sentences are known in advance [7].

III. APPROACH

This work is inspired by the work of Hinaut and Dominey
[5], who proposed a neural computational model for thematic
role labeling from incoming grammatical constructions [17],
while our approach does not employ neural networks. One
hypothesis for language acquisition is that children learn “tem-
plates of sentences” [9] (i.e. grammatical constructions [17]).
The θRARes model proposed by Hinaut and Dominey [5]
learns to assign θ-roles (thematic roles) to the semantic words
(SW) or content words in a sentence. E.g. the sentence ‘put
the pyramid on the cube’ is mapped to the predi-
cate ‘put(pyramid, cube)’. This mapping is called a
grammatical construction [9]. The sentence is preprocessed by
removing all semantic words from the sequence and replacing
them by the wildcard token X: ‘X the X on the X’. In
this work, we call these structures like ‘X the X on the
X’ a Sentence Template (ST), which is adapted from Hinaut
and Dominey [5]. These STs are used for our language model.
The system learns to assign the slots of the predicate to the
SWs of the sequence and requires to determine if a word is
a SW and has to be replaced by the wildcard token. As SWs
are an open class words, which means that new words can be
added to this class, it is not trivial to determine if a word is a
SW or not. Instead, non-semantic words, which are function
words, are identiﬁed, as they belong to a closed class meaning
that there is only a ﬁnite number. All words that do not belong
to the closed class of function words are considered to be SWs.
As function words belong to a closed class, they are known
beforehand even for new domains.

The idea of our work is to be able to perform domain-
restricted language modeling while also making use of
general-purpose language models. The text output coming
from Google’s ASR is not ignored like in previous approaches
(see Sec. II-B) where only the phonemic representation was
processed. This approach also handles the syntactic structure
in the form of STs. We consider function words to be domain
independent, which makes STs consist of domain-independent
elements. Instead of only processing the phonemic represen-
tation of a hypothesis, we are now able to exploit the beneﬁts
of a general-purpose language model.

Fig. 1. Architecture. This ﬁgure shows the processing pipeline of our system.
First, the system tries to use the Sentence Template Grammar. If the conﬁdence
for individual words or subsequences is below a threshold, this part of the
hypothesis is postprocessed by the Sentence Template N-gram module.

Figure 1 depicts the processing pipeline of our system.
We take the text hypothesis coming from Google’s speech
recognition and build the ST out of it. The training data that
we possess for our restricted domain consists of grammatically
and syntactically correct sentences. As the hypothesis coming
from Google may be grammatically or syntactically incorrect,
we try to ﬁnd the closest sentence producible from the training
data. To be able to generate variations that are not covered
by the training data, we do not match the concrete sentence
(like in Sec. II-B), but only its ST against the ST of the
training sentences using the normalized Levenshtein distance
mentioned in section II-B. This produces a ranked list of STs
closest to the one that was generated by Google’s ASR.

Inspired by the work of Hinaut and Dominey [5] we
interpret the SWs of the training data as terminal words (non-
replaceable words) in a grammar, and the wildcard slots as the
non-terminals to be replaced. We call these sets of possible
SWs Terminal Bags (TB; e.g. t0, t1 below). This way, the
model is able to generate variations of sentences for the same
ST that may not be contained literally in the training data. The
following example clariﬁes the principle. The training data
consists of the sentences:

• ‘put the pyramid on the cube’
• ‘move the prism on the block’
• ‘move the prism to the left’

The ST for the ﬁrst two sentences is ‘X the X on the
X’ and ‘X the X to the X’ for the third one. The
training is performed by generating a grammar:
• (cid:104)s0(cid:105) = (cid:104)t0(cid:105) the (cid:104)t1(cid:105) on the (cid:104)t2(cid:105)

Google's ASR (10 best)Sentence TemplateDetectionGrapheme-to-PhonemeConversionPhonemic LevenshteinScoringSemantic WordCombination GenerationN-gram RetrievalGrapheme-to-PhonemeConversionPhonemic LevenshteinScoringWord TransitionViolation DetectionN-best RerankingN-best RerankingSentenceTemplateGrammarSentenceTemplateN-grams• (cid:104)s1(cid:105) = (cid:104)t3(cid:105) the (cid:104)t4(cid:105) to the (cid:104)t5(cid:105)
• (cid:104)t0(cid:105) = put | move
• (cid:104)t1(cid:105) = pyramid | prism
• (cid:104)t2(cid:105) = cube | block
• (cid:104)t3(cid:105) = move
• (cid:104)t4(cid:105) = prism
• (cid:104)t5(cid:105) = left

This grammar is able to generate, e.g., the sentence ‘put
the prism on the cube’, which is not present in the
training data. Processing a speech utterance is performed by
generating a hypothesis using e.g. Google’s ASR. In this
example, the reference text is ‘put the prism on the
cube’, Google may produce the hypothesis ‘pull the
pistol on the cube’. This hypothesis is transformed
to its ST, which is ‘X the X on the X’. We employ the
normalized Levenshtein distance (see eq. 3) to calculate the
best matching ST from the training data, which is in this case
also ‘X the X on the X’. Then, the SWs are matched
against each other to ﬁnd the best matching word fro the
TB. This means that we calculate the normalized Levenshtein
distance of ‘pull’ against ‘put’ and ‘pull’ against
‘move’ on phoneme level for the ﬁrst SW gap. We do this
for all SW gaps and get the most probable sequence ‘put
the prism on the cube’.

In some cases, the hypothesis coming from Google cannot
be converted to a correct ST, e.g. ‘put them prism on
the cube’. In this case, we calculate the best match-
ing ST and generate possible combinations for the incor-
the sequence, e.g. ‘put the pyramid’,
rect part of
‘put the prism’, ‘move the prism’, ‘move the
pyramid’. This guarantees a correct ST which can be
interpreted by a θ-role assignment model (e.g. [5]).

As the normalized Levenshtein distance can be used as a
conﬁdence for each word, words that the system is uncertain
about can be identiﬁed. This information can be used to repair
a hypothesis in a second step in case some of the words possess
a low conﬁdence (threshold 0.5).

For long and nested sentences the correct hypothesis cannot
be constructed due to the lack of training data, meaning not
enough samples for a speciﬁc ST and a lack of possible
SWs. This means, the TBs do not contain many words and
possibly not the word that was uttered. We use a threshold for
each word and repair all words that were recognized with a
conﬁdence lower than this threshold (0.5). For this, we train
the system by collecting all n-grams inside the training data up
to 10-grams. For example, if we have the input sentence ‘put
the yellow prism on the cube’, we possibly could
only generate ‘put the blue prism on the cube’,
as the word ‘yellow’ was for example never used in this
SW gap. The function words ‘the’ and ‘on’ can be used as
“anchors” to retrieve n-grams starting with ‘the’ and ending
with ‘on’. Then, another scoring process is started using the
normalized Levenshtein distance to calculate the best matching
n-gram from the training data. This way, we can generalize
from other sentences and STs in the training data.

To reduce the number of incorrect sentences, we check

Fig. 2. Example from the corpus. An example for a board scene before (left
board) and after (right board) the command ‘Move the red brick on
top of the blue brick’.

each generated hypothesis for non-existing word transitions
(bigrams) in the training data and, if occurring, we can drop
that hypothesis. Also, we perform the postprocessing for the
10 best hypotheses coming from Google, and afterwards sort
our generated hypotheses by conﬁdence.

IV. EXPERIMENTS
We evaluate our system we chose the Train Robots dataset
[18]. It consists of instructions directed at a robot arm
that can move objects around like boxes and pyramids in
a discrete world and is inspired by SHRDLU [19]. The
text corpus contains 2500 training sentences and 909 test
sentences, which are linguistically rich,
including ellipses,
anaphoric references, multi-word spatial expressions and
lexical disambiguation. The dataset has a “before” and “after”
scene for each of the commands, Fig. 2 shows an example.
e.g. ‘Move the red brick on
The
top of the blue brick’ or ‘Pick up the blue
block from the top of the green block and
put it down on the blue block which lies
next to another green block’.

sentences

are

The original corpus was created via crowd-sourcing and
contains a lot of grammatical and syntactic errors, which
would make a model learn incorrect syntax. A prerequisite for
our approach to work is that the system learns a correct syntax
of the language. It could be possible to learn a correct syntax
from data that contains partially incorrect syntax, as incorrect
syntax could be identiﬁed via outlier or anomaly detection,
which could possibly be integrated into an extension of this
approach.

For restricted domains like the Train Robots scenario we
believe that there is not enough data available to be able to
identify incorrect syntax in an unsupervised way. That is why
we corrected grammatical errors in the training data to make
sure the system learns only correct syntax. We recorded audio
data for the 909 test sentences with 9 different non-native
speakers, each one uttering 101 sentences from the test set.

We compare the performance of our system with Google’s
ASR, DOCKS Bigram and DOCKS Sentencelist and measure
Word Error Rate (WER) and Sentence Error Rate (SER),
which is the rate of incorrect hypotheses (ones that con-
tain at least one error). Also, we calculate the number of
grammatically and syntactically correct sentences. The 2500

TABLE I
PERFORMANCE REGARDING CONVENTIONAL ASR METRICS ON THE Train
Robots DATA SET.

Approach

Google

DOCKS Bigram

DOCKS Sentencelist

ST Grammar

ST N-gram

Word Error Rate

Sentence Error Rate

17.3%

24.2%

32.2%

22.6%

11.0%

77.7%

76.9%

81.7%

71.2%

53.355%

training sentences are used to train DOCKS Bigram, DOCKS
Sentencelist and our models. For DOCKS Sentencelist the list
of expectable sentences is not given in this scenario, but the
training sentences are used to build the sentence list.

Table I shows the results of our experiments. Google’s
Search by Voice ASR achieves a WER of 17.343%, which
is better than DOCKS Bigram (24.338%) and shows that the
corpus is too challenging to improve the results with Bigram
postprocessing. When using Sentencelist postprocessing, the
WER rises to 32.174%, which shows that the corpus contains
so many variations that the intersection between training data
and test data is not
large enough to make this approach
usable for this data set. By performing the ﬁrst postprocessing
step (ST Grammar), the WER also increases while the SER
decreases, which means that more sentences are recognized
correctly but fewer words in total. The second postprocessing
step (ST N-gram) reduces the WER to 11.038% and the SER
to 53.355%, which is a large boost in performance regarding
conventional ASR benchmark metrics.

For the second evaluation, we measure the number of
syntactically and grammatically correct sentences produced
by the baseline approach (Google) and our best approach.
The ST N-gram approach produced 90.319% syntactically
and grammatically correct sentences and no incorrect word
transitions. When incorrect syntax was produced, there were
incorrect verbs inside a correct phrase structure in most of
the cases, e.g. ‘hold the yellow cube on top of
the single blue cube’, which would be correct
if
‘hold’ was replaced by ‘move’. A known ST was created
in 100% of the cases. The hypotheses produced by Google’s
Search by Voice were syntactically and grammatically correct
for only 42.684% of the test sentences (according to the
subjective measure of an expert), while we even ignored errors
concerning wrong capitalization, which would lead to almost
0% syntactically and grammatically correct hypotheses. Also,
more than 50% of the sentences contained words that did
not belong to the vocabulary of the domain like ‘Musa
red pyramid for sale spec’, because words were
confused with similarly pronounced out-of-domain words. The
original sentence here was ‘move the red pyramid to
the left edge’.

V. DISCUSSION AND CONCLUSION

In the current robotic application, our domain-dependent
postprocessor clearly outperforms Google’s ASR regarding

conventional metrics like WER and SER. The model is able to
generalize for unseen sentences and is producing syntactically
correct hypotheses in 9 out of 10 cases. The increase in
WER and decrease in SER for the ST Grammar is caused
by the lack of training data in the domain, as long sentences
(and their ST) only have few samples and though not all
combinations of SWs can be produced. For shorter sentences,
where more examples are available, the system is performing
much better. The ST N-Gram approach does not have this kind
of problems, as SW gaps are ﬁlled with n-grams coming from
other training sentences. The main contribution of this model
is the possibility to generate syntactically correct sentences
without having to hand-craft a grammar, instead, the grammar
is learned by our approach in an unsupervised way. Also a list
of all possible sentences that can be uttered is not necessary
any more.

One disadvantage is that sometimes syntactically incor-
rect sentences are produced by the ST N-gram model like
‘hold the yellow cube on top of the single
blue cube’, which is caused by the local context of the
n-gram. If the training data of the ST Grammar approach was
augmented, these cases would not occur for that method, as
such cases would not be present in the training data. The
system could be used for interactive data augmentation by
generating new training sentences using the ST N-gram model
and adding those sentences to the training data of the ST
Grammar if validated by a human teacher. We interpret this
model as a function that is necessary to produce syntactically
correct sentences and is not present in other systems.

The θ-RARes model of Hinaut and Dominey [5] is also
based on STs and can be used to generate predicates containing
the thematic roles of a hypothesis. As most of the sentences are
syntactically correct and all of them possess a correct ST, the
model is also able to understand incorrect hypotheses which
are expected to be rejected completely by other systems. In
previous work [6], we investigated the problems of integrat-
ing inconsistent multimodal input (coming from speech and
vision), where the θ-RARes model was also employed. The
results showed that 50% of the spoken input coming from the
user was inconsistent with the input coming from the vision
module. For example, the user was describing a position of an
object and confusing left and right. In that work, we showed
that feedback generated from incorrect input is useful for the
user to interact with an agent.

In future work, we plan to connect the θ-RARes model to
our system to be able to interpret both correct and incorrect
sentences and provide feedback if necessary instead of only
rejecting incorrect input. Also, we plan to integrate a visual
component which is able to interpret the generated predicates
if possible or otherwise provide suggestions to change speciﬁc
words inside a hypothesis to make it plausible and consistent
with the visual context. We believe that this idea cannot be
implemented using the raw results coming from Google’s
ASR or other comparable ASR systems, as they often contain
out-of-domain words or are syntactically and grammatically
incorrect.

MacGregor et al. [20] and Visser et al. [21] state that lexical-
semantic categorization is necessary to interpret words and
cannot be performed for invented words. Given the Google hy-
pothesis ‘Musa red pyramid for sale spec’, our
system “does not know” words like ‘Musa’, ‘sale’ or
‘spec’, as they are not contained in the training data. These
words are treated as out-of-domain words and cannot be
interpreted by the system. These cases do not occur in the hy-
potheses generated by our system, as no out-of-domain words
can be produced, which guarantees a lexical-semantic cat-
egorization. Additionally, errors like ‘hold the yellow
cube on top of the single blue cube’ can be
prevented using the θ-RARes model, as such a predicate was
never contained in the training data and could be rejected.

Diaz and McCarthy [22] found out that function words
cause activity in different brain areas compared to SWs, which
supports our hypothesis that a speech recognition system
should treat function words differently than SWs, as our
the P600 Event-Related-Potential
system does. In general,
(ERP) that could be recorded from the brain is thought to be
an indication for a syntactic reanalysis and repair mechanism
[23]. Therefore we think syntactic postprocessing mechanism
is necessary for an ASR system.

Also, new words can easily be added to our language model
by declaring a new word to be equivalent to an existing word
given the context. E.g., the unknown word ‘cuboid’ can be
declared to be used in the same context as the known word
‘box’. The word ‘cuboid’ is then added to all Terminal
Bags of the ST Grammar where the word ‘box’ is present.
Also, all n-grams of the ST N-gram model containing the word
‘box’ are taken, the word ‘box’ is replaced by ‘cuboid’
and the new n-gram is added to the training data. This process
is performed automatically after only providing the grapheme
representation of the new word ‘cuboid’.

The proposed method could also help with improving
performance in other HRI disciplines, like child ASR [24].
The system was running in realtime on a computer (Intel(R)
Core(TM) i5-4590 CPU @ 3.30GH, 16GB RAM) which
makes it also interesting for other HRI tasks.

ACKNOWLEDGMENT

We would like to thank Tobias Hinz for correcting the
grammatical errors in the data, and all speakers for their
participation. We gratefully acknowledge partial support for
the project “ LingoRob - Learning Language in Developmental
Robots” by Campus France PHC Procope Project 37857TF
and by DAAD Project 57317821 in the F¨orderprogramm
“Projektbezogener Personenaustausch Frankreich”.

REFERENCES

[1] H. Sak, A. Senior, K. Rao, and F. Beaufays, “Fast and accurate recurrent
neural network acoustic models for speech recognition,” arXiv preprint
arXiv:1507.06947, 2015.

[2] H. Sak, A. W. Senior, and F. Beaufays, “Long short-term memory
recurrent neural network architectures for large scale acoustic modeling.”
in Interspeech, 2014, pp. 338–342.

[3] T. N. Sainath, O. Vinyals, A. Senior, and H. Sak, “Convolutional,
long short-term memory, fully connected deep neural networks,” in
2015 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP).

IEEE, 2015, pp. 4580–4584.

[4] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro,
J. Chen, M. Chrzanowski, A. Coates, G. Diamos et al., “Deep speech 2:
End-to-end speech recognition in english and mandarin,” arXiv preprint
arXiv:1512.02595, 2015.

[5] X. Hinaut and P. F. Dominey, “Real-time parallel processing of grammat-
ical structure in the fronto-striatal system: a recurrent network simulation
study using reservoir computing,” PloS one, vol. 8, no. 2, p. e52946,
2013.

[6] J. Twiefel, X. Hinaut, M. Borghetti, E. Strahl, and S. Wermter, “Using
natural language feedback in a neuro-inspired integrated multimodal
robotic architecture,” in 25th IEEE International Symposium on Robot
and Human Interactive Communication (RO-MAN).
IEEE, 2016, pp.
52–57.

[7] J. Twiefel, T. Baumann, S. Heinrich, and S. Wermter, “Improv-
ing domain-independent cloud-based speech recognition with domain-
dependent phonetic post-processing,” in Twenty-Eighth AAAI. Qu´ebec
City, Canada, 2014, pp. 1529–1535.

[8] X. Hinaut, M. Petit, G. Pointeau, and P. F. Dominey, “Exploring the
acquisition and production of grammatical constructions through human-
robot interaction with echo state networks,” Frontiers in Neurorobotics,
vol. 8, 2014.

[9] M. Tomasello, Constructing a language: A usage based approach to
language acquisition. Cambridge, MA: Harvard University Press, 2003.
[10] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

computation, vol. 9, no. 8, pp. 1735–1780, 1997.

[11] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber, “Connectionist
temporal classiﬁcation: labelling unsegmented sequence data with re-
current neural networks,” in 23rd international conference on Machine
learning. ACM, 2006, pp. 369–376.

[12] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[13] P. Mermelstein, “Distance measures for speech recognition – psycholog-
ical and instrumental,” in Joint Workshop on Pattern Recognition and
Artiﬁcial Intelligence, 1976, pp. 374–388.

[14] P. Lamere, P. Kwok, W. Walker, E. Gouvea, R. Singh, and P. Wolf,
“Design of the cmu sphinx-4 decoder,” in 8th European Conference on
Speech Communication and Technology (EUROSPEECH).
ISCA, 9
2003, pp. 1181–1184.

[15] M. Bisani and H. Ney, “Joint-sequence models for grapheme-to-
phoneme conversion,” Speech Communication, vol. 50, no. 5, pp. 434–
451, 2008.

[16] V. I. Levenshtein, “Binary codes capable of correcting deletions, in-
sertions, and reversals,” Soviet Physics – Doklady, vol. 10, no. 8, pp.
707–710, 2 1966.

[17] A. Goldberg, Constructions: A construction grammar approach to

argument structure. University of Chicago Press, 1995.

[18] K. Dukes, “Train robots: A dataset for natural language human-robot
spatial interaction through verbal commands,” in ICSR. Embodied Com-
munication of Goals and Intentions Workshop, Bristol, UK, 2013.
[19] T. Winograd, “Understanding natural language,” Cognitive psychology,

vol. 3, no. 1, pp. 1–191, 1972.

[20] L. J. MacGregor, F. Pulverm¨uller, M. Van Casteren, and Y. Shtyrov,
“Ultra-rapid access to words in the brain,” Nature Communications,
vol. 3, p. 711, 2012.

[21] M. Visser, E. Jefferies, and M. L. Ralph, “Semantic processing in the
anterior temporal lobes: a meta-analysis of the functional neuroimaging
literature,” Journal of cognitive neuroscience, vol. 22, no. 6, pp. 1083–
1094, 2010.

[22] M. T. Diaz and G. McCarthy, “A comparison of brain activity evoked
by single content and function words: an fmri investigation of implicit
word processing,” Brain research, vol. 1282, pp. 38–49, 2009.

[23] A. D. Friederici, “The brain basis of language processing: from structure
to function,” Physiological reviews, vol. 91, no. 4, pp. 1357–1392, 2011.
[24] J. Kennedy, S. Lemaignan, C. Montassier, P. Lavalade, B. Irfan, F. Pa-
padopoulos, E. Senft, and T. Belpaeme, “Child speech recognition in
human-robot interaction: Evaluations and recommendations,” in 12th
Annual ACM International Conference on Human-Robot Interaction,
2017.

