Analyzing large-scale spike trains data with
spatio-temporal constraints
Hassan Nasser, Olivier Marre, Selim Kraria, Thierry Viéville, Bruno Cessac

To cite this version:

Hassan Nasser, Olivier Marre, Selim Kraria, Thierry Viéville, Bruno Cessac. Analyzing large-scale
spike trains data with spatio-temporal constraints. NeuroComp/KEOpS’12 workshop beyond the
retina: from computational models to outcomes in bioengineering. Focus on architecture and dynamics
sustaining information flows in the visuomotor system., Oct 2012, Bordeaux, France. ￿hal-00756467￿

HAL Id: hal-00756467

https://inria.hal.science/hal-00756467

Submitted on 23 Nov 2012

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Analyzing large-scale spike trains data with
spatio-temporal constraints

Hassan Nasser∗, O. Marre †, Selim Kraria‡, Thierry Vieville§, Bruno Cessac¶,

October 5, 2012

1 Context

Recent experimental advances have made it possible to record several hundred
neurons simultaneously in the retina as well as in the cortex. Analyzing such a
huge amount of data requires to elaborate statistical, mathematical and numer-
ical methods, to describe both the spatio-temporal structure of the population
activity and its relevance to sensory coding. Among these methods, the maxi-
mum entropy principle has been used to describe the statistics of spike trains.
Recall that the maximum entropy principle consists of ﬁxing a set of constraints,
determined with the empirical average of quantities (”observables”) measured
on the raster:
for example average ﬁring rate of neurons, or pairwise corre-
lations. Maximising the statistical entropy given those constraints provides a
probability distribution, called a Gibbs distribution, that provides a statistical
model to ﬁt the data and extrapolate phenomenological laws. Most approaches
were restricted to instantaneous observables i.e. quantities corresponding to
spikes occurring at the same time (singlets, pairs, triplets and so on. See [6],[5]
and [4]). The corresponding Gibbs distributions are only successful in predict-
ing the probability of patterns lying within one time bin [2] but fail to predict
the temporal statistics of the neural activity.

2 Extension of the maximum entropy to spatio-

temporal statistics

We ﬁrst deﬁne the Gibbs potential as:

Hβ =

K

X
k=1

βkOk,

(1)

where the βks are real numbers and free parameters. Oks are the observables
that deﬁnes the constraints. Fitting the data to this Gibbs potential model aims

∗NeuroMathComp team, INRIA, 2004 Route des Lucioles, 06902 Sophia-Antipolis, France.
†Institut de la vision, 17 rue Moreau, 75012, Paris, France.
‡Dream team, INRIA, 2004 Route des Lucioles, 06902 Sophia-Antipolis, France.
§Cortex team, INRIA, 2004 Route des Lucioles, 06902 Sophia-Antipolis, France.
¶NeuroMathComp, INRIA, 2004 Route des Lucioles, 06902 Sophia-Antipolis, France.

1

to reproduce the probability distribution of the data, i.e, ﬁnding a theoretical
distribution whose observables probabilities are close to the empirical probabil-
ities measured on the raster, using the maximum entropy principle. We have
extended this principle considering spatio temporal constraints (i.e. neurons in-
teractions with time lags) and allowing the characterization of spatio-temporal
patterns statistics. This approach, based on a transfer matrix technique, avoids
the computation of the normalization factor (partition function) and directly
provides the moment generating function (free energy density). We ﬁrst de-
veloped a numerical method to learn the parameters of a maximum entropy
model with spatio-temporal constraints on real spike train data [7]. However,
this exact method could only be applied to small subsets of neurons, since it
required construction of huge matrices whose size increases exponentially with
the number of neurons.

To circumvent this combinatorial explosion, we then adopted a Monte-Carlo
approach to compute the observable averages for a given set of model parameters
(see [3] for more details). We ﬁrst tested the eﬃciency of this new algorithm
in the case where a raster plot is generated from a known Gibbs distribution
[Fig. 1]. We then make the algorithm ﬁnd back this Gibbs distribution from that
raster. This allowed us to quantify both convergence rate and model accuracy as
a function of the number of neurons. The main advantage of this new technique
is its computational speed. Fig. 2 shows the diﬀerence between the computation
time taken using transfer matrix technique and the Monte-Carlo technique:
The computation time with Monte-Carlo increases linearly with the number of
neurons while it increases exponentially with transfer matrix based technique.

3 Learning the parameters βk

We have also developed an eﬃcient algorithm to compute the parameters βk
deﬁning the Gibbs distribution, even for large data sets. We adopted the method
used in [1] and we extended it to the spatio-temporal case. Their method is
based on the minimization a criterion, the negative likelihood, given by (Eq. 2)

Lπ(T )

ω

(β) = −π(T )

ω [log µβ],

(2)

ω

where βks are the parameters to be inferred, µβ is the probability distribu-
tion estimated using Monte-Carlo technique, π(T )
is the empirical distribution
ω
(β) represent the likelihood between the data and the model,
and ﬁnally. Lπ(T )
to be minimized. We have tested the method considering large spatio temporal
data sets where the number N of neurons is large (N ∼ 100) as well as the model
range R, where the exact transfer matrix computation becomes intractable. In
Fig. 3 we present the maximal distance (Eq. 3) between the exact value of
the coeﬃcient and the estimated value, averaged on a set of 10 random Gibbs
potentials.

The error is given by the following formula:

Error =

1
L X
k=1...L

βk − ˆβk (cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(3)

where βk is the exact value of coeﬃcient k, ˆβk is the estimated value and L

is the number of terms in the potential.

2

N=3
N=4
N=5
N=6
N=7
y=x

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

 0

-0.1

 0

 0.1

 0.2

 0.3

 0.4

 0.5

 0.6

 0.7

 0.8

 0.9

 1

Figure 1: Comparison between the real (x − axis) and estimated (y − axis) val-
ues of observable averages (Points represent the mean value of the observable
averages and segments represents the standard deviation, over 10 trials).Given
a random Gibbs potential, we computed the probability distribution of observ-
ables with the transfer-matrix based technique and with the Monte-Carlo based
technique. Since the transfer matrix cannot handle for large sets, we were lim-
ited to a small number of neurons in order compare the empirical and theoretical
averages. The values obtained with Monte-Carlo are closed to those obtained
with the transfer matrix technique since they are all aligned to the y = x axis.

4 Conclusion

We have extended the maximum entropy technique to handle spatio-temporal
temporal constraint and developped a Monte-Carlo based method which allows
to infer statistical properties for large data set, in less time than the transfer
matrix based technique. This work will help to analyse spike train sorted from
large MEA (Multi-Electrode array) recordings and discover the structure of the
hidden statistics behind the data. The choice of the Gibbs potential form (what
are the terms that we put in the potential) remains a challenge and will be
subject for future studies.

3

MC Av.
12.4\*N^1.32
Th. Av.
0.00015\*exp(2.24\*N)

)
s
(

i

e
m
T
U
P
C

1e+06

100000

10000

1000

100

10

1

0.1

0.01

2

4

N

8

Figure 2: The CPU time necessary to obtain the observable averages presented
in Fig. 1, for the Monte-Carlo Average (MC Av.) and for the theoretical (Th.
Av.), as a function of N (Here, the theoretical average is exact since the potential
is generated synthetically with known probability distribution). The full lines
correspond to ﬁt. The computation time with Monte-Carlo increases linearly
with the number of neurons while it increases exponentially with transfer matrix
based technique.

N=5, R=3
N=4,R=4
N=20,R=3
N=30,R=4
N=60,R=5
convergence bound

r
o
r
r
E

 0.14

 0.12

 0.1

 0.08

 0.06

 0.04

 0.02

 0

 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40

Number L of Monomials

Figure 3: Max distance between the exact values of coeﬃcients and the esti-
mated values, averaged on a set of 10 random potentials for several values of
N, R. Since for each (N, R) value the potential is not the same, then the max-
imum number of monomials (L) will not be the same and by consequence we
observe that the curves do not have all the same number of points.

References

[1] M. Dud´ık, S. Phillips, and R. Schapire. Performance guarantees for reg-
ularized maximum entropy density estimation. In Proceedings of the 17th

4

 
 
Annual Conference on Computational Learning Theory, 2004.

[2] O. Marre, S. El Boustani, Y. Fr´egnac, and A. Destexhe. Prediction of spa-
tiotemporal patterns of neural activity from pairwise correlations. Phys. rev.
Let., 102:138101, 2009.

[3] H. Nasser, O. Marre, and B. Cessac. Spatio-temporal spike trains analysis
for large scale networks using maximum entropy principle and monte-carlo
method. Journal Of Statistical Mechanics, to appear, 2012.

[4] E. Schneidman, M.J. Berry, R. Segev, and W. Bialek. Weak pairwise cor-
relations imply strongly correlated network states in a neural population.
Nature, 440(7087):1007–1012, 2006.

[5] J. Shlens, G.D. Field, J.L. Gauthier, M.I. Grivich, D. Petrusca, A. Sher,
A.M. Litke, and E.J. Chichilnisky. The structure of multi-neuron ﬁring
patterns in primate retina. Journal of Neuroscience, 26(32):8254, 2006.

[6] G. Tkacik, Elad Schneidman, Michael J. Berry II, and William Bialek. Spin
glass models for a network of real neurons. arXiv: 0912.5409v1, 2009.

[7] J-C Vasquez, T. Vi´eville, and B. Cessac. Entropy-based parametric estima-

tion of spike train statistics. INRIA Research Report, 2010.

Acknowledgements: This work was supported by the INRIA, ERC-NERVI

number 227747, KEOPS ANR-CONICYT and BrainScales projects.

5

