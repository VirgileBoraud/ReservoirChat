Reservoir Computing : traitement eﬀicace de séries
temporelles avec ReservoirPy
Nathan Trouvain, Xavier Hinaut

To cite this version:

Nathan Trouvain, Xavier Hinaut. Reservoir Computing : traitement eﬀicace de séries temporelles avec
ReservoirPy. Dataquitaine 2022 - 5ème journée IA, Recherche Opérationnelle & Data Science, Feb
2022, Bordeaux, France. ￿hal-03945994￿

HAL Id: hal-03945994

https://inria.hal.science/hal-03945994

Submitted on 18 Jan 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Reservoir Computing : traitement efficace de séries 
temporelles avec ReservoirPy 

Nathan Trouvain, nathan.trouvain@inria.fr, Inria, LaBRI, IMN, Bordeaux (Orateur) 

Xavier Hinaut, xavier.hinaut@inria.fr, Inria, LaBRI, IMN, Bordeaux (Orateur) 

Thématique : 6 – Intelligence Artificielle 

Résumé 

De la météo au langage, extraire l’information de flux de données est un enjeu primordial en 
Intelligence Artificielle. Le Reservoir Computing (RC) est particulièrement adapté pour bien 
prendre en compte ces dynamiques temporelles. C’est un paradigme d’apprentissage 
automatique sur des données séquentielles où un réseau de neurones artificiel n'est que 
partiellement entrainé. Un des intérêts majeurs de ces réseaux de neurones récurrents est 
leur coût computationnel réduit et la possibilité d'apprendre aussi bien "en-ligne" que "hors-
ligne". Leurs applications sont très variées, qu'il s'agisse de prédiction/génération de séries 
chaotiques ou de discrimination de séquences audio, comme la reconnaissance de chants 
d'oiseaux. Nous verrons les aspects théoriques du RC : comment ce "réservoir de calculs" 
fonctionne grâce à des projections aléatoires en grandes dimensions, et s'apparente ainsi à 
des Machines à Vecteurs Supports (SVM) temporelles. 

Nous présenterons également ReservoirPy : une bibliothèque Python à la fois simple et 
efficace basée sur la pile logicielle scientifique de Python (Numpy, Scipy, Matplotlib). 
ReservoirPy met l'accent sur les Echo State Networks (ESN), l'instance la plus connue de 
RC. Elle permet la conception d'architectures développées dans la littérature, allant des plus 
classiques aux plus complexes. ReservoirPy embarque plusieurs règles d'apprentissage 
(online et offline), une implémentation distribuée de l'entraînement des ESN, la possibilité de 
créer des réseaux hiérarchiques comportant des boucles de feedback complexes, et des 
outils d'aide à l'optimisation des hyperparamètres. La documentation, des tutoriels, des 
exemples et des jeux de données sont disponibles sur la page GitHub de ReservoirPy : 
github.com/reservoirpy 

Mots clés : Séries temporelles, Réseaux de neurones récurrents, Prédiction, Génération, 
Classification, Reconnaissance de la parole 

1. Introduction 

Appréhender la dimension temporelle d’un jeu de données est un problème particulièrement 
complexe en apprentissage machine. Elle est cependant primordiale dans de nombreux 
domaines, comme le traitement du langage naturel ou la prédiction de séries temporelles. 
En particulier, appliquer à des séries temporelles les techniques fructueuses du deep 
learning impose une complexité algorithmique et mathématique grandissante, des LSTM [4] 
aux Transformers [8]. Les réseaux de neurones récurrents mettent ainsi à profit une version 
modifiée de l’algorithme de descente du gradient d’erreur pour apprendre des relations 

 
 
 
temporelles complexes. Ces techniques sont souvent très coûteuses en calculs et en 
mémoire et sont en général moins robustes face au bruit ou à des comportements 
chaotiques, observés dans de nombreux problèmes de la recherche comme de l’industrie. 

Nous nous proposons de présenter le Reservoir Computing [5], une technique de machine 
learning permettant de tirer parti de réseaux de neurones récurrents en utilisant des 
mécanismes d’apprentissage plus rapides et plus simples que ceux utilisés en deep 
learning. Le Reservoir Computing offre de nombreuses perspectives en traitement robuste 
des séries temporelles, utile pour la robotique, le traitement de données audio, ou la 
prédictions de systèmes chaotiques par exemple. 

2. Méthodologie 

 Le Reservoir Computing est une méthode tirant parti d’un réseau de neurones récurrents 
aléatoirement construit pour produire une grande variété de représentations des données en 
entrée, en y intégrant une dimension temporelle. L’apprentissage n’a pas lieu dans la partie 
récurrente du réseau, appelée reservoir, mais uniquement à sa sortie, où une unique couche 
de neurones, appelée readout, est chargée de produire le signal désiré. Ainsi, il est possible 
d’effectuer l’apprentissage à l’aide d’une simple régression linéaire [5]. 

Nous présenterons des applications du Reservoir Computing à des tâches considérées 
comme complexes dans la littérature : prédiction de séries temporelles chaotiques, 
classification de sons… Nous introduirons également les concepts clés de cette technique, 
et quelques exemples issus de la littérature, au travers notamment des Echo State Networks 
(ESN), une technique courante de Reservoir Computing. Nous présenterons également le 
bibliothèque Python ReservoirPy [7], développée au sein de l’Inria pour diffuser et appliquer 

ces techniques. 

Figure 1 - Exemple de résultats obtenus à l’aide de techniques de Reservoir Computing (ESN) 
appliqué à une tâche d’annotation de chant de canaris [6]. La tâche consiste à identifier chaque 
type de motif sonore dans le chant (tâche similaire à la reconnaissance de la parole chez 
l’humain). La figure A montre le type de données traitées (ici, les composantes spectrales du 
son). La figure B montre l’adéquation entre la séquence de motifs identifiés par le reservoir 
(ESN) et la séquence attendue (Target). On y voit également la comparaison avec les 
performances d’un LSTM. 

3. Originalité / perspective 

Quoique plus discrète et à contre courant des politiques d’augmentation du nombre de 
paramètres des modèles d’apprentissage profond rencontrés de nos jours, le Reservoir 
Computing offre une alternative pertinente et économique (voire écologique) aux réseaux de 
neurones récurrents plus classiques. Le Reservoir Computing est également appliqué avec 
succès dans sa version hardware, où des composants neuromorphiques ou optiques [1] 
remplacent les transistors électroniques pour produire un reservoir physique. 

Dans un autre registre, 
le Reservoir Computing offre également de nombreuses 
perspectives en neurosciences computationnelles, où certains indices semblent indiquer que 
le cerveau possède des structures neuronales semblables lui permettant de traiter la 
dimension temporelle de son environnement [2, 3]. C’est dans ce cadre que les membres de 
l’équipe Mnemosyne l’utilisent, où cette technique permet de modéliser divers processus 
cognitifs et sensorimoteurs. 

Références 

[1] Brunner, D., Soriano, M. C., & Van der Sande, G. (Eds.). (2019). Photonic Reservoir 
Computing: Optical Recurrent Neural Networks. De Gruyter. 

[2] Dominey, P. F. (2021). Cortico-Striatal Origins of Reservoir Computing, Mixed Selectivity, 
and Higher Cognitive Function. In K. Nakajima & I. Fischer (Eds.), Reservoir Computing: 
Theory, Physical Implementations, and Applications (pp. 29–58). Springer. 

[3] Enel, P., Procyk, E., Quilodran, R., & Dominey, P. F. (2016). Reservoir Computing 
Properties of Neural Dynamics in Prefrontal Cortex. PLOS Computational Biology, 12(6), 
e1004967. https://doi.org/10.1371/journal.pcbi.1004967 

[4] Gers, F. A., Schmidhuber, J., & Cummins, F. (1999). Learning to forget: Continual 
prediction with LSTM. 1999 Ninth International Conference on Artificial Neural Networks 
ICANN 99. (Conf. Publ. No. 470), 2, 850–855 vol.2. 

[5] Jaeger, H. (2001). The" echo state" approach to analysing and training recurrent neural 
networks-with an erratum note’. Bonn, Germany: German National Research Center for 
Information Technology GMD Technical Report, 148 

[6] Trouvain, N., & Hinaut, X. (2021). Canary Song Decoder: Transduction and Implicit 
Segmentation with ESNs and LTSMs. In I. Farkaš, P. Masulli, S. Otte, & S. Wermter (Eds.), 
Artificial Neural Networks and Machine Learning – ICANN 2021 (pp. 71–82). Springer 
International Publishing 

[7] Trouvain, N., Pedrelli, L., Dinh, T. T., & Hinaut, X. (2020). ReservoirPy: An Efficient and 
User-Friendly Library to Design Echo State Networks. In I. Farkaš, P. Masulli, & S. Wermter 
(Eds.), Artificial Neural Networks and Machine Learning – ICANN 2020 (pp. 494–505). 
Springer International Publishing. 

[8] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., 
& Polosukhin, I. (2017). Attention is All you Need. Advances in Neural Information 
Processing Systems, 30. 

 
 
