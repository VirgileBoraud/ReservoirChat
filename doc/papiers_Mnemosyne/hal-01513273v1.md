Diversity in Reproducibility
Olivia Guest, Nicolas P. Rougier

To cite this version:

Olivia Guest, Nicolas P. Rougier. Diversity in Reproducibility. IEEE CIS Newsletter on Cognitive
and Developmental Systems, 2016, 13 (2). ￿hal-01513273￿

HAL Id: hal-01513273

https://inria.hal.science/hal-01513273

Submitted on 24 Apr 2017

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Diversity in Reproducibility

Olivia Guest
Department of Experimental Psychology
University College London, United Kingdom

Nicolas P. Rougier
INRIA Bordeaux Sud-Ouest, Talence, France
Institut des Maladies Neurodégénératives, Université
Bordeaux, Centre National de la Recherche Scientiﬁque,
UMR 5293, Bordeaux, France
LaBRI, Université de Bordeaux, Institut Polytechnique de
Bordeaux, Centre National de la Recherche Scientiﬁque,
UMR 5800, Talence, France

In our previous contribution, we proposed computational
modelling-related deﬁnitions for replicable,
i.e., experi-
ments within a model can be recreated using its original
codebase, and reproducible, i.e., a model can be recreated
based on its speciﬁcation. We stressed the importance of
speciﬁcations and of access to codebases. Furthermore, we
highlighted an issue in scholarly communication — many
journals do not require nor facilitate the sharing of code.
In contrast, many third-party services have ﬁlled the gaps
left by traditional publishers (e.g., Binder, 2016; GitHub,
2007; Open Science Framework, 2011; ReScience, 2015).
Notwithstanding, journals and peers rarely request or expect
use of such services. We ended by asking: are we ready
to associate codebases with articles and are we prepared to
ensure computational theories are well-speciﬁed and coher-
ently implemented?

Scope of Evaluation

Dialogue contributions include proposals for: intermedi-
ate levels between replicability and reproducibility (Crook,
Hinsen); going beyond reproducibility (Kidd); encompass-
ing computational science at large (Gureckis & Rich, Varo-
quaux); and addressing communities as a function of exper-
tise (French & Addyman). On the one hand, some replies
discuss evaluation more broadly, empirical data collection,
and software engineering. On the other hand, some delve
into the details of evaluating modelling accounts. We will
discuss the former ﬁrst.

In Varoquaux’s contribution,

reproducibility includes
replicability and code rot (e.g., in fMRI: Eklund, Nichols, &
Knutsson, 2016). However, the titular computational repro-
ducibility is orthogonal to maintaining a re-usable codebase.
Software and hardware inevitably go out of fashion mean-
ing codebases expire. Nevertheless, the overarching theory
encapsulated by modelling software could withstand the ef-
fects of entropy if speciﬁed coherently, e.g., early artiﬁcial
neural network codebases are not required to understand nor

reproduce these models. Indubitably, there is a balance to be
struck between reimplementation and re-use.

In contrast, Gureckis and Rich extend their scope to the
empirical replication crisis in psychology. They mention that
implicit knowledge often goes unpublished and thus only
fully automated on-line experiments are computationally re-
producible psychology.

Epistemically, empirical and software replication and re-
production are distinct from their modelling-related counter-
parts — they are six related endeavours. The diﬀerence be-
tween software for science (e.g., a statistical test) and science
that is software (e.g., a cognitive model) is an important one
to underline.
In the former case the code is a tool, in the
latter it constitutes an experiment. Notwithstanding, all such
evaluations have scientiﬁc merit.

Levels of Evaluation

We mentioned two of the levels in which modelling work
is evaluated. Unanimity is reached on replication as a min-
imum check, however some dialogue contributions go fur-
ther. To wit, Hinsen separates this endeavor into three steps.
Speciﬁcally we must check that a model is: bug-free; repro-
ducible as presented; congruent with empirical data. These
roughly map onto the levels of talking about modelling work
more generally, as Kidd notes (Marr, 1982).

Implementation Level

With respect to the implementation level, as Crook ex-
plains, re-running code both within a lab and by others al-
lows for checking for bugs and, importantly, if assumed-to-
be-irrelevant variables, e.g., the random seed, are not driving
the results. This also ensures documentation is appropriate.
Success at this level indicates a model is replicable.

Model Level

To evaluate the quality of the speciﬁcation, we may re-
write, i.e., reproduce, the model from scratch. This provides

2

OLIVIA GUEST

evidence for or against depending on the reimplementation’s
success. As Kidd mentions, and as we discovered (Cooper
& Guest, 2014), this process allows us to: discern when im-
plementation details must be elevated to the theory level and
vice versa; evaluate the speciﬁcation; and uncover bugs.

Theory Level

Many methods exist for testing theories. One such method
involves computationally implementing a theory — another
is to test predictions by gathering empirical data. As Crook
points out, such data is also used to evaluate models and
should be associated with the original article and codebase.
In such cases, empirical data requires re-collecting. This is
because if the phenomenon to-be-modelled, Hinsen warns,
does not occur as described by the overarching theoretical
account, then both theory and model are brought into ques-
tion. “A\* is a model of [...] A to the extent that [we] can
use A\* to answer questions [...] about A.” (Minsky, 1965,
p. 426)

Conclusions

Even though deﬁnitions for terms across the replies do not
fully converge,1 all contributors agree that change is needed
and imminent. A notable divergence of opinion can be found
in the reply by French and Addyman, who believe speciﬁ-
cations are less vital than we do. Importantly, we agree on
some fundamentals: sharing codebases; linking articles with
codebases; and reproducing models (e.g., ReScience, 2015).
In response to our question: Hinsen proposes mod-
ellers include a speciﬁc article section on evaluation; while
Crook lists community-driven initiatives for sharing code-
bases and speciﬁcations. Crook hopes, as we do, for top-
down publisher-enforced sharing of resources in partially-
centralised repositories. However, this does not preclude,
and may in fact require, grassroots demands. If the scientiﬁc
community rightfully yearns for change, we are required to
act to make this happen.

Cooper, R. P., & Guest, O. (2014). Implementations are not
speciﬁcations: Speciﬁcation, replication and experi-
mentation in computational cognitive modeling. Cog-
nitive Systems Research, 27, 42 - 49. doi: 10.1016/
j.cogsys.2013.05.001

Eklund, A., Nichols, T. E., & Knutsson, H.

(2016).
Cluster failure: Why fmri inferences for spatial ex-
Proceed-
tent have inﬂated false-positive rates.
ings of the National Academy of Sciences, 113(28),
7900-7905. Retrieved from http://www.pnas.org/
doi: 10.1073/
content/113/28/7900.abstract
pnas.1602413113

GitHub.

(2007). GitHub. Retrieved 2017-01-02, from

http://github.com/

Marr, D. (1982). Vision: A Computational Investigation into
the Human Representation and Processing of Visual
Information. Freeman.
Mesnard, O., & Barba, L. A.

(2016, May). Reproducible
and replicable CFD: it’s harder than you think. ArXiv
e-prints.

Minsky, M. (1965). Matter, mind and models. In Interna-
tional federation of information processing congress.
Open Science Framework. (2011). OSF. Retrieved 2017-01-

02, from http://osf.io/
Patil, P., Peng, R. D., & Leek, J.

A sta-
tistical deﬁnition for reproducibility and replicabil-
ity. bioRxiv. Retrieved from http://biorxiv.org/
content/early/2016/07/29/066803
doi: 10
.1101/066803

(2016).

ReScience. (2015). The ReScience Journal. Retrieved 2017-
01-02, from http://rescience.github.io/

References

Binder. (2016). mybinder.org. Retrieved 2017-01-02, from

http://mybinder.org/

1We do not wish to prescriptively enforce our terms and deﬁni-
tions — and we are open to suggestions, especially based on the use
of such terms by computationally-based disciplines (e.g., Mesnard
& Barba, 2016; Patil, Peng, & Leek, 2016).

