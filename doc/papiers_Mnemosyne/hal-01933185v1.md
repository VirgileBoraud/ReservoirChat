L’approche systémique : simuler moins pour modéliser
plus en neurosciences
Frédéric Alexandre

To cite this version:

Frédéric Alexandre. L’approche systémique : simuler moins pour modéliser plus en neurosciences.
Actes du colloque ”Modélisation: succès et limites”, CNRS et Académie des Technologies, CNRS -
Académie des Technologies, pp.12, 2018. ￿hal-01933185￿

HAL Id: hal-01933185

https://inria.hal.science/hal-01933185

Submitted on 23 Nov 2018

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

L’approche systémique :
simuler moins pour modéliser plus
en neurosciences

Frédéric ALEXANDRE

Inria Bordeaux Sud-Ouest
LaBRI, UMR 5800
Institut des Maladies Neurodégénératives
F-33076 Bordeaux

Frederic.Alexandre@inria.fr

RÉSUMÉ. Le recours à la modélisation et à la simulation permet aujourd’hui des performances
considérables pour les prévisions météorologiques ou pour la conception d’objets technolo-
giques très complexes. Il est tentant de poursuivre ces efforts et de les orienter vers d’autres
sujets particulièrement complexes comme l’étude du cerveau. Il est cependant très important de
bien analyser les principes de la démarche de modélisation et de simulation pour les appliquer
au mieux dans un cadre systémique, le plus adapté pour étudier le cerveau, et de se rendre
compte ainsi qu’il ne s’agit pas de construire les modèles les plus précis et les plus lourds mais
les plus adaptés à la question que l’on se pose.

ABSTRACT. The use of modeling and simulation now allows considerable performance for mete-
orological forecasting or for the design of very complex technological objects. It is tempting to
pursue these efforts and direct them to other particularly complex subjects such as the study of
the brain. However, it is very important to analyze the principles of the modeling and simulation
approach in order to best apply them in a systemic framework, the most adapted to study the
brain, and to realize that it is not a question to build the most precise and heaviest models but
the most adapted to the question that one asks.

MOTS-CLÉS : modélisation, simulation, approche systémique, neurosciences.

KEYWORDS: modeling, simulation, systemic approach, neurosciences.

2

F. Alexandre

1. Introduction

Avec le développement de l’informatique, son ancrage solide dans les mathéma-
tiques et les progrès technologiques associés, le recours à la modélisation est devenu
une pratique très courante pour rendre compte de phénomènes de plus en plus com-
plexes, et ceci d’autant plus que les moyens de calcul actuels permettent d’exploi-
ter plus facilement ces modèles. En plus de réalisations impressionnantes dans de
nombreux domaines de la physique, l’approche modélisatrice s’attaque également à
différents domaines du vivant, avec des succès également notables et une notoriété
croissante. C’est le cas en particulier pour les neurosciences computationnelles qui
ont acquis une popularité importante aujourd’hui pour l’étude du cerveau.

En première analyse, le domaine des neurosciences computationnelles pourrait
évoquer un oxymore. D’une part, les neurosciences peuvent être perçues comme des
sciences éminemment descriptives qui, par observation, expérimentation et recueil de
données, visent à décrire le cerveau (ou plus généralement le système nerveux) dans
sa réalité. D’autre part, le computationnel évoque plutôt des sciences normatives qui,
à l’aide de modèles et de simulations informatiques, se proposent de représenter un
objet d’étude (ici le cerveau) à travers un certain prisme, en suivant des formalismes
mathématiques ou informatiques, ce qui semble l’éloigner de sa réalité vivante (hu-
mide, diraient les biologistes).

Il est donc légitime de se demander comment des approches issues de sciences
dites exactes peuvent être utilisées dans les sciences du vivant, domaines plutôt tradi-
tionnellement associés à l’exploitation des données. Nous nous proposons de creuser
un peu plus ici les rapports entre ces deux formes d’étude du cerveau, de montrer
qu’elles sont au contraire très compatibles et d’observer que cette discussion fait éga-
lement émerger un certain nombre de recommandations pour leur meilleure associa-
tion. Cette analyse sera également l’occasion de rappeler les règles et les fondements
de la modélisation, ce qui permettra aussi d’aborder un autre sujet important dans ce
contexte, le développement de modèles de plus en plus complexes et le recours à des
moyens de calcul énormes pour les simuler, avec en ﬁligrane la question de savoir
si cette escalade dans la complexité et la puissance des calculs est nécessaire ou si
d’autres voies sont possibles, voire préférables.

2. Les neurosciences : une approche descriptive ?

John von Neumann faisait déjà remarquer que la vérité est beaucoup trop complexe
pour permettre autre chose que des approximations [NEU 47]. Cette remarque s’ap-
plique particulièrement bien au cerveau et les biologistes ont toujours été confrontés
à la difﬁculté de l’observer sans biais. On peut bien sûr être admiratif devant l’explo-
sion de la puissance des moyens technologiques développés pour mieux observer le
cerveau, de Ramon y Cajal qui travaillait au début du vingtième siècle avec une colo-
ration de Golgi et un simple microscope et dessinait ses observations à la plume et à
l’encre, jusqu’aux techniques plus récentes de microscopie biphotonique et d’expres-

Simuler moins pour modéliser plus

3

sion de protéines ﬂuorescentes (technique Brainbow) ou plus récemment encore la
technique Clarity [CHU 13] qui rend les tissus cérébraux transparents pour permettre
une encore plus grande précision dans les détails. Mais dans tous les cas, des biais
importants subsistent. Les techniques de coloration sont sélectives, les observations
concernent des animaux sacriﬁés et les tissus sont transformés par la préparation.

Comme le suggère l’évocation de ces limites technologiques, le cerveau est un
objet d’étude particulièrement délicat à observer pour de multiples raisons. D’une
part, par sa structure, il est fragile, difﬁcile d’accès, multi-structures et multi-échelles.
D’autre part, par ses fonctions, il est vivant et difﬁcilement dissociable d’autres entités
qui l’hébergent, comme le corps, ou interagissent avec lui, comme son environnement
immédiat mais aussi plus largement son histoire ou son contexte social, sans oublier le
fait que des considérations éthiques peuvent également limiter son exploration. Enﬁn,
pour le rendre encore plus particulier, on peut aussi noter que le cerveau peut être
vu tout aussi bien comme une machine physico-chimique que comme un système de
traitement de l’information et de communication.

On peut donc considérer que les neuroscientiﬁques ont toujours été confrontés à
l’intérêt majeur de cet objet d’étude mais aussi aux difﬁcultés, intrinsèques à sa struc-
ture et à sa fonction, de l’observer sans y introduire de biais. Ils ont donc dès l’origine
dû mener une réﬂexion élaborée pour développer des expériences (des techniques ou
des protocoles) permettant de se rapprocher le plus possible de ce souhait de décrire
la réalité du cerveau et, ce faisant, ont emprunté une démarche similaire à celle de la
modélisation que l’on évoquera dans la section suivante.

On peut ainsi mentionner la découverte d’animaux modèles, comme par exemple
des rongeurs qui semblent développer naturellement des maladies neurodégénératives
[ARD 12] et que l’on pourra étudier plus simplement ou plus invasivement que des
humains, ainsi que la mise au point de modèles animaux, par exemple relativement à
l’observation que l’injection d’une neurotoxine, le MPTP, peut induire chez le singe
les symptômes de la maladie de Parkinson [POR 12]. On parlera ici de modèle non
parce que le système à étudier est plus simple mais parce qu’il permet un accès faci-
lité à l’étude d’une question, ce qui est tout à fait compatible avec la déﬁnition d’un
modèle qu’on reprendra ci-dessous.

C’est aussi dans cette même perspective que des procédés expérimentaux ont dû
être développés dans les neurosciences, en particulier par électrophysiologie ou par
imagerie, pour observer des phénomènes autrement inaccessibles. Mais le dévelop-
pement de ces technologies s’est également accompagné d’un débat sur leurs limites
et sur l’interprétation de leurs résultats. Quelles sont par exemple les limites de réso-
lutions spatiales et temporelles en IRM fonctionnelle où le signal BOLD mesuré est
relatif au niveau d’oxygénation local des tissus et pas (directement) à l’activité neuro-
nale ? On voit à travers ces exemples que les neurosciences, considérant la complexité
de leur objet d’étude, doivent recourir massivement à des médiations entre cet objet et
les connaissances qu’elles veulent en extraire, suivant en cela une démarche similaire
à la modélisation.

4

F. Alexandre

Il est notable en particulier qu’aujourd’hui la plupart des avancées récentes en
neurosciences reposent sur des plateformes technologiques de plus en plus impres-
sionnantes mais aussi parfois dont il est de plus en plus difﬁcile de maîtriser les biais
potentiels et dont on peut parfois penser, comme on le notera aussi plus bas pour les
neurosciences computationnelles, qu’elles sont uniquement élaborées pour le plaisir
de complexiﬁer. Inversement, on peut aussi noter que sur certains sujets anatomiques,
les dessins plus que centenaires de Cajal restent une référence, probablement car, plus
qu’une observation, ils incluent l’intuition du Maître sur ce qu’il fallait observer...

3. Modélisation et simulation

La théorisation est probablement la plus aboutie des sciences normatives et vise
à décrire un objet d’étude en fournissant des explications ou des connaissances, sous
forme, par exemple, de relations entre ses variables d’état (comme la loi d’Ohm, par
exemple). Cependant, et particulièrement pour un objet complexe, une théorie com-
plète est généralement hors d’atteinte ou nécessite au mieux une mise au point par
démarche itérative, en attaquant successivement différents aspects de cet objet, en ré-
pondant à des séries de questions. C’est ainsi que l’on peut déﬁnir cette autre approche
normative qu’est la modélisation, comme une médiation entre expérience et théorie,
qui va pouvoir faciliter certains aspects de ce passage, en particulier au niveau de l’ex-
périmentation (par exemple le modèle animal) ou de la formulation (voir les modèles
phénoménologiques évoqués plus bas) [VAR 13]. C’est aussi dans cette perspective
que d’autres auteurs indiquent qu’un modèle est avant tout fait pour répondre à une
question [MIN 65] et qu’il a cette vertu analogique par rapport à l’objet qu’il repré-
sente, pour ce qui concerne cette question [THO 72].

Ainsi, selon R. Thom, faire fonctionner un modèle, c’est le questionner sur le sujet
pour lequel il a été conçu. On peut voir que ceci s’applique particulièrement bien à
un modèle animal. Dans une vision positiviste, où tout peut être expliqué par des phé-
nomènes physico-chimiques et décrit par des équations mathématiques [BUL 99], on
pourra aussi construire des modèles dits de connaissance, utilisant souvent l’algèbre
et les systèmes dynamiques et ainsi tenter d’expliquer certaines propriétés de l’objet
d’étude (c’est le rôle de justiﬁcation théorique du modèle). Cette approche a connu
un essor extraordinaire dans la seconde moitié du XXème siècle, avec le développe-
ment de l’informatique et de l’analyse numérique, en particulier pour rendre compte
de phénomènes naturels (océans, météorologie) ou pour développer des dispositifs
technologiques complexes (aviation, industrie nucléaire).

Dans une vision plus moderne, où l’on est capable de rassembler de grandes col-
lections de données (Big Data) comme traces de fonctionnement d’un phénomène, et
de développer des approches statistiques adaptatives (Machine Learning), on parlera
plutôt de modèles de représentation ou de modèles phénoménologiques qui, n’étant
pas fondés sur une analyse structurelle, n’auront pas de vertu explicative mais plu-
tôt un pouvoir prédictif. On parlera alors de l’efﬁcacité pragmatique d’un modèle. Ce
type d’approches a connu un regain d’intérêt spectaculaire récemment, en particulier

Simuler moins pour modéliser plus

5

grâce au développement de l’Internet permettant un meilleur accès aux données, et
des capacités de calcul permettant de calculer des modèles de plus grande taille, au
point où dans certains domaines où la théorie est difﬁcile (fortement non-linéaire par
exemple), il est plus efﬁcace d’approximer les équations principales par apprentis-
sage à partir de données [BRU 16]. On pourra aussi constater que dans le domaine
du traitement du langage naturel, après des décennies de théorisation, les meilleurs
systèmes de traduction automatique sont aujourd’hui basés sur des statistiques et sont
donc phénoménologiques...

La simulation, qui s’attache à la mise en œuvre numérique de modèles de connais-
sance, peut, d’un certain point de vue, être également considérée comme un modèle
phénoménologique. Alors que l’étape de modélisation proprement dite vise à déﬁnir
la représentation des connaissances et le formalisme de calcul qui seront les plus adap-
tés à la question posée, la simulation a pour but de mettre effectivement en œuvre le
modèle calculatoire pour répondre à des interrogations de type “Qu’est-ce qui se passe
si ... ?” (What if), en construisant des scénarios permettant par exemple de considérer
l’effet des paramètres choisis. Depuis de nombreuses années, des domaines entiers
de l’informatique et des mathématiques ont été développés pour étudier la mise au
point de schémas numériques efﬁcaces et pour permettre leur mise en œuvre perfor-
mante sur des architectures de calcul distribuées, au point que dans certains domaines
de l’algèbre linéaire, les progrès de la simulation sont autant dus à l’algorithmique
qu’à l’accroissement des puissances de calcul. La simulation peut être effectivement
considérée comme un modèle phénoménologique dans la mesure où cette étape de
calcul, malgré sa puissance, ne reste qu’un moyen sans vertu explicative et qu’il faut
ensuite procéder à une étape d’exploitation des résultats obtenus, le plus souvent par
visualisation mais aussi par d’autres moyens d’évaluation ou de mesure.

Comme il a été mentionné plus haut, à terme, l’aboutissement de telles approches
de modélisation pourrait être de construire une théorie ou en tout cas de contribuer
à son établissement progressif. Ceci se traduit par l’expression de trois étapes princi-
pales lors de la réalisation de modèles. Il s’agit premièrement de choisir la question de
connaissance à laquelle on souhaite répondre, typiquement une question sur laquelle
les théories courantes ne sont pas satisfaisantes, et de construire le modèle qui sera le
plus adapté pour y répondre, en choisissant en particulier la structure du système de
représentation et son état initial ainsi que le formalisme de calcul le plus adapté pour
rendre compte des variables importantes et de leurs relations (il est ainsi superﬂu d’in-
troduire des aspects de l’objet d’étude qui ne sont pas concernés par ces dimensions).
Dans un deuxième temps, si on ne peut pas répondre analytiquement à la question
ou si le modèle échappe à la compréhension car il devient trop complexe pour être
considéré dans son ensemble, on pourra exécuter une simulation jusqu’à l’étape où
les résultats générés peuvent être interprétés pour permettre de répondre à la question
et, le cas échéant, de fournir des explications.

Dans un troisième temps, on peut participer à la réfutation du modèle, en sui-
vant la démarche proposée par Karl Popper [POP 34] et en comparant les productions
ultérieures du modèle (et de ses simulations) avec la réalité ou en provoquant cette

6

F. Alexandre

comparaison, en proposant des prédictions sur des sujets qui n’étaient pas a priori
prévus lors de l’établissement du modèle. Une comparaison défavorable va sufﬁre à
mettre en cause le modèle (à le réfuter) et conduira à le modiﬁer plus ou moins radica-
lement pour lui permettre d’intégrer ce nouveau cas alors qu’une comparaison favo-
rable ne permettra de rien conclure d’autre que le fait que le modèle courant reste la
meilleure explication à notre disposition (en attendant une prochaine réfutation éven-
tuelle), puisqu’il faudrait pouvoir tester le modèle dans toutes les circonstances pour
l’adopter déﬁnitivement.

Cette démarche itérative a été utilisée pour le rafﬁnement de nombreux modèles,
avec ses bons et ses moins bons aspects. Si elle a permis de construire des modèles
complexes en considérant successivement différentes facettes d’une question assez
générale, elle a aussi donné lieu à des dérives, en créant des modèles exagérèment
complexes, constitués d’excroissances et de rustines destinées à rendre compte de cas
particuliers ou de questions relativement annexes au problème considéré, alors qu’il
ne faut pas oublier que la vocation d’un modèle n’est pas de rendre compte de la réalité
d’un objet d’étude sous tous ses aspects, mais seulement de fournir un substrat pour
répondre à une question particulière. On évoquera plus bas de tels exemples dans le
champ des neurosciences computationnelles. On observera également dans ce cas que
le problème principal de mise au point d’un modèle n’est pas tant de réduire progres-
sivement un écart de précision que de savoir y intégrer des connaissances hétérogènes,
ce qui reste un des points les plus délicats non abordés ici car relevant souvent plutôt
d’un savoir faire : le passage de la question à la forme du modèle le mieux adapté pour
y répondre. On évoquera seulement pour conclure la loi de l’instrument proposée par
A. Maslow [MAS 66] : “Il est tentant, si le seul outil que vous avez est un marteau, de
tout traiter comme si c’était un clou”.

4. Les Neurosciences Computationnelles

Les neurosciences computationnelles peuvent être déﬁnies comme le domaine
scientiﬁque visant à étudier les relations entre les structures et les fonctions du cer-
veau par le moyen de techniques de traitement de l’information [SCH 90, DAY 01].
Il a été évoqué plus haut la grande efﬁcacité de l’approche modélisatrice pour rendre
compte, dans une vision très positiviste, de phénomènes physico-chimiques à l’aide
d’équations différentielles. Il est notable à ce propos que l’histoire des neurosciences
computationnelles trouve son origine dans une description du fonctionnement d’un
neurone sous le couvert des lois physiques de l’électricité. Le premier modèle histo-
rique de neurone [BRU 07] et, plus tard, celui sur lequel une grande partie des neuros-
ciences computationnelles s’est construite, le modèle de Hodgkin-Huxley [NEL 95],
écrivent l’équation du potentiel de membrane d’un neurone en appliquant simplement
les lois d’Ohm et de Kirchoff (lois de l’électricité). De façon intéressante, on notera
que le modèle de Hodgkin-Huxley, plus complexe que son ancètre, ajoute à ce modèle
de connaissance d’autres équations phénoménologiques qui ont été déterminées expé-

Simuler moins pour modéliser plus

7

rimentalement par Hodgkin et Huxley en 1952 sur l’axone de calamar géant, rendant
compte de phénomènes comme la probabilité d’ouverture de canaux ioniques.

On pourra donc noter qu’à un certain niveau de description, il n’y a rien à com-
prendre de ce modèle, sauf qu’il traduit des observations de biologistes, mais il n’en
reste pas moins vrai qu’il a connu un succès retentissant (valant en particulier le prix
Nobel à ses auteurs), car il pouvait permettre de mimer avec une grande précision le
comportement électrique d’un neurone isolé soumis à des créneaux de courant en en-
trée. Cette précision s’est améliorée ultérieurement en étendant le modèle spatialement
mais aussi en descendant progressivement dans les niveaux de description des boîtes
phénoménologiques ou en ajoutant d’autres détails, relatifs aux synapses par exemple.
Outre les premiers essais pour étudier des assemblages de tels modèles de neurones,
parallèlement, des modèles plus intégrés ont été développés [AMA 77, WIL 73], choi-
sissant comme niveau de description l’activité électrique moyenne d’une population
de neurones et visant à rendre compte de phénomènes plus globaux de propagation de
cette activité électrique [COO 05].

Alors qu’il est important de mentionner que ces modèles élémentaires n’ont été
confrontés à la biologie que dans des cas artiﬁciels de petits nombres de neurones
soumis à des stimulations externes tout aussi artiﬁcielles, et qu’ils étaient essentiel-
lement dédiés à répondre à des questions du type “comment est-ce que les neurones
calculent ?”, ces approches de modélisation, très fructueuses dans leur domaine d’in-
vestigation initial, ont également suscité des attentes énormes et ont été réinterprétées
dans des approches ascendantes visant à simuler des morceaux importants de cerveau
(simuler alors qu’avant on parlait de modélisation). C’est en particulier le cas du Blue
Brain Project, ancêtre de l’actuel Human Brain Project.

Le Blue Brain Project, dont on peut aujourd’hui commencer à tirer des bilans
[MAR 15], se proposait d’assembler des modèles de neurones très détaillés, en agré-
geant des données de neuroanatomie correspondant à trente mille neurones et quarante
millions de synapses du cortex sensoriel du rat, sans mécanisme de plasticité céré-
brale et sans questionnement sur un éventuel calcul sous-jacent. Comme ses modèles
ancêtres, il inclut également des parties phénoménologiques, par exemple concernant
l’activité électrique des neurones, extraite par observation statistique, à coté d’autres
aspects très détaillés, plutôt basés sur des connaissances. Ce processus de rétroin-
génierie consistant à assembler un grand nombre de briques élémentaires pour mieux
comprendre l’objet global a pu faire évoquer de la complexité pour le plaisir de la com-
plexité [CHI 16] et en tout cas a conduit à se demander si ce type de projet ascendant
est sufﬁsamment contraint pour permettre de passer ainsi de niveaux sub-neuronaux à
des niveaux à bien plus large échelle tout en retrouvant, juste par agrégation de don-
nées, les propriétés des niveaux macroscopiques [FR˜14]. Parmi ces propriétés que l’on
voudrait voir ainsi émerger, il y a en particulier des aspects cognitifs, car c’est ce qui
est évidemment une justiﬁcation majeure d’une telle démarche intégratrice, pour ne
pas mentionner l’Human Brain Project qui, lui, vise le cerveau humain dans son en-
semble, c’est-à-dire un réseau d’une taille deux millions de fois supérieure au modèle
précédent.

8

F. Alexandre

Même si c’est également un aspect très important de ces projets et si cet aspect
a généré des questions scientiﬁques très intéressantes, nous ne discuterons pas ici en
détails les travaux relatifs à l’implantation matérielle et à la réalisation concrète de
tous les calculs sous-jacents. On donnera seulement des ordres de grandeurs en in-
diquant que calculer de tels modèles peut impliquer des milliers de processeurs et
des dizaines voire des centaines de téraﬂops (milliers de milliards d’opérations par
seconde).Toujours pour rester dans les ordres de grandeur, on remarquera que de tels
calculs peuvent générer une consommation électrique de l’ordre du Méga Watt, à com-
parer avec les vingt Watts consommés par notre cerveau.

Les neurosciences computationnelles semblent donc proposer des modèles inté-
ressants lorsque l’on considère des petits systèmes de neurones, vus comme des ma-
chines physico-chimiques stimulées artiﬁciellement. Ceci peut être précieux pour ré-
pondre à des questions et faire des prédictions dans un certain nombre de situations
relatives par exemple aux modes de fonctionnement ou d’apprentissage de neurones
isolés, mais semble plus difﬁcile à transposer au cerveau dans son ensemble ou à des
fonctions cognitives complexes. Nous pensons que cette difﬁculté est due à plusieurs
raisons que nous résumons brièvement ici. Tout d’abord, le cerveau est un système
ouvert, la cognition est incarnée dans un corps et le cerveau se construit et fonctionne
en interaction avec l’environnement. Ensuite, le cerveau est un système adaptatif et
changeant. La cognition résulte de différentes formes d’apprentissage et d’interaction
entre différentes formes de mémoires et s’élabore de façon plus ou moins autonome
tout au long de la vie. Enﬁn, le cerveau est un système multimodal et multi-niveaux et
l’on pourra donc poser des questions totalement différentes selon que l’on considère
des sensations élémentaires comme plaisir et douleur ou des perceptions beaucoup
plus structurées comme la vision et l’audition et selon que l’on s’intéresse au rôle des
hormones ou à celui du langage dans le fonctionnement du cerveau. Il n’est donc pas
évident que l’on puisse s’attaquer à tous et à chacun de ces sujets en agrégeant sim-
plement des modèles de neurones à l’aide d’un simple plan de connexion, en suivant
un simple processus de rétroingénierie. Ce tableau du cerveau, vu comme un système
complexe, dépendant potentiellement d’un nombre astronomique de variables et de
paramètres, impliqué dans des boucles d’interaction avec son environnement incluant
le corps et dépendant de son histoire récente et ancienne semble donc disqualiﬁer la
méthode de modélisation traditionnellement utilisée pour d’autres objets complexes
comme un avion ou un océan, quand il s’agit d’aborder des afﬁrmations telles que
celle formulée par P. Cabanis au XVIIIème siècle : “Le cerveau sécrète la pensée
comme le foie sécrète la bile” ...

D’autres chercheurs, intéressés par mettre les neurosciences computationnelles au
service de l’exploration de fonctions cognitives, ont fait ce constat. Ils ont également
bien compris que faire des modèles, si précis soient-ils, n’est pas simuler la réalité ni
tout en expliquer, qu’il reste des approximations et des aspects purement phénoméno-
logiques et que faire un modèle, c’est construire un cadre, éventuellement simpliﬁé et
comportant des a priori, dans le but d’explorer une question précise. C’est dans cette
perspective que, pour modéliser le cerveau et ses fonctions cognitives, des forma-
lismes de calcul ont été proposés [ROU 12, O’R 00, STE 11], adaptés aux questions

Simuler moins pour modéliser plus

9

que ces chercheurs se posent, et qui permettent d’explorer ensuite certaines fonctions
cognitives comme les phénomènes attentionnels [FIX 11], la décision [O’R 06] ou la
coordination sensorimotrice pour manipuler de différentes façons des séquences per-
ceptives [ELI 12]. Ces modèles ne sont pas purement ascendants mais introduisent
également des a priori et des hypothèses concernant des niveaux de description inter-
médiaires ou même parfois, de façon descendante, des cadres conceptuels. Il ne s’agit
donc pas de comparer ou d’évaluer ces modèles en fonction de la quantité de détails
qu’ils ont pu agréger, mais plutôt de se demander s’ils reposent effectivement sur des
a priori ou des hypothèses cohérentes par rapport à ce que l’on sait aujourd’hui du
cerveau, s’ils arrivent effectivement à apporter des éléments de réponse pertinents par
rapport aux questions qui avaient été choisies, si les méthodes d’évaluation sont so-
lides et éventuellement s’ils proposent des prédictions qui pourraient permettre de les
réfuter ou de les faire évoluer.

Dans le cadre d’une démarche de modélisation de fonctions cognitives, ajouter la
contrainte de la prise en compte du substrat neuronal peut également avoir des effets
bénéﬁques tout au long de la démarche, pour aider à formuler la question, construire le
cadre et le formalisme et évaluer le modèle dans un contexte plus connu, plus classique
à décrire et plus facile à expérimenter. Si l’on considère par exemple la compréhension
du conditionnement répondant, une série de modèles purement comportementaux ont
été proposés [LEP 04], chaque modèle se traduisant par la complexiﬁcation du précé-
dent en ajoutant un terme dans une équation phénoménologique pour rendre compte
de résultats nouveaux ayant réfuté le précédent. De façon contrastée, considérer ce
même paradigme de conditionnement en le faisant reposer sur son implantation neu-
ronale décrite dans le lobe temporal médial [CAR 15] permet de proposer une solution
plus simple où les comportements complexes reposent simplement sur une compé-
tition entre plusieurs voies neuronales élémentaires. Une telle approche de modéli-
sation cognitive par les neurosciences computationnelles permet de plus de générer
des prédictions pouvant être vériﬁées expérimentalement à des niveaux différents, par
exemple pharmacologiques [CAL 06].

5. Discussion

Dans ce chapitre, nous avons tout d’abord rappelé le recours croissant à l’utilisa-
tion de modèles dans de nombreux domaines de la physique et dans l’industrie. Ces
modèles rendent compte de phénomènes trop complexes pour être directement décrits
par une théorie mais pour lesquels la connaissance de leurs mécanismes élémentaires
par la théorie ou l’observation phénoménologique permet de construire un modèle et
de l’utiliser ensuite lors de campagnes de simulation. Le succès de telles entreprises
est dû aux progrès considérables réalisés dernièrement en mathématiques et en infor-
matique pour construire ces modèles et pour les calculer efﬁcacement. Mais il repose
également sur une utilisation réﬂéchie et maitrisée de ces outils très puissants. Nous
avons identiﬁé deux types de risques associés à une mauvaise compréhension et à un
mauvais usage des modèles et des simulations.

10

F. Alexandre

De façon générale, il est important de se rappeler qu’un modèle n’est pas une des-
cription de la réalité qu’il conviendrait de rendre de plus en plus précise mais un outil
de médiation construit entre l’expérience et la théorie pour répondre à une question
particulière. Si nous avons insisté sur cette déﬁnition de la modélisation, c’est pour
rappeler que la qualité première d’un modèle n’est pas, dans l’absolu, la ﬁnesse du
niveau de description utilisé mais sa capacité à répondre à la question qui était posée.
Il y a de toutes façons le plus souvent des aspects phénoménologiques inclus dans
les modèles qui empêchent de descendre dans la ﬁnesse du niveau d’explication et le
plus important est donc de vériﬁer que les hypothèses retenues sont cohérentes avec
le cadre choisi et donc la question à traiter. Modéliser plus ﬁnement et donc devoir
simuler plus n’est pas un but en soi.

De façon plus particulière, concernant les neurosciences (ou d’autres sciences étu-
diant des systèmes complexes), nous avons mis en garde contre l’extension de la seule
vision positiviste dans un contexte systémique. Il n’est pas satisfaisant de décrire le
cerveau comme une simple machine physico-chimique à modéliser de façon ascen-
dante mais il convient plutôt de le remettre dans un cadre systémique en considérant
des boucles d’interaction avec son corps, son environnement, ses niveaux d’échelle
ou encore son histoire à différentes constantes de temps. La complexité de cette des-
cription est une raison de plus pour disqualiﬁer une modélisation reposant trop sur
l’afﬁnement du niveau de description mais va plutôt inciter à réﬂéchir ﬁnement aux
questions à poser et aux hypothèses et aux formalismes à retenir. Pour ces raisons, le
choix d’un cadre général bio-inspiré et d’hypothèses reposant sur des niveaux de des-
cription intermédiaires est une voie intéressante pour tenter de rejoindre expériences et
théories. Pour autant, considérant la complexité de l’objet d’étude et les effets d’émer-
gence associés à ces boucles et à ces niveaux d’échelle et de temps, le recours à la
simulation reste un outil important pour explorer ce cadre de modélisation.

6. Bibliographie

[AMA 77] AMARI S., « Dynamics of pattern formation in lateral-inhibition type neural

ﬁelds », Biological Cybernetics, vol. 27, no 2, 1977, p. 77–87.

[ARD 12] ARDILES A. O., TAPIA-ROJAS C. C., MANDAL M., ALEXANDRE F., KIRKWOOD
A., INESTROSA N. C., PALACIOS A. G., « Postsynaptic dysfunction is associated with
spatial and object recognition memory loss in a natural model of Alzheimer’s disease. »,
Proceedings of the National Academy of Sciences, vol. 109, no 34, 2012, p. 13835-40,
National Academy of Science.

[BRU 07] BRUNEL N., VAN ROSSUM M. C. W., « Lapicque’s 1907 paper : from frogs to

integrate-and-ﬁre », Biological Cybernetics, vol. 97, no 5, 2007, p. 337–339.

[BRU 16] BRUNTON S. L., PROCTOR J. L., KUTZ J. N., « Discovering governing equations
from data by sparse identiﬁcation of nonlinear dynamical systems », Proceedings of the
National Academy of Sciences, vol. 113, no 15, 2016, p. 3932–3937, National Academy
of Sciences.

[BUL 99] BULLOCK A., TROMBLEY S., The Fontana Dictionary of Modern Thought, Lon-

don : Harper-Collins, 1999.

Simuler moins pour modéliser plus

11

[CAL 06] CALANDREAU L., TRIFILIEFF P., MONS N., COSTES L., MARIEN M., MARI-
GHETTO A., MICHEAU J., JAFFARD R., DESMEDT A., « Extracellular hippocampal ace-
tylcholine level controls amygdala function and promotes adaptive conditioned emotional
response. », The Journal of neuroscience : the ofﬁcial journal of the Society for Neuros-
cience, vol. 26, no 52, 2006, p. 13556–13566.

[CAR 15] CARRERE M., ALEXANDRE F., « A pavlovian model of the amygdala and its in-
ﬂuence within the medial temporal lobe », Frontiers in Systems Neuroscience, vol. 9, no
41, 2015.

[CHI 16] CHI K. R., « Neural modelling : Abstractions of the mind », Nature, vol. 531, no

7592, 2016, p. S16–17.

[CHU 13] CHUNG K., DEISSEROTH K., « CLARITY for mapping the nervous system », Nat.

Methods, vol. 10, no 6, 2013, p. 508–513.

[COO 05] COOMBES S., « Waves, bumps and patterns in neural ﬁeld theories », Biol. Cybern.,

vol. 93, 2005, p. 91-108.

[DAY 01] DAYAN P., ABBOTT L. F., Theoretical neuroscience : computational and mathema-

tical modeling of neural systems, Cambridge, MA : MIT Press, 2001.

[ELI 12] ELIASMITH C., STEWART T. C., CHOO X., BEKOLAY T., DEWOLF T., TANG Y.,
TANG C., RASMUSSEN D., « A large-scale model of the functioning brain. », Science
(New York, N.Y.), vol. 338, no 6111, 2012, p. 1202–1205, American Association for the
Advancement of Science.

[FIX 11] FIX J., ROUGIER N. P., ALEXANDRE F., « A Dynamic Neural Field Approach to
the Covert and Overt Deployment of Spatial Attention », Cognitive Computation, vol. 3,
2011, p. 279–293.

[FR˜14] FRÉGNAC Y., LAURENT G., « Where is the brain in the Human Brain Project ? »,

Nature, vol. 513, 2014.

[LEP 04] LE PELLEY M. E., « The role of associative history in models of associative lear-
ning : a selective review and a hybrid model. », The Quarterly Journal of Experimental
Psychology, vol. 57, no 3, 2004, p. 193–243.

[MAR 15] MARKRAM H., MULLER E., RAMASWAMY S., REIMANN M. W., ABDELLAH
M., SANCHEZ C. A., AILAMAKI A., ALONSO-NANCLARES L., ANTILLE N., ARSEVER
S., KAHOU G. A., BERGER T. K., BILGILI A., BUNCIC N., CHALIMOURDA A., CHIN-
DEMI G., COURCOL J.-D., DELALONDRE F., DELATTRE V., DRUCKMANN S., DUMUSC
R., DYNES J., EILEMANN S., GAL E., GEVAERT M. E., GHOBRIL J.-P., GIDON A., GRA-
HAM J. W., GUPTA A., HAENEL V., HAY E., HEINIS T., HERNANDO J. B., HINES M.,
KANARI L., KELLER D., KENYON J., KHAZEN G., KIM Y., KING J. G., KISVARDAY Z.,
KUMBHAR P., LASSERRE S., LE BÉ J.-V., MAGALHÃES B. R. C., MERCHÁN-PÉREZ
A., MEYSTRE J., MORRICE B. R., MULLER J., MUÑOZ CÉSPEDES A., MURALIDHAR
S., MUTHURASA K., NACHBAUR D., NEWTON T. H., NOLTE M., OVCHARENKO A., PA-
LACIOS J., PASTOR L., PERIN R., RANJAN R., RIACHI I., RODRÍGUEZ J.-R., RIQUELME
J. L., RÖSSERT C., SFYRAKIS K., SHI Y., SHILLCOCK J. C., SILBERBERG G., SILVA R.,
TAUHEED F., TELEFONT M., TOLEDO-RODRIGUEZ M., TRÄNKLER T., VAN GEIT W.,
DÍAZ J. V., WALKER R., WANG Y., ZANINETTA S. M., DEFELIPE J., HILL S. L., SE-
GEV I., SCHÜRMANN F., « Reconstruction and Simulation of Neocortical Microcircuitry »,
Cell, vol. 163, no 2, 2015, p. 456–492, Elsevier.

[MAS 66] MASLOW A., The Psychology of Science, New York : Harper and Row, 1966.

12

F. Alexandre

[MIN 65] MINSKY M., « Matter, Mind and Models », Proc. International Federation of

Information Processing Congress, 1965, p. 45-49.

[NEL 95] NELSON M., RINZEL J., « The Hodgkin-Huxley model. », chapitre 4, p. 27-51,

Bower and Beeman. The book of Genesis, Springer, New York, 1995.

[NEU 47] VON NEUMANN J., « The Mathematician », HEYWOOD R., Ed., The works of the

mind, University of Chicago Press, 1947.

[O’R 00] O’REILLY R., MUNAKATA Y., Computational Explorations in Cognitive Neuros-
cience : Understanding the Mind by Simulating the Brain, MIT Press, Cambridge, MA,
USA, 2000.

[O’R 06] O’REILLY R. C., FRANK M. J., « Making Working Memory Work : A Computatio-
nal Model of Learning in the Prefrontal Cortex and Basal Ganglia », Neural Computation,
vol. 18, no 2, 2006, p. 283–328.

[POP 34] POPPER K., The Logic of Scientiﬁc Discovery, Routledge, Edition 2002, 1934.

[POR 12] PORRAS G., LI Q., BEZARD E., « Modeling Parkinson’s disease in primates : The
MPTP model », Cold Spring Harb Perspect Med, vol. 2, no 3, 2012, page a009308.
[ROU 12] ROUGIER N. P., FIX J., « DANA : Distributed numerical and adaptive modelling
framework », Network : Computation in Neural Systems, vol. 23, no 4, 2012, p. 237–253.
[SCH 90] SCHWARTZ E. L., Computational Neuroscience, MIT Press, Cambridge, MA, USA,

1990.

[STE 11] STEWART T. C., BEKOLAY T., ELIASMITH C., « Neural representations of com-
positional structures : representing and manipulating vector spaces with spiking neurons »,
Connection Science, vol. 23, no 2, 2011, p. 145–153, Taylor and Francis, Inc.

[THO 72] THOM R., Stabilité structurelle et morphogénèse : essai d’une théorie générale des

modèles, W. A. Benjamin Reading, Mass, 1972.

[VAR 13] VARENNE F., « Modèles et simulations dans l’enquête scientiﬁque : variétés tradi-
tionnelles et mutations contemporaines », VARENNE F., SILBERSTEIN M., Eds., Modéli-
ser et simuler. Epistémologies et pratiques de la modélisation et de la simulation, p. 11-49,
Editions Matériologiques, 2013.

[WIL 73] WILSON H., COWAN J., « A mathematical theory of the functional dynamics of

cortical and thalamic nervous tissue », Kybernetic, vol. 13, 1973, p. 55–80.

