Recurrent neural network weight estimation through
backward tuning
Thierry Viéville, Xavier Hinaut, Thalita F Drumond, Frédéric Alexandre

To cite this version:

Thierry Viéville, Xavier Hinaut, Thalita F Drumond, Frédéric Alexandre. Recurrent neural network
weight estimation through backward tuning. [Research Report] RR-9100, Inria Bordeaux Sud-Ouest.
2017, pp.1-54. ￿hal-01610735￿

HAL Id: hal-01610735

https://inria.hal.science/hal-01610735

Submitted on 5 Oct 2017

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Recurrent neural
network weight
estimation
though backward tuning

Thierry Viéville , Xavier Hinaut , Thalita F. Drumond , Frédéric
Alexandre

G
N
E
+
R
F
-
-
0
0
1
9
-
-

/

R
R
A
R
N

I

I

RESEARCH
REPORT
N° 9100
October 2017

Project-Team Mnemosyne

N
R
S

I

9
9
3
6
-
9
4
2
0
N
S
S

I

Recurrent neural network weight
estimation
though backward tuning

∗ Thierry Viéville ∗, Xavier Hinaut ∗, Thalita F.
Drumond ∗, Frédéric Alexandre ∗

Project-Team Mnemosyne

Research Report n° 9100 — October 2017 — 50 pages

Abstract: We consider another formulation of weight estimation in recurrent net-
works, proposing a notation for a large amount of recurrent network units that helps
formulating the estimation problem. Reusing a “good old” control-theory principle,
improved here using a backward-tuning numerical stabilization heuristic, we obtain
a numerically stable and rather eﬃcient second-order and distributed estimation,
without any meta-parameter to adjust. The relation with existing technique is dis-
cussed at each step. The proposed method is validated using reverse engineering
tasks.

Key-words:

recurrent network, machine learning, backward tuning

∗ Mnemosyne team, INRIA Bordeaux

RESEARCH CENTRE
BORDEAUX – SUD-OUEST

351, Cours de la Libération

Bâtiment A 29

33405 Talence Cedex

Estimation des poids d’un réseau récurrent
par ajustement rétroactif

Résumé : Nous considérons une formulation alternative de l’estimation du poids
dans les réseaux récurrents, proposant une notation integrant une grande quan-
tité d’unités de réseau récurrentes qui aide à formuler ce problème d’estimation.
Réutilisant un «bon vieux» principe de la théorie du contrôle, amélioré ici à l’aide
d’une heuristique de stabilisation numérique rétroactive, nous obtenons une esti-
mation distribuée du 2ème ordre, numériquement stable et plutôt eﬃcace, sans
aucun méta-paramètre à ajuster. La relation avec les techniques existantes est
discutée à chaque étape. La méthode proposée est validée en utilisant des tâches
d’ingénierie inverse.

Mots-clés :
tif

réseaux récurrents, apprentissage automatique, ajustement rétroac-

Backward tuning

1

Introduction

3

Artiﬁcial neural networks can be considered as discrete-time dynamical systems,
performing input-output computation, at the higher level of generality [49]. The
computation is deﬁned by the adjustment of the network connection weights and
related parameters1 In fact, only speciﬁc feed-forward or recurrent architectures
are considered in practice, because of network parameters estimation, as reviewed
now.

In the artiﬁcial neural network literature, feed-forward networks parameter
learning is a rather well-solved problem. For instance, the back-propagation algo-
rithms, based on speciﬁc architectures of multi-layer feed-forward networks, allows
one to propose well-deﬁned implementation [2], though it has been shown at the
theoretical and empirical levels that "shallow" architectures are ineﬃcient for rep-
resenting complex functions [44, 7], or at the cost of huge network sizes as in, e.g.,
extreme learning [31].

Deep-networks are speciﬁc feed-forward architectures [7] which can have very
impressive performances, e.g. [22]. The key idea [32] is that, at least for threshold
units with positive weights, reducing the number of layers induces an exponential
complexity increase for the same input/output function. On the reverse, it is a
reasonable assumption, numerically veriﬁed, that increasing the number of layers
yields a input/output function compact representation (in the sense of [32], i.e., as
a hierachical composition of local functions). One drawback is related to weights
supervised learning in deeper layers, since readout layers may over-ﬁt the learning
set, the remedy being to apply unsupervised learning on deeper layers (see [5] for
an introduction). This problem is highly reduced with speciﬁc architectures such
as CNN [36].

It also remains restrictive by the fact that the architecture is mainly a pipe-
line including some parallel tracks or short-cuts, while each layer is a feed-forward
network (e.g. a convolutional neural layers) or with a very speciﬁc recurrent con-
nectivity (e.g., restrained Boltzman machines). Starting with LeNet-5 [36], dif-
ferent successful architectures in term of performance have been proposed (e.g.,
AlexNet[35], ZF net [60], Overfeat [47], VGG [50], GoogLeNet [53], Inspection [52],
residual nets [26]).

In the brain, more general architectures exist (e.g. with shortcuts between
deeper and lower layers, as it happens in the visual system regarding the thalamus
[48]) and each layer is a more general recurrent network (e.g., with short and long
range horizontal connections). Breaking this pipe-line architecture may overcome
the problem of deeper layer weight adjustment, and the need of huge architecture

1Other network parameters include the unit leak, intrisic plasticity, parameters of the non-
linearity (or activation function). However, in this paper we are going to use a notation allowing
us to consider all these parameters as connection weights for an extended set of state variables.

RR n° 9100

4

Alexandre & Drumond & Hinault & Viéville

in order to obtain high performances. This is the origin of the present work.

Feed-forward networks are obviously far from the computational capacity of
recurrent networks [25, 46, 10]. Therefore, speciﬁc multi-layer architectures with
recurrent links within a layer and speciﬁc forward/backward connections between
layers have been proposed instead. The ﬁrst dynamic neural model, the model by
Hopﬁeld [30], or its randomized version as a Boltzman machine, was very speciﬁc.
For such speciﬁc networks, such as bidirectional associative memory [1], speciﬁc
learning methods apply. Further solutions include Jordan’s network [34], Elman’s
Networks [19], Long short term memory (LSTM) by Hochreiter and Schmidhuber
[28]. This latter architecture being very performant [46].

Another track is to consider recurrent networks with a “reservoir” of reccurent
units but without explicit weight adjustement [58]. Units in such architectures
are linear or sigmoid artiﬁcial neurons, including soft-max units, or even spiking
neurons. Such network architectures, such as Echo State Networks [33] and Liquid
State Machines [38], are called “reservoir computing” (see [58] for uniﬁcation of
reservoir computing methods at the experimental level), while extreme learning is
based on a closed idea [31]. In such architectures the recurrent weigts of hidden
units are not explicitly learned, but recurrent weights are either randomly ﬁxed,
likely using a sparse connectivity, or adjusted using unsupervised learning mecha-
nism, without any direct connection with the learning samples (though the hidden
unit statistics, for instance, is sometimes adjusted in relation with the desired out-
put) [42]. It appears that reservoir computing yields good results [58], but without
over-passing recent deep-layer architecture performances [18].

The general problem of learning recurrent neural networks has also been widely
addressed as reviewed in [17] for 90’s studies and in [39] for recent advances, and
methods exist far beyond basic methods such as back-propagation through time,
but is still not a well-solved problem.

In the present paper, we revisit the general problem of recurrent network weight
learning, not as it, but because it is related to modern issues related to both ar-
tiﬁcial networks and brain function modeling. Such issues include: Could we
adjust the recurrent weights in a reservoir computing architecture ? Is it possible
to consider deep-learning architecture, with more general inter and intra layers
connectivity ? Would it be possible to not only use some speciﬁc recurrent archi-
tecture as exempliﬁed here, but to learn also the architecture itself (i.e. learn the
weight value and learn if the connection weight has to be set to zero, cutting the
connection) ?

We are not going to address more than weight adjustment in this paper, and
only on small architectures since we precisely target being able to solve complex
computational tasks with reasonable architectures, in order the parameters to be
learnable on not so big data [20]. As a consequence, learning issues (e.g., boosting

Inria

Backward tuning

5

[23]) are not within the scope of this paper: Neither representation learning [6], nor
other complex issues [25] are considered, this contribution being only an alternate
tool for variational weight optimization. See [20] for a recent discussion on such
issues.

We are also not going to consider biological plausibility in the sense of [8], but
will show that the proposed method is compliant with several distributed biological
constraints or computational properties: local weight adjustment, backward error
propagation, Hebbian like adjustment rules. A more rigorous discussion about the
link with computational neuroscience aspects is however beyond the scope of this
work.

In the next section we choose a notation to state the estimation problem, and
Appendix A makes explicit how this notation applies to most of the usual frame-
works, while Appendix B compares the method with related recurrent weights
estimation methods. We then address the estimation problems and introduce the
proposed modiﬁed solution, while Appendix C further discuss how it can be used
for several estimation problems. In the subsequent section the method is imple-
mented and numerically evaluated. Finally, Appendix D illustrates how certain
estimation problem reduce to trivial computation problems, given a suitable units
and architecture, while Appendix E reviews how statistical problems can be re-
duced to an estimation problem compatible with our framework.

This is a short paper with a new proposal for weight estimation, but in link
with quite a lot of other issues in the ﬁeld. This is the reason why the core of the
paper is short while several appendices are added.

2 Problem position

Notations. Vectors and matrix are written in bold, only basic linear algebra is
used. For instance, xn(t) stands for the value of the n-th node at time t, xn for the
whole values of the node along time, x(t) the whole values of the nodes at time t
and x all network values.

The Heaviside function writes H(u) (considering H(0) = 1/2) and the sign

function writes sg(u) = 2 H(u) − 1:




1
1/2
0
Partial derivatives are written in compact form, e.g., ∂xn(t) f (x) means ∂f (x)
∂xn(t)
.

if u > 0
if u = 0
if u < 0.

H(u) def=



,

while ∂xn(t) xn(cid:48) (t(cid:48)) f (x) means

∂2f (x)
∂xn(t) ∂xn(cid:48) (t(cid:48))

The notation δP stands for 1 is the property P is true and 0 otherwise (e.g.,

δ2>1 = 1).

RR n° 9100

6

Alexandre & Drumond & Hinault & Viéville

Other notations are made explicit as soon as used.

A general recurrent architecture.

Figure 1: A General recurrent architecture maps a vectorial input sequence i(t)
onto an output o(t), via an internal state x(t) of hidden units. It is parameterized
by a recurrent parameter matrix W. The dynamics is deﬁned by the network
recurrent equations.

As schematized in ﬁgure 1, we consider a recurrent network

with nodes of the form:

xn(t) = Φnt (· · · , xn(cid:48)(t(cid:48)), · · · , im(s), · · · )

xn(t) = Φn0t (· · · , xn(cid:48)(t(cid:48)), · · · , im(s), · · · )

+ (cid:80)Dn
on(t) = xn(t), n < N0

d=1 Wnd Φndt (· · · , xn(cid:48)(t(cid:48)), · · · , im(s), · · · )

(1)

i.e., deﬁned as a linear combination of some kernel Φndt(). We show in Appendix A
that this a very general form (e.g., including when considering the adjustment of
unit parameters that are not connection weights).
More precisely, equation (1) elements deﬁne:
- N nodes of value xn(t) indexed by n ∈ {0, N {,

- with a maximal state recurrent causal range of R and with either,
- t − R ≤ t(cid:48) < t (i.e., taking into account previous value up to R time-steps

in the past) or

- t(cid:48) = t and n < n(cid:48) (i.e., taking into account present value, of subsequent

Inria

Backward tuning

nodes, in a causal way).

7

- while N0 ≤ N of these nodes are output;
- M input im(s) indexed by m ∈ {0, M {, t − S ≤ s < t,
- 1 + Dn predeﬁned kernels Φndt () for each node, deﬁning the network structure;
- (cid:80)

n Dn static adjustable weights Wnd, deﬁning the network parameter.

Considering equation (1) we notice that :

• The distinction between output or hidden node is simply based on the fact
that we can (or not) observe the on(t) node value. Here, without loss of
generality, output nodes are the N0 ≤ N ﬁrst ones.

• Though, in order to keep compact notations, we mixed node with either
- unit ﬁrmware parameter-less function, i.e. with Φn0t(), or
- unit learnware linear combination of elementary kernels, i.e. with

d Wnd Φndt(),

(cid:80)
in all examples these two kinds of node will be separated. This constraint is
not mandatory, but will help clarifying the role of each node.

• A given state value depends either on previous time values (t−R ≤ t(cid:48) < t) or
subsequent indexed nodes (t(cid:48) = t and n < n(cid:48)), yielding a causal dependency
in each case.

• By design choice, as made explicit in appendix A for all examples, 0 ≤
∂xn(cid:48) (t(cid:48))Φndt() ≤ 1 (non-decreasing contractive non-linearity), is veriﬁed. This
constraint is not mandatory, but will help at the numerical conditioning level.

• We further assume, just for the sake of simplicity2, that initial conditions are

equal to zero, i.e., x(t) = 0, t < 0 and i(s) = 0, s < 0.

• We also assume that the dynamic is regular enough3 for weight estimation

to be numerically stable.

The key point here, is that some state variables xn are additional intermediate
internal variables in order the weight estimation to be a simple linear problem

2It is an easy task to introduce non-zero initial conditions as additional network parameter

to learn, or consider then as a transient additional random input.

3Here, we assume that input and output are bounded, while the system is regular enough
for the subsequent estimation to be numerically stable. Chaotic behaviors likely require very
diﬀerent numerical methods (taking explicitly the exponential dependency on previous value
variations into account) [10]. In practice, not only contracting systems can be considered, as
soon as the observation times are not too large with respect to cumulative rounding errors. As
far as computing capabilities are considered, systems at the edge of chaos (but not chaotic) seem
to be interesting to consider [9, 37], which ﬁts with the present requirement.

RR n° 9100

8

Alexandre & Drumond & Hinault & Viéville

as a function of these additional variables (and at the cost of higher dimensional
problem).

The claim of this paper is that this choice of notation has two main conse-

quences developed in the next sections:

1. All known computational networks architecture can be speciﬁed that way.

This is made explicit in appendix A.

2. The weight estimation problem writes in a quite simple way, with this refor-

mulation. This is discussed now.

3 Recurrent weight estimation

We implement the recurrent weight estimation as a variational problem, i.e. deﬁne:

W = arg min

minxmaxε L(W, x, ε),

W

(2)

for adjustable network parameters or weights W, given state values x and auxilary
variables ε, writing:

L(W, x, ε) def= ρ(· · · , xn(t), · · · )
+ (cid:80)
+ R(W)

desired values

regularization

nt εnt (˜xn(t) − xn(t)) network dynamic constraint

where ˜xn(t) is a shortcut for equation (1):

(cid:40)

˜xn(t)

def= Φn0t (· · · , xn(cid:48)(t(cid:48)), · · · , im(s), · · · )
+ (cid:80)Dn

d=1 Wnd Φndt (· · · , xn(cid:48)(t(cid:48)), · · · , im(s), · · · )
while εnt are Lagrange multipliers, and in most of the case4 we use:
ρ(· · · , xn(t), · · · ) def= (cid:80)

nt ρnt(xn(t)).

Here ρ() is a cost-function (acting both as supervised or unsupervised varia-
tional term) and R(W) some regularization term, as made explicit in Appendix C.
The cost function includes both the term attached to the data, i.e., the fact that
output values have a desired values, and regularization. These ingredients can
be used to get the approximate desired output, yield sparse estimation, reduce
artifact inﬂuence, obtain activity orthogonality, etc (see Appendix C for details).
In a nutshell, ρ() and R allows one to specify the estimation problem, as a
function of the unknows W, x and ε. Stating the estimation this way, leads us
to a simpliﬁed form of the Pontryagin’s minimum principle, well-known in control

4More precisely, here, in the deterministic case, a simple additive criterion is used, while this

is not the case for statistical criterion, as further discussed in appendix C and E.

Inria

Backward tuning

9

theory [3], and reviwed in the next section. In short, the eﬀective related solution
is derived from the normal equations of the proposed criterion.

This formulation is not new and has been formalized, by, e.g.

[17]. Here we
restate it at a higher level of generality, with two new aspects: (i) making explicit
the role of the Lagrange multiplier (also called adjoint state in this context) for hid-
den units and (ii) proposing a 2nd order local estimation mechanism. The relation
with other recurrent weights estimation methods is discussed in Appendix B.

Applying standard derivations, the criterion gradient writes:

∂εnt L = ˜xn(t) − xn(t)

∂xn(cid:48) (t(cid:48)) L = −εn(cid:48)t(cid:48) + ρ(cid:48)

n(cid:48)t(cid:48) + (cid:80)

nt,

t(cid:48) < t ≤ t(cid:48) + R
or t(cid:48) = t, n < n(cid:48)

βn(cid:48)t(cid:48)
nt εnt

∂Wnd L = (cid:80)

n(cid:48)(cid:48),Wn(cid:48)(cid:48)d=Wnd

(cid:80)

t φn(cid:48)(cid:48)dt εn(cid:48)(cid:48)t + ∂Wnd R

ρ(cid:48)
nt

def= ∂xn(t)ρ(· · · , xn(t), · · · )

φndt

def= Φndt (· · · , xn(cid:48)(t(cid:48)), · · · , im(s), · · · )

= ∂Wnd ˜xn(t)

βn(cid:48)t(cid:48)
nt

def= ∂xn(cid:48) (t(cid:48))φn0t + (cid:80)Dn

d=1 Wnd ∂xn(cid:48) (t(cid:48))φndt = ∂xn(cid:48) (t(cid:48)) ˜xn(t)

writing :






The sum (cid:80)

nt, t(cid:48) < t ≤ t(cid:48) + R or t(cid:48) = t, n < n(cid:48) encounters for previous values
and subsequent node values. This sum includes terms with βn(cid:48)t(cid:48)
(cid:54)= 0, i.e. terms
nt
for which there is a recurrent connection from the node of index n at time t onto
the node of index n(cid:48) at time t(cid:48). We simply write (cid:80)
in the sequel, without any
risk of ambiguity.
The sum (cid:80)

encounters for weight sharing, i.e., the fact that weights
from diﬀerent units may be constrained to have the same value. We will simply
write (cid:80)

n(cid:48)(cid:48) in the sequel, without any risk of ambiguity.

Let us now review and discuss how we can implement such a minimization.

n(cid:48)(cid:48),Wn(cid:48)(cid:48)d=Wnd

nt

The minimization steps

Forward simulation
The equation ∂εntL = 0 yields xn(t) = ˜xn(t). This simply means that xn(t) is
iven by the network equation, i.e., equation (1). Since ˜xn(t) depends on previous
values at time t(cid:48) < t, it provides a closed-form formula to evaluate xn(t) from
the beginning to the end. This simply corresponds to the fact that the dynamic
is simulated. This step depends on the weights Wnd but not on the Lagrange

RR n° 9100

10

Alexandre & Drumond & Hinault & Viéville

multipliers εnt. At the end of the step the equality ∂εnt L = 0 is obtained, and the
criterion value itself does not depends on ε since the constraints are veriﬁed. As a
consequence, the criterion value L can be calculated during this step.

The forward simulation complexity corresponds to the network simulation and
is of order O(N DT ) with a memory resources of O(N T ) since we must buﬀer the
calculated output, for subsequent calculations.

Backward tuning

The equation ∂xn(cid:48) (t(cid:48))L = 0 also provides a closed-form formula to evaluate εn(cid:48)t(cid:48) as
a linear function of subsequent values εnt, t > t(cid:48), so that the calculation is to be
done from the last time t = T − 1 backward to the ﬁrst time t = 0:

εn(cid:48)t(cid:48) = ρ(cid:48)

nt +

βn(cid:48)t(cid:48)
nt εnt.

(cid:88)

nt

(3)

This is the key feature of such a variational approach, allowing backward tun-
ing, i.e., take into account the fact that adjusting the system parameters for a
node n at time t is interdependent with the state of subsequent computations.

This makes the key diﬀerence with respect to usual approaches based on gradi-
ent back-propagation: Here the output error is back-propagated. This calculation
may be recognized as a kind of back-propagation, but it is mathematically dif-
ferent. This method is thus quite diﬀerent from back-propagation-though-time
recurrent network or other standard alternatives.

As mentioned by [17], βn(cid:48)t(cid:48)
nt

is nothing more than the ﬁrst order approximation
of the backward dynamics, technically the product of the weight matrix with the
system Jacobian.

This backward computation is local to a given unit in the sense that only
eﬀerent units (i.e., units this unit is connected to) are involved in the computation
of the related Lagrange parameter. This step depends on both weights and output
values, and the equality ∂εnt L = 0 is obtained at the end.

The backward tuning step has the same order of magnitude in terms of cal-
culation O(N DT ) and memory resources of O(N T ) (in fact of O(N R), because
the obtained result may be immediately re-used to compute the 2nd and 1st order
weight adjustment quantities, discussed in the sequel).

nt Bnt

Parameter interpretation. We obtain, from equation (3) after some algebra
εn(cid:48)t(cid:48) = (cid:80)
n(cid:48)t(cid:48) (not
made explicit here) which are unary coeﬃcient polynomial in βn(cid:48)t(cid:48)
. This made
nt
explicit the fact εn(cid:48)t(cid:48) is a linear function of subsequent errors, i.e., a backward
tuning error.

, with ﬁnite summations and for some quantities Bnt

n(cid:48)t(cid:48) ρ(cid:48)
nt

Inria

Backward tuning

11

If βn(cid:48)t(cid:48)

nt = 0, there is no dependency of xn(t) on xn(cid:48)(t(cid:48)), i.e. no recurrent con-
nection. If the unit has no recurrent connection, i.e. is a not a function of other
units, then εn(cid:48)t(cid:48) = ρ(cid:48)
In the
nt
nt = xnt − ¯ont is the output
least-square case (i.e.
error.

is simply related to the cost function derivative.
if ρnt = 1

2 (xnt − ¯ont)2), then ρ(cid:48)

Real-time aspects. Such a formulation is deﬁnitely not “real-time”, since we “go
back in time”. It is however, the only solution for hidden layers to be tuned, since
the output adjustment is a function of hidden activity in the past, the estimation
must thus take future information into account in order to properly adapat.

However, in a real-time paradigm, it must be noted that each computation
is also local in time: It only depends on values in a “near future” within a time
range equal to the system time range. In other words, at a given time we obtain
the value with a lag equal to system time-range. It is an interesting perspective
of this work to explore if, considering only a bounded window-time may provide
numerically relevant values for on-the-ﬂy backward tuning.

Numerical stability. This back-propagation of tuning error, may suﬀer from
the same curse than back-propagation of gradient, as reviewed in e.g., [29]: Either
error explosion (if |βn(cid:48)t(cid:48)
nt | > 1), or error extinction (if |βn(cid:48)t(cid:48)
nt | < 1). Based on this
remark, the key idea of LSTM [29] is to consider memory carousel (detailled in
Appendix A) to guaranty (cid:12)
(cid:12)
(cid:12) (cid:39) 1 and thus a stable back-propagation for at least
some recurrent link, but this means that the designer of the network architecture
has to consider such predeﬁned units, which is a strong constraint.

(cid:12)βn(cid:48)t(cid:48)

nt

In our case, since all kernels are contracting with max |∂xn(cid:48) (t(cid:48))φndt| = 1 we are
in a situation where the a-priory numerical conditioning is optimal. We also have
the bound, writing βmax

def= maxnt |βn(cid:48)t(cid:48)
(cid:12)
0 ≤ (cid:12)
(cid:12) ≤ βmax ≤ 1 + (cid:80)
(cid:12)βn(cid:48)t(cid:48)
without any thinner inequality in the general case. This means that we “must”
accept error potential explosion as soon as the weights values are not below one,
which can not be a manageable constraint.

d |Wnd|

nt | :

nt

To avoid backward explosion or extinction, we are going to introduce another
heuristic: We are going to bias the backward error given in equation (3). We
deﬁne:

εn(cid:48)t(cid:48) (cid:39) ρ(cid:48)

nt + g

βn(cid:48)t(cid:48)
nt εnt

,

(4)

(cid:32)

(cid:88)

(cid:33)

considering a function g(u), shown in Fig. 2. It is the identity function except for
small vanishing values that are raised using a simple quadratic proﬁle, an huge
values saturated by an exponential proﬁle, and providing a continuously derivable

nt

RR n° 9100

12

Alexandre & Drumond & Hinault & Viéville

Figure 2: The backward guard proﬁle, deﬁned in (5), with a bias for tiny values
and a saturation for huge values.

function. This design choice writes, for ﬁxed meta-parameters ω, α, ν:

g(u) def= sg(u)






ω − α e− |u|−ω
|u|
ν + u2
4 ν

α −1 ω − α ≤ |u|

2 ν ≤ |u| ≤ ω − α
|u| ≤ 2 ν,

(5)

where sg() is the sign function. To ﬁx these meta-parameters we consider the order
of magnitude of the output error:

¯ρ(cid:48) def=

(cid:80)

nt,ρ(cid:48)

nt

nt(cid:54)=0 ρ(cid:48)
nt(cid:54)=0 1 ,

(cid:80)
nt,ρ(cid:48)
and a reasonable choice to preserve the numerical conditioning is ν = 10−6 ¯ρ(cid:48) and
ω = 106 ¯ρ(cid:48), with e.g., α = 10−3 ω. They are very likely not to be adjusted because
they only correspond to order of magnitude of numerical calculation. We have
observed that using double precision ﬂoating numbers on a standard processor for
such kind of calculations corresponds to such rough numbers.

The 2nd order unit weight adjustment

We now have to estimate the weights W and are left with the last normal equation
∂Wnd L = 0 which is not an explicit function of the weights. On track is to use the

Inria

Backward tuning

13

gradient to minimize the criterion using a 1st order method, this is discussed in
the next sub-section. Interesting enough is the fact that we can also propose a 2nd
order method as made explicit and derived now. In other words, we reintroduce a
linear estimation of the weights assuming that the criterion is locally quadratic.

We thus propose to use the following 2nd order weight adjustment:

(cid:88)

n(cid:48)(cid:48)

bn(cid:48)(cid:48), d =

(cid:88)

Dn(cid:88)

n(cid:48)(cid:48)

d(cid:48)=1

An(cid:48)(cid:48), d d(cid:48) Wn(cid:48)(cid:48)d(cid:48)

(6)

writing, for some κnt:



bn, d

def= (cid:80)

t φndt (εnt + κnt (ˆxn(t) − φn0t)) + ∂Wnd R( ˆW),



An, d d(cid:48)

def= (cid:80)

t κnt φndt φnd(cid:48)t,

where:
- ˆxn(t) is best present estimate of xn(t),
- ˆW is the best estimate of W at the present step.

This allows us to obtain a new weight value W solving a linear system of
equation for each unit and the closest solution5 with respect to ˆW is considered.

The derivation6.

5Minimal distance pseudo-inverse. We consider:

minW ||W − ˆW||, b = A ˆW
which is directly obtained using the singular value decomposition of the symmetric matrix A =
U S UT :

W = ˆW + A† (b − A ˆW),

where A† is the pseudo-inverse of A.
—————————————————

6Deriving the 2nd order adjustment form. Let us omit the R() term and avoid consid-
ering weight sharing in this derivation, in order to lighten the notations. The complete derivation
would have obviously led to similar results.

Given a desired value estimate ˆxn(t), without loss of generality we can write, for some general

quantity κnt(W, x, ε):

yielding for ∂Wnd L:

L(W, x, ε) = (cid:80)

nt

κnt(W,x,ε)
2

(˜xn(t) − ˆxn(t))2 ,

(cid:80)

t κnt(W, x, ε) φndt (˜xn(t) − ˆxn(t)) + (cid:80)

t ∂Wndκnt(W, x, ε) (˜xn(t) − ˆxn(t))2/2 = (cid:80)

t φndt εnt.
For a simple least-square criterion, κnt ∈ {0, 1} depending on the fact that the desired output
¯on(t) is deﬁned or not, and it is straight-forward to verify in this particular case that the proposed
2nd order weight adjustment reduces to an exact linear system of equation, in the absence of
recurrent links of the given unit, since φndt is only function of the input. Otherwise, φndt is also
a function of both the network unknown output and hidden node values.
(i) In our case the output and backward error estimation is εnt, i.e., we can set ˆxn(t) def= ˜xn(t)−εnt
as a corrected value of the last estimate ˜xn(t). Given this hypothesis, it is obvious to verify that
κnt(W, x, ε) = 1 veriﬁes the equation.
(ii) A step further, for a general value ˆxn(t), a suﬃcient condition now writes:

κnt(W, x, ε) = 2 εnt/(˜xn(t) − ˆxn(t)),

RR n° 9100

14

Alexandre & Drumond & Hinault & Viéville

More sophisticated estimated values can be considered7.
The weight adjustment is local to each unit, providing a true distributed mech-
anism (unless if weight sharing is considered, because weights from diﬀerent units
are to be estimated together using the proposed equations). This corresponds to
a 2nd order minimization scheme. Each step requires O(N (DT + D3)- operation,
solving a linear system of equations. The O(N D3) is critical if the network connec-
tivity D is high, and this does not depend on the linear system resolution method
(e.g., SVD or Cholesky decomposition). The implemented method stands on the
singular-value-decomposition of the matrices An.

This oﬀers an alternative to 2nd order adjustment methods such as [39] or

other methods reviewed in [25].

In fact, a standard 2nd order adjustment can be derived in closed form8, di-
rectly from the 2nd order criterion derivatives.
It is not used here because the
computation involves not only the local node parameters, but also the connected
node parameters, and the calculation is rather heavy.

thus knt is proportional to εnt and must decrease with the prediction error increase. Considering
the case (i), it is straightforward to verify that if κnt is constant, and assuming ˆxn(t) is ﬁxed, we
obtain the related least-square linear equations given in (6).
—————————————————

7Improving the best estimate of the state value. The best estimate of the state value

ˆxn(t) given output values ¯on0(t) is not obtained by the simulation since ˆxn0 (t) (cid:54)= ¯on0(t).

If we consider the value obtained by simulation (i.e., the ˜xn(t) values), corrected by the error
estimate thus ˆxn(t) = ˜xn(t) − εnt, for a least-square criterion, it is easy to verify that this yields
ˆxn0(t) = ¯on0(t).

For output node value the ¯on(t) desired value could be enforced, limiting recurrent perturbation
and yielding φndt values closed to the ideal value, which is interesting in reverse-engineering
estimation, i.e. when an exact solution is expected [45], whereas a bias in the estimation is
otherwise expected, since hidden units simulated values and output values are not coherent.

A step further, we propose to retro-propagate the output value through the recurrent network,

given weights values ˆW, i.e., estimate:
(cid:80)
ˆxn(t) = arg minxn(t)M, M = 1
2

n,n≥N0 t(xn(t) − Φnt (· · · , xn(cid:48)(t(cid:48)), · · · ))2, xn0(t) = ¯on(t)

in words ﬁnd the state values for which the simulation errors yielding the desired output are
minimal. Considering the normal equation ∂xn(cid:48) (t(cid:48))M = 0 we obtain the recurrent equation:
n(cid:48) < N0
n(cid:48)(t(cid:48)) =
ˆxk
(t(cid:48)), · · · (cid:1)) N0 ≤ n(cid:48),

(cid:26) ¯on(cid:48)(t(cid:48))
Φnt

(t(cid:48)), · · · (cid:1) − (cid:80)

(cid:0)· · · , ˆxk−1

(cid:0)· · · , ˆxk−1

(t) − Φnt

nt βn(cid:48)t(cid:48)

nt

(ˆxk−1
n

i.e., the simulation value is corrected considering a backward propagation of the simulation error.
In fact, it is to verify that we implicitly solve a system of N T equations in N T unknowns,
the numerical scheme allowing to converge to a solution closed to the simulation values. This
has been numerically veriﬁed in the experimentation.

n(cid:48)

n(cid:48)

It has been implemented as an option in the software in order to help improving the convergence

of the recurrent weight adjustment.
—————————————————

8Calculating the standard 2nd order weight adjustment. The criterion Hessian, omit-

ting the regularization term and weight sharing to lighten the notations, writes:

Inria

Backward tuning

15

The 1st order unit weight adjustment
The calculation of ∂Wnd L allows us to propose a 1st order gradient descent ad-
justment of the weights, providing that ∂εntL = 0 after network simulation and
∂xn(cid:48) (t(cid:48))L = 0 after backward tuning.

It yields a Hebbian weight adaptation rule (as the sum of products between
an output unit error term εnt (combining the supervised error and the backward
tuning multiplier) and an input quantity φndt. This rule applies to both output
unit of index n < N0 with a desired output and hidden units of index N0 ≤ n
that indirectly adapt their behavior to optimize the output, via the backward
tuning values. The gradient calculation is local to a given unit and average over
time, through another O(N DT ) computation, unless weight sharing is considered.
In that case, this 1st order unit weight adjustment is either to be done globally
at the whole node set level, or locally for each unit, but with inter-unit weight
adjustment, not discussed here.

A step further, we can enhance this method considering the so-called momen-
tum gradient mechanism (based on a temporal averaging of the gradient values).
To this end we consider:

gk(t) = (1 − 1/k) gk(t − 1) + 1/k ∂Wnd L(t), k ∈ {1, 2, 4, 8, 16, 32}
in words, a 1st order exponential ﬁltering of the gradient value obtained at time t,
and the algorithm is going to compare these 6 options and choose the one with a
maximal criterion decrease (avoiding introducing a meta-parameter at this stage).
Here we mainly would like to explore several direction of descent if the criterion is






∂εntεn(cid:48) t(cid:48) L = 0
∂xn(cid:48) (t(cid:48))εnt L = βn(cid:48)t(cid:48)
∂Wndεn(cid:48) t(cid:48) L = δn=n(cid:48) φndt(cid:48)

nt

∂xn(cid:48) (t(cid:48))xn(cid:48)(cid:48) (t(cid:48)(cid:48)) L = H nt
∂Wndxn(cid:48) (t(cid:48)) L = J nd
n(cid:48)t(cid:48)
∂WndWn(cid:48) d(cid:48) L = 0,

n(cid:48)t(cid:48)n(cid:48)(cid:48)t(cid:48)(cid:48)

def= (cid:80)
def= (cid:80)

nt ∂xn(cid:48) (t(cid:48))xn(cid:48)(cid:48) (t(cid:48)(cid:48))
t εnt ∂xn(cid:48) (t(cid:48))φndt

(cid:16)

ρ() + φn0t + (cid:80)Dn

d=1 Wnd φndt

(cid:17)

writing βnt
nt

def= −1.
The 1st remark is that H nt

n(cid:48)t(cid:48) are not local to one node, whereas the summation
involves all nodes connected to the given one. Furthermore if ρ() is not a sum of local terms but
a statistical criterion H nt

n(cid:48)t(cid:48)n(cid:48)(cid:48)t(cid:48)(cid:48) is a function of the whole network.

Then the standard 2nd order scheme 0 (cid:39) ∇L + ∇2L δ(W, x, ε) writes in our case where

n(cid:48)t(cid:48)n(cid:48)(cid:48)t(cid:48)(cid:48) and J nd

∂εnt L = ∂xn(cid:48) (t(cid:48))L = 0:






(cid:80)

nt βn(cid:48)t(cid:48)
nt
(cid:80)
t φndt

δεnt + (cid:80)
δεnt +

(cid:80)
n(cid:48)(cid:48)t(cid:48)(cid:48) H nt
(cid:80)

n(cid:48)t(cid:48) βn(cid:48)t(cid:48)

nt

n(cid:48)t(cid:48)n(cid:48)(cid:48)t(cid:48)(cid:48)
n(cid:48)t(cid:48) J nd
n(cid:48)t(cid:48)

(cid:80)
δxn(cid:48)(t(cid:48)) +
d φndt
δxn(cid:48)(cid:48) (t(cid:48)(cid:48)) + (cid:80)
nd J nd
n(cid:48)t(cid:48)
δxn(cid:48)(t(cid:48)) + (cid:80)
t φndt εnt

δWnd (cid:39) 0nt
δWnd (cid:39) 0n(cid:48)t(cid:48)
(cid:39) 0nd,

and δεnt and δxn(cid:48)(t(cid:48)) can be eliminated in order to obtain a linear equation in δWnd. This
however requires the inversion the βn(cid:48)t(cid:48)
nt matrix (and its transpose), which is a O(N T × N T )
matrix, not necessarily sparse if the network is fully connected. We thus consider that the
resulting calculation is too greedy to be performed at each step of the minimization.
—————————————————

RR n° 9100

16

Alexandre & Drumond & Hinault & Viéville

not numerically regular.

This leads to a 1st order adjustment of the weights, i.e. it provides the direction
for the weight variation, not its magnitude.
In order to manage this issue we
very simply automatically adjust a step meta-parameter υk, initialized to any
reasonnable small value and:

- Calculates: ˜Wnd = ˆWnd − υk gk.
- Performs a rough line-search minimization minαk L(αk ˜W + (1 − αk) ˆW)
(here using the Brent-Dekker method with a 10−2 relative precision).
- Updates υk ← 2 αk υk.

In words we look for a weight value between both previous and new values that
decreases the criterion, and set the new step value to twice the last optimal value.
Each line-search step requires a simulation to compute L.

This is a bit heavy, but it is only a fall-back of the 2nd order adjustment
(e.g., for concave parts of the criterion). For the same reason, more sophisticated
methods such as conjoint gradient methods (taking into account several subse-
quent gradient directions in order to infer an approximate 2nd order minimization
method) have not been considered.

The complete weight adjustment

Collecting the previous steps the ﬁnal iterative weight adjustment writes

-1- Perform a forward simulation and a backward tuning, calculating the 1st
order gradient and 2nd order elements during the backward estimation.
-2.a- Perform a 2nd order weight adjustment.
-2.b- If it fails, attempt to perform a 1st order weight adjustment.
-3- Repeat -1- unless steps -2.b- fails.

The 2nd order adjustment also uses a line search, because our experimental ob-
servation is that the 2nd order estimation tends to overestimate the local minimum.
The 2nd order adjustment is not performed if the connectivity of the network is
too high since it has a cubic cost.

Though the algorithm can be implemented in a complete distributed frame-
work, in this preliminary study, the 2nd or 1st order adjustment is global, in order
to limit the number of iteration on a simple sequential machine. The complete
algorithmic structure is schematized in Fig. 3.

Inria

Backward tuning

17

Figure 3: The algorithmic structure of the estimation algorithm: A forward sim-
ulation yields the current criterion value, while the backward tuning allows us
to obtain the 2nd order and 1st order local weight adjustment elements. The
algorithm can be implemented in a complete distributed framework.

RR n° 9100

t = 0t = TForward simulationBackward tuningLocal weight adjustment18

Alexandre & Drumond & Hinault & Viéville

4 Experimentation

In this experimental part we study the numerical stability and limit of the method
considing toy benchmark problems. Supervised learning is targeted since it is a
direct way to evaluate the method eﬃciency and robustness. Let us remember
that we do not evaluate learning performances here, only the way we can adjust
recurrent network weights.

Software implementation

In order to provide so called reproducible science [54], the code is implemented as
a simple, highly modular, fully documented, open source, object oriented, easily
forkable, self contained, middle-ware, and is available here:
https://vthierry.github.io/mnemonas.
A minimal set of standard mechanisms (random number generation, histogram
estimation, linear system resolution, system calls) is used. The main part of the
implementation hierarchy is show in Fig. 4.

Figure 4: A view of the class-hierarchy: A Input simply provides a xn(t), n ∈
{0, N {, t ∈ {0, T { values, while a Transform provides such values given another
Input, while other objects deﬁned here derive from such an oversimple abstract
class, and are precisely deﬁned and discussed in Appendix A.

Regarding the estimations described in Appendix C, the KernelSupervisedEstimator

class implements, quadratic estimation, bounded and unbounded robust estima-
tion and Boolean estimation, while the KernelObservableEstimator class imple-
ments some basic stochastic models estimation.

For run-time performances and inter-operability with diﬀerent programming
languages a C/C++ implementation (with the compilation scripts) is proposed, the
wrapping to other programming languages (e.g., Python, available in the present
implementation) being straightforward, using e.g. swig.

Inria

Backward tuning

19

The ﬁrst experimental veriﬁcation is that it is quite simple to deﬁne the main
unit structures in Appendix section A from KernelTransform as claimed in the
paper, see Fig 5 for an example with AIF nodes and the code source for the LNL
LinearNonLinearTransform implementation and the SoftMax SoftMaxTransform
implementation.

Figure 5: The implementation of the AIF node, translating equation (9) into the
notation of equation (1) in the IntegrateAndFireTransform object.

Using reverse engineering

As being in a deterministic context, we are going to rely on a reverse engineer-
ing setup, in order to evaluate the performances and limit of the method. An
input/output learning sequence is going to be generated by a input/output root
network of ¯N units and another learning network with random initialization is
going to re-estimate the transform. This guaranties the existence of an exact
solution.

How relevant is it to use such a reverse engineering setup ? On one hand, sur-
prisingly enough perhaps, such networks (at least deep networks [61]) behave with
the same order of magnitude of performances, the input being either “meaningful”
or not, in the sense it represents data with a semantic or not. We thus can expect
simple random input/output tests to be relevant estimation of performance, even
for more semantic application. On the other hand, as developed in Appendix D,

RR n° 9100

20

Alexandre & Drumond & Hinault & Viéville

several “challenging” tests are in fact highly dependent on the chosen architecture,
with often trivial solution, as soon as the hidden architecture is well chosen. The
key point is thus to see if several kind of nodes can be adjusted with this mecha-
nism. For these reasons we have considered the reverse engineering paradigm as a
ﬁrst test.

In most of the cases, they are several solutions (e.g., in a linear case, up to a
permutation of the units, or some linear combination). We consider a root network
of ¯N units for a sequence of time T , for a M = 1 scalar random input, considering
either L (for linear), LNL, AIF or SoftMax units, with random weights (drawn
from a Gaussian distribution with 0 mean and σ (cid:39) 1/N standard deviation, which
is know to guaranty a stable non-trivial dynamic). Only the unit of index n = 0 is
considered as output units, i.e., N0 = 1, the N − 1 remainder units activity being
hidden to the estimation. This choice is related to the fact that the adjustment of
the hidden units weights is the key challenge.

In this deterministic case, we observe the following parameters: number of
steps to convergence, ﬁnal precision criterion value, and we also ﬁt an exponential
decay curve9 in order to estimate the decay time-constant and ﬁnal criterion bias.
Examples of results for diﬀerent kind of units are reported in Fig. 6, and two

typical criterion decay curves are shown in Fig.7.

No surprise, the method converges in each case, while performances depend
on the input and weight random draws. The reported result corresponds to the
observed variability, as illustrated in Fig. 8. We have never observed run where
the estimation fails.

A key point is that the convergence corresponds to an exponential decay proﬁle,
and the decay magnitude almost corresponds to the 2nd order adjustment, when
the criterion is locally quadratic, while 1st order fall-back mechanism is mainly cho-
sen by the algorithm in the other cases (e.g. concave criterion), again as expected.
We only observed that the 2nd order adjustment may generates weights that can,
mainly in the linear case, generates divergent sequences. Despite this caveat, the
optimization algorithm recovers by reducing the weight variation amplitude, thus

9Exponential decay ﬁt. The criterion value model to ﬁt is of the form:

c(t) def= α e−t/τ + β.
The time decay τ is ﬁtted in the least-square sense on log(c(t) − c(t − 1)) = k − t/τ , for k def=
log(α (1 − e1/τ )) and the bias β is ﬁtted, given τ , on c(t) = (c(t) − c(t − 1))/(e 1
τ − 1) + β. More
precisely, the least-square problems write:

min1/τ,k

(cid:80)T

t γT −t (k − t/τ − log(c(t) − c(t − 1)))2,

for an exponential window of width W = log(1−r)
this window (typically 90%), while the bias is estimated minimizing:

log(γ) where r is the fraction of data average within

minβ

(cid:80)T

t=1 δ0< ˆβ(t)

This kind of unit, in addition to NLN units, or LSTM units form the basic

components of deep-learning architectures [5, 18].

The 1st line is a ﬁrmware global equation19 which is a function of all units

value of the same layer.

We encounter such a construction in restricted Boltzmann machine (RBM)
(also using LNL network with the logistic sigmoid, but in a context of stochas-
tic activation of the units in this case) [5]. We mention this possibility for the

17 The relation with a max operator comes from the fact that:

xn(t) def= e

zn (t)
(cid:15)
zn(t)
(cid:15)

(cid:80)

n e

⇒ lim(cid:15)→0

(cid:80)

n xn(t) zn(t) = maxn (zn(t)) .

In words the softmax weighted sum of values approximates these values maximum.

18See, e.g., https://en.wikipedia.org/wiki/Softmax\_function.
19It is worthwhile mentioning that:

∂zn(cid:48) (t)on(t) = on(t) (δn=n(cid:48) − on(cid:48)(t)) ∈ [0, 1], δn=n(cid:48) =

(cid:26) 1 n = n(cid:48)

0

otherwise

,

thus numerically well deﬁned, with no singularity, the transformation being contracting, i.e.,
|∂zo| ≤ 1, with max |∂zo| = 1.

Inria

Backward tuning

31

completeness of the discussion, making explicit the fact that the present frame-
work includes such equation. However, the estimation problem addressed in RBM
completely diﬀers (as being a stochastic estimation paradigm) from the determin-
istic estimation considered here, the key diﬀerence being the fact we want relevant
results event on small data sets.

Other aspects of the proposed notation It is also straightforward to verify
that the reservoir computing equations [58] also ﬁt with this framework, as being
a particular of LNL network, since they simply correspond to a recurrent reservoir
of interconnected units, plus a read-out layer.

Since there is no restriction on the architecture, depending on the choice of
the kernels, it also can represent a two-layers non-linear network, or even better a
multi-layers deep network. The trick is simply to choose kernels corresponding to
the desired inter-layer and intra-layer connectivity.

A step further, in a given architecture, we can adjust both the number of layers
and the choice between one or another computation layer. This aspect if further
discussed in [21]. We also would like to consider not only a sequence of layers, but a
more general acyclic graph of layers, noticing that shortcuts can strongly improve
the performance thanks to what is called residual-learning [26]. Following [20],
the key-point is that we want to have this structural optimization as a parameter
continuous adjustment and not a meta-parameter combinatory adjustment. The
proposal is thus to consider an architecture with versatile layers where the choice
of the non-linearity is performed via a linear combination, obtained with sparse
estimation, thus acting as a soft switch. Furthermore, adding shortcuts allows to
deﬁne an adjustable acyclic graph with the output as supremum and the input as
inﬁmum. On the reverse, [20] points out that any acyclic graph can obviously be
deﬁned in this framework. Of course, we do not expect this method to generate
the best acyclic graph and combination of modules, but to improve an existing
architecture by extending usual optimization to the exploration of structural al-
ternatives.

RR n° 9100

32

Alexandre & Drumond & Hinault & Viéville

B Comparison with related recurrent weight esti-

mation methods

In this section we brieﬂy discuss how this method compares with existing methods
of recurrent weight methods estimation.

The back-propagation through time (BPTT) is a gradient-based technique
used, .e.g., in Elman’s Networks [19], where the standard back-propagation al-
gorithm is applied to both the network recurrent layers and through time. It is
based on the propagation of the error gradient, and it generally remains on two
assumptions that the cost is additive with respect to training examples and that it
can be written as a function of the network output (see, e.g., [40]). With respect
to this basic method, our method:
- does not rely on the cost gradient propagation, but the error backward propaga-
tion (or tuning), while gradients remain local to a unit.
- has been stated including for non additive costs (such as statistical criteria) and
for both supervised criterion based on the network output error, or other unsuper-
vised criteria.

Our formulation has been formalized, by, e.g.

[17], but without proposing a
second order estimation method, considering explicitly the backward tuning of
the error with a heuristic to avoid extinction and explosion. Moreover, the fact
this formalism has been applied on the formulation propose in section 2 with
intermediate variables makes the backward tuning proposal more eﬃcient, than if
non linearity and weights linear combination have been mixed.

Furthermore, as made explicit in [59] when comparing back-propagation with
contrastive Hebbian learning, or in [17], our backward tuning mechanism corre-
sponds gradient back-propagation up to a change of variable. However contrary
to [59] or [29], there is no need to introduce further approximation (such as, e.g,
only considering diagonal terms) in order to write the backward propagation rule.
This variant is well-founded, simpler to write and seems to be numerically more
stable.

A step further, artiﬁcial neuron network back-propagation has been related to
biological back-propagation in neurons of the mammalian central nervous system
(see, e.g., [51]) and it is clear that the propagation of a learning or adaptive
error, is more likely to be related to backward tuning of an error, than an energy
or criterion gradient minimization. Regarding biological plausibility, our method
only involves local distributed adjustments, as a version of back-propagation that
can be computed locally using bi-directional activation recirculation [27] instead
of back-propagated error derivatives is more biologically plausible, and has been
improved by [41]. In its generalized form it also communicates error signals, being
inspired by contrastive learning, and using the Pineda and Almeida algorithm [43].

Inria

Backward tuning

33

All these methods operate on the current estimate of the derivative of the error,
not the backward tuning error deﬁned here, while related to speciﬁc cost function.
The proposed method also enjoy an interesting interpretation related to the
2nd order estimation method, as made explicit in footnotes7 and 6. Thanks to
the simple formulation, and either from the backward tuning of the estimation
error in the case of footnote7 or by direct estimation in the case of footnote 6
we obtain an estimation not only of the output desired value, but also of hidden
state desired value. This corresponds to a deterministic estimation / minimization
algorithmic scheme : estimation of the desired hidden state value, given the current
weight values followed by the local minimization of the criterion adjusting the unit
weights.

As it, even if in relation with the usual standard back-propagation method, the

proposed method is a real alternative.

RR n° 9100

34

Alexandre & Drumond & Hinault & Viéville

C Using this framework in diﬀerent contexts

In this section we make explicit mechanism of estimation that can make use of the
previous variational mechanism.

Considering a supervised learning paradigm.

If we focus on a supervised learning paradigm, we consider learning sequences of
size T with desired output ¯o(t), 0 ≤ t < T , corresponding to the input ¯i(t), in
order to adjust the weights.

This setup includes without loss of generality the possibility to use several
epochs (i.e., several sequences): They are simply concatenated with a period of
time with state reset at the end of each epoch, in order to guaranty to have
independent state sequences, see Fig. 12).

Figure 12: If the supervised learning is performed with diﬀerent epoch of data,
this is equivalent to a unique epoch, providing a reset segment of length R, the
maximal recurrent range, is inserted before each new epoch. During reset segment,
we set κnt = 0.

Least-square adjustment

With respect to desired output ¯on(t) we can write, in the L2 case:
ρnt(ˆxn(t)) = κnt

2 (ˆxn(t) − ¯on(t))2

On one hand, we choose κnt > 0 if ¯on(t) is deﬁned (output node) and κnt = 0
otherwise (hidden unit, missing data, or segmentation of the sequence in diﬀerent
epochs, while since κnt ∈ [0, +∞[ it can also act as error gain, taking related
precision into account.

Robust criterion

One aspect of the estimation is related to robustness, i.e., being able to take into
It is
account the fact that errors and artifacts may occur in the learning set.

Inria

Backward tuning

35

implemented here as a M-estimator, i.e., not a least-square function but another
alternative cost function, with a smaller slope for higher values, as made explicit
in Fig 13. This has been addressed, e.g, by [14].

With respect to usual M-estimators20, we propose here to use reweighted quadratic

criterion, i.e., consider a previous estimation ˆx of x, in order to locally work at
each step with a least-square criterion, namely:

1ρnt(x) ≡

x2
ν + |ˆx|

or 0ρnt(x) ≡

x2
ν + ˆx2 ,

for unbounded and bounded proﬁles. This is equivalent to use an approximate
criterion derivative.

For small values of ν the criterion allows to perform sparse estimations. Any-
way, the value of ν is not to be adjusted manually, but can simply be set at a
fraction of the criterion minimal value, say ν (cid:39) 10−3 ρmin.

13:

Figure
Two
bounded proﬁle, molliﬁcation of
L2

criterion for

low values,

examples

the L1
and to a L1

of M1

proﬁles.

criterion,

Leftward,
thus

un-
a
criterion for high values:

close

an

to

ρnt(x) ≡ x2

ν+|x| = x2/ν + O(|x|3) = |x| − ν + O(1/|x|),

for ν = 1, 2, 4, 8. Rightward, a bounded proﬁle, molliﬁcation of the L0 criterion:

ρnt(x) ≡ x2

ν+x2 = x2/ν + O(x4) = 1 + O(1/x2),

for ν = 1, 2, 4, 8.

20For instance, for a bounded criterion a usual choice is the twice continuous diﬀerentiable
biweight proﬁle ρnt(x) ≡ (x6 − 3 x4 + 3 x2) H(1 − |x|) + H(|x| − 1), with a hard threshold at
xthres = 1.

RR n° 9100

36

Alexandre & Drumond & Hinault & Viéville

Boolean adjustment

Another aspect of the estimation is the fact that we may have to estimate Boolean
value, i.e., with the notations of this paper, requires the values ˆxn(t) to be lower
or higher than 1/2, according to ¯on(t). Taking a margin ν into account, and
considering a quadratic criterion, a natural choice writes:
2 q (ν − sg(¯on(t) − 1/2) (ˆxn(t) − 1/2)) ,

where sg() is the sign function, which is a criterion that vanishes if and only if
|ˆxn(t) − 1/2| > ν and has the right sign, while it behaves as a quadratic criterion
otherwise.

q(v) def= H(v) v2

ρnt(ˆxn(t)) = κnt

Stochastic adjustments

A step further, we may not be interested to perform a deterministic adjustment,
but to optimize the output probabilistic distribution with respect to the desired
output distributions, as e.g., in [16] for spiking neuron networks adjustment. As
developed in appendix E, the weight estimation problem can be related to the
following criterion:

ρ(x) def= (cid:80)

k |λk|

¯Ωk − 1
T −τk

(cid:80)

t ωk(t)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ,

for some observable ωk(t), with average value ¯Ωk, and parameters λk made explicit
in appendix E and considered here as input.

We may for instance consider mean and auto-correlation as in appendix E, or
instantaneous momenta at time t, e.g., mean and variance, considering a Gaussian
distribution.

Considering static estimation.

The present framework stands for dynamic estimation of a temporal sequence. It
can also simply be applied to a static estimation at the ﬁnal time step T −1 consid-
ering ¯on(T −1) only the previous values on(t) being unconstrained. In that case the
value T corresponds to the number of iteration to obtain the desired estimation.
In a non-recurrent architecture this value is easy to derive from the architecture,
it corresponds to the number of computation steps. In a recurrent architecture,
the situation is more complex since computation loops have to converged, and
the number of computation steps is an explicit parameter, unless the system is
tuned to converge to a ﬁxed point, while considering T → +∞ which is a rather
straightforward extension of the present work.

Inria

Backward tuning

37

Considering constrained architecture and weights values.

It is precious to also introduce constraints on the connection weights. Typical
constraints include:
- sparse connectivity, which reduces the total amount of computation, and allows
internal sub-assemblies to emerge,
- positive or negative weight values (corresponding to excitatory or inhibitory
connections).

The design choice of the kernels allows us to constraint the network connectiv-
ity. It is possible to specify partial connectivity allowing to distinguish diﬀerent
layers (e.g. hidden layers not connected to input and/or output). This may be,
for instance, a 2D-topography with local horizontal connections, or several layers
with, e.g., either point to point, or divergent connectivity between layers.

However, if the architecture itself has to be learned, the present framework may
be used in another way: Starting from a given connected network and performing
a sparse estimation, may lead to a result with zero weight values for connections
not present in the estimated architecture, and non zero values otherwise. This is a
sparse estimation, i.e. not only minimizing the metric not only with respect to the
weights values, but also with respect to the fact that some weights have either zero
or non-zero values, i,e, with respect connection sets. Sparse estimation methods
(see e.g. [55, 56] for a didactic introduction) can be used to this end.

One application could be modulatory weighted connections, allowing to en-

hance or cancel sub-parts of the network connectivity.

One track is to simply choose, for some meta-parameters νnd:
R(W) = (cid:80)

νnd
(cid:15)+| ˆWnd|

W 2
nd

nd

where ˆWnd stands for the best a-priory or previous estimation of the weight. This
leads to a reweighted least-square criterion, where small weights value minimiza-
tion is reinforced, up to 0, yielding sparse estimation.

The case where we consider excitatory or inhibitory connections (i.e., weight
values that only positive or negative), or the case where the weights are bounded,
is managed at the implementation level, as a hard constraint in the minimization.
Very simply, if the value is beyond the bound it is reprojected on on the bound.
This may lead to a sub-optimal estimation, but avoids the heavy management of
Karush-Kuhn-Tucker conditions.

As an example, let us consider the adjustable leak γnt, 0 ≤ γnt ≤ 0.99 (cid:39) 1 of a
NLN unit. If the minimization process yields a negative value, the value is reset
to zero (it means that we better have no leak). If the minimization process yields
an unstable value higher than one, it is reset to, say, 0.99 to be sure the system
will not diverge.

RR n° 9100

38

Alexandre & Drumond & Hinault & Viéville

Considering un-supervised regularization.

In order to ﬁnd an interesting solution, we have to constraint the hidden activity to
be estimated. Interesting properties includes sparseness, orthogonality, robustness
and bounds.

Sparse activity (i.e., with a maximal number of values closed or equal to zero),
which is known to correspond to unit assemblies tuned to a given class of input
statistics, can be speciﬁed as a reweighted least-square criterion again, for some
meta-parameters κnd:

ρnt(xnt) = κnd

(cid:15)+|ˆxnt| x2

nt

where ˆxnd stands for the previous estimation, with an initial value equal to κnd, as
previously discussed.

Orthogonality of hidden unit activities, in order to avoid redundancy and max-
imize the dynamic space dimension in the recurrent network, can also be speciﬁed,
the same way as :

(cid:80)
again as as, now not local but global, reweighted least-square criterion, now mini-
mizing the dot products between unit activities, thus minimal when orthogonal.

ρnt(xnt) = κnd

t xnt ˆxn(cid:48)t)2

n(cid:48)(cid:54)=n((cid:80)

Another aspect concerns the fact we may have to control the activity bound,
e.g., a weak constraint of the form xnt (cid:22) b. Following the same heuristic, we may
introduce a cost of the form:

ρnt(xnt) = κnd ek (xnt−b)
with k > 0 in order to have a fast increasing function as soon as the bound is
violated.

Inria

Backward tuning

39

D Closed forms solution for neural network tasks

Let us illustrate how the type of used units has a strong inﬂuence on the diﬃ-
culty of the task. Here we consider deterministic tasks only. The remark is that
tasks considered as quite complex [29, 24, 39] for certain architectures are trivial
for others. In particular, the use of AIF neurons simpliﬁes certain problems, e.g.
requiring long short-term memory. We illustrate this point here considering de-
terministic sequence generation and long-term non-linear transform, and provide
explicit simple solutions for those problems.

Generating long term sequential signals

The lever is that it is straightforward to generate a delayed step signal (i.e., equal
to 0 before t = τ , and 1 after) using AIF units, e.g.:

sτ (t) = 1

2 (1 − Υ(sτ (t − 1))) sτ (t − 1) + hτ

with

hτ

def=

1

1−2

(cid:16)

4

2 −τ (cid:17) ∈ [h∞ = 1/4, h1 (cid:39) 0.85],

1

for which we easily obtain21 Υ(sτ (t)) = δt≥τ .

The numerical limit of this method is the fact that for huge value of τ the
parameter precision must be of order O (2−τ ). To avoid this constraint, either
an architecture with several units building a delay line, or with a ramp unit and
adaptive thresholds (see next section) can be considered.

From this basic element we can generate a delayed clock signal22 or another
long-term mechanism, such a as ﬂip-ﬂop23, which is a fundamental building blocks

21Delayed step signal. Starting with sτ (0) = 0 this ﬁrst order recurrent equation yields:
sτ (t) = 2 h (1 − 2−t) ∈ [0, 2 h],
which is an bounded increasing negative exponential proﬁle, for which the parameter h has been
chosen to maintain sτ (t) < 1/2, t < τ , and reach sτ (t) > 1/2, for t ≥ τ .
—————————————————

22Delayed clock signal. Modifying the delayed step signal, and adding a memory carousel

unit in order to reset the signal after the step and keep it reseted, we obtain:

cτ (t) = 1
dτ (t) = dτ (t − 1) + Υ(cτ (t − 1)),

2 (1 − Υ(cτ (t − 1))) cτ (t − 1) + hτ (1 − Υ(dτ (t − 1)))

with Υ(cτ (t)) = dτ (t) = 0, t < τ , until cτ (τ ) > 1/2, As a consequence dτ (τ + 1) = 1, thus
cτ (τ + 1) = 0, which is a stable ﬁxed point, values remaining constant beyond. Finally we obtain
Υ(cτ (t)) = δt=τ in this case.
—————————————————

23Deﬁning a ﬂip-ﬂop latch. Let us deﬁned a SR-latch (i.e., a ﬂip-ﬂop) with:

yielding the following behavior:
- R-state: If i0(t) < 1/2 and i1(t) < 1/2 (no-input) and z(t − 1) < 1/2, then z(t) = 0 < 1/2, the

z(t) = Υ(z(t − 1)) + Υ(i1(t)) − Υ(i0(t))

RR n° 9100

40

Alexandre & Drumond & Hinault & Viéville

of any digital transform, in conjunction with logic gates such as a xor gate24.

If we consider a molliﬁcation instead of a step function (i.e., replacing Υ withΥ(cid:15)
in the previous equation), we obtain the behavior for suﬃciently large slopes. More
precisely25, for instance, we numerically observed the same qualitative behavior in
the delayed step signal case, with h ∈ [h∞ = 0.376, h1 = 0.5], while h is not given
in closed form in this case.

Further on this track, it is clear that we can compile any sequential circuit in
such networks, which is far from being new. The add-on here is about that the
fact we provide explicit solutions, using AIF neurons, with a lower complexity in
terms of network nodes than using LSTM units. Let us see two paradigms where
this enlighten the problem complexity.

Long term non-linear transform

In many experiments, a variant of a sequence of the form:

time :
input:
output:

0
a
∗

1
b
∗

∗
∗

· · ·
· · ·

T
∗
∗
∗ a b

reset state is maintained.
- S-state: If i0(t) < 1/2 and i1(t) < 1/2 (no-input) and z(t − 1) > 1/2, then z(t) = 1 > 1/2, the
set state is maintained.
- R-S transition: If i0(t) < 1/2 and i1(t) > 1/2 and z(t − 1) < 1/2, then z(t) = 1 > 1/2, ﬂipping
to a set state; if it was already in the set state, we still have z(t) = 2 > 1/2.
- S-R transition:If i0(t) > 1/2 and i1(t) < 1/2 and z(t − 1) > 1/2, then z(t) = 0 < 1/2, ﬂipping
to a reset state; f it was already in a reset state, we still have z(t) = −1 < 1/2.
- no instability: i0(t) > 1/2 and i1(t) > 1/2 contrary to a standard digital RS-latch we simply
have z(t) = Υ(z(t − 1)) providing it was in set of reset state, without any meta-stability.
—————————————————

24Deﬁning the xor function. It is straightforward to notice that:

veriﬁes

x•(t) = Υ(xa(t − 1)) + Υ(xb(t − 1)) + −2 Υ(xo(t))
xo(t) = Υ(xa(t − 1)) + Υ(xb(t − 1)) − 1

Υ(xo(t)) = Υ(xa(t − 1)) and Υ(xb(t − 1))
Υ(x•(t)) = Υ(xa(t − 1)) xor Υ(xb(t − 1))

,

while other logic gates are easy to build in a similar manner.
A step further the expression

x†(t) = 1/2 − 2 (Υ(xa(t − 1)) − 1/2) (Υ(xb(t − 1)) − 1/2)
now considering a multiplication unit, directly calculates the xor function, but does no correspond
to some AIF unit.
—————————————————

25This is obtained, e.g., by the following piece of maple code:

upsilon := (u) ->

1/(1+exp(-4\*(u - 1/2)/epsilon)):
c\_n := c -> (1 - upsilon(c)) \* c / 2 + h:
bounds := [solve(c\_n(1/2) = 1/2, h), solve(c\_n(0) = 1/2, h)];

—————————————————

Inria

Backward tuning

41

where a and b are variable input, ∗ are random distractors and a b the desired
delayed output (here a product, but it could be another calculation). Such setup
combines several non-trivial aspects, long short term memory, distractor robust-
ness, and operation which may not explicitly hardwired in the network, presently
a product. The LSTM approach was shown to be particularly eﬃcient for such
computation, because of the notion of “memory carousel”.
In fact, the explicit
implementation of such a mechanism on the given example is trivial26.

What do we learn from this very simple development? While authors have
already made explicit the fact that such computations rely on “gate unit” and
“memory unit”, it seems that “delayed unit” (i.e.
learning a time delay) are also
basic components. It is also an example of how deterministic computations might
become simple, if we introduce a-priory information on the computation, via ded-
icated units.

Deterministic sequence generation

What is the complexity of the task of generating a deterministic time sequence
¯on(t), n ∈ {0, N0{, t ∈ {0, T {, with a recurrent network of N ≥ N0 units of range
R? This could be an unpredictable sequence, without any algorithm to generate
it, unless copying all sample (i.e., with a maximal Kolmogorov complexity).

On one hand, O(N0) independent linear recurrent units of range R = T , solves
the problem of generating an exact sequence of N0 T samples, in closed form27.
This solution requires a very large recurrent range, and the numerical precision is
limited by the fact that errors accumulate along the recurrent calculation.

On the other hand, feed-forward units of range R = 1 solve explicitly the

26An example of long term computation. One solution writes:

o0(t) = (1 − Υ(cT (t)) i(t) + (Υ(cT (t)) − 1) xa(t) xb(t)+
xa(t) = (1 − Υ(c0(t)) xa(t − 1) + (Υ(c0(t)) − 1) i(t)
xb(t) = (1 − Υ(c1(t)) xb(t − 1) + (Υ(c1(t)) − 1) i(t)

while cτ (t) = δt=τ are clock signals, as deﬁned previously, and it is easy to verify that xa(t)
“opens” the memory at time t = 0, and stores the previous value otherwise, with a similar
behavior for xb(t), while o0(t) simply mirror the input until t = T , where the expected result
is output. Obviously, these are no more AIF units but introduce multiplications between state
values

27Long range sequence generation. Let us consider units of the form:

xn(t) = (cid:80)d=T −1

d=1 Wnd xn(t − d) + Wn0,

thus with N0 T weights. Since xn(t) = 0, t < 0, providing ¯on(1) (cid:54)= 0, we immediately obtain
Wn0 = ¯on(1) and for d > 0:

Wnk = (¯on(k + 1) − Wn0 − (cid:80)d=k−1

d=1 Wnd ¯on(t − d))/¯on(1),

providing that ¯on(1) (cid:54)= 0, thus a closed-form solution. If ¯on(1) = 0 we simply have to generate
the sequence, say, ¯o(cid:48)
n(t) − 1, using
now an additional node.
—————————————————

n(t) = ¯on(t) + 1 and add a second unit of the form xn(t) = x(cid:48)

RR n° 9100

42

Alexandre & Drumond & Hinault & Viéville

problem using T clock units and N0 readout units, with O(N0 T ) weights. This
requires no more than N0 + T units considering binary information28, and no
more that N0 + 1 units if the numerical precision is suﬃcient and unit threshold
def= (cid:112)N0 T /R linear or NLN
adjustable29. A step further, considering less than N•
units of range R, we can not generate a solution in the general case30.

The generation of periodic signal of period T , is a very similar problem, as
studied in [45], for N = N0. In a nutshell, we simply must add equations such
that x(T ) = x(0) to guaranty the periodicity.

From this discussion, we see that the complexity of signal generation problem
highly depends on the kind of “allowed units” and reduces to a trivial problem as
soon as suitable operation are allowed. Furthermore, there exist a R = 1 network
of at most N0 + T units that exactly solves the problem, without requiring huge
precision, while a linear network, a NLN network or a AIF network can generate
such a sequence in the general case, with either a closed form solution, or solving
a linear system of equation.

28Long sequence generation with delay lines. Let us consider N0 readout units and T

clock units of the form:
xn0 (t) = (cid:80)T −1
xN0 (t) = 1

n=0 (¯on0(n) − ¯on0(n + 1)) Υ(xN0+n(t))

2 (1 − Υ(xN0 (t − 1))) xN0 (t − 1) + h1

xN0+n(t) = xN0+n−1(t − 1)

0 ≤ n0 < N0, writing ¯on0 (T ) def= 0

0 < n < T

thus providing T delayed step signals such that Υ(xN0+n(t)) = δt>n, allowing us to generate
the desired sequence combining these signals. If we now consider molliﬁcation of he threshold
function, the previous system of equation is going to generate a temporal partition of unity.
Since xN0+n are simple shifts of xN0, the clock units obviously span the output signal space and
output units can easily linearly adjust there related combination to obtain the desired values.
—————————————————

29Long sequence generation with a ramp unit. If we can consider units of the form:

xn0 (t) = (cid:80)T −1
xN0 (t) =

n=0 (¯on0(n) − ¯on0(n + 1)) Υ(xN0+n(t) − θn)
xN0(t − 1) + 1

with the ramp unit xN0(t) precision being of order O(1/T ), while we now can introduce adaptive
thresholds θn = n, it is obvious to verify that we solve the problem with two units.
—————————————————

30Long sequence generation with fully connected network. Considering the linear

network system:

xn(t) = (cid:80)m=N

m=1 Wnmr

(cid:80)r=R

r=1 xm(t − r) + Wn0,

with 0 ≤ n0 < N0 output units and N0 ≤ n < N hidden units, using vectorial notations, with
the shift operator S deﬁned as Sx(t − 1) = x(t), we obtain:
(cid:18) ¯o
¯x

(cid:18) ¯o
¯x

N0 T (cid:108)
(N −N0) T (cid:108)

+ W0

= W

where ¯o are the desired output. It is a bi-linear system of N T equations in N 2 R+N independent
unknowns, i.e., the weights, while the (N − N0) T hidden values are entirely speciﬁed as soon as
the weights are given. In terms of number of degree of freedom we can not have N 2 R+N < N0 T
for this algebraic system of equation to have a solution in the general case.

(cid:19)

(cid:19)

S

Inria

Backward tuning

43

E Stochastic adjustment of the network weights

Let us consider the problem of optimizing the probabilistic distribution ˜p(˜x =
x) of a network output, as a function of the desired distribution ¯p(¯o = o) of a
root network. Since we are in a multi-dimensional and dynamic framework, with
continuous values, it is intractable to consider as it the distribution, but only a
parametric model of it, and adjust the parameters of this model.

An illustrative example

Let us, for instance, consider that it is important that the network output mean
˜Ωn,• and auto-correlation in a τ = {0, ∆} time window ˜Ωn,τ , correspond to some
desired values:
¯Ωn,•
¯Ωn,τ

def= ¯on(t)
def= ¯on(t) ¯on(t − τ ),

ωn,•(t)
ωn,τ (t) ωn,τ (t)

1
T −τ
the normalized temporal auto-correlation being:

t=0 ωn,•(t),
(cid:80)T −τ −1
t=0

def= 1
T
def=

(cid:80)T −1

We thus do not constraint the output desired values directly but only some mo-
menta expectation.

Cn,τ = ( ¯Ωn,τ − ¯Ω2

n,•)/( ¯Ω0,τ − ¯Ω2

n,•).

Beyond this example, we thus consider observable ωk(xn(t) · · · xn(t − τk)) of a
given rank τk and their expectation on the desired distribution Ωk. We could also
have considered higher order momenta, e.g., consider mean, standard-deviation,
skewness and kurtosis, or spatial correlations, and so on.

Considering a general model

Adapting the development given in [57] for binary distribution, we propose to mini-
mize the KL-divergence, considering maximal entropy Gibbs distributions. We are
going to propose to adjust the network weights in order to minimize an approxi-
mation of the KL-divergence between the desired and simulated distribution.

If we look for a distribution of probability with maximal entropy and which

observable ωk correspond to some expectation values Ωk, we obtain:
p(x) =
where the denominator guaranties (cid:82)
x p(x) = 1 and is called the partition func-
tion31, topological pressure or free energy. The quantity Zp(λ) has no closed form
beyond simple cases, and can be numerically estimated as:

k λk ωk(x))
Zp(λ)

exp((cid:80)

,

Zp(λ) = (cid:82)

x exp ((cid:80)

k λk ωk(x)) (cid:39) 1

T −τ

(cid:80)T −τ

t=0 exp ((cid:80)

k λk ωk(t))

31Maximal entropy distribution. Given expectation Ωk of observable ωk(t) we state that
we look for a probability distribution of maximal entropy which corresponds to the observable
expectation. This writes, with Lagrangian multipliers λk:

RR n° 9100

44

Alexandre & Drumond & Hinault & Viéville

, under the ergodic assumption, τ being chosen for all observable ωk(t) to be
deﬁned.

Fitting a Gibbs distribution

A step further, it appears that minimizing the KL-divergence between the observed
distribution ¯p(¯o) and the Gibbs model corresponds to adjust the parameters ¯λ in
order the predicted observable expectation Ωk(λ) to get as closed as possible to
the desired observable expectation ¯Ωk, which is a standard estimation problem (in
a nutshell, the trick is to minimize the criterion gradient, not the criterion itself32).

(cid:90)

minλ

p(x) log(p(x))

+ λ0

(cid:18)(cid:90)

(cid:19)

p(x) − 1

−

(cid:18)(cid:90)

(cid:88)

λk

p(x) ωk − Ωk

x

x

(cid:123)(cid:122)
(cid:123)(cid:122)
entropy
normalization
and the functional derivative of this criterion yields:

(cid:124)

(cid:124)

(cid:125)

(cid:125)

k

(cid:124)

x

(cid:123)(cid:122)
observations

k λk ωk(x)) /Zp(λ),
as easily obtained from the normal equation derivation, see e.g.:

p(x) = exp ((cid:80)

(cid:19)

(cid:125)

https://en.wikipedia.org/wiki/Maximum\_entropy\_probability\_distribution#Proof.

—————————————————

32Fitting the Gibbs parameters distribution. For the sake of completeness, let us detail
how such estimation can be performed. If we consider the KL-divergence between the observed
distribution ¯p(¯o) and the model approximate distribution q(x)), we easily derive:

dKL(¯p(¯o)(cid:107)q(x)) = (cid:82) ¯p(¯o) log

(cid:17)

(cid:16) ¯p(¯o)
q(x)

= (cid:82) ¯p(¯o) log (¯p(¯o)) − (cid:82) ¯p(¯o) log (q(x))
= −h¯o − (cid:82) ¯p(¯o) log (q(x))
= −h¯o − (cid:82) ¯p(¯o) ((cid:80)
= −h¯o − (cid:80)

k λk ¯Ωk + 1 log(Zq(λ))

k λk ωk − log(Z(λ)))

def= − (cid:82) ¯p(¯o) log(¯p(¯o)) is the observed
combining the previous equations, and since the term h¯o
entropy and is constant with respect to the parameter to estimate, we are left with the following
criterion, which in fact corresponds to cross-entropy maximization minλ J , with:

writing ωkl(t) = ωk(t) ωl(t), and Ωkl = E[ωkl]. This computation comes from the fact that:

J = log (Zq(λ)) − (cid:80)

k λk ¯Ωk

∂λk J = Ωk(λ) − ¯Ωk

∂λk λl J = Ωkl(λ)

Zq(λ) = (cid:82)
∂λk Zq(λ) = (cid:82)
= (cid:82)
= Zq(λ) Ωk(λ),

x exp ((cid:80)
x exp ((cid:80)
x Zq(λ) q(x) ωk(x)

k λk ωk(x))
k λk ωk(x)) ωk(x)

and it is easy to approximate:

under the ergodic assumption.

Ωl(λ)

def= (cid:82)
(cid:39)

x

exp((cid:80)

k λk ωk)

Zq(λ)
(cid:80)

1
T −τl

t ωl(t)

ωl(t)

As a consequence, despite the caveat that Zq(λ) calculation is usually not tractable, this allows
us to implement some paradigm that tends to minimize the criterion gradient (since at a criterion
minimum, the gradient vanishes):

¯λ = arg minλ|Ωl(λ) − ¯Ωk|.

Inria

Backward tuning

45

As a consequence, given a desired output ¯o and a choice of observable ωk we can
estimate the maximal entropy parameters ¯λ.

Statistical weight adjustment from the parametric model

Given set of desired observable values ¯Ωk, with the corresponding Gibbs model ˆp(¯o)
parameterized by ¯λ and adjusted on the reference samples ¯o, we now can state the
problem of adjusting the network weights. We consider the KL-divergence between
the observed distribution ¯p(¯o), approximated by the related Gibbs model, and the
network simulation ˜pW(˜x), parameterized by the network weights W. The network
is viewed here as a parametric model of the observed distribution.

Since the network simulation is brought to the desired reference samples distri-
bution, modeled as a Gibbs distribution, we are going to assume that the network
simulation can itself be represented by a Gibbs distribution:

(cid:16)(cid:80)
k
yielding, using similar algebra as before:

˜p(˜x) (cid:39) exp

(cid:17)
˜λk ωk(x)

/Z˜p(˜λ),

dKL(¯p(¯o)(cid:107)˜p(˜x)) = (cid:82) p(¯o) log
(cid:39) (cid:82) p(¯o) log
= (cid:80)

(cid:17)

(cid:17)

(cid:16) ¯p(¯o)
˜p(˜x)
(cid:16) ˆp(¯o)
˜p(˜x)

k(¯λk − ˜λk) ¯Ωk + log(Z˜p(˜λ)/Z¯p(¯λ))

with the goal to adjust the weights in order the related ˜λ to minimize this diver-
gence. As before, we can replace the KL-divergence minimization by the minimiza-
tion of the gradient magnitude. This design choice is valid because the topological
pressure is convex with respect to λ, so that the criterion is convex [57]. As a
consequence, the criterion is minimal when the gradient magnitude vanishes, i.e.
is minimal too, while the criterion decreases with the gradient magnitude, thanks
to being a convex criterion.
The gradient writes ∂˜λk

dKL(¯p(¯o)(cid:107)˜p(˜x)) = ˜Ωk(˜λW) − ¯Ωk, and we propose to

One example of algorithm writes:

Input : The desired observable values ¯Ωk and the distribution samples ¯o.
Output : The estimated ¯λk.
- Starts with λ0 = 0 and a regularization parameter υ = 1.
- At a given iteration i
– Computes Ωk(λ) and Ωkl(λ) for a given value of λ from a random draw π(t).
– In order to obtain λi = dλ + λi−1 solve the regularized linear problem:

dλ = arg mindλ|dλ|, υ ∂J + (1 − υ) ∂2J λi−1 = ∂2J dλ

calculating the SVD of ∂2J in order to consider its pseudo-inverse.

– If (cid:107)∂J (cid:107) does not decreases reduce υ and repeat until υ vanishes.

—————————————————

RR n° 9100

46

Alexandre & Drumond & Hinault & Viéville

consider the following weighted L1 norm:

ρ(x) def=

(cid:88)

|¯λk|

k

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

¯Ωk −

1
T − τk

(cid:12)
(cid:12)
(cid:12)
ωk(t)
(cid:12)
(cid:12)

.

(cid:88)

t

(12)

The reason of this second design choice is that it has the same order of magnitude
as the dKL(¯p(¯o)(cid:107)˜p(˜x)) with respect to the observable, i.e.:

|∂ ¯ΩkdKL(¯p(¯o)(cid:107)˜p(˜x))| = |∂ ¯Ωkρ(x)| = |¯λk|,
so that we expect the numerical condition of the original criterion and the related
gradient magnitude to be similar. At the experimental level we have observed that
such L1 criterion seems more eﬃcient than the corresponding L2 criterion.

Inria

Backward tuning

References

47

[1] Maria Elena Acevedo-Mosqueda, Cornelio Yáñez Márquez, and Marco Antonio Acevedo-Mosqueda. Bidirec-
tional associative memories: Diﬀerent approaches. ACM Comput. Surv., 45(2):18:1–18:30, March 2013.

[2] D.J. Amit. Modeling brain function—the world of attractor neural networks. Cambridge University Press,

New York, NY, USA, 1989.

[3] K.J. Astrom. Theory and application of adaptive control: a survey. Automatica, 19:471–486, 1983.

[4] David Balduzzi and Muhammad Ghifary. Strongly-typed recurrent neural networks. CoRR, abs/1602.02218,

2016.

[5] Yoshua Bengio. Learning Deep Architectures for AI. Now Publishers Inc, Hanover, Mass., October 2009.

02705.

[6] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspec-

tives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828, 2012.

[7] Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards ai.

In L. Bottou, O. Chapelle,

D. DeCoste, and J. Weston, editors, Large-Scale Kernel Machines. MIT Press, 2007.

[8] Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, and Zhouhan Lin. Towards biologically plausible deep

learning. arXiv preprint arxiv:1502.0415, page 10, feb 2016.

[9] N. Bertschinger and T. Natschläger. Real-time computation at the edge of chaos in recurrent neural networks.

Neural Computation, 16:1413–1436, 2004.

[10] B. Cessac. A View of Neural Networks as Dynamical Systems. International Journal of Bifurcation and

Chaos, 20(06):1585–1629, June 2010. 00020.

[11] Bruno Cessac. A discrete time neural network model with spiking neurons. Rigorous results on the sponta-
neous dynamics. Journal of Mathematical Biology, 56(3):311–345, 2008. 00004 56 pages, 1 Figure, to appear
in Journal of Mathematical Biology.

[12] Bruno Cessac, Hélène Paugam-Moisy, and Thierry Viéville. Overview of facts and issues about neural coding

by spikes. J. Physiol. Paris, 104(1-2):5–18, February 2010.

[13] Bruno Cessac, Rodrigo Salas, and Thierry Viéville. Using event-based metric for event-based neural network

weight adjustment. page 18 pp. Louvain-La-Neuve : I6doc.com, April 2012. 00000.

[14] D. S. Chen and R. C. Jain. A robust backpropagation learning algorithm for function approximation. IEEE

Transactions on Neural Networks, 5(3):467–479, May 1994.

[15] Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation.
CoRR, abs/1406.1078, 2014.

[16] Rodrigo Cofre and Bruno Cessac. Exact computation of the Maximum Entropy Potential of spiking neural

networks models. Technical report, May 2014. working paper or preprint.

[17] Yann Le Cun. A Theoretical Framework for Back-Propagation. 1988.

[18] Li Deng. Deep Learning: Methods and Applications. Foundations and Trends® in Signal Processing,

7(3-4):197–387, 2014. 00003.

[19] J. L. Elman. Finding structure in time. Cognitive Science, 14(2):179–211, 1990.

[20] Thalita F. Drumond, Thierry Viéville, and Frédéric Alexandre Alexandre. Not-so-big data deep learning: a

review. 2017. in preparation.

RR n° 9100

48

Alexandre & Drumond & Hinault & Viéville

[21] Thalita F. Drumond, Thierry Viéville, and Frédéric Alexandre. From shortcuts to architecture optimization

in deep-learning. 2017.

[22] Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. Learning Hierarchical Features for
Scene Labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1915–1929, August
2013. 00578.

[23] Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram Singer. An eﬃcient boosting algorithm for combining

preferences. The Journal of machine learning research, 4:933–969, 2003.

[24] Felix A. Gers, Nicol N. Schraudolph, and Jürgen Schmidhuber. Learning precise timing with lstm recurrent

networks. J. Mach. Learn. Res., 3:115–143, March 2003.

[25] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.

[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In

IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.

[27] Geoﬀrey E Hinton and James L. McClelland. Learning representations by recirculation. In D. Z. Anderson,
editor, Neural Information Processing Systems, pages 358–366. American Institute of Physics, 1988.

[28] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780, 1997.

[29] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780,

November 1997.

[30] J. J. Hopﬁeld. Neural networks and physical systems with emergent collective computational abilities. Proc.

National Academy of Sciences, USA, 79:2554–2558, 1982.

[31] Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew. Extreme learning machine: Theory and applica-

tions. Neurocomputing, 70(1):489 – 501, 2006. Neural Networks.

[32] Johan Håstad and Mikael Goldmann. On the power of small-depth threshold circuits. Computational

Complexity, 1(2):113–129, June 1991. 00000.

[33] H. Jaeger. Adaptive nonlinear system identiﬁcation with Echo State Networks. In S. Becker, S. Thrun, and
K. Obermayer, editors, NIPS\*2002, Advances in Neural Information Processing Systems, volume 15, pages
593–600. MIT Press, 2003.

[34] M. I. Jordan. Attractor dynamics and parallelism in a connectionist sequential machine. Proceedings of the

8th Annular Conference Cognitive Science Society, pages 531–546, 1986.

[35] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E. Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In F. Pereira Weinberger, C. J. C. Burges, L. Bottou, and K. Q., editors, Advances in
Neural Information Processing Systems 25, pages 1097–1105. Curran Associates, Inc., 2012.

[36] Y. Lecun, L Bottou, Y Bengio, and P Haﬀner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE, 86(11):2278–2324, 1998.

[37] Robert Legenstein and Wolfgang Maass. Edge of chaos and prediction of computational performance for
neural circuit models. Neural Networks, 20(3):323 – 334, 2007. Echo State Networks and Liquid State
Machines.

[38] W. Maass, T. Natschläger, and H. Markram. Real-time computing without stable states: A new framework

for neural computation based on perturbations. Neural Computation, 14(11):2531–2560, 2002.

[39] James Martens and Ilya Sutskever. Learning Recurrent Neural Networks with Hessian-Free Optimization.

[40] Michael Nielsen. Neural Networks and Deep Learning. Determination Press, 2015.

Inria

Backward tuning

49

[41] R. C. O’Reilly. Biologically plausible error-driven learning using local activation diﬀerences: The generalized

recirculation algorithm. Neural Computation, 8(5):895–938, July 1996.

[42] Hélène Paugam-Moisy, Régis Martinez, and Samy Bengio. Delay learning and polychronization for reservoir

computing. Neurocomputing, 71:1143–1158, 2008.

[43] Fernando J. Pineda. Generalization of back-propagation to recurrent neural networks. Phys. Rev. Lett.,

59:2229–2232, Nov 1987.

[44] Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and when
can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of
Automation and Computing, Mar 2017.

[45] Horacio Rostro-Gonzalez, Bruno Cessac, and Thierry Viéville. Exact spike-train reproduction with a neural

network model. Journal of Computational Neuroscience, 2010. submitted.

[46] J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85–117, 2015. Pub-

lished online 2014; based on TR arXiv:1404.7828 [cs.NE].

[47] Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. Overfeat:

Integrated recognition, localization and detection using convolutional networks. dec 2013.

[48] S. Murray Sherman and R. W. Guillery. The role of the thalamus in the ﬂow of information to the cortex.
Philosophical Transactions of the Royal Society B: Biological Sciences, 357(1428):1695–1708, December
2002.

[49] Hava T. Siegelmann and Eduardo D. Sontag. Turing Computability With Neural Nets. 1991.

[50] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.

In International Conference on Learning Representations (ICRL), pages 1–14, sep 2015.

[51] Greg Stuart, Nelson Spruston, Bert Sakmann, and Michael Häusser. Action potential initiation and back-

propagation in neurons of the mammalian cns. Trends in Neurosciences, 20(3):125 – 131, 1997.

[52] Christian Szegedy, Sergey Ioﬀe, Vincent Vanhoucke, and Alex Alemi. Inception-v4, inception-resnet and the

impact of residual connections on learning. Arxiv, page 12, feb 2016.

[53] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,

Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. sep 2014.

[54] Meropi Topalidou, Arthur Leblois, Thomas Boraud, and Nicolas P Rougier. A long journey into reproducible

computational neuroscience. Frontiers in Computational Neuroscience, 9:30, 2015.

[55] Joel A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Inform. Theory,

50:2231–2242, 2004.

[56] Joel A. Tropp. Just relax: Convex programming methods for subset selection and sparse approximation.

Technical report, Texas Institute for Computational Engineering and Sciences, 2004.

[57] Juan Carlos Vasquez, Thierry Viéville, and Bruno Cessac. Parametric Estimation of Gibbs distributions as
general Maximum-entropy models for the analysis of spike train statistics. Research Report RR-7561, March
2011. This work corresponds to an extended and revisited version of a previous Arxiv preprint, submitted
to HAL as http://hal.inria.fr/inria-00534847/fr/.

[58] D. Verstraeten, B. Schrauwen, M. D’Haene, and D. Stroobandt. An experimental uniﬁcation of reservoir

computing methods. Neural Networks, 20(3):391–403, 2007.

[59] Xiaohui Xie and H. Sebastian Seung. Equivalence of backpropagation and contrastive hebbian learning in a

layered network. Neural Computation, 15(2):441–454, 2003.

[60] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. Lecture Notes

in Computer Science, 8689:818–833, 2014.

[61] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep

learning requires rethinking generalization. nov 2016.

RR n° 9100

50

Alexandre & Drumond & Hinault & Viéville

Contents

1 Introduction

2 Problem position

3 Recurrent weight estimation

4 Experimentation

5 Conclusion

A Major examples ﬁtting this architecture.

B Comparison with related recurrent weight estimation methods

C Using this framework in diﬀerent contexts

D Closed forms solution for neural network tasks

E Stochastic adjustment of the network weights

3

5

8

18

24

26

32

34

39

43

Inria

RESEARCH CENTRE
BORDEAUX – SUD-OUEST

351, Cours de la Libération

Bâtiment A 29

33405 Talence Cedex

Publisher
Inria
Domaine de Voluceau - Rocquencourt
BP 105 - 78153 Le Chesnay Cedex
inria.fr

ISSN 0249-6399

