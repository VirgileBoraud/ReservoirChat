DANA: Distributed (asynchronous) Numerical and
Adaptive modelling framework
Nicolas P. Rougier, Jérémy Fix

To cite this version:

Nicolas P. Rougier, Jérémy Fix. DANA: Distributed (asynchronous) Numerical and Adaptive
modelling framework. Network: Computation in Neural Systems, 2012, 23 (4), pp.237-253.
￿10.3109/0954898X.2012.721573￿. ￿hal-00718780￿

HAL Id: hal-00718780

https://inria.hal.science/hal-00718780

Submitted on 18 Jul 2012

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

DANA: Distributed (asynchronous) Numerical and Adaptive
modelling framework

Nicolas Rougier∗1 and J´er´emy Fix2

1INRIA Bordeaux - Sud Ouest, 351, Cours de la Lib´eration, 33405 Talence Cedex, France
2IMS, SUPELEC, 2 rue Edouard Belin, F-57070 Metz, France

July 18, 2012

Abstract

DANA is a python framework (http://dana.loria.fr) whose computational paradigm is grounded

on the notion of a unit that is essentially a set of time dependent values varying under the inﬂuence
of other units via adaptive weighted connections. The evolution of a unit’s value are deﬁned by a
set of diﬀerential equations expressed in standard mathematical notation which greatly ease their
deﬁnition. The units are organized into groups that form a model. Each unit can be connected to
any other unit (including itself) using a weighted connection. The DANA framework oﬀers a set
of core objects needed to design and run such models. The modeler only has to deﬁne the equa-
tions of a unit as well as the equations governing the training of the connections. The simulation
is completely transparent to the modeler and is handled by DANA. This allows DANA to be used
for a wide range of numerical and distributed models as long as they ﬁt the proposed framework
(e.g. cellular automata, reaction-diﬀusion system, decentralized neural networks, recurrent neural
networks, kernel-based image processing, etc.).

Keywords: python, software, computational neuroscience, distributed, numerical, adaptive, mean-
rate, simulation, computation

Introduction

The original artiﬁcial neural network (ANN) paradigm describes a ﬁne grain computational system, that
is distributed among a population of elementary processing units analogous to the processing taking place
within the brain. Those units, the so called artiﬁcial neurons, communicate with each other through
weighted connections. Such a system can be described by a graph in which the vertices are the pro-
cessing components and the edges are the communication channels allowing the transfer of information
within the system. The diﬀerent ANN architectures proposed by the community can be distinguished by
the type of units (logical, rate, spikes, etc.) and topology (feed-forward, feedback, recurrent, etc.) used.
They can be ordered graphs where some units are systematically evaluated before others (e.g. perceptron
or multi-layer perceptron (MLP)) or they can be recurrent networks where no a priori evaluation order
can be deﬁned. The system may be endowed with a central supervisor able to look at the whole graph
in order to take some global decision depending on meta-conditions (e.g. winner takes all in the Koho-
nen self organizing maps (SOM) or back-propagation learning in the MLP) or can be fully distributed,
relying only on local computations. This description is indeed loose enough to allow virtually any kind
of network, from a very simple network of two neurons that is able to ride a bike1 up to the blue brain
project2 gathering thousands of highly detailed models of the neocortical column. Obviously, if these
two models fall into the generic neural network paradigm, they are quite diﬀerent both in their essence
and their respective goals. The dual neuron network is built around a concept of a neuron that is very

∗Corresponding author: Nicolas.Rougier@inria.fr
1http://www.paradise.caltech.edu/~cook/
2http://bluebrain.epfl.ch/

1

similar to a logical unit able to carry out quite complex computations while in the case of the blue brain
project, the concept of a neuron is directly related to precise anatomical, electrical and morphological
data on the composition, density and distribution of the diﬀerent cortical cells.

Motivations

Between these two extremes, there exist several mid-level approaches which promote population models
for the study of complex functions and structures and there are generally dedicated software such as
Emergent (Aisa et al., 2008) (previously PDP++ (O’Reilly and Y.Munakata, 2000)), Miind (de Kamps
et al., 2008), Topographica (Bednar, 2008), NEST initiative (Gewaltig and Diesmann, 2007), etc. These
simulators are mostly based on high-level interfaces where a user can build a model by using a set of
predeﬁned objects such as neurons, connections, training algorithms, etc. and also oﬀer the possibility of
deriving new models from these base objects. While those high-level approaches yield several well-known
advantages, the main problem in our view is the lack of control of what the model is actually computing
since the underlying computations may be buried very deep in the software. Such high level interfaces
can be thus delicate to use because they oﬀer no guarantee of strongly enforcing an actual parallel,
distributed, numerical and adaptive approach. Let us for example consider the winner-takes-all (WTA)
algorithm which is usually implemented as the maximum() function over the activities of a given layer
such that, only the most active unit remains active at the end of the simulation. This implementation
requires an examination of all units at once and modifying parameters, such as synpatic eﬃcacy according
to the result of the function. There is then a question of who is examining and acting; the model or the
modelling software?

This means that part of the properties of the system may indeed be due to the supervisor, without
specifying how the supervisor itself is controlled. This also means that one would not be able to reproduce
this network using, for instance, only hardware artiﬁcial neurons linked together without introducing an
ad-hoc surrounding architecture for the coordination of the network. A question arises here as to what
extent such artifacts could impact results and claims. We will not answer in the general case; however,
in the context of the study of emergence where we seek to understand properties of a population of
units whose behavior depend only on local interactions between the units, we think it is critical to both
strongly constrain the modeling framework, in order to be able to guarantee results, and to still oﬀer the
modeler the freedom to design its own model as is the case, for example, in the Brian simulator.

Brian limitations

The Brian simulator (Goodman and Brette, 2008) has become very popular among neuroscientists in
just a few years and it has been ranked 2nd most popular neural simulator in a recent survey (Hanke
and Halchenko, 2011), just after the well known Neuron software (Carnevale and Hines, 2006). The
reason behind this success is certainly the intuitive and highly ﬂexible interface oﬀered by Brian as
well as eﬃcient vectorized computations that make Brian quite competitive with regular C code for large
networks. Designing a non-standard spiking neuron model using Brian is also straightforward as you just
need to provide the model’s equations in the usual mathematical notation. In just a few lines of codes,
you can easily and very quickly simulate a whole model. However, Brian has been designed speciﬁcally
for spiking neuron models with dedicated optimization for the sparse (in time) transfer of information
from one neuron to the other. If one wants to model a mean-rate neuron (or any model that relies on
continuous quantities), it is still possible to tweak Brian to simulate it; however, doing this prevents the
model from beneﬁting from Brian’s optimizations and makes the overall simulation quite slow since it
requires a dedicated network operation (see appendix).

Reclaiming control

One of the goals of the DANA framework is to give the user full control over its model such that he knows
what is actually computed at every timestep. We think it is critical to frame what is precisely computed
by the model in order to avoid any modeling artifact such as a central supervisor or homunculus. One
way to achieve this is to use a constrained computational framework that guarantees the model to be
consistent regarding a deﬁnition of what we think are distributed, asynchronous, numerical and adaptive

2

computations. The DANA framework (http://dana.loria.fr) aims to provide such a deﬁnition in the
form of a set of computational principles (namely, distributed, asynchronous, numerical and adaptive)
and to allow for the rapid design of models in an easy and intuitive way. These constraints may change
radically the way models are designed. For example, ﬁgure 1 shows the implementation of a dynamic
neural ﬁeld, which under some conditions leads to the emergence of a localized bubble of activity. This
can be interpreted as a k-WTA. However, this implementation does not require an implicit supervisor
and the ﬁnal equilibirum state is the result of unit computations only.

τ

∂U (x, t)
∂t

= −U (x, t) +

(cid:90) +∞

−∞

(cid:124)

W (|x − y|)

V (y,t)
(cid:125)(cid:124)

(cid:122)
f (U (y, t)) dy

(cid:123)

+I(x, t) + h

(1)

(cid:123)(cid:122)
L(x,t)

(cid:125)

from dana import \*

n , p = 256 , 513
h , tau = 0.0 , 1.0

def f ( X ): np . maximum (X ,0)

src = zeros (n , ’V ’)
tgt = zeros (n ,

’’’ dU / dt = ( - U + L + I + h )/ tau

V = f ( U ); I ; L ’’’)

Sp ars eCo nn ect ion ( src ( ’V ’) , tgt ( ’I ’) , 1)
Sh are dCo nn ect ion ( tgt ( ’V ’) , tgt ( ’L ’) ,

0.1\* gaussian (p ,0.1) -0.075\* gaussian (p ,1))

src . V = 1.1\* gaussian (n , 0.1 , -0.5) \

+ 1.0\* gaussian (n , 0.1 , +0.5)

Z = []
@clock . every (10\* millisecond )
def register ( time ):

Z . append ( tgt . V . copy ())

run ( time =1.5\* second , dt =1\* ms )

Figure 1: A one dimensional neural ﬁeld in DANA, with code on the right, model equation on the top and
ﬁeld activity evolution over time at the left. The diﬀerential equation describes the spatio-temporal evolution of
a coarse grained variable (such as synaptic or ﬁring rate value) for a spatially continuous population of neurons.
V is the membrane potential, W a neighborhood function, f the transfer function of a single neuron (mapping
the membrane potentials to the ﬁring rates), τ a time constant, I the input and h the resting potential. In the
simulation script, L gathers the lateral contributions through the weights W . The model has been simulated for
1.5 second using a time-step of 1 millisecond and activity has been registered every 10 milliseconds.

Distributed, Numerical and Adaptive computing

The computational paradigm supporting the DANA framework is grounded on the notion of a unit that
is essentially a set of time dependent values varying under the inﬂuence of other units via trainable
weighted connections (ﬁgure 2). The evolution of the units value are deﬁned by a set of diﬀerential
equations expressed in standard mathematical notation. The units are organized into groups that form
a network. Each unit can be connected to any other unit (including itself) using a weighted connection.
The DANA framework oﬀers a set of core objects needed to design and run such networks. The modeler

3

020406080100120140Time (10ms)050100150200250Neural position0.000.080.160.240.320.400.480.560.640.72who uses DANA only has to specify the equations of his model (computations, training), in usual
mathematical notation, and does not have to handle how the model is simulated which greatly facilitates
the deﬁnition and testing of a model.

Figure 2: DANA framework. A unit is a set of one to several values (Vi). A group is a set of one to several
homogeneous units. A layer is a subset of a group restricted to a unique value Vi. A layer is a group. A link
is a weighted connection between a source group to a target group. A group can be linked to any other group
including itself.

The framework has been implemented as a Python3 library using fast vectorized computation brought
by the NumPy4 library. In the next paragraphs, we illustrate the main concepts of DANA through the
simulation of the Bienenstock-Cooper-Munro (BCM, (Bienenstock et al., 1982)) learning rule. This
learning rule describes a model of synaptic plasticity in the visual cortex that is able to account for the
selectivity of the visual neurons.
It relies on a sliding threshold ensuring the stability of the rule as
shown in equations (2) to (4) where d is the input, m the synaptic connections with the input, c the
post-synaptic activity, θ the adaptive modiﬁcation threshold and η the learning rate. τ and ¯τ are time
constants. Deﬁning and simulating this learning rule is straightforward as shown on ﬁgure 3.

Computing

We explained earlier that a unit is the elementary processing element and a group is a set of one to
several homogeneous units. However, there is no such explicit concept of a unit within the proposed
implementation. In DANA, we deﬁne groups as a set of units sharing the same update rules. For the
BCM example, we have two groups; one that holds the current stimulus (src) and one that computes
the actual activity and the sliding threshold (tgt):

src = np . zeros ( n )
tgt = zeros ( n , ’’’ dC / dt = (I - C )\*(1/ tau )

dT / dt = ( C \*\*2 - T )\*(1/ tau\_ )
I ’’’ )

The src group is deﬁned as a regular numpy array while the tgt group is deﬁned as a DANA group
with three values. The evolution of the values C and T is governed by diﬀerential equations 2 and 3. The
value I holds the output of the weighted connections from the src group. In DANA, group values can
be of three diﬀerent types:

• A declaration speciﬁes the existence of a value in a group. It can be used to receive the output of

projections from one group to another or the result of some local computations,

• An equation of the form y = f (y, x, ...) describes how a given variable is computed when the
equation is called. The name of the variable is given on the left side of the equation and the update
function is represented on the right side of the equation,

• A diﬀerential equation represents a ﬁrst order ordinary diﬀerential equation of the form dy/dt =
f (y, x, t, ...) and describes how a given variable is updated when the equation is called (using Euler,

3http://python.org/
4http://scipy.numpy.org/

4

GroupGroupUnitLinkLayerLinkValueValueValueτ ˙c = m.d − c
¯τ ˙θ = c2 − θ,
˙w = ηc(c − θ)d

(2)

(3)

(4)

from random import choice
from dana import \*

n = 10
tau , tau\_ , eta = 1.0 , 0.1 , 0.01

stims = np . identity ( n )
src = np . zeros ( n )
tgt = zeros ( n , ’’’ dC / dt = (I - C )\*(1/ tau )

dT / dt = ( C \*\*2 - T )\*(1/ tau\_ )
I ’’’)

Dense Co nnectio n ( src , tgt ( ’I ’) ,

rnd . uniform ( 0 , 1 , (n , n ) )
’ dW / dt = eta \* post . C \*( post .C - post . T )\* pre ’)

@before ( clock . tick )
def set\_stimulus (\* args ):

src [:] = choice ( stims )

run ( n =10000)

Figure 3: Implementation of the BCM learning rule. Starting from an initial random weight matrix,
units learn to become responsive to a unique stimulus and to remain silent for the others.

Runge-Kutta second and fourth order or exponential Euler). The name of the variable is given
by the left side of the equation and the update function is represented by the right side of the
equation. Considering only ﬁrst order diﬀerential equations is not restrictive as any higher order
equation can be expressed by a system of ﬁrst order diﬀerential equations.

Connecting

The connections are directly speciﬁed between groups. Considering a source group of size n and a target
group of size m, a connection is a matrix (Lij)1≤i≤n,1≤j≤m describing individual connections between
a source unit i and a target unit j. The connection output is computed as the matrix product of the
source group and the connection matrix which corresponds to the classic weighted sum. This output is
made available to the target group for the deﬁnition of value equations. The deﬁnition of a connection
involves the involved source and target group values.

A group does not have a topology per se and the shape is mainly used to access group elements or
display it in a terminal or a ﬁgure. If the given weight matrix shape is diﬀerent from m × n, the kernel
is interpreted as a prototype kernel for a single target unit that is to be used for every other unit, as
In this case, DANA internally builds a new matrix such that any target unit
illustrated on ﬁgure 4.
receives the output of the prototype kernel multiplied by the source unit and its immediate neighborhood
(that spans the kernel size) as illustrated on ﬁgure 4.

Depending on the type of connections, DANA oﬀers various optimizations:

• Shared connection: when the same kernel is shared by all the units from the target group, the

output can be computed using a convolution or Fast Fourier Transform (FFT),

• Sparse connection: when the connections are sparse, the connection matrix is stored in a sparse

array,

5

Learned prototypeUnitsInitial stateLearned prototypeUnitsFinal stateFigure 4: The ﬁgure shows how the output of target unit at position (2,0) is computed according to the
prototype kernel. First, the shape of the target is used to assign a set of normalized coordinates to the unit and
these normalized coordinates are used to ﬁnd the corresponding source unit. The prototype kernel is then centered
on this unit and an element wise multiplication is performed where kernel and source overlaps. They are then
summed up and stored at the target unit position within the target group.

• Dense connection: the most generic non optimized connection.

User can also decide to deﬁne its own type of connection by sub-classing the Connection class and
providing an output() method. For the BCM example, the connection is made dense because each
output unit is connected to every input unit with their own (i.e. not shared) weights.

Learning

A diﬀerential equation can be associated to sparse and dense connections.
It deﬁnes the temporal
evolution of the weights as a function of any of the target group values (or post-synaptic group, identiﬁed
as post), source group values (or pre-synaptic group, identiﬁed as pre) and connection output value
identiﬁed by its name. For example, in case of the BCM learning rule, the connection reads:

Den seConnection ( src , tgt ( ’I ’) ,

rnd . uniform ( 0 , 1 , (n , n ) )
’ dW / dt = eta \* post . C \*( post .C - post . T )\* pre ’ )

This diﬀerential form of the learning rule allows to write various standard learning rules such as the
Hebbian learning rule (pre\*post). A broader range of learning rules can also be implemented. As an
example, one can consider the general deﬁnition of Hebb like learning rules given in Gerstner (2002) and
expressed by equation 5.

dwij
dt

= F (wij, νi, νj)

(5)

where νj and νj are respectively the pre- and post- synaptic activities and wij is the connection strength
between these two neurons. A Taylor expansion at νi = νj = 0 of equation (5) leads to a general
expression for Hebb like learning rules (see eq. (6)).

dwij
dt

=

∞
(cid:88)

∞
(cid:88)

k=0

l=0

cklνk

i νl
j

(6)

Various classical learning rules, such as Hebb, Oja (Oja, 1982) or BCM (Bienenstock et al., 1982) can all
be expressed within such a formulation and easily deﬁned within DANA. Finally, following the guidelines
that no supervisor should coordinate and deﬁne when learning or computation takes place, the learning
rules are evaluated unconditionally at each time step. Learning could certainly be modulated by some
signals gating when some synapses should be potentiated or depressed but these modulatory signals
must be explicitly speciﬁed within the model. An example of these in Neuroscience is the modulatory
inﬂuence of dopamine on the plasticity of corticostriatal synapses in the basal ganglia (Reynolds and
Wickens, 2002).

6

11111KernelTargetSourceKernel1x1+1x1+1x1+1x1446469664111111111111111111Simulating

Once the model has been fully speciﬁed, it is possible to run it for a speciﬁed period of time using:

run ( time =t , dt =∆t) ,

This running loop will evaluate the connections, update the group values through their equations and
update the weights through their equations. As the model is running, it is also possible to trigger some
events based on a clock (ﬁg. 5). The events can be triggered once, after a certain time has elapsed, on a
regular basis for the entire duration of the simulation or starting and ending at speciﬁc times, and ﬁnally
before or after the tick of the internal clock. This way, it is possible to record the values within a model
or to provide a new input at regular time intervals.

Figure 5: Time management. Unique, regular and time-bounded events can be plugged into the simulation
through the use of decorated functions relative to the clock. A Unique event at 2ms. B Regular event every 2ms.
C Regular event every 1ms, starts at 5ms and never stops. D Regular event every 2ms, starts at 4ms, stops at
8ms. E Regular event just before clock tick. E Regular event just after clock tick.

Performances

In order to evaluate the performances of DANA, we compared the execution times of a script written in
DANA and its dedicated C++ counterpart. As a ﬁrst experiment, we consider a 2D neural ﬁeld with
diﬀerence of Gaussian lateral connections. With appropriately deﬁned lateral connections and a random
input exciting the ﬁeld, a localized bump of activity emerges. In this case, as the weights are ﬁxed and
not subject to learning, we use shared connections involving the FFT to update the lateral contribution.
The DANA script to simulate this example and the C++ and DANA execution times are shown in ﬁg-
ure 6. With no surprise, the dedicated C++ script is faster than the DANA implementation. However,
DANA is only two times slower than the dedicated script while oﬀering much more ﬂexibility and being
more generic than the C++ counterpart (please note that due to the use of the FFT, the execution times
are in O(n2 log(n)) with ﬁelds of size n×n). In this simulation, all the units of the group were connected.
As a second example, we consider the reaction diﬀusion Gray-Scott model Pearson (1993) (see ﬁg. 12).
In this model, the kernel size is 3 × 3. Therefore, we make use of the Sparse Connections of DANA since
using the FFT is not optimal for such small kernels. The DANA simulation script and an illustration of
the model are shown in ﬁgure 12. The execution time of a single step in both DANA and a dedicated
C++ script is given on ﬁgure 7. Again, the C++ dedicated script is faster than its DANA translation.
In summary, in both situations a C++ dedicated script runs faster than the equivalent model written
in DANA. However, this cost remains reasonable when we consider the ﬂexibility that DANA oﬀers for
deﬁning the models (and the overhead it induces).

Case studies

We report here several examples of models taken from the literature in order to illustrate the versatility
of the DANA framework. Each of these models ﬁt the proposed framework at various degrees and the

7

C →D →B →A →012345678910E →F →C: @clock.every(1ms,start=5ms)B: @clock.every(2ms)A: @clock.at(2ms)D: @clock.every(2ms,start=4ms,stop=8ms)E: @before(clock.tick)F: @after(clock.tick)from dana import \*

n , p = 256 , 513
h , tau = 0.0 , 1.0/0.9

def f ( X ): np . minimum ( np . maximum (X ,0) ,1)
focus = Group (( n , n ) ,

’’’ dV / dt = ( - V + L + I )/ tau

U = f ( V ); I ; L ’’’)

Sh are dCo nn ect ion ( focus ( ’U ’) , focus ( ’L ’) ,

0.062\* gaussian (( p , p ) ,0.15)

-0.059\* gaussian (( p , p ) ,1) ,

fft = True , toric = True )

focus [ ’I ’] = 1
focus [ ’I ’] += np . random . random (( n , n ))
run ( time =100\* second , dt =100\* ms )

Figure 6: Execution time comparison between a DANA and C++ implementation of a two dimensional
neural ﬁeld. The execution time is averaged over 1000 steps.

Du , Dv , F , k = 0.16 , 0.08 , 0.020 , 0.055
Z = Group ( (128 ,128) ,

""" du / dt = Du \* Lu - Z + F \*(1 - U ) : float
dv / dt = Dv \* Lv + Z - ( F + k )\* V : float
: float
U = np . maximum (u ,0)
: float
V = np . maximum (v ,0)
Z = U \* V \* V
: float
Lu ; Lv """ )

K = np . array ([[ np . NaN ,

[
1. ,
[ np . NaN ,

-4. ,

1. , np . NaN ] ,
] ,

1.
1. , np . NaN ]])

Sp ars eCo nn ectio n ( Z ( ’U ’) , Z ( ’ Lu ’) ,K , toric = True )
Sp ars eCo nn ectio n ( Z ( ’V ’) , Z ( ’ Lv ’) ,K , toric = True )

run ( n =1000)

Figure 7: Execution time comparison between a DANA and C++ implementation for the Gray-Scott
reaction diﬀusion model. The time to execute a single step is averaged over 1000 steps.

8

0.00.51.01.52.02.53.03.5n2log(n),kernelsizen2×1070.00.51.01.52.02.53.0Executiontimeperstep(s.)ExecutiontimesfunctionoftheneuralﬁeldsizeDanaC++020040060080010001200Mapsizen0.00.10.20.30.40.50.60.70.8Executiontimeperstep(s.)Executiontimesfunctionofthemodel’ssizeDanaC++resulting script is generally straightforward from the mathematical description.

Saccadic exploration of a visual environment Fix et al. (2011, 2006)

In (Fix et al., 2011, 2006) we proposed a distributed mechanism for exploring a visual environment with
saccadic eye movements (see ﬁg. 8). In this section and in the simulation script provided online, we detail
how this mechanism can be implemented within the DANA framework. The experimental setup consists
of three identical targets that have to be scanned successively with saccadic eye movements. The tar-
gets are identical (same shape, same visual features), a target is only characterized by its spatial position.

The experimental studies on the monkey brain areas involved in the control of saccadic eye movements
indicate that most of these areas encode a saccadic target in an eye-centered frame of reference (such as
in the Lateral Intraparietal area (LIP) and the Frontal Eye Fields (FEF)). In addition, some areas such
as the dorsolateral prefrontal cortex (dlPFC) exhibit sustained activities when a spatial position, relevant
for the task at hand, has to be memorized. These areas are part of the saccadic system. They project on
the basal ganglia with which they form partially segregated loops. The loop with FEF is proposed to be
involved in action selection. The loop involving dlPFC may be involved in spatial memory updating in
particular because dlPFC has sustained activities when a spatial information has to be memorized and
because it interacts with the mediodorsal nucleus of thalamus which has been shown recently to be the
target of a corollary discharge signal sent from the superior colliculus (SC).

In agreement with the above mentioned biological data, the model presented in (Fix et al., 2011,
2006) considers that the spatial position of the targets are encoded in an eye centered frame of reference.
In order to successfully perform the saccadic scanpath, we further proposed that the position of the
previously focused targets are stored in an eye-centered frame of reference, and updated by anticipation
with each saccadic eye movement. This update involves sigma-pi connections (i.e. weighed sum of
products). These connections are not provided by DANA. However, as shown on ﬁgure 8, it is suﬃcient
to subclass the Connection type and to overload the output method in order to deﬁne custom connectivity
patterns.

A computational model of action selection Gurney et al. (2001b)

Gurney and his colleagues proposed in (Gurney et al., 2001a,b) a biologically plausible model of the
basal ganglia (BG) complex based on the hypothesis that action selection is the primary role of the BG.
The model is made of 9 diﬀerent structures: Posterior Cortex (PC), Striatum D1 (St1), Striatum D2
(St2), Sub-Thalamic Nucleus (STN), External Globus Pallidus (GPe), Internal Globus Pallidus (GPi),
Ventro-Lateral Thalamus (VLT), Prefrontal Cortex (PFC) and the Thalamic Reticular Nucleus (TRN).
Each of these structures possesses its own equation for computing the activity as well as a speciﬁc set of
connections with other structures.

The overall dynamic of the model is illustrated in ﬁgure 9 which reproduces faithfully results from
(Gurney et al., 2001b). The DANA framework allowed us to transcript the model in a straightforward
way. First, we deﬁned all the structures according to their respective equations with U and V being
respectively the core activity and the bound activity and the connections are made between individual
structures, enforcing their respective topologies (one to one or one to all). Before running the model, we
also plugged some timed events that change the input (activity in PC) on a regular time basis. Figure 9
shows the temporal evolution of the activity of the main structures.

Discussion and Future Work

Asynchrony

The original implementation of the DANA framework oﬀered the posibility to evaluate models asyn-
chronously where units were updated in random order. The motivation for such numerical scheme was
to reject the idea of a central clock that would signal any unit when to update and to avoid any implicit
synchronization eﬀect. The standard procedure to evaluate a time-dependent diﬀerential system is is to
synchronously evaluate the values of each variables: the state at time t + ∆t is exclusively computed

9

class S ig m aPi Con nect ion ( Connection ):

def \_\_init\_\_ ( self , source = None , modulator = None , target = None , scale =1 , direction =1):

Connection . \_\_init\_\_ ( self , source , target )
self . \_scale = scale
self . \_direction = +1
names = modulator . dtype . names
self . \_ act ual \_mo dul ator = modulator

def output ( self ):

src = self . \_actual\_source
mod = self . \_ac tual \_mo dul ato r
tgt = self . \_actual\_target
R = np . zeros ( tgt . shape )
if len ( tgt . shape ) == len ( src . shape ) == len ( mod . shape ) == 1:

R = convolve1d ( src , mod [:: self . \_direction ])

elif len ( tgt . shape ) == len ( src . shape ) == len ( mod . shape ) == 2:

R = convolve2d ( src , mod [:: self . \_direction ,:: self . \_direction ])

else :

raise NotImplemented

return R \* self . \_scale

Figure 8: Schematic view of the architecture of a model for the saccadic exploration of a visual environment. The
image captured by a camera is ﬁltered and represented in the saliency map. This information feeds two pathways :
one to the memory and one to the focus map. A competition in the focus map leads to the most salient location
that is the target for the next saccade. The anticipation circuit predicts the future state of the memory with its
current content and the programmed saccade. This is done through the use of a sigma-pi connections whose code
correspond to the bottom part of the ﬁgure.

10

SaliencyMemoryAnticipationUpdateSaccadeCompetitionBiasFocusPredictionGatingLateral...
# Striatum D2 : medium spiny neurons of the striatum with D2 dopamine receptors
St2 = zeros (n , """ dU / dt = 1/ tau \*( - U + PC\_ + PFC\_ )

= np . minimum ( np . maximum (U - eSt2 ,0) ,1)

V
PC\_ ; PFC\_ """ )

...
# St2 connections
Sp ar se Con nection ( PC ( ’V ’) ,
PC\_St2 \* np . ones (1) )
Sp ar se Con nection ( PFC ( ’V ’) , St2 ( ’ PFC\_ ’) , PFC\_St2 \* np . ones (1) )
...

St2 ( ’ PC\_ ’) ,

# Timed events
@clock . at (2\* second )
def update\_PC ( t ):

PC . V [0] = .4

...

# Run simulation for 10 seconds
run ( time =10\* second , dt =1\* millisecond )

Figure 9: Computational model of action selection in the basal ganglia from Gurney et al.
(2001b). Full script with all deﬁnitions and graphic code is 250 lines (model is made of more than 18 equations,
9 groups and 20 connections between groups).

11

0123450.00.20.40.60.81.0Posterior Cortex (PC)0123450.00.20.40.60.81.0Prefrontal Cortex (PFC)0123450.00.20.40.60.81.0Sub-Thalamic Nucleus (STN)0123450.00.20.40.60.81.0Striatum D1 (St1)0123450.00.20.40.60.81.0Striatum D2 (St2)0123450.00.20.40.60.81.0External globus pallidus (GPe)0123450.00.20.40.60.81.0Internal globus pallidus (GPi)0123450.00.20.40.60.81.0Ventro Lateral Thalamus (VLT)0123450.00.20.40.60.81.0Thalamic Reticular Nucleus (TRN)0246810Time (s)0.10.00.10.20.30.40.50.6GPi channel 10246810Time (s)0.10.00.10.20.30.40.50.6GPi channel 20246810Time (s)0.10.00.10.20.30.40.50.6GPi channel 3-6from the state at time t. The usual way to perform such a synchronization is to explicitly implement
a temporary buﬀer at the variable level and store the computed value at time t + ∆t. Once the value
of all the vatiables has been evaluated at time t + ∆t, the public value is replaced by the content of the
buﬀer (there exist other ways of doing this synchronous evaluation, however the idea remains to separate
information between time t and time t + ∆t). To perform such a synchronization, there is thus a need
for a global signal indicating to the variables that the evaluation period is over and that they can replace
their previous value with the newly computed one.

At the computational level, this synchronization is rather expensive and is mostly justiﬁed by the
diﬃculty of mathematically handling asynchronous models (Rougier and Hutt, 2009). For example,
cellular automata have been extensively studied during the past decades for the synchronous case and
many theorems have been proved in this context. However, recent works on asynchronous cellular
automata (Fates, 2009) showed that the behavior of the models and their associated properties may
be of a radically diﬀerent nature depending on the level of synchrony of the model (only a sub-part
of all the available automata can be evaluated asynchronously). Since we want to reject any kind of
centralized process, we introduced asynchronous computations relying on results from Robert (1994)
(discrete systems) and Mitra (1987) (continuous systems). However, if Mitra and Robert give conditions
to ensure the proper convergence of the relaxation, we do not introduce such constraints within the
framework since it might be quite a complex implementation. However, our recent study (Taouali
et al., 2011) explained that even if asynchronous evaluation may provide a richer behavior in some
circumstances, allowing a dynamical system to reach additional ﬁxed points, the simple addition of noise
provides an almost equivalent behavior. Consequently, we removed the asynchronous evaluation
from the DANA framework in favor of a simple noise addition when necessary. This is left to the
responsibility of the modeler.

Finite transmission speed

Until now, we have considered the velocity of connection between two groups to be inﬁnite leading to an
instantaneous transmission of information. However, considering such an inﬁnite velocity in connections
does not make sense from a biological point of view and forbids de facto the experimental study of some
phenomena directly dependent on a ﬁnite transmission speed. We have proposed in (Hutt and Rougier,
2010) a fast numerical procedure (associated with its mathematical proof) that allows us to use delayed
diﬀerential equations (DDEs) instead or regular diﬀerential equations. This requires a dedicated solver
that has been detailed in (Hutt and Rougier, 2010) but has not yet been implemented within DANA.

Integration into Brian

As we already mentioned, Brian is a famous python library for simulating spiking neural networks. DANA
is more dedicated to time continuous systems such as mean-rate model of neurons. Integrating DANA
within Brian would oﬀer the community a uniﬁed library for simulating both types of neural networks.
Indeed, we tried to ensure a maximum compatibility with Brian in order to ease this integration and we
already share a lot of common objects (equations, groups and connections) but the code currently diﬀers
(see for example ﬁg. 10 and ﬁg. 11). This integration is the subject of the BEP (Brian Enhancement
Proposal) 24. However, before proposing this integration to the Brian development team, we aim at
solving the case of ﬁnite transmission speeed connections in DANA that we mentioned in the previous
section.

Acknowledgment

Declaration of interest

The authors report no conﬂicts of interest.

12

A Brian/DANA comparison

A.1 Game of Life

from brian import \*
from scipy . signal import convolve2d
N = 256
K = array ([[1 ,1 ,1] ,
[1 ,0 ,1] ,
[1 ,1 ,1]])
G = NeuronGroup ( N \*N , ’ alive :1 ’)
G . alive = randint (2 , size = N \* N )
A = reshape ( G . alive , (N , N ))

@ ne t w o rk \_ op er a ti o n
def update ():

n = convolve2d (A , K , mode = ’ same ’)
A [:] = maximum (0 ,1.0 -

(n <1.5) -( n >3.5) -( n <2.5)\*(1 - A ))

from dana import \*

n = 256
src = Group (( n , n ) ,

’’’V = maximum (0 ,1.0 -( N <1.5) - \
(N >3.5) - \
(N <2.5)\*(1 - V )); N ’’’)

C = S par seC onn ect ion ( src ( ’V ’) , src ( ’N ’) ,

np . array ([[1. , 1. , 1.] ,
[1. , 0. , 1.] ,
[1. , 1. , 1.]]))

src . V = rnd . randint (2 , src . shape )

run (500\* defaultclock . dt )

run ( n =500)

Figure 10: Game of life written using Brian (left) and DANA (right). The Brian version is much slower only
due to the lack of dedicated connections (SparseConnection in this example).

A.2 Integrate and Fire

from brian import \*
tau = 20 \* msecond
Vt = -50 \* mvolt
Vr = -60 \* mvolt
El = -49 \* mvolt
psp = 0.5 \* mvolt

G = NeuronGroup (40 ,

from dana import \*
msecond = 0.001
mvolt = 0.001
tau = 20.0 \* msecond
Vt = -50.0 \* mvolt
Vr = -60.0 \* mvolt
El = -49.0 \* mvolt
psp = 0.5 \* mvolt

" dV / dt = -(V - El )/ tau : volt " ,
threshold = Vt , reset = Vr )

C = Connection (G , G )
C . connect\_random (

G = zeros (40 , """ dV / dt = -(V - El )/ tau + I \* psp

S = V > Vt ; I """ )

W = ( rnd . uniform (0 ,1 ,(40 ,40))

sparseness =0.1 , weight = psp )

\* ( rnd . random ((40 ,40)) < 0.1))

G . V = Vr + rand (40) \* ( Vt - Vr )

C = S par seC onn ect ion ( G ( ’S ’) , G ( ’I ’) , W )
G . V = Vr + rnd . random (40) \* ( Vt - Vr )

@after ( clock . tick )
def spike ( t ):

G . V = np . where ( G .V > Vt , Vr , G . V )

run (1 \* second )

run (1 \* second )

Figure 11: Integrate and ﬁre model written using Brian (right) and DANA (left). While it is possible to deﬁne
a spiking neuron model in DANA, the resulting simulation is highly ineﬃcient compared to Brian performances.

13

B Beyond neuroscience

B.1 A reaction-diﬀusion system Pearson (1993)

Du , Dv , F , k = 0.16 , 0.08 , 0.020 , 0.055
Z = Group ( (128 ,128) ,

""" du / dt = Du \* Lu - Z + F \*(1 - U ) : float
dv / dt = Dv \* Lv + Z - ( F + k )\* V : float
: float
U = np . maximum (u ,0)
: float
V = np . maximum (v ,0)
Z = U \* V \* V
: float
Lu ; Lv """ )

K = np . array ([[ np . NaN ,

1. ,
[
[ np . NaN ,

-4. ,

1. , np . NaN ] ,
] ,

1.
1. , np . NaN ]])

Sp ars eCo nn ect io n ( Z ( ’U ’) , Z ( ’ Lu ’) ,K , toric = True )
Sp ars eCo nn ect io n ( Z ( ’V ’) , Z ( ’ Lv ’) ,K , toric = True )

run ( n =10000)

Figure 12: Reaction Diﬀusion Gray-Scott model from Pearson (1993). Full script with graphic code is 110
lines.

References

Aisa, B., Mingus, B., O’Reilly, R., 2008. The emergent neural modeling system. Neural Networks 21, 1146–1152.

Bednar, J., 2008. Topographica: Building and analyzing map-level simulations from python, c/c++, matlab,

nest, or neuron components. Frontiers in Neuroinformatics 3 (8).

Bienenstock, E., Cooper, L., Munro, P., 1982. Theory for the development of neuron selectivity: orientation

speciﬁcity and binocular interaction in visual cortex. Journal of Neuroscience 2 (1), 32–48.

Carnevale, N., Hines, M., 2006. The NEURON Book. Cambridge University Press.

de Kamps, M., Baier, V., Drever, J., Dietz, M., M¨osenlechner, L., van der Velde, F., 2008. 2008 special issue:

The state of miind. Neural Networks 21 (8), 1164–1181.

Fates, N., 2009. Asynchronism induces second order phase transitions in elementary cellular automata. Journal

of Cellular Automata 4 (1), 21–38.

Fix, J., Rougier, N. P., Alexandre, F., 2011. A Dynamic Neural Field Approach to the Covert and Overt Deploy-

ment of Spatial Attention. Cognitive Computation 3 (1), 279–293.

Fix, J., Vitay, J., Rougier, N. P., 2006. A Distributed Computational Model of Spatial Memory Anticipation
During a Visual Search Task. In: Butz, M. V., Sigaud, O., Pezzulo, G., Baldassarre, G. (Eds.), SAB ABiALS.
Vol. 4520 of Lecture Notes in Computer Science. Springer, pp. 170–188.

Gerstner, W., 2002. Spiking Neuron Models. Cambridge University Press, Ch. 10.

Gewaltig, M.-O., Diesmann, M., 2007. Nest (neural simulation tool). Scholarpedia 2 (4), 1430.

Goodman, D., Brette, R., 2008. Brian: a simulator for spiking neural networks in python. Frontiers in Neuroin-

formatics 2 (5), 1–10.

Gurney, K., Prescott, T., Redgrave, P., 2001a. A computational model of action selection in the basal ganglia i:

A new functional anatomy. Biological Cybernetics 84, 401–410.

Gurney, K., Prescott, T., Redgrave, P., 2001b. A computational model of action selection in the basal ganglia ii:

Analysis and simulation of behaviour. Biological Cybernetics 84, 411–423.

14

Hanke, M., Halchenko, Y., 2011. Neuroscience runs on gnu/linux. Frontiers in Neuroinformatics 5 (8).

Hutt, A., Rougier, N., 2010. Activity spread and breathers induced by ﬁnite transmission speeds in two-

dimensional neural ﬁelds. Physical Review E: Statistical, Nonlinear, and Soft Matter Physics 82.

Mitra, D., 1987. Asynchronous relaxations for the numerical solution of diﬀerential equations by parallel proces-

sors. SIAM journal on scientiﬁc and statistical computing 8 (1), 43–58.

Oja, E., 1982. Simpliﬁed neuron model as a principal component analyzer. Journal of Mathematical Biology

15 (3), 267–273.

O’Reilly, R., Y.Munakata, 2000. Computational Explorations in Cognitive Neuroscience: Understanding the

Mind by Simulating the Brain. MIT Press, Cambridge, MA, USA.

Pearson, J., 1993. Complex patterns in a simple system. Science 261 (5118), 189–192.

Reynolds, J. N., Wickens, J. R., 2002. Dopamine-dependent plasticity of corticostriatal synapses. Neural Networks

15, 507–521.

Robert, F., 1994. Les syst`emes dynamiques discrets. Vol. 19 of Math´ematiques et Applications. Springer.

Rougier, N., Hutt, A., 2009. Synchronous and Asynchronous Evaluation of Dynamic Neural Fields. Journal of

Diﬀerence Equations and Applications To appear.

Taouali, W., Vieville, T., Rougier, N., Alexandre, F., 2011. No clock to rule them all. Journal of Physiology -

Paris 105 (1–3), 83–90.

15

