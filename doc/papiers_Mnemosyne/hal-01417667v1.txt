Recurrent Neural Network Sentence Parser for Multiple Languages with
Flexible Meaning Representations for Home Scenarios Xavier Hinaut,
Johannes Twiefel

To cite this version:

Xavier Hinaut, Johannes Twiefel. Recurrent Neural Network Sentence
Parser for Multiple Languages with Flexible Meaning Representations for
Home Scenarios. IROS Workshop on Bio-inspired Social Robot Learning in
Home Scenarios, Oct 2016, Daejon, South Korea. ￿hal-01417667￿

HAL Id: hal-01417667

https://inria.hal.science/hal-01417667

Submitted on 15 Dec 2016

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

Recurrent Neural Network Sentence Parser for Multiple Languages with
Flexible Meaning Representations for Home Scenarios

Xavier Hinaut1 and Johannes Twiefel2

Abstract— We present a Recurrent Neural Network (RNN), namely an Echo
State Network (ESN), that performs sentence comprehension and can be
used for Human-Robot Interaction (HRI). The RNN is trained to map
sentence structures to meanings (i.e. predicates). We have previously
shown that this ESN is able to generalize to unknown sentence structures
in English and French. The ﬂexibility of the predicates it can learn to
produce enables one to use the model to explore language acquisition in
a developmental approach. This RNN has been encapsulated in a ROS module
which enables one to use it in a cognitive robotic architecture. Here,
for the ﬁrst time, we show that it can be trained to learn to parse
sentences related to home scenarios with higly ﬂexible predicate
representations and variable sentence structures. Moreover we apply it
to various languages, including some languages that were never tried
with the architecture before, namely German and Spanish. We conclude
that the representations are not limited to predicates, other type of
representations can be used.

I. INTRODUCTION

For humans, communicating with current robots is chal- lenging. It
involves either learning a programming language or using complex
interfaces. The most sophisticated ones, that recognise command given
orally, are limited to a pre- programmed set of stereotypical sentences
such as “Give me cup”. In this paper, we propose an approach that allows
one to use natural language when interacting with robots. Our
architecture enables one to train a sentence parser easily on
potentially many different contexts, including home scenarios
(e.g. grasping remote objects or cleaning furnitures or rooms) [1]. It
can also enable the users to directly teach new sentences to the robot
[2].

Current approaches typically involve developing methods speciﬁcally
aimed at — or only tested on — the English language. Speciﬁc parsers
then need to be designed for other languages, often adapting the
algorithms and methods to their speciﬁcities. Here, we propose to
overcome these limitations by providing a simple and ﬂexible way of
training a sentence parser for any arbitrary language, without requiring
any new programming effort. This enables to easily create parsers even
for languages that are spoken by a small population, thus broadening the
potential application of robotics to new communities.

*This work was partly supported by a Marie Curie Intra European
Fellowship within the 7th European Community Framework Programme:
EchoRob project (PIEF-GA-2013-627156).

1X. Hinaut is with Inria Bordeaux Sud-Ouest, Talence, France; LaBRI, UMR
5800, CNRS, Bordeaux INP, Universit´e de Bordeaux, Talence, France; and
Institut des Maladies Neurod´eg´en´eratives, UMR 5293, CNRS, Univer-
sit´e de Bordeaux, Bordeaux, France xavier.hinaut@inria.fr

2J. Twiefel

Informatik, twiefel@informatik.uni-hamburg.de

Universit¨at

is with Knowledge Technology Group, Department Germany 22527

Hamburg,

Hamburg,

Here, we propose to overcome these limitations by pro- viding a simple
and ﬂexible way of training a sentence parser for potentially any
language. We demonstrate the potential of our model in home scenarios
for four common European languages: English, German, Spanish and French.
To make home robotics possible and useful, we need robots that are able
to provide social adaptation to the user, and able to adapt continuously
over their whole lifetime. Our architecture is inspired from
neurosciences [3] and language acquisition theories [4][5][6] with the
goal of modelling human language use: thus it aims to have these social
and life-long adaptabilities.

First, we present the general ESN architecture. Then, we detail the
reservoir sentence processing model. Afterwards, we show how it can be
used within a ROS module, and give some perspectives. Finally, we
provide corpora examples for a home scenario in four languages, and show
how ﬂexible the meaning representations can be.

II. METHODS & RESULTS

A. Echo State Networks

The language module is based on an ESN [7] with leaky neurons: inputs
are projected to a random recurrent layer and a linear output layer
(called “read-out”) is modiﬁed by learning (which can also be done in an
online fashion). The units of the recurrent neural network have a leak
rate (α) hyper-parameter which corresponds to the inverse of a time
constant. These equations deﬁne the update of the ESN:

x(t + 1) = (1 − α)x(t) + αf (W inu(t + 1) + W x(t)) (1)

y(t) = W outx(t)

(2) 

with x(t), u(t) and y(t) the reservoir (i.e. hidden) state, the input
and the read-out (i.e. output) states respectively at time t, α the leak
rate, W , W in and W out the reservoir, the input and the output
matrices respectively and f the tanh activation function. After the
collection of all reservoir states the following equation deﬁnes how the
read-out (i.e. output) weights are trained:

W out = Y d[1; X]+

(3) 

with Y d the concatenation of the desired outputs, X the concatenation
of the reservoir states (over all time steps for all train sentences)
and M + the Moore-Penrose pseudo- inverse of matrix M . Hyper-parameters
that can be used for this task are the following: spectral radius: 1,
input scaling: 0.75, leak rate: 0.17, number of reservoir units: 100.

model performs:

• “Please give me the mug” → give(mug, me) • “Could you clean the table
with the sponge?”

→ clean(table, sponge)

• “Find the chocolate and bring it to me” → ﬁnd(chocolate),
bring(chocolate, me)

As shown, the system can robustly transform different types of
sentences. Recently, we have explored how ﬂexible our system is: we
found that it can handle various kinds of representations at the same
time. For instance, it allows to use nouns as main elements of a
predicate, and use its arguments to ﬁll in adjectives:

• “Search for the small pink object”

→ search(object), object(small, pink)

It also allows to process various kinds of complex sentences: • “Bring
me the newspaper which is on the table in the kitchen” →
bring(newspaper, me), newspaper(on, table), table(in, kitchen)

C. Preprocessing

Preprocessing to identify semantic words can be done in two ways, one
may be more convenient than the other depending on the application:

• deﬁne a list of function words (i.e. all

the words that are not in the meaning representation part of the
construction) and consider any other word as semantic word;

• deﬁne a list of semantic words (i.e. words that are in

meaning representations).

For some language like Spanish, one may need to extract some word
sufﬁxes (or preﬁxes) from semantic words. For instance, “sirve -lo” is
usually written as one word (“sirvelo”) even if its translation to
English is composed of two words (“serve it”).

Currently, there was no need to use part-of-speech (POS) tagging, but it
may help to add this complementary informa- tion in the input
representation of words.

D. Usage of the ROS Module

The proposed model was encapsulated in a ROS module [9]. It is coded in
the Python language and uses the rospy library. When running the
program, the reservoir model is trained using a text ﬁle and the ROS
service is initialized. The training text ﬁle contains sentences and
corresponding predicates and can be edited easily. If predicates
represent multiple actions that have to be performed in a particular
order, the predicates have to be speciﬁed in chronological order:

• bring coffee me, clean table; bring me the coffee before you clean the
table

• bring coffee me, cleaning table;

before cleaning the table bring me the coffee

This predicate representation enables one to easily integrate this model
into a robotic architecture. It enables the users to

Fig. 1. Sentences are converted to a sentence structure by replacing
semantic words by a SW marker. The ESN is given the sentence structure
word by word. Each word activates a different input unit. During
training, the connections to the readout layer are modiﬁed to learn the
mapping between the sentence structure and the arguments of the
predicates. When a sentence is tested, the most active units are bound
with the SW kept in the SWs memory to form the resulting predicate.
(Adapt. from [2].)

B. Reservoir Sentence Processing Model

How do children learn language? In particular, how do they associate the
structure of a sentence to its meaning? This question is linked to the
more general issue: how does the brain associate sequences of symbols to
internal symbolic or sub-symbolic representations? We propose a
framework to understand how language is acquired based on a simple and
generic neural architecture (Echo State Networks) [7] which is not
hand-crafted for a particular task.

The reservoir sentence processing model has been adapted from previous
experiments on a neuro-inspired model for sentence comprehension using
ESN [8] and its application to HRI [2]. The model learns the mapping of
the semantic words (SW; e.g. nouns, verbs) of a sentence onto the
different slots (the thematic roles: e.g. action, location) of a basic
event structure (e.g. action(object, location)). As depicted in Fig. 1,
the system processes a sentence as input and generates corresponding
predicates. Before being fed to the ESN, sentences are transformed into
a sentence structure (or grammatical construction) semantic words (SW),
i.e. nouns, verbs and adjectives that have to be assigned a thematic
role, are replaced by the SW item. The processing of the grammatical
construction is sequential (one word at a time) and the ﬁnal estimation
of the thematic roles for each SW is read-out at the end of the
sentence.

By processing constructions [5] and not sentences per se, the model is
able to bind a virtually unlimited number of sentences to these sentence
structures. Based only on a small training corpus (a few tens of
sentences), this enables the model to process future sentences with
currently unknown semantic words if the sentence structures are similar.
One major advantage of this neural network language module is that no
parsing grammar has to be deﬁned a priori: the system learns only from
the examples given in the training corpus. Here are some input/output
transformations that the language

theontoandSW…Sentence:PALput (toy, left)P(A, L)Semantic words
(SWs)Recurrent NeuralNetworkSW1SW2SW3P: PredicateA: AgentL:
LocationMeaning: P (A, L)Read-outLayerInputLayeron the left put the toy
.SW1SW2SW3putlefttoySWs MemoryFunction
words(FWs)PALPALleftputtoyInactive connectionActive connectionLearnable
connectionsFixed connectionsConnectionSW: Semantic word
item(e.g. toy)thisdeﬁne which kind of predicate representation they
want to use. Moreover, parentheses were removed in order to make the
typing and editing quicker: by convention the ﬁrst word is the
predicate, and the following words are the arguments. to the ROS
service: it accepts a sentence (text string) as input and returns an
array of predicates in real time. With an ESN of 100 units, the training
of 200 sentences takes about one second on a laptop computer. Testing a
sentence is of the order of 10 ms.

Once initialized, a request could be sent

E. Perspectives

In Hinaut et al. [10], it has been shown that the model can learn to
process sentences with out-of-vocabulary words. Moreover, we
demonstrated that it can generalize to unknown constructions in both
French and English at the same time. To illustrate how the robot
interaction works, a video can be seen at youtu.be/FpYDco3ZgkU [11][12].
The source code, implemented as a ROS module, is available at github.
com/neuronalX/EchoRob1.

This ROS module could be employed to process various hypotheses
generated by a speech recognition system (like in [12]), then returning
the retrieved predicates for each hypothesis, thus, enabling a semantic
analyser or world sim- ulator to choose the predicates with the highest
likelihood. Preliminary work has shown that the model could be trained
fully incrementally [13]: we plan to add this feature to the ROS module
in the future.

In a nutshell, the objectives of this model are to improve HRI and
provide models of language acquisition. From the HRI point of view, the
aim of using this neural network- based model is (1) to gain
adaptability because the system is trained on corpus examples (no need
to predeﬁne a parser for each language), (2) to be able to process
natural language sentences instead of stereotypical sentences (i.e. “put
cup left”), and (3) to be able to generalize to unknown sentence
structures (not in the training data set). Moreover, this model is quite
ﬂexible when changing the output predicate repre- sentations, as we have
shown here. From the computational neuroscience and developmental
robotics point of view, the aim of this architecture is to model and
test hypotheses about child learning processes of language acquisition
[4].

III. HOME CORPORA IN MULTIPLE LANGUAGES

A. Introduction

In this section, we show that the network is able to learn to parse
sentences related to home scenarios. In particular, similar networks
(with the same hyper-parameters) can learn to map sentences to
predicates in English, German, Spanish and French.

Each line contains a sentence together with its correspond- ing meaning
representation: left part of the semi-column is the meaning, right part
is the sentence. One can see that the way to write predicates is fairly
intuitive and can be done

1We would be very grateful if you could share you corpus by uploading it
to the repository. Thus we could enhance the model based on various
corpora.

without prior knowledge of a predeﬁned structure. Hereafter, we show the
four home corpora.

B. English

1)  open door ; open the door
2)  answer phone ; answer the phone
3)  water plants ; water the plants
4)  clear table ; clear the table
5)  take coffee, pour coffee into mug ; take the coffee and

pour it into the mug

6)  clean table, put mug on table ; clean the table and put

the mug on it

7)  put mug on left ; put the mug on the left
8)  get phone, bring phone me ; get the phone and bring

it to me

9)  go to bathroom ; go to the bathroom

10) make tea me; make me some tea

11) tell joke me ; tell me a joke

12) make sandwhich me, sandwhich tomatoes ; make me

a sandwhich with tomatoes

13) bring newspaper me, newspaper on table,

table in kitchen ; bring me the newspaper which is on the table in the
kitchen

14) bring dress me, dress blue, dress in closet ; bring me

the blue dress that is in the closet

15) bring pen me, pen blue, pen beside cup, cup red ; bring

me the blue pen beside the red cup

16) dance for me, clean ﬂoor, clean same time ; dance for

me and clean the ﬂoor at the same time

17) switch off light ; switch off the light
18) ﬁnd bottle, bottle water ; ﬁnd the bottle of water
19) search object, object small pink ; search for the small

pink object

20) search recipe internet, recipe tiramisu ; search the

recipe of tiramisu on internet

21) check if, if husband home, husband my, if ﬁve ; check

if my husband is at home at ﬁve

C. German

¨offne T¨ur ; ¨offne die T¨ur

1)  
2)  geh an Telefon ; geh an das Telefon
3)  gieße Pﬂanzen ; gieße die Pﬂanzen
4)  leere Tisch ; leere den Tisch
5)  nimm Kaffee, gieß Kaffee in Tasse ; nimm den Kaffee

and gieß ihn in die Tasse

6)  reinige Tisch, stell Tasse auf Tisch ; reinige den Tisch

und stell die Tasse auf ihn

7)  stell Tasse nach links ; stell die Tasse nach links
8)  nimm Telefon, bring Telefon mir ; nimm das Telefon

und bring es zu mir

9)  geh in Badezimmer ; geh in das Badezimmer

10) mach Tee mir ; mach mir etwas Tee

11) erz¨ahl Witz mir ; erz¨ahl mir einen Witz

12) mach Sandwich mir, Sandwich Tomaten ; mach mir

ein Sandwich mit Tomaten

13) bring Zeitung mir, Zeitung auf Tisch ; bring mir die

Zeitung, die auf dem Tisch ist

14) bring Kleid, Kleid blaue, Kleid im Schrank ; bring mir

das blaue Kleid , das im Schrank ist

15) bring Stift mir, Stift blauen, Stift neben Becher, Becher roten ;
    bring mir den blauen Stift neben dem roten Becher

16) tanz f¨ur mich, reinige Boden, reinige gleichen Zeit ; tanz f¨ur
    mich und reinige den Boden zur gleichen Zeit

17) schalte aus Licht ; schalte das Licht aus

18) ﬁnde Flasche, Flasche Wasser ; ﬁnde die Flasche mit

Wasser

19) such Objekt, Objekt kleine pinke ; such das kleine

pinke Objekt

4)  d´ebarrasse table ; d´ebarrasse la table
5)  prends caf´e, verse caf´e tasse ; prends le caf´e et verse

le dans la tasse

6)  nettoie table, met tasse dessus table ; nettoie la table

et met la tasse dessus

7)  met tasse gauche ; met la tasse `a gauche
8)  prends t´el´ephone, am`ene t´el´ephone moi ; prends le

t´el´ephone et am`ene le moi

9)  va dans salle de bain ; va dans la salle de bain

10) fais th´e moi ; fais moi du th´e

11) raconte blague moi ; raconte moi une blague

12) fais sandwich moi, sandwhich tomates ; fais moi un

13) such Rezept Internet, Rezept Tiramisu ; such das

sandwhich avec des tomates

21) 

Rezept f¨ur Tiramisu im Internet ¨uberpr¨ufe ob, ob Mann Hause, Mann
mein, ob f¨unf ; ¨uberpr¨ufe , ob mein Mann um f¨unf zu Hause ist

D. Spanish

1)  abre puerta ; abre la puerta
2)  contesta tel´efono ; contesta el tel´efono
3)  riega plantas ; riega las plantas
4)  liempia mesa ; limpia la mesa
5)  toma caf´e, sirve caf´e en taza ; toma el caf´e y sirve -lo

en la taza

6)  limpia mesa, pon taza sobre mesa ; limpia la mesa y

7)  apporte journal moi,

table dans cuisine ; apporte moi le journal qui est sur la table dans la
cuisine

journal sur table,

14) apporte robe moi, robe bleu, robe dans placard ; apporte moi la robe
    bleue qui est dans le placard

15) apporte stylo moi, stylo bleu, stylo cˆot´e tasse, tasse rouge ;
    apporte moi le stylo bleu `a cˆot´e de la tasse rouge

16) danse pour moi, nettoie sol, nettoie mˆeme temps; danse

pour moi et nettoie le sol en mˆeme temps

17) ´eteins lumiere ; ´eteins la lumiere
18) trouve bouteille, bouteille eau ; trouve la bouteille d’

pon la taza sobre ella

eau

7)  pon taza en izquierda ; pon la taza en la izquierda
8)  toma tel´efono, trae tel´efono -me ; toma el tel´efono y

trae -me -lo

9)  ve ba˜no ; ve al ba˜no

10) prepara t´e -me; prepara -me t´e

11) di broma -me ; di -me una broma

12) prepara sandwich -me, sandwich tomates ; prepara -me

13) cherche objet, objet petit rose ; cherche le petit objet

rose

20) cherche recette internet, recette tiramisu ; cherche la

recette du tiramisu sur internet

21) v´eriﬁe si, si mari maison, mari mon, si heures cinq ; v´eriﬁe si
    mon mari est a la maisona cinq heures

un sandwich con tomates

F. Remarks

13) trae periodico -me, periodico sobre mesa, mesa en cocina ; trae -me
    el peri´odico que est´a sobre la mesa en la cocina

14) trae vestido -me, vestido azul, vestido en armario ; trae

-me el vestido azul que est´a en el armario

15) trae l´apiz -me, l´apiz azul, l´apiz junto taza, taza roja ;

trae -me el l´apiz azul junto a la taza roja

16) baila para mi, limpia piso, limpia mismo tiempo ; baila

para mi y limpia el piso al mismo tiempo

17) apaga luz ; apaga la luz
18) encuentra botella, botella agua ; encuentra la botella

de agua

19) busca objeto, objeto rosado peque˜no ; busca el objeto

rosado peque˜no

20) busca receta internet, receta tiramis´u ; busca la receta

de tiramis´u en internet

All sentences in German, Spanish and French correspond to the
translation from English sentences, but there are some speciﬁcities in
each language which make the predicates not a direct translation word by
word. For instance, in the German sentence “geh an Telefon ; geh an das
Telefon” (“answer phone ; answer the phone”), the “an” is not present in
the English predicate because it is a “verb particle” (a German
speciﬁcity). In a similar way, for the Spanish sentence “toma el
telefono y trae -me -lo” (“get the phone and bring it to me”): the
direct translation of “to me” would be “a mi”, but it is more natural in
Spanish to attach “-me” to the verb. When grammatical markers such as
“-me” are used in the meaning representation, we use “-” to indicate it
is not a word by itself, and the direction indicates to which word this
marker should be attached.

21) chequea si, si marido casa, marido mi, si cinco ;

chequea si mi marido est´a en casa a las cinco

G. Results

E. French

1)  ouvre porte ; ouvre la porte
2)  r´eponds t´el´ephone ; r´eponds au t´el´ephone
3)  arrose plantes ; arrose les plantes

A network of 100 neurons is able to learn a set of 21 sentence-predicate
pairs in any of these languages (i.e. reproduce exactly the same
predicates for each sentence). In particular, a network of 75 units is
able to learn separately any of English, French or Spanish corpora. Only
the German

corpus needs a 100 unit reservoir to be learned perfectly2. The
difference of units needed is not surprising since some language
speciﬁcities (for this tiny highly variable data set) may be more
difﬁcult than others to learn3. Moreover, we found for instance that a
network with just 150 neurons was able to learn both English and German
corpora at the same time. A network of size 325 was needed to learn the
four corpora at the same time. Thus, the learning capabilities of the
network seems to increase linearly when adding new languages4. This
needs to be conﬁrmed with further experiments.

We did not investigate generalization concerning these corpora, even if
we already demonstrated that the same network could generalize on both
English and French at the same time [10]. It was out of scope here; we
focus on the ﬂexibility of the meaning representations, and in
particular in home scenarios. Furthermore, the corpora are of tiny size
(only 21 sentences for each language) with a high variability in their
structure. Consequently, each sentence is nearly unique in its structure
(and its use of function words), which does not enable generalization as
shown in previous studies.

IV. DISCUSSION

We proposed an approach that allows people to use natural language when
interacting with robots. Our architecture enables to train a sentence
parser easily on home scenarios: no parsing grammar has to be deﬁned a
priori, it only relies on the particular corpus used for training.
Moreover, we showed that this neural parser is not limited to one
language, but was able to learn corpora in four different languages at
the same time: English, German, Spanish and French. In [3] an anterior
version of the model was able to learn a Japanese copora, thus this
neural model is applicable to many languages.

We used an Echo State Network (ESN) for the core part of the module for
two main reasons. Firstly, it aims at modelling brain processes of
language comprehension and language acquisition [8]. Reservoir Computing
paradigm (such as ESNs) is considered to be a biologically plausible
model of “canonical circuits” (i.e. generic pieces of cortex) [14][15]
and is also used to decode neural activity of primate prefrontal cortex
[16]. Secondly, we believe that HRI applica- tions need modules that can
be trained quickly (e.g. via one- shot ofﬂine learning), executed in
tens of milliseconds, and can process (and provide outputs) in an
incremental fashion (word by word).

The main drawback of the architecture is the fact that it only relies on
function words to interpret a sentence: this may cause some ambiguities
when an identical sentence structure corresponds to different meanings
(see [8] for more details). However, reliance on function words is also
its strength since it enables to generalize to new inputs with only tens

2We only changed the network size by 25 unit steps. 3In our experiments,
the German sentence-predicate pair “switch off light; switch off the
light” was the only one not learned by a 75 units network.
475(English)+75(Spanish)+75(F rench)+100(German) = 325

of sentence structures in the training corpus. We believe this is not a
limitation of the architecture itself since some current unpublished
experiments show that real semantic words could be used instead of SW
markers (i.e. ﬁllers).

The ability of the model to scale to larger copora depends on the
similarity of the sentence structures in the corpus. If sentence
structures are different variants of similar ex- pressions (usually in a
same home context), then there is no need to use an high number of units
in the reservoir. For instance in [8] we have shown that a 5.000 units
reservoir could generalize up to 85% on unkown sentence structures on a
corpus of size 90.000. Conversely, if sentence structures are highly
different, the number of units should be increased (apparently linearly)
with the number of sentences. Moreover, if sentence structures are
similar or applied to the same home context (e.g. kitchen, living room,
…), the generalization to new sentence structures will come for free
(with a quick tuning of main parameters: spectral radius, leak rate and
input scaling). In future work, we could limit the number of reservoir
unit needed if some contextual inputs are provided to the network: for
instance, kitchen or living room environment will not provide the same
context input, thus giving the network more information to
discriminate/parse input sentences.

The way to write predicates (left hand of each line) is fairly intuitive
and can be done without prior knowledge of a predeﬁned structure like
Robot Control Language (RCL) commands [17]. In other words, we propose
to let the users deﬁne which meaning/predicate representation they want
to use. For instance, some words like “on” can be used in the meaning
representation when needed (e.g. “clean table, put mug on table ; clean
the table and put the mug on it”), and can be discarded when they are
not necessary (e.g. “search recipe internet, recipe tiramisu ; search
the recipe of tiramisu on internet”). Accordingly, one can adapt the
meaning representation to any kind of “series of slots” as soon as there
are consistent with each other: consistency should be kept both within
slots and in the linear organization of the slots. In conclusion, this
means that we are not limited to predicates, and our architecture could
be used to learn other type of representations.

ACKNOWLEDGMENT

We thank Ikram Chraibi Kaadoud, Pramod Kaushik, Louis Devers, and Iris
Wieser for their ideas on the home corpora, Francisco Cruz for his nice
help on the Spanish corpus, and Fabien Benureau for his very/damn useful
feedback.

REFERENCES

[1] F. Cruz, J. Twiefel, S. Magg, C. Weber, and S. Wermter. Interactive
re- inforcement learning through speech guidance in a domestic scenario.
In Proc. of IJCNN, pages 1–8. IEEE, 2015.

[2] X. Hinaut, M. Petit, G. Pointeau, and P. F. Dominey. Exploring the
acquisition and production of grammatical constructions through
Frontiers in human-robot interaction with echo state networks.
Neurorobotics, 8, 2014.

[3] P.F. Dominey, M. Hoen, and T. Inui. A neurolinguistic model of gram-
matical construction processing. Journal of Cognitive Neuroscience,
18(12):2088–2107, 2006.

[15] W. Maass, T. Natschl¨ager, and H. Markram. Real-time computing
without stable states: A new framework for neural computation based on
perturbations. Neural computation, 14(11):2531–2560, 2002. [16] M.
Rigotti, O. Barak, M.R. Warden, X.J. Wang, N.D. Daw, E.K. Miller, and S.
Fusi. The importance of mixed selectivity in complex cognitive tasks.
Nature, 497(7451):585–590, 2013.

[17] K. Dukes. Semeval-2014 task 6: Supervised semantic parsing of

robotic spatial commands. SemEval 2014, page 45, 2014.

[4] M. Tomasello. Constructing a language: A usage based approach to
language acquisition. Cambridge, MA: Harvard University Press, 2003.

[5] A.E. Goldberg. Constructions: A construction grammar approach to

argument structure. University of Chicago Press, 1995.

[6] E. Bates, B. Wulfeck, and B. MacWhinney. Cross-linguistic research
in aphasia: An overview. Brain and language, 41(2):123–148, 1991. [7] H.
Jaeger. The “echo state” approach to analysing and training re- current
neural networks. Bonn, Germany: German National Research Center for
Information Technology GMD Technical Report, 148:34, 2001.

[8] X. Hinaut and P.F. Dominey. Real-time parallel processing of gram-
matical structure in the fronto-striatal system: a recurrent network
simulation study using reservoir computing. PloS one, 8(2):e52946, 2013.

[9] X. Hinaut, J. Twiefel, and S. Wermter. Recurrent neural network for
syntax learning with ﬂexible predicates for robotic architectures. In
Proc. of the IEEE Conference on Development and Learning and Epigenetic
Robotics (ICDL-EpiRob). IEEE, 2016.

[10] X. Hinaut, J. Twiefel, M. Petit, P. Dominey, and S. Wermter. A
recurrent neural network for multiple language acquisition: Starting In
NIPS 2015 Workshop on Cognitive with english and french. Computation:
Integrating Neural and Symbolic Approaches, 2015.

[11] X. Hinaut, J. Twiefel, M. Borghetti Soares, P. Barros, L. Mici, and
S. Wermter. Humanoidly speaking – learning about the world and language
with a humanoid friendly robot. In IJCAI Video competition, Buenos
Aires, Argentina. https://youtu.be/FpYDco3ZgkU, 2015. [12] J. Twiefel,
X. Hinaut, M. Borghetti, E. Strahl, and S. Wermter. Using Natural
Language Feedback in a Neuro-inspired Integrated Multimodal In Proc. of
RO-MAN, New York City, USA, Robotic Architecture. 2016.

[13] X. Hinaut and S. Wermter. An incremental approach to language
acquisition: Thematic role assignment with echo state networks. In Proc.
of ICANN 2014, pages 33–40, 2014.

[14] P.F. Dominey. Complex sensory-motor sequence learning based on
recurrent state representation and reinforcement learning. Biological
cybernetics, 73(3):265–274, 1995.


