Canary Vocal Sensorimotor Model with RNN Decoder
and Low-dimensional GAN Generator
Silvia Pagliarini, Arthur Leblois, Xavier Hinaut

To cite this version:

Silvia Pagliarini, Arthur Leblois, Xavier Hinaut. Canary Vocal Sensorimotor Model with RNN Decoder
and Low-dimensional GAN Generator. ICDL 2021- IEEE International Conference on Development
and Learning, Aug 2021, Beijing, China. ￿hal-03482372￿

HAL Id: hal-03482372

https://inria.hal.science/hal-03482372

Submitted on 15 Dec 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Canary Vocal Sensorimotor Model with RNN
Decoder and Low-dimensional GAN Generator

Silvia Pagliarini
Inria Bordeaux Sud-Ouest, Talence, France.
LaBRI, UMR 5800, CNRS, Bordeaux INP,
IMN, UMR 5293, CNRS,
Universit´e de Bordeaux, France.
silvia.pagliarini@inria.fr

Arthur Leblois\*
IMN, UMR 5293, CNRS,
Universit´e de Bordeaux, France.
arthur.leblois@u-bordeaux.fr

Xavier Hinaut\*
Inria Bordeaux Sud-Ouest, Talence, France.
LaBRI, UMR 5800, CNRS, Bordeaux INP,
IMN, UMR 5293, CNRS,
Universit´e de Bordeaux, France.
xavier.hinaut@inria.fr

Abstract—Songbirds, like humans, learn to imitate sounds pro-
duced by adult conspeciﬁcs. Similarly, a complete vocal learning
model should be able to produce, perceive and imitate realistic
sounds. We propose (1) to use a low-dimensional generator model
obtained from training WaveGAN on a canary vocalizations,
(2) to use a RNN-classiﬁer to model sensory processing. In this
scenario, can a simple Hebbian learning rule drive the learning
of the inverse model linking the perceptual space and the motor
space? First, we study how the motor latent space topology
affects the learning process. We then investigate the inﬂuence
of the learning rate and of the motor latent space dimension. We
observe that a simple Hebbian rule is able to drive the learning
of realistic sounds produced via a low-dimensional GAN.

I. INTRODUCTION

Vocal learning represents the ability to produce new sounds
via imitation. In humans, vocal learning allows infants to learn
to produce speech through the parallel development of speech
perception and production ability [1], [2]. Among complex
vocal learners, songbirds represent the most studied model
organisms for vocal learning. Songbirds share with humans
similar vocal development [3]. Vocal learning starts with a
sensory learning phase, when infants and juvenile songbirds
learn to discriminate the sounds they hear from conspeciﬁc
adults. Then, vocal learning resumes with a sensori-motor
phase during which the infants/juvenile start to produce their
own vocalizations. Human babies start producing non-speech
sounds (e.g., cries) considered as precursor of speech at birth,
then produce vowel-like sounds around the third and seventh
month of their life [4], [5] and produce canonical babbling
after seven months [1], [5]. Vocal production in juvenile
songbirds is also gradual: they start with a variable babbling
behavior, then slowly adapt their vocalizations to incorporate
elements of the tutor song and ﬁnally produce highly complex,
stereotyped motifs in adulthood [6]. While humans and song-
birds share analogous brain circuits for vocal learning [7], a
circuit dedicated to song learning in birds facilitates the study
of the underlying neuronal mechanisms [6].

The basic structure of a vocal learning schema involves
three spaces (motor, sensory, perceptual), the motor control
the sensory response function, and the learning
function,

\* Corresponding authors that co-supervised the study.

architecture [8], [9]. The motor space contains the motor
coordinates. The sensory space contains the real sounds. The
perceptual space represents the encoding of the sounds cate-
gories: such perceptual categories can be seen as perceptual
goals. The motor control function allows the production of
sound. The sensory response function processes the sound and
encodes it in a low-dimensional space (the perceptual space).
Alternatively, the sensory response function can provide a
reward to the model. The learning architecture deﬁnes the
learning algorithm, and the exploration strategy. Several learn-
ing frameworks have been proposed to model vocal learning in
humans and birds. A recent comparative review summarizes
a wide set of models, their objectives and how the various
components have been deﬁned [9].

In the songbird literature, the motor control function has
been often deﬁned using a system of ordinary differential
equations that model the anatomy of the syrinx (i.e., the birds’
vocal organ) [10], or the features of sound [11]. Recently,
generative networks have been introduced to solve tasks such
as image, music, and speech generation or classiﬁcation and
have been used to investigate visual pathways in the brain [12].
The advantages of using generative neural networks are to
obtain resemblance of the generated data with the real data
from an uniformly distributed low-dimensional motor space.
We propose a canary sensorimotor model where the motor
function and the sensory response function are implemented
in a novel way. On the one hand, the motor function is im-
plemented using a low-dimensional generator model obtained
from a Generative Adversarial Network (GAN) generator. As
shown in [13], such a model shows the ability of producing
realistic sounds (canary syllables), and represents an alterna-
tive to previously proposed vocal tract models. On the other
hand, the sensory response function is deﬁned as a Recurrent
Neural Network (RNN) classiﬁer [14] implemented with the
ReservoirPy library [15]. For this study, the classiﬁer and the
GAN are pre-trained. The connections between perceptual and
motor spaces – that form the inverse model – are learned
through activity-dependent plasticity. We check whether or not
a simple Hebbian learning rule combined with random motor
exploration are sufﬁcient to build an inverse model between
perceptual and motor representations of a 16-syllables canary

repertoire.

Section II introduces the components of the proposed vocal
learning model. Section III shows the results, including the
exploration of the structure of the motor latent space, and
the inﬂuence of various conditions on learning. Section IV
summarizes the advantages and the limitations of the model,
and discusses possible perspectives to expand this work.

All

the details of the implementation are available at

github.com/spagliarini/canary-vocal-sensorimotor-model

II. METHODS
The proposed model contains three spaces (perceptual,
motor, sensory), a motor control function, a sensory response
function, and an inverse model (Figure 1). Section II-A
introduces the structure of the model. Section II-B details
the learning algorithm in the inverse model, Section II-C the
motor control, and Section II-D the sensory system. Finally,
Section II-E contains the experimental setup.

A. General architecture

A one-layer perceptron models the connections between
the perceptual space and the motor space, see Figure 1. The
ﬁrst layer (P1, .., PnP ) represents the perceptual space P. The
second layer (M1, .., MnM ) represents the motor space M.
At each time step t, the perceptual units are deﬁned as a
nP -dimensional vector Pt, where nP represents the size of
the perceptual layer. The motor units are deﬁned as a nM -
dimensional vector Mt, where nM represents the number of
motor parameters. The synaptic weights at t of the inverse
model describing the connections between the motor and the
perceptual space are deﬁned by matrix Wt. Given a motor
pattern Mt, the motor control function G provides a real sound
St (i.e., an element of the sensory space). The sensory space
S is the domain of the sensory response function: at each time
step t, the sensory response A is a function of the actual sounds
produced St (i.e., Pt = A(St)).
B. Inverse model and Hebbian learning

The aim of the inverse model I is to learn the link between
perceptual and motor space. At each time step t, a motor
pattern Mt is drawn from [−1, 1]nM and enables a sensory
response Pt. Learning is driven by the Hebbian learning rule

∆Wt = ηMtPt,

(1)

where Wt represents the synaptic weight and η the learning
rate. The synaptic weights Wt=t0 are initialized as random
uniform values and vary according with Equation 1 until time
t = tf . The motor space is explored using random exploration.
C. Motor control

As motor control function G, the generator part of a nM -
dimensional GAN (where nM ∈ {1, 2, 3, 6}) is used to pro-
duce sounds: e.g. during motor exploration or when we want to
evaluate the architecture during training. The generator model
has been previously obtained by training WaveGAN [17] on a
dataset of canary syllables [18]: it is able to provide syllables
similar to the real ones [13]. As one could expect, one syllable
class can be produced using multiple motor conﬁgurations.

Fig. 1: Vocal learning model schema.. The model contains three spaces:
the perceptual space, the motor space, and the sensory space. A one-layer
perceptron connects the perceptual space (P1, .., PnP ) to the motor space
(M1, .., MnM ). Wt represents the synaptic connections between perceptual
and motor spaces at each time step t. The motor control function G is a nM -
dimensional generator of a GAN that enables sound production. At each time
step t, the sensory response Pt is a function of the actual sound production
(i.e., Pt = A(St), where St is the actual sound produced by G at time
step t. The sensory response function is composed of a RNN-classiﬁer and a
normalization layer to restrict the obtained activation in the interval [0, 1].

Fig. 2: Repertoire and training dataset. Low-dimensional representation
of the training dataset obtained using Uniform Manifold Approximation and
Projection (UMAP) [16]. Each point represents the spectrogram of a syllable,
and each colored cluster represents a class Ri. A template syllable of each
class is highlighted with the corresponding spectrogram (an arrow connects
each cluster to the corresponding template). Image from [13].

D. Sensory system

The sensory system is able to detect syllables belonging to a
canary repertoire R composed of N different syllable classes

and an alternative class X. We deﬁne the vocabulary V as the
set containing all classes V = R ∪ {X}. The repertoire R is
composed by N classes Ri for i ∈ [1, N ]. The class X repre-
sents distorted syllables that the classiﬁer is not able to assign
to a class of the repertoire (e.g. inter-syllabic transitions) and
has been built using bad WaveGAN generations (e.g. early
epochs of GAN training) and white noise [13]. Note that the
generator model G was trained with R (not V ).

In our experiments, the dimension of the perceptual space
is given by nP = N , as the repertoire contains N different
syllable classes. The output of the sensory response function
has always dimension N + 1 since class X is included.
The sensory response function A (see Fig 1) is made of
two components. First, a RNN classiﬁer takes a sound St (a
syllable) as input and provides a distribution of classes - the
probability of St to belong to each class of the repertoire [14].
Then, a normalization layer scales the obtained activation to
values in [0, 1]. As the RNN-classiﬁer produces a distribution
of activations that depends on the syllables class (some are
often very high and some often very low), we need to nor-
malize the classiﬁer outputs in order to have a more balanced
distribution among syllable classes. This will reduce the bias
of the perceptual part of the architecture during learning.
Hereafter, we explain the normalization that we performed.
First, we precomputed the 95-percentile for each class of the
vocabulary as follows:

1) for all the precomputed motor patterns Mi (16k in total)
the sensory output Si has been generated and processed
by the classiﬁer;

2) from the classiﬁer outputs, we selected1 the peak of the

most active output YM (S);

3) for each class in the repertoire, we computed the 95-
percentile p95 from all YM (S) obtained (from all gen-
erated motor patterns in step 1).

Then, during motor random exploration at time step t, the
perceptual activation P Ri
of a sound is computed, for each
class Ri, i ∈ [1, nP ], as the maximum activation YM divided
element-wise by the global 95-percentile obtained for class
Ri, i ∈ [1, nP ]:

t

P Ri
t = ARi(S) =






1
Y Ri
M (S)
p95Ri

if Y Ri

M (S) > p95Ri,
otherwise,

(2)

where Ri represents each class of the vocabulary, Y Ri
M (St)
represents the maximum activation of sound St provided by
the classiﬁer for Ri, and p95Ri represents the precomputed
global 95-percentile of Ri. Such deﬁnition for auditory acti-
vation leads to ∼ 5% of the motor latent space leading to
the production of a given syllable class. This ﬁnal vector
corresponds to the activation of the perceptual layer of the
inverse model.

E. Experimental setup

The training dataset used for this work has been obtained
from a larger set of adult canary recordings [18]. In [13],
WaveGAN was trained with the syllables classes containing
enough samples leading to a nP = 16 classes repertoire R.
Syllable examples of each class Ri can be seen in Figure 2.
The representation obtained using Uniform Manifold Approx-
imation and Projection (UMAP) [16] shows the clusters that
compose the training dataset.

During learning, at each time step t, a random motor
vector Mt (i.e. nM -dimensional vector taking random values
in [−1, 1]) is given as input to the motor control function G.
This generator model produces a syllable sound St = G(Mt),
and the sensory response function computes the corresponding
perceptual representation Pt. For simplicity, a set of 16k
motor vectors, the corresponding syllables waveforms, and
the corresponding perceptual representations have been pre-
computed beforehand. The inverse model synaptic weights
W ∈ M 16×nM are initialized as Wt0 ∈ U [−0.001, 0.001].

We studied (1) the inﬂuence of the motor space dimension
on the learning, (2) the inﬂuence of using different learning
rates (η = 0.01 versus η = 0.1) on the learning. We stopped
the learning after 3k time steps. We trained three model
instances for each condition keeping ﬁxed the initial synaptic
weights Wt0 between motor and perceptual spaces, but varying
the random motor exploration across instances. Indeed, as the
initial weights are close to zero, they have negligible inﬂuence
on the learning. Conversely, the random motor exploration
affects more the initial phases of learning.

F. Evaluation

To evaluate the inverse model during learning, we need to
see if each given perceptual neuron (each corresponding to one
syllable class) can activate the motor layer and consequently
produce the sound of the correct syllable. Thus, we evaluate
(for each syllable) what would be the evoked activation of
the perceptual layer obtained via the full sensorimotor loop:
we evaluate every 15 time steps. To do so, we (1) activate
one given perceptual unit i of the inverse model ˆP Ri (i.e.
the ideal perceptual pattern of syllable i), (2) record the
motor pattern ˜M Ri
produced through Wt, (3) use this motor
pattern to generate a sound St through the GAN generator,
(4) record the evoked perceptual activation ˜P Ri
. If the evoked
activation is at 1 (i.e. the perceptual goal is reached), then
the syllable is considered to be learned by the inverse model.
Figures 7 and 8 show these evoked perceptual activations
for each given syllable. For each class Ri (i ∈ [1, N ]) of
the repertoire, the ideal perceptual activation is encoded as
one-hot vector. During the perceptual evaluation, the motor
pattern at time t is deﬁned as ˜M Ri
t = f (Wt ˆP Ri), where Wt
is the matrix of the synaptic weights at time step t, ˆP Ri is
2, and f a piecewise
the ideal auditory pattern of class Ri

t

t

1Please note that one generated syllable has a length of 1 sec. and in average
the longest syllables have a duration of 300 ms. Thus, we restricted the output
activity to the ﬁrst 500 ms before selecting the maximum activation.

2We do no use goal-babbling here, but if we were to do so, ˆP Ri would
be the perceptual goal driving the exploration to produce target syllable Ri.

Fig. 3: Three-dimensional motor latent space. Each point represents a 3-dimensional motor pattern M belonging to the motor latent space. Each
motor pattern takes values in [−1, 1]3. Each ﬁgure represents a slice of the three dimensional cube: z1 component has been ﬁxed to a given interval (e.g.,
z1 ∈ [−0.25, 0] in the right panel), while components z0 and z2 are free. Black points represent motor patterns that lead to the production of syllables
belonging to class X, each other color correspond to a class of the repertoire. These patterns are often making junctions between patterns which lead to the
production of two different syllables of the repertoire.

nM = 1
nM = 3

A
5.02
5.43

B1
7.19
6.84

B2
4.79
6.64

C
6.38
5.38

D
5.48
5.89

E
5.04
5.23

H
4.86
4.76

J1
5.86
5.4

J2
5.82
5.4

L
4.87
5.77

M
4
5.12

N
4.7
3.81

O
6.85
4.09

Q
1.31
1.92

R
2.8
3.64

V
7.39
6.03

X
17.64
18.64

TABLE I: Motor latent space composition. Percentage of syllables belonging to each class for motor dimensions 1 and 3.

linear function3 that restricts the values of ˜M Ri
in the interval
[−1, 1]. By construction, the input space for the generator lies
in [−1, 1].

t

III. RESULTS

This section shows the results, including the exploration of
the structure of the motor latent space, and the inﬂuence of
various conditions on learning.

A. Structure of the motor latent space

We remind that at each time step t, the model explores a
motor pattern Mt which enables the generations of a sound
and, consequently, the corresponding sensory response. Each
motor pattern is randomly chosen from a predeﬁned set
containing 16k motor patterns, and is a nM -dimensional vector
in [−1, 1]nM . In Figures 3 and 4, we observe that motor
patterns are organized in clusters in the motor latent space,
when coloring each motor pattern with the class of sound it is
generating. We observe that transitions between two clusters
(i.e. between two different classes) are often characterized by
the presence of patterns that lead to the production of syllables
belonging to class the alternative class X.

Interestingly,

the structure of the latent space can help
understanding the learning dynamics (see Figures 7 and 8).
For instance, when nM = 3, syllable O (blue navy dots in
Figure 3) is characterized by a sparse “cluster” intermixed
with the alternative classe. This may result in O being a more
challenging syllable to learn for the model. This is coherent
with the fact that, in biological and robotic systems, some
gestures may be easier to learn than others. Additionally, this

3The piecewise linear function we use in this work is deﬁned as follows:
j > 1, ˜M Ri
j < −1, and

j = −1 if ˜M Ri

j = 1 if ˜M Ri

∀j ∈ [0, nM ], ˜M Ri
˜M Ri

j = ˜M Ri

j

otherwise.

Fig. 4: One-dimensional motor latent space. Structure of the motor latent
space when nM = 1: each point represents a 1-dimensional motor pattern
M . Legend is the same of Figure 3. Similarly to the 3-dimensional space (see
Figure 3), class X is spread in space and often makes the junction between two
different clusters. It shows that the GAN is interpolating in-between existing
classes. The pairs of syllables B1/B2 and J1/J2 are very similar, which explains
why they are intermixed in the latent space.

representation shows that the latent space does not contain a

z1 = [0, 0.25]z1 = [-0.5, -0.25]z1 = [-0.25, 0]z1 = [0, 0.25]z1 = [-1, -0.75]z1 = [-0.5, -0.25]z1 = [-0.5, -0.25]z1 = [-1, -0.75]z1 = [0, 0.25]z0AB1B2CDEHJ1J2LMNOQRVXAB1B2CDEHJ1J2LMNOQRVXneutral position: any point of the space will generate a sound
that can be classiﬁed. This is why the sensory response can
be greater than zero at the beginning of learning for some
syllables (i.e. when the synaptic connections are weak and
close to 0). In particular, for motor patterns close to the
origin O ∈ R3, syllables J2 and syllables J1 are produced
for nM = 3. Respectively, syllables L and syllables J2 are
produced for nM = 1.

Finally, the motor latent space is not a balanced space:
syllables are not equally represented (in percentage) in the
motor latent space (Table I). Extreme values are represented
in bold: for both nM = 1 and nM = 3, class Q is the
least represented whereas class X is the most represented. In
particular, the percentage of syllables belonging to class X
represents ∼ 18% of the total amount: this introduces learning
difﬁculties because these syllables should not be learned. Both
for nM = 1 and nM = 3, patterns producing class X syllables
(i.e. black points in Figures 3 and 4) lies at the junction of
repertoire syllables. Such a distribution of X syllables shows
that the generator model is able to generalize and produce
not only syllables belonging to the training dataset but also
intermediate syllables interpolating from different classes [13].

B. Evolution of learning and sounds produced

For each class Ri, i ∈ [1, nP ] of the repertoire R, the per-
ceptual activation P Ri across time is described by Equation 2
and takes values in [0, 1]. The learning is considered achieved
when P Ri stabilizes at 1. For most syllables, P Ri = 0 at time
t = t0. Then, oscillatory dynamics can be observed until the
activation stabilizes, as can be observed in the left panel of
Figure 5 for syllable R. A similar example can be found in
the right panel of Figure 5 for class B1. Although realistic
syllables can be produced since the beginning (due to the
absence of neutral position), the random motor exploration
allows to produce the correct syllables towards the end of the
training (bottom panel of Figure 5). One can notice that at
t = 0 the same syllable is produced both for class B1 and
R: indeed, as mentioned in Section III-A, there is no neutral
position for the motor pattern. Instead, syllable J1 is produced.
Towards the end of the training, the produced syllables become
similar to the corresponding target in the repertoire.

In Figure 6, we see that the sounds produced, when the
perceptual activation is higher than a certain threshold thsP =
0.99, are stable for the majority of the classes. Empty boxes
mean that the perceptual activation never crosses thsP (for
syllables O and syllable B2). In the top-left panel of Figure 6,
we see that class A represents an exception with respect to
the other classes: syllables belonging to other classes (in
particular, to class R) inﬂuences the mean spectrogram4. In
Figure 3, we can see that A (lime green points) and R (light
pink points) can be blended with alternative class X in the
motor latent space. Thus, it is probable that some sounds
generated by the GAN are actually interpolations between A

4Shape features of syllable A can be seen in Figure 2.

and R which are classiﬁed as A5. This is a potential explanation
for the mixed shape of the mean spectrogram of class A.

C. Evolution of learning and learning rate

A learning rate of η = 0.1 (red lines in Figure 7) can
induce faster changes in the synaptic weights (i. e., in Wt)
with respect to η = 0.01 (blue lines in Figure 7). Nevertheless,
both for η = 0.1 and η = 0.01, the perceptual activation
increases and reaches the optimal plateau (a value of 1) for
14 syllables over 16. Moreover, one can expect that syllable
B2 can be learned if a longer simulation is performed. Some
syllables, like C and H have a decay after having reached the
plateau. Such decay is due to the fact that the learning is driven
by a simple Hebbian learning rule which is not expected to
converge (due to the absence of normalization). Thus, a decay
could also be expected for all syllables if the simulation would
go on, except for the syllables that are at the boundaries of the
motor latent space (−1 or 1 for each coordinate). That is, if
the time is long enough, we expect a decay for all the classes.

D. Evolution of learning and motor space topology

The structure of the motor latent space has an inﬂuence on
the learning. Observing both (Table I) and Figure 7, one can
see that even if syllable Q is the least represented the model is
able to reach P Q = 1 during learning. Alternatively, the model
struggles in learning syllable O: although it is well represented
in the motor latent space. As mentioned in Section III-A, the
“cluster O” (blue navy points in Figure 3) seems to introduce a
challenge for the model because it is not convex6. Clear convex
clusters are probably not necessary for the simple learning rule
we used, but convexity of the motor space helps the learning.

E. Evolution of learning and motor space dimension

A higher motor dimension allows the model to learn a
higher number of classes of the repertoire. As we saw previ-
ously, a 3-dimensional motor space (yellow lines in Figure 8)
allows the learning of almost all
the syllables. A similar
behavior can be observed when nM ∈ 2, 6 (respectively, blue
and blak lines in Figure 8). Alternatively, a 1-dimensional
motor space (red lines) prevents the learning of most classes
of the repertoire: the perceptual activation never reaches one,
but for syllables J2 and syllable L. A higher learning rate
does not result in the learning of a higher number of classes
(see Figure 7). The 1-dimensional null motor space produces
a syllable classiﬁed as L (light green blue points in the fourth
panel of Figure 4).7

5Remind that the perceptual space P does not include the alternative class

X.

6If we were to take an intermediate point between two random points of

the “cluster O”, this intermediate point may not lie inside cluster O.

7However, it is not because the produced sound is classiﬁed as L that the
perceptual activation should be necessarily at 1 (i.e. indicating a well produced
sound similar to a real canary syllable L): the colored dot obtained by the
classiﬁcation only indicates to which class this sound is the closest. Indeed,
as the learning goes on, the perceptual activation decays quickly.

(a) Syllable R

(b) Syllable B1

Fig. 5: Evolution of sound produced during learning for syllables R and B1. (top) Evolution of the sensory response activation P R of unit R (left) and
B1 (right) obtained during one instance of training. (bottom) Evolution of the corresponding sounds produced over time for 9 selected time steps. Parameter
values: nP = 16, nM = 3, η = 0.01, Wt0 ∈ U [−0.001, 0.001], tf = 3000.

IV. DISCUSSION

We built a vocal learning model with a full action-perception
loop [9]. The aim of the model is to learn a repertoire of
16 different classes of canary syllables. The motor space is
a low-dimensional latent space obtained from training Wave-
GAN [17] on a dataset of canary syllables [13]. The motor
control function is a generator model that enables the pro-
duction of syllables resembling real recordings. The sensory
space is the actual produced sound (and not a spectrogram
or formants as it could happen in other models [9]). The
sensory response function encodes the sound in a rather low-
dimensional space, i.e. the perceptual space which plays the
role of a goal space. We used the normalized output of a
reservoir-based classiﬁer [14] to model the sensory response
function. The learning of the inverse model between the
perceptual space and the motor space is driven by a simple
Hebbian learning rule and an motor random uniform explo-
ration. We tested how the learning is inﬂuenced by (1) different
learning rates, (2) different motor space dimensions and (3) the
structure of the motor latent space. A higher motor space
dimension allows the learning of a higher number of classes of
the repertoire (Figure 8). Similar performance can be observed
from the models using the 3- and 6-dimensional GAN genera-
tors, suggesting that adding more than 3 dimensions does not
help to enhance the learning. Interestingly, the structure of
the motor latent space can reﬂect which syllables are more
challenging for the model. Moreover, some syllables show
a synchronous behavior when learned using different latent
space dimensions (see syllable M in Figure 8 for dimension
3 - yellow line and 6 - black line): one can hypothesize that
such a syllable has a similar distance from the origin in both
latent spaces. Another hypothesis could be that such syllable
is contained in a subspace of the latent space that co-exists

Fig. 6: Syllables produced by the learning model. Mean spectrogram
obtained from all the sounds generated when the perceptual activation is higher
than 0.99 during learning (i.e., if a sound St produced at a certain time is
such that P Ri = ARi (St) ≥ 0.99). An empty box in panel means that for
all the produced sounds of that class we obtain P Ri = ARi (S) < 0.99,
∀S, ∀t0 < t < tf . Parameter values: nP = 16, nM = 3, η = 0.01,
Wt0 ∈ U [−0.001, 0.001], tf = 1500.

0.000.050.100.150.200.25Time (seconds)010002000300040005000600070008000Frequency (Hz)0t=0 t=510 t=810 t=1110 t=1500 t=1800 t=2310 t=2505 t=3000Time (in number of time steps)0.000.020.040.060.08Time (seconds)010002000300040005000600070008000Frequency (Hz)0Sensory response0.000.050.100.150.200.250.30Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.020.040.060.080.100.12Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.020.040.060.080.100.12Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.050.100.15Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.050.100.15Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.050.100.150.200.25Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.050.100.150.200.250.30Time (seconds)010002000300040005000600070008000Frequency (Hz)0050010001500200025003000Time (in number of time steps)0.00.20.40.60.81.0Activationt=0 t=510 t=810 t=1110 t=1500 t=1800 t=2310 t=2505 t=3000050010001500200025003000Time (in number of time steps)0.00.20.40.60.81.0ActivationTime (in number of time steps)0.0000.0250.0500.0750.1000.1250.150Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.020.040.060.08Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.020.040.060.080.100.12Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.020.040.060.080.100.12Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.020.040.060.080.100.12Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.020.040.060.080.100.12Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.020.040.060.080.100.12Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.020.040.060.080.100.12Time (seconds)010002000300040005000600070008000Frequency (Hz)00.000.020.040.060.080.100.12Time (seconds)010002000300040005000600070008000Frequency (Hz)0ActivationTime (ms)010002000300040005000600070008000ATime (ms)B1Time (ms)B2Time (ms)Frequency (Hz)C010002000300040005000600070008000DEHFrequency (Hz)J1010002000300040005000600070008000J2LMFrequency (Hz)N05101520010002000300040005000600070008000O05101520Q05101520R05101520Frequency (Hz)VFig. 7: Evolution of learning: inﬂuence of the learning rate for nM = 3. Evolution of the perceptual activation for η = 0.1 (red lines) and η = 0.01
(blue lines). Each line represents the average activation obtained from 3 different instances of training. For all the classes except O and B2, the perceptual
activation increases (more or less sharply) and reaches, even if does not stabilizes at, a value of 1. A higher learning rate results in faster but more unstable
learning dynamics. Parameter values: nP = 16, nM = 3, Wt0 ∈ U [−0.001, 0.001], tf = 3000.

Fig. 8: Evolution of learning: inﬂuence of motor space dimension. Evolution of the perceptual activation for nM = 1 (red lines), nM = 2 (blue lines),
nM = 3 (yellow lines) and nM = 6 (black lines). Each line represents the average activation obtained from 3 different instances of training. The perceptual
activation remains generally low for nM = 1 (red lines): it never reaches 1 but for syllables J2 and syllable L (starting position). Alternatively, the perceptual
activation almost always enables learning for nM = 2 (blue lines), nM = 3 (yellow lines), and nM = 6 (black lines). Eventually, the stability drops after a
certain time. Parameter values: nP = 16, η = 0.01, Wt0 ∈ U [−0.001, 0.001], tf = 3000.

0100020003000Time (in number of time steps)0.000.250.500.751.00Average AA10e-110e-20100020003000Time (in number of time steps)0.000.250.500.751.00B10100020003000Time (in number of time steps)0.000.250.500.751.00B20100020003000Time (in number of time steps)0.000.250.500.751.00C01000200030000.000.250.500.751.00Average AD01000200030000.000.250.500.751.00E01000200030000.000.250.500.751.00H01000200030000.000.250.500.751.00J101000200030000.000.250.500.751.00Average AJ201000200030000.000.250.500.751.00L01000200030000.000.250.500.751.00M01000200030000.000.250.500.751.00N01000200030000.000.250.500.751.00Average AO01000200030000.000.250.500.751.00Q01000200030000.000.250.500.751.00R01000200030000.000.250.500.751.00VTime (in number of time steps)Time (in number of time steps)Time (in number of time steps)Time (in number of time steps)Average activationAverage activationAverage activationAverage activation10e-110e-20100020003000Time (in number of time steps)0.00.51.0Average A1236A0100020003000Time (in number of time steps)0.00.51.0B10100020003000Time (in number of time steps)0.00.51.0B20100020003000Time (in number of time steps)0.00.51.0C01000200030000.00.51.0Average AD01000200030000.00.51.0E01000200030000.00.51.0H01000200030000.00.51.0J101000200030000.00.51.0Average AJ201000200030000.00.51.0L01000200030000.00.51.0M01000200030000.00.51.0N01000200030000.00.51.0Average AO01000200030000.00.51.0Q01000200030000.00.51.0R01000200030000.00.51.0VAverage activationAverage activationAverage activationAverage activationTime (in number of time steps)Time (in number of time steps)Time (in number of time steps)Time (in number of time steps)both for dimension 3 and 6. Further studies are needed to
investigate the structure of higher dimensional latent spaces
and compare within different space dimensions. The sparsity
and probable non-convexity of the syllable clusters A and O
are probably what is inducing an approximate learning (for A)
or no-learning (for O) (see Section III-D).

Several modelers have proposed dynamical systems to
model the motor control function in songbirds: in such case,
the motor space describes the time-dependent motor articu-
lations parameters which control the dynamics of the syrinx
(e.g., air pressure, syringeal labial tension) [11], [19], [10],
[20]. Instead of using a dynamical system that would produce
sounds of approximate realism, we implemented the motor
function using the generator part of low-dimensional GAN.
Biologically,
the GAN would represent a premotor layer
rather than the control parameters of a vocal organ. Such a
model learns well how to produce syllables resembling the
training data (see Figure 2) from a low-dimensional latent
space. We focused on a low-dimensional space, because we
showed previously that a high dimensional motor space could
result
in slower learning convergence [21]. Although the
training and the evaluation of a generator model are not
trivial, once the model has been validated, it represents a
powerful computational tool to produce realistic sounds. At
the same time, the latent space (i.e., motor space) of GANs is
redundant by construction: very similar sounds (i.e., syllables
belonging to the same class of the repertoire and impossible
to distinguish acoustically) can be produced by several latent
vectors (i.e., motor patterns). As a consequence of the motor
space redundancy, the target of the model is not a motor target
but rather a perceptual target. For this reason, we did not
introduce a normalization in the learning rule like we did
in [21]. This choice is motivated by the fact that the motor
space of the GAN cannot be normalized.

The sensory response function models the encoding of
the sound in the birds’ brain. The categorical classiﬁcation
provided by the classiﬁer qualitatively describes the response
that young birds develop in highly auditory area when they
are memorizing the song [22].

A simple Hebbian learning rule allows the models to learn
but does not prevent divergence after a critical time tcritic. The
value of tcritic is syllable-speciﬁc (Figure 7) and depends on
the learning rate. A higher learning rate results in faster learn-
ing dynamics and, thus, in an earlier tcritic. A simple stopping
criteria could solve the decay problem. The introduction of
a reinforcement signal could (1) speed the learning and (2)
help the learning to stabilize after having reached the optimal
plateau (i.e. the region of the motor space that enables the
production of the correct perceptual goal). One could test how
the introduction of a neutral position inﬂuences the learning.
The model could be forced to use a syllable belonging to
class X as initial position. The inﬂuence on the learning of
the initial condition could then be tested. Moreover, a goal-
directed strategy (e.g. goal babbling) could be used to enable
the learning of the more challenging syllables (e.g. O or A).
In future work, we believe this vocal model could be ex-

tended to learn full songs (i.e. sequences of syllables) instead
of single syllables. Relying on reservoirs for the sensory
response function is a good choice for such extension, as it was
shown that reservoirs can handle different levels of abstrac-
tion for language-like inputs [23]. A hierarchical architecture
processing raw sounds but also syntactic representations [24]
could be necessary to learn full songs.

AKNOWLEDGMENTS

We would like to thank Catherine Del Negro, Aurore Cazala
and Juliette Giraudon for the canary recordings, and Nathan
Trouvain for the classiﬁer. We also thank Inria for the CORDI-
S PhD fellowship, the ANR for grant ANR-16-CE37-0020-01,
and the LabEx BRAIN for the PhD extension.

REFERENCES

[1] PK Kuhl. Early language acquisition: cracking the speech code. Nature

reviews neuroscience, 5(11):831, 2004.

[2] P Kuhl. A new view of language acquisition. Proceedings of the

National Academy of Sciences, 97(22):11850–11857, 2000.

[3] AJ Doupe and PK Kuhl. Birdsong and human speech: common themes
and mechanisms. Annual review of neuroscience, 22(1):567–631, 1999.
[4] DK Oller. The emergence of the speech capacity. Psychology Press,

2000.

[5] A. Warlamount. The Cambridge Handbook of Infant Development:
Brain, Behavior, and Cultural Context, chapter Infant vocal learning
and speech production., pages 602–631. Camb. Uni. Press, 2020.
[6] MS Brainard and AJ Doupe. What songbirds teach us about learning.

Nature, 417(6886):351, 2002.

[7] M Chakraborty and ED Jarvis. Brain evolution by brain pathway

duplication. Phil Trans Roy Soc B, 370(1684):20150056, 2015.

[8] PY Oudeyer. The self-organization of speech sounds.

Journal of

Theoretical Biology, 233(3):435–449, 2005.

[9] S Pagliarini, A Leblois, and X Hinaut. Vocal imitation in sensorimotor

learning models: a comparative review. IEEE TDCS, 2020.

[10] A Amador, YS Perl, GB Mindlin, and D Margoliash. Elemental gesture
dynamics are encoded by song premotor cortical neurons. Nature,
495(7439):59, 2013.

[11] K Doya and TJ Sejnowski. A computational model of birdsong learning
In Central auditory

by auditory experience and auditory feedback.
processing and neural modeling, pages 77–88. Springer, 1998.

[12] CR Ponce et al. Evolving images for visual neurons using a deep
generative network reveals coding principles and neuronal preferences.
Cell, 177(4):999–1009, 2019.

[13] S Pagliarini, N Trouvain, A Leblois, and X Hinaut. What does the canary
say? WaveGAN applied to birdsong. HAL preprint hal-03244723, 2021.
[14] N. Trouvain and X. Hinaut. Canary song decoder: Transduction and
implicit segmentation with ESNs and LTSMs. In ICDL-EpiRob, 2021.
[15] N Trouvain et al. Reservoirpy: an efﬁcient and user-friendly library to
design echo state networks. In ICANN, pages 494–505. Springer, 2020.
[16] L McInnes et al. Umap: Uniform manifold approximation and projection
for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.
[17] Chris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio

synthesis. arXiv preprint arXiv:1802.04208, 2018.

[18] G Giraudon, N Trouvain, A Cazala, C Del Negro, and X Hinaut. Labeled
songs of domestic canary m1-2016-spring (serinus canaria) (version
0.0.1) [data set]. Zenodo, http://doi.org/10.5281/zenodo.4736597, 2021.
[19] IR Fiete, MS Fee, and HS Seung. Model of birdsong learning based
on gradient estimation by dynamic perturbation of neural conductances.
Journal of neurophysiology, 98(4):2038–2057, 2007.

[20] RG Alonso et al. A circular model for song motor control in serinus

canaria. Frontiers in computational neuroscience, 9:41, 2015.

[21] S Pagliarini, X Hinaut, and A Leblois. A bio-inspired model towards
vocal gesture learning in songbird. In ICDL Epirob, 2018. IEEE, 2018.
[22] MM Solis and AJ Doupe. Anterior forebrain neurons develop selectivity
by an intermediate stage of birdsong learning. J Neuro, 17(16):6447–
6462, 1997.

[23] X Hinaut. Which input abstraction is better for a robot syntax acquisition
In ICDL-

model? phonemes, words or grammatical constructions?
EpiRob, pages 281–286. IEEE, 2018.

[24] L Pedrelli and X Hinaut. Hierarchical-task reservoir for online semantic

analysis from continuous speech. HAL preprint hal-03031413, 2020.

