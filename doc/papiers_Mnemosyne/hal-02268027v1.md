Exploration de la notion de méta-apprentissage
Matthieu Zimmer, Yann Boniface, Alain Dutech, Nicolas P. Rougier

To cite this version:

Matthieu Zimmer, Yann Boniface, Alain Dutech, Nicolas P. Rougier. Exploration de la notion de
méta-apprentissage. [Rapport de recherche] Université de Lorraine, CNRS, Inria, LORIA, UMR 7503.
2012. ￿hal-02268027￿

HAL Id: hal-02268027

https://inria.hal.science/hal-02268027

Submitted on 21 Aug 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Université de LorraineStage de Licence 3, option InformatiqueExploration de la notion de méta-apprentissageMatthieu ZIMMER2012Effectué au LORIAsous la supervision de Yann BONIFACE, Alain DUTECH & Nicolas ROUGIERMini-RapportExplorationdelanotiondeméta-apprentissageDansquellemesureunsystèmeapprenantpeutprendreconsciencedesesperformancesetaltérersoncomportement?YannBoniface,AlainDutech,NicolasRougierMatthieuZimmer27avril2012122DUPLIQUERLEPREMIERRÉSEAU1IntroductionNotreintérêts’esttournéversl’article[CleeremansAlex,2007]etses2typesderé-seauxproposés.Dansunpremiertemps,nousavonscherchéàreproduireetexpliquerlesrésultatsdonnés,etensuiteàdessolutionspourtirerproﬁtdesparisréalisés.Nousnoussommeségalementpenchéssur[PasqualiAntoine,2010]dontnousavonsre-produitlesexpériences,maisleursenjeuxnoussemblentencorevagues.2Dupliquerlepremierréseau2.1LesbasesEnpremierlieu,rappelonslastructuredesréseauxetlesrésultatsdel’article:Figure1–[CleeremansAlex,2007]Architectureconnexionnisteavecméta-représentationsLaformuleRMSutiliséeàuneépoqueeestlasuivante:rmsproportione=rmse=s1nnPi=1(oi,e−di)2max(rmse0),∀e0∈epochswithn:numberofneuronsontheoutputlayeroi,e:valueobtainedfortheithneuronattheethepochdi:valuedesiredfortheithneuronNousavonsdécomposéleserreurspourmieuxcomprendrelefonctionnementdel’ar-chitecture.2.1Lesbases3Figure2–ErreurRMSsansproportion.Leréseausupérieurà5unitéscachéesn’estplusconsidéré,etceluià10estdécoupéen3courbespourreprésenterles3couchesàreproduire.Lacourbevioletteestdonclasommedes3courbesdescouches.Onpeutenconclure2choses:–lacouchecachéeetlacouchedesortieneposentaucunproblèmesd’apprentissage–lesperformancesdusecondréseaudépendentprincipalementdesacapacitéàrepro-duirelesentréesIlresteunequestionensuspend:Pourquoilesecondréseauapprendrait-ilplusrapidementquelepremier?Nousn’avonspasderéponsedéﬁnitive.Cependant,nouspouvonsnousintéresseràl’erreurdeclassiﬁcation.Figure3–Erreurdeclassiﬁcation.Leréseausupérieurà5unitéscachéesn’estplusconsidéré,etceluià10estdécoupéen3courbespourreprésenterles3couchesàreproduire.42DUPLIQUERLEPREMIERRÉSEAUIlestalorsbeaucoupmoinsﬂagrantquelesecondréseauapprendmieuxquelepremier.Évidemment,parlerdeclassiﬁcationdelapremiercouche(courbe20inputs)etdelacouchecachée(courbe5hidden)estrelativementdérisoire.Laquestiondelarapiditéd’apprentissagerestedoncouverte.2.2LesrouagesNotreattentions’estégalementportéesurlesmécanismesquipermettaientlaréalisa-tiondecettearchitecture.Nousavonsalorspuremarquerquelesneuronesdelacouchecachéedupremierréseausestabilisaienttrèsrapidement(autourdela50ièmeépoqueenmoyenne),letoutpermettantausecondréseaud’avoirdesentréestrèspeuvariables,favorisantetpermettantdoncsonapprentissage.Figure4–Valeursdiscrétisésdelacouchecachéedupremierréseau.Chaquecouleursreprésentantundes10chiﬀresenentrée.Lescourbesdeviennentrapidementstables.2.3Ré-haussementPourreveniràl’importancedesentrées,nousavonsréaliséquelesecondréseaun’étaitcapablededupliquerlepremierqu’uniquementparcequecesentréessonttriviales.Ainsinousavonsaugmentélenombred’entréesenpassantsurdeschiﬀresmanuscrits[SemeionResearchCenter,1994],toutenaugmentantproportionnellementlenombredeneuronesdescouchescachées:2.3Ré-haussement5Figure5–Architectureavecméta-représentationpourchiﬀresmanuscritsLesperformancesdusecondréseausesontalorsécroulées:Figure6–ErreurRMSsansproportiondel’architecturesurdeschiﬀresmanuscrits.L’erreurdusecondréseauesttoujoursdiviséen3.Iln’estalorspluscapabledereproduirelacouched’entrée(quiaunpoidstrèsimportant).Tentonsmaintenantd’expliquercettechute:Lapremièrechoseàcomprendreestquel’augmentationdunombredeneuronen’arienàvoiraveccetteperte.Eneﬀet,nousavonsfaitlessimulationsavecleschiﬀrestriviales(i.e.seulement10formesdiﬀérentes)agranditen16×16ettoutsepassealorstrèsbien.62DUPLIQUERLEPREMIERRÉSEAUCettechuteprovientdufaitquelesentréesproposéessonttrèsnombreusesetdiﬀérentes.Ils’opèreainsiunepremièreétapedeclassiﬁcationdanslacouchecachéedupremierréseau.Ainsidiﬀérentesformespeuventêtrereprésentéesparlamêmecouchecachée.Saufquelesecondréseaun’ayantenentréequelaversionfactorisé(lacouchecachée),illuiestimpossiblederetrouverlesformesinitiales:ilestentraînépourlamêmeentrée,àdiﬀérentessorties.Ilestdoncnaturelquelescourbessoientdésavantageuses,saufquelorsqu’onregardelesformes,onn’estpassiloindelaréalité.Figure7–Enpremièreligne,10entréesaléatoires.Enseconde,premièrepartiedelasortiedusecondréseauSil’onestoptimiste,onpeutmêmeconsidérerqueceuxsontdesreprésentationspersonnellesdel’agentpourchaquechiﬀrediﬀérentsqu’ilavujusqu’alors.2.4ReprésentationsEnﬁn,nousavonsremarquéqu’enbloquantl’apprentissageentrelacouchecachéeetlesentréesdupremierréseau,puisenchangeantdetâche,leréseauétaitcapablederéapprendrelanouvelletâche,cequiprouvebienlaprésenced’unereprésentationdesentréesdanslacouchecachée.Figure8–Erreurdeclassiﬁcationdel’architecturesurdeschiﬀresmanuscrits.Ap-prentissagebloquéàl’époque800.Changementdetâcheauxépoques:800,2000,32002.5Remarque72.5RemarqueIlfautcependantremarquerqu’unsimpleperceptronestsuﬃsantpourréaliserlatâchedupremierréseau(mêmesurleschiﬀresmanuscrits),etdonc,qu’ilestpossiblequedanslecasd’unproblèmenonlinéairementséparablecettearchitecturesoitinvalidée.Figure9–Erreurdeclassiﬁcationd’unperceptronetd’unperceptronmulti-couchessurlabasedechiﬀresmanuscrits.3Pariersurlepremierréseau3.1LesbasesRappelonségalementlastructuredesréseauxetlesrésultats:Figure10–[CleeremansAlex,2007]Architectureconnexionnisteavecparis83PARIERSURLEPREMIERRÉSEAULapremièrechosequenousavonsfaitesàétéd’améliorerlesperformancesdusecondréseauenmodiﬁantquelquesparamètres(initialisationdespoidssur[-1;-1],momentumà0.5).Figure11–Performancedeclassiﬁcation.Àgauchelabase,àdroitelaversionaméliorée.Onneconsidèreplusleréseauavecapprentissagefaible,maisonaajoutéletauxdeparishauts.Contrairementàceluidel’article,ilnesecontenteraplussimplementdeparierhautàchaquecoups(après40époques).Ilauraunelongueurd’avancesurlepremierréseausurtouteladuréedel’apprentissage.Onpourradonctirerproﬁtdecetteavance.3.2FeedbacksÀpartirdecettediﬀérencedeperformances,nousavonsimaginéplusieursarchitec-tures,quiaméliorentplusoumoinslesperformancesdereconnaissanceduréseausurdeschiﬀresmanuscrits:Figure12–Architectureavec3ièmeréseauDansces2architectures,nousnouscontentonsdeconnecterun3ièmeréseauquidoittirerdesconclusionsàpartird’informationssurles2premiers.3.2Feedbacks9Figure13–Performancedeclassiﬁcationdes2architectures.Lesdeuxarchitecturesapportentungain.Ilestplusaccentuélorsqueladernièrecouchedisposedeplusd’informations(i.e.connectésurlacouchecachée).Figure14–ArchitectureparfusionIci,nousmélangeonsunapprentissagepardescentedegradient(surlepremieretsecondréseau)etunapprentissageperceptron(entreles2couchesdesorties).L’algorithmepeutêtretrouvédansl’annexeA.103PARIERSURLEPREMIERRÉSEAUFigure15–Performancedeclassiﬁcationdel’architecturefusion.Legainestaussinotabled’autantqu’ilnenécessiteaucunajoutdeneurone.Ilestd’ailleursassezsimilaireaugaindelapremièrearchitecture,puisqu’elleestaussibaséesurles2couchesdesortiesdes2réseaux(néanmoins,ledébutdiﬀère,maisc’estletempsquelesperceptronapprennent).Figure16–ArchitectureparintuitionsCettearchitectureestlégèrementdiﬀérentedanslesensoùellen’enregistreplusdeparimaisl’indicedunièmeneuroneleplusactifcontenantlabonneréponse.Exemple:leréseausupérieursort2->laréponseestle3ièmeneuroneleplusactifdelacouchedesortiedupremierréseau.11Figure17–Performancedeclassiﬁcationdel’architectureintuition.Danscecasmalheureusement,lesperformancessonttrèspeuaméliorées.Nousavonsaussiessayéquelquesmodèlesoùleréseausupérieurservaitdesupervi-seuràl’apprentissagedupremierréseau.Parexemple,s’ilpariehaut,l’apprentissagedupremierréseauserafaible,sinonilseraaccentué.4LasuiteCequenouscontinuonsd’étudier:–validationsurdesexpériencespluscomplexes(quinepeuventêtrerésoluedirecte-mentparunperceptron)–relationentrelatailledelacouchecachéedupremierréseauetletauxdeparisavantageux–approfondirlesintérêtsdusecondarticle[PasqualiAntoine,2010]–denouvellesarchitecturesaxéesméta-apprentissageoùleréseaud’ordresupérieurcontrôlelepremier(tauxd’apprentissage,momentum,entréesàapprofondir,...)Références[CleeremansAlex,2007]CleeremansAlex,TimmermansBert,P.A.(2007).Consciousnessandmetarepresentation:Acomputationalsketch.doi:10.1016/j.neunet.2007.09.011.[PasqualiAntoine,2010]PasqualiAntoine,TimmermansBert,C.A.(2010).Knowthy-self:Metacognitivenetworksandmeasuresofconsciousness.[SemeionResearchCenter,1994]SemeionResearchCenter,o.S.o.C.(1994).Semeionhandwrittendigitdataset.1593handwrittendigitsfromaround80persons.12AALGORITHMEFUSIONGRADIENT/PERCEPTRONAAlgorithmefusiongradient/perceptronEnsembled’instructionsexécutépouruneépoqueetuntiraged’entrées/sortiesàap-prendre.Cetalgorithmesedérouleenplusieurstemps:a)Oncalculelasortiedupremierréseauenignorantlesecond(valeurà0)b)Oncalculelasortiedusecondréseauc)Oncalculelasortiedupremierréseauaveclessortiesdeb)d)Lesperformancessontcalculéesavecc)e)Lesecondréseauapprendàpariersurlerésultata)(sansprendrecomptedelui-même)f)Lepremierréseauapprendàdiscriminerenprennentcomptedeb)first\_order.calc\_hidden\_layer(samples.inputs)high\_order.calc\_output\_layer(first\_order.hidden\_layer)first\_order.calc\_output\_layer(first\_order.hidden\_layer,[0,...,0])h\_output←ampli(high\_order.output\_layer)right\_houtput←[0,0]ifgood\_answer(first\_order)thenright\_houtput[1]←1elseright\_houtput[0]←1endiffirst\_order.calc\_output\_layer(first\_order.hidden\_layer,h\_output)calc\_stats()high\_order.train(first\_order.hidden\_layer,right\_houtput)first\_order.train(samples.inputs,samples.outputs,h\_output)Intéressons-nousjustementàl’étapef):functiontrain(inputs,outputs,add)fori=0→output\_neurons.lengthdoyoutput[i]←g0(output\_neurons[i].a)×(outputs[i]−output\_neurons.state)endforfori=0→hidden\_neurons.lengthdow\_sum←output\_neurons.lengthPj=0output\_neurons[j].weights[i]×youtput[j]yhidden[i]←g0(hidden\_neurons[i].a)×w\_sumendforupdate\_weights\_hidden\_layer(yhidden)13fori=0→output\_neurons.lengthdooutput\_neurons[i].update\_weights\_gradient(youtput[i],hidden\_neurons,add)output\_neurons[i].update\_weights\_perceptron(outputs[i],hidden\_neurons,add)endforendfunctionLadiﬀérenceprincipalesesituesurledernierfor,avecles2méthodesdemiseàjoursdespoids.functionupdate\_weights\_gradient(error,intputs,add)calc\_output(inputs+add)forj=0→inputs.lengthdodw←weights[j]−last\_weights[j]p←error×inputs[j]weights[j]←weights[j]+learning\_rate×p+momentum×dwendforendfunctionfunctionupdate\_weights\_perceptron(goal,intputs,add)calc\_output(inputs+add)forj=inputs.length→inputs.length+add.lengthdodw←weights[j]−last\_weights[j]p←(goal−state)×add[inputs.length−j]weights[j]←weights[j]+learning\_rate×p+momentum×dwadd.lengthendforendfunction