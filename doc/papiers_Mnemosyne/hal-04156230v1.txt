De Cambridge Analytica à ChatGPT, comprendre comment l’IA donne un sens
aux mots Frédéric Alexandre

To cite this version:

Frédéric Alexandre. De Cambridge Analytica à ChatGPT, comprendre comment
l’IA donne un sens aux mots. The Conversation France, 2023.
￿hal-04156230￿

HAL Id: hal-04156230

https://inria.hal.science/hal-04156230

Submitted on 12 Jul 2023

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International
License

De Cambridge Analy0ca à ChatGPT, comprendre comment l’IA donne un sens
aux mots

ALEXANDRE Frédéric, Centre Inria de l’université de Bordeaux, CNRS,
Bordeaux INP

Un des problèmes majeurs que l’IA n’a toujours pas résolus aujourd’hui
est celui de l’ancrage du symbole (symbol grounding problem),
c’est-à-dire comment des symboles (des mots par exemple) peuvent être
associés à leur signiﬁcaPon. Par exemple, si je dis : « le chat dort sur
son coussin car il est faPgué », la plupart des êtres humains comprendra
sans eﬀort que « il » renvoie à « chat » et pas à « coussin ». C’est ce
qu’on appelle le raisonnement de bon sens. Par contre, ce raisonnement
va être beaucoup plus diﬃcile en IA et je vais expliquer pourquoi. Mais
je vais aussi présenter la technique dite de plongement lexical qui, si
elle ne résout pas tous les problèmes, est cependant d’une redoutable
eﬃcacité. Il est important de connaitre les principes de ce[e technique
car c’est celle qui est uPlisée dans la plupart des modèles d’IA
récents, dont ChatGPT.

Le problème de l’ancrage du symbole a été idenPﬁé bien avant l’IA, par
des philosophes et des linguistes, et a été résumé, au début du XXème
par trois concepts regroupés dans un triangle dit sémioPque, dont les
trois sommets renvoient respecPvement au symbole (le signiﬁant), l’objet
qu’il représente et le concept (ou le signiﬁé), correspondant à l’image
mentale que nous nous sommes construite, suite à nos expériences avec
cet objet. Ce sont ces expériences (et en parPculier leurs valeurs
émoPonnelles) et les propriétés de cet objet qu’elles nous ont permis
d’extraire qui nous perme[ent d’avoir ce bon sens pour raisonner à son
sujet et qui sont singulièrement absentes d’un système d’IA et qui lui
donnent ce[e limitaPon d’accès à la sémanPque des symboles.

John Searle a illustré ce problème en 1980 avec son expérience de pensée
dite de la chambre chinoise. Dans ce[e chambre, un être humain qui ne
parle pas le chinois reçoit des quesPons écrites en chinois. Il a accès
à un ensemble de règles qui lui dit mécaniquement comment réagir à
chaque caractère (ou combinaison de caractères) possibles, ce qui lui
permet d’écrire une réponse à la quesPon et de l’envoyer à l’extérieur
de la chambre. Ceci permet de montrer que si, pour une tâche donnée (un
ensemble de quesPons), on est capable d’écrire l’ensemble des règles qui
régissent son traitement, alors un système mécanique (ou un humain qui
ne comprend pas le chinois) peut donner l’illusion à l’extérieur qu’il
est intelligent. Mais même dans ce cas (et on peut imaginer qu’écrire un
tel ensemble de règles n’est pas simple voire impossible pour de
nombreuses tâches), l’agent ne comprendra pas ce qu’il fait et n’aura
donc pas accès à la signiﬁcaPon des symboles qu’il manipule (ce que fera
très naturellement toute personne parlant le chinois). Searle en
déduisait alors qu’une approche purement mécanique, comme celle d’un
ordinateur, quelle que soit la complexité des règles ou des algorithmes
qu’il manipule, ne donnera donc jamais accès à la capacité de comprendre
le sens de ce que l’on fait.

Considérant que notre cerveau est tout à la fois capable de manipuler
des symboles (des mots par exemple) et d’apprendre leur signiﬁcaPon à
travers ses interacPons avec le monde, Stevan Harnard a repris ces idées
en 1990 pour reformuler le problème de l’ancrage du symbole, en
proposant qu’on pouvait le résoudre à une condiPon : L’interprétaPon
sémanPque d’un système de symboles doit être faite de façon intrinsèque
à ce système, si on

veut qu’il ait eﬀecPvement accès au sens des symboles qu’il manipule.
Il s’agirait donc d’ancrer notre système de symboles dans nos
expériences sensorimotrices avec le monde, pour apprendre des relaPons
symboles-signiﬁcaPons. De ce[e concepPon est née l’approche de l’IA
incarnée, qui postule qu’un tel système (un robot par exemple) devrait
avoir un corps pour ressenPr et interagir avec le monde, si il veut
pouvoir donner un sens aux symboles qu’il manipule. Il reste maintenant
à voir comment faire ça en praPque. C’est ce que propose, dans le cadre
très restreint de l’analyse de textes, le plongement lexical.

Ce[e technique consiste à remplacer un mot (qui peut être vu comme un
symbole abstrait, impossible à relier directement à sa signiﬁcaPon) par
un vecteur numérique (une liste de nombres). Notons que ce passage au
numérique fait que ce[e représentaPon peut être directement uPlisée par
des réseaux de neurones et bénéﬁcier de leurs capacités d’apprenPssage.
En parPculier, ces réseaux de neurones vont, à parPr de très grands
corpus de textes, apprendre à plonger un mot dans un espace numérique de
grande dimension (typiquement 300) où chaque dimension calcule la
probabilité d’occurrence de ce mot dans certains contextes. En
simpliﬁant, on remplace par exemple la représentaPon symbolique du mot
‘chat’ par 300 nombres représentant la probabilité de trouver ce mot
dans 300 types de contextes diﬀérents (texte historique, texte
animalier, texte technologique, etc.) ou de co- occurrence avec d’autres
mots (oreilles, moustache ou avion). Même si ce[e approche peut sembler
très pauvre, elle a pourtant un intérêt majeur en grande dimension :
elle code avec des valeurs numériques proches des mots dont le sens est
proche, ce qui va alors perme[re de déﬁnir des noPons de proximité et de
distance pour comparer le sens de symboles, ce qui est un premier pas
vers leur compréhension. Pour donner une intuiPon de la puissance de
telles techniques (en fait, de la puissance des staPsPques en grande
dimension pour des phénomènes avec des régularités telles qu’on en voit
dans notre monde cogniPf), prenons un exemple similaire dont beaucoup
ont entendu parler.

C’est en eﬀet avec une approche similaire que des sociétés comme
Cambridge AnalyPca ont pu agir sur le déroulement d’élecPons en
apprenant à associer des préférences électorales (représentaPons
symboliques) à diﬀérents contextes d’usages numériques (staPsPques
subPlisées à parPr de pages Facebook d’usagers). Leurs méthodes reposent
sur une publicaPon scienPﬁque parue en 2014 dans la revue PNAS qui
comparait des jugements humains et des jugements issus de staPsPques sur
des proﬁls Facebook. L’expérimentaPon reportée dans ce[e publicaPon
demandait à quelques dizaines de milliers de parPcipants de déﬁnir
certains de leurs traits psychologiques (sont-ils consciencieux,
extraverPs, etc.). Ces parPcipants avaient donc des éPque[es (dites
symboliques) représentant ces traits. On pouvait également les
représenter par une éPque[e (dite numérique) comptant les ‘Likes’ qu’ils
avaient mis sur Facebook sur diﬀérents sujets (sports, loisirs, cinéma,
cuisine, etc). On pouvait alors, par des staPsPques dans cet espace
numérique de grande dimension, apprendre à associer certains endroits de
cet espace à certains traits psychologiques. Ensuite, pour un nouveau
sujet, uniquement en regardant son proﬁl Facebook, on pouvait voir dans
quelle parPe de cet espace il se trouvait et donc de quels types de
traits psychologiques il est le plus proche. On pouvait également
comparer ce[e prédicPon à ce que connait de ce sujet ses proches. Le
résultat principal de ce[e publicaPon est que, si on s’en donne les
moyens (dans un espace d’assez grande dimension, donc avec assez de
‘Likes’ à récolter, et avec assez d’exemples, ici plus de 70 000
sujets), le jugement staPsPque peut être plus précis que le jugement
humain. Autrement dit,

qu’avec 10 Likes, on en sait plus sur vous que votre collègue de bureau
; 70 Likes que vos amis ; 275 Likes que votre conjoint. Ce[e publicaPon
avait tout d’abord comme but de nous alerter sur le fait que, quand on
recoupe diﬀérents indicateurs en grand nombre, nous sommes très
prévisibles et qu’il faut donc faire a[enPon quand on laisse des traces
sur les réseaux sociaux car de nombreux acteurs sur internet peuvent
nous faire des recommandaPons ou des publicités ciblées avec une très
grande eﬃcacité. L’exploitaPon de telles techniques est d’ailleurs la
principale source de revenu de ces acteurs.

Cambridge AnalyPca est allée un cran plus loin en subPlisant les proﬁls
Facebook de millions d’américains et en apprenant à associer leurs Likes
avec leurs préférences électorales pour mieux cibler des campagnes
électorales. De telles techniques ont également été uPlisées lors du
vote sur le Brexit, ce qui a conﬁrmé leur eﬃcacité. Notons que c’est
uniquement l’aspiraPon illégale des proﬁls Facebook qui a été reprochée
par la jusPce, ce qui doit conPnuer à nous rendre méﬁants quant aux
traces que l’on laisse sur Internet. En exploitant ce même pouvoir des
staPsPques en grand dimension, les techniques de plongement lexical
uPlisent de grands corpus de textes (que l’on trouve facilement sur
Internet (Wikipédia, livres numérisés, réseaux sociaux) pour associer
des mots avec leur probabilité d’occurrence dans diﬀérents contextes
(dans diﬀérents types de textes). Comme on l’a vu plus haut, ceci permet
de considérer une proximité dans cet espace de grande dimension comme
une similarité sémanPque et donc de calculer avec des mots en prenant en
compte leur signiﬁcaPon. Un exemple classique qui est rapporté est de
prendre le vecteur numérique représentant le mot Roi, de lui soustraire
le vecteur (de même taille car reportant les probabilité d’occurrence
sur les mêmes critères) représentant le mot Homme, de lui ajouter le
vecteur représentant le mot Femme, pour obtenir un vecteur très proche
de celui représentant le mot Reine. Autrement dit, on a bien réussi à
apprendre une relaPon sémanPque de type A est à B ce que C est à D.

Le principe retenu ici (dans ce monde lexical) pour déﬁnir la sémanPque
est que deux mots proches sont uPlisés dans des mêmes contextes. On
parle ici de sémanPque distribuPonnelle. C’est ce principe de codage des
mots qu’uPle ChatGPT, auquel il ajoute d’autres techniques, comme on
peut le voir dans le [texte] (« De quoi ChatGPT est-il le nom].

On a indiqué plus haut que coder avec des valeurs numériques proches des
mots dont le sens est proche permet d’aider à leur compréhension. On
peut se demander ce qu’il manque pour se rapprocher encore de la façon
qu’a notre cerveau de comprendre un concept. On aborde ce[e quesPon dans
le [texte] (ChatGPT est-il intelligent comme un humain ?).


