Recurrent Neural Network for Syntax Learning with
Flexible Representations
Xavier Hinaut

To cite this version:

Xavier Hinaut. Recurrent Neural Network for Syntax Learning with Flexible Representations. IEEE
ICDL-EPIROB Workshop on Language Learning, Dec 2016, Cergy, France. ￿hal-01417060￿

HAL Id: hal-01417060

https://inria.hal.science/hal-01417060

Submitted on 15 Dec 2016

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Recurrent Neural Network for Syntax Learning with Flexible
Representations

Xavier Hinaut1

Abstract— We present a Recurrent Neural Network (RNN),
namely an Echo State Network (ESN), that performs sentence
comprehension and can be used for Human-Robot Interaction
(HRI). The RNN is trained to map sentence structures to
meanings (e.g. predicates). We have previously shown that this
ESN is able to generalize to unknown sentence structures in
English and French. The meaning representations it can learn
to produce are ﬂexible: it enables one to use any kind of
“series of slots” (or more generally a vector representation)
and are not limited to predicates. Moreover, preliminary work
has shown that the model could be trained fully incrementally.
Thus, it enables the exploration of language acquisition in a
developmental approach. Furthermore, an “inverse” version of
the model has been also studied, which enables to produce
sentence structure from meaning representations. Therefore, if
these two models are combined in a same agent, one can inves-
tigate language (and in particular syntax) emergence through
agent-based simulations. This model has been encapsulated in
a ROS module which enables one to use it in a cognitive robotic
architecture, or in a distributed agent simulation.

I. INTRODUCTION

How do children learn language? In particular, how do
they associate the structure of a sentence to its meaning?
This question is linked to the more general issue: how does
the brain associate sequences of symbols to internal symbolic
or sub-symbolic representations? We propose a framework to
understand how language is acquired based on a simple and
generic neural architecture (Echo State Networks) [1] which
is not hand-crafted for a particular task.

First, we present the general ESN architecture. Then, we
detail the reservoir sentence processing model. Finally we
give some perspectives.

II. METHODS & RESULTS

A. Echo State Networks

The language module is based on an ESN [1] with leaky
neurons: inputs are projected to a random recurrent layer
and a linear output layer (called “read-out”) is modiﬁed by
learning (which can also be done in an online fashion). The
units of the recurrent neural network have a leak rate (α)
hyper-parameter which corresponds to the inverse of a time
constant. These equations deﬁne the update of the ESN:

x(t + 1) = (1 − α)x(t) + αf (W inu(t + 1) + W x(t)) (1)

y(t) = W outx(t)

(2)

1X. Hinaut is with Inria Bordeaux Sud-Ouest, Talence, France; LaBRI,
UMR 5800, CNRS, Bordeaux INP, Universit´e de Bordeaux, Talence, France;
and Institut des Maladies Neurod´eg´en´eratives, UMR 5293, CNRS, Univer-
sit´e de Bordeaux, Bordeaux, France xavier.hinaut@inria.fr

with x(t), u(t) and y(t) the reservoir (i.e. hidden) state,
the input and the read-out (i.e. output) states respectively
at time t, α the leak rate, W , W in and W out the reservoir,
the input and the output matrices respectively and f the tanh
activation function. After the collection of all reservoir states
the following equation deﬁnes how the read-out (i.e. output)
weights are trained:

W out = Y d[1; X]+

(3)

with Y d the concatenation of the desired outputs, X the
concatenation of the reservoir states (over all time steps
for all train sentences) and M + the Moore-Penrose pseudo-
inverse of matrix M . Hyper-parameters that can be used for
this task are the following: spectral radius: 1, input scaling:
0.75, leak rate: 0.17, number of reservoir units: 100.

Sentences are converted to a sentence structure by replacing
Fig. 1.
semantic words by a SW marker. The ESN is given the sentence structure
word by word. Each word activates a different input unit. During training,
the connections to the readout layer are modiﬁed to learn the mapping
between the sentence structure and the arguments of the predicates. When
a sentence is tested, the most active units are bound with the SW kept in
the SWs memory to form the resulting predicate. (Adapt. from [2].)

B. Reservoir Sentence Processing Model

The reservoir sentence processing model has been adapted
from previous experiments on a neuro-inspired model for
sentence comprehension using ESN [3] and its application
to HRI [2]. The model learns the mapping of the semantic
words (SW; e.g. nouns, verbs) of a sentence onto the different
slots (the thematic roles: e.g. action, location) of a basic
event structure (e.g. action(object, location)). As depicted
in Fig. 1, the system processes a sentence as input and
generates corresponding predicates. Before being fed to the

theontoandSW…Sentence:PALput (toy, left)P(A, L)Semantic words (SWs)Recurrent NeuralNetworkSW1SW2SW3P: PredicateA: AgentL: LocationMeaning: P (A, L)Read-outLayerInputLayeron the left put the toy .SW1SW2SW3putlefttoySWs MemoryFunction words(FWs)PALPALleftputtoyInactive connectionActive connectionLearnable connectionsFixed connectionsConnectionSW: Semantic word item(e.g. toy)thisESN, sentences are transformed into a sentence structure (or
grammatical construction) semantic words (SW), i.e. nouns,
verbs and adjectives that have to be assigned a thematic
role, are replaced by the SW item. The processing of the
grammatical construction is sequential (one word at a time)
and the ﬁnal estimation of the thematic roles for each SW
is read-out at the end of the sentence.

By processing constructions [4] and not sentences per se,
the model is able to bind a virtually unlimited number of
sentences to these sentence structures. Based only on a small
training corpus (a few tens of sentences), this enables the
model to process future sentences with currently unknown
semantic words if the sentence structures are similar. One
major advantage of this neural network language module is
that no parsing grammar has to be deﬁned a priori: the system
learns only from the examples given in the training corpus.
Here are some input/output transformations that the language
model performs:

• “Please give me the mug” → give(mug, me)
• “Could you clean the table with the sponge?”

→ clean(table, sponge)

• “Find the chocolate and bring it to me”
→ ﬁnd(chocolate), bring(chocolate, me)

As shown, the system can robustly transform different
types of sentences. Recently, we have explored how ﬂexible
our system is: we found that it can handle various kinds
of representations at the same time. For instance, it allows
to use nouns as main elements of a predicate, and use its
arguments to ﬁll in adjectives:

• “Search for the small pink object”

→ search(object), object(small, pink)

It also allows to process various kinds of complex sentences:
• “Bring me the newspaper which is on the table in
the kitchen” → bring(newspaper, me), newspaper(on,
table), table(in, kitchen)

C. Perspectives

In Hinaut et al. [5], it has been shown that the model
can learn to process sentences with out-of-vocabulary words.
Moreover, we demonstrated that it can generalize to unknown
constructions in both French and English at the same time. To
illustrate how the robot interaction works, a video can be seen
at youtu.be/FpYDco3ZgkU [6][7]. The source code,
implemented as a ROS module, is available at github.
com/neuronalX/EchoRob. With an ESN of 100 units,
the training of 200 sentences takes about one second on a
laptop computer. Testing a sentence is of the order of 10 ms.
This ROS module could be employed to process various
hypotheses generated by a speech recognition system (like
in [7]),
then returning the retrieved predicates for each
hypothesis, thus, enabling a semantic analyser or world sim-
ulator to choose the predicates with the highest likelihood.
Preliminary work has shown that the model could be trained
fully incrementally (i.e. changing weights at each time step)
[8].Moreover, an “inverse” version of the model has been
also studied, which enables to learn mapping from meaning

representations to sentence structure [2]. The representations
may not be limited to “series of slots”: neural networks
are able to deal with hierarchical structures when they are
embedded in a vector representation (e.g. Fluid Construction
Grammar using Holographic Reduced Representations like
in [9]. With these properties, one can investigate language
syntax emergence through agent-based simulations.

In a nutshell, the objectives of this model are to improve
HRI and provide models of language acquisition. From the
HRI point of view, the aim of using this neural network-
based model is (1) to gain adaptability because the system
is trained on corpus examples (no need to predeﬁne a parser
for each language), (2) to be able to process natural language
sentences instead of stereotypical sentences (i.e. “put cup
left”), and (3) to be able to generalize to unknown sentence
structures (not in the training data set). Moreover, this model
is quite ﬂexible when changing the output predicate repre-
sentations, as we have shown here. From the computational
neuroscience and developmental robotics point of view, the
aim of this architecture is to model and test hypotheses about
child learning processes of language acquisition [10].

ACKNOWLEDGMENT

I thank William Schueller for interesting discussions and

ideas.

REFERENCES

[1] H. Jaeger. The “echo state” approach to analysing and training re-
current neural networks. Bonn, Germany: German National Research
Center for Information Technology GMD Technical Report, 148:34,
2001.

[2] X. Hinaut, M. Petit, G. Pointeau, and P. F. Dominey. Exploring
the acquisition and production of grammatical constructions through
Frontiers in
interaction with echo state networks.
human-robot
Neurorobotics, 8, 2014.

[3] X. Hinaut and P.F. Dominey. Real-time parallel processing of gram-
matical structure in the fronto-striatal system: a recurrent network
simulation study using reservoir computing. PloS one, 8(2):e52946,
2013.

[4] A.E. Goldberg. Constructions: A construction grammar approach to

argument structure. University of Chicago Press, 1995.

[5] X. Hinaut, J. Twiefel, M. Petit, P. Dominey, and S. Wermter. A
recurrent neural network for multiple language acquisition: Starting
In NIPS 2015 Workshop on Cognitive
with english and french.
Computation: Integrating Neural and Symbolic Approaches, 2015.
[6] X. Hinaut, J. Twiefel, M. Borghetti Soares, P. Barros, L. Mici, and
S. Wermter. Humanoidly speaking – learning about the world and
language with a humanoid friendly robot. In IJCAI Video competition,
Buenos Aires, Argentina. https://youtu.be/FpYDco3ZgkU, 2015.
[7] J. Twiefel, X. Hinaut, M. Borghetti, E. Strahl, and S. Wermter. Using
Natural Language Feedback in a Neuro-inspired Integrated Multimodal
In Proc. of RO-MAN, New York City, USA,
Robotic Architecture.
2016.

[8] X. Hinaut and S. Wermter. An incremental approach to language
acquisition: Thematic role assignment with echo state networks.
In
Proc. of ICANN 2014, pages 33–40, 2014.

[9] Y. Knight, M. Spranger, and L. Steels. A vector representation of Fluid
Construction Grammar using Holographic Reduced Representations.
2015.

[10] M. Tomasello. Constructing a language: A usage based approach
to language acquisition. Cambridge, MA: Harvard University Press,
2003.

