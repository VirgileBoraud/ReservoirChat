A Modular Network Architecture Resolving Memory Interference through
Inhibition Randa Kassab, Frédéric Alexandre

To cite this version:

Randa Kassab, Frédéric Alexandre. A Modular Network Architecture
Resolving Memory Interference through Inhibition. Merelo, J.J.
Computational Intelligence, 669, Springer, pp.407-422, 2016, Studies in
Computational Intelligence, ￿10.1007/978-3-319-48506-5￿. ￿hal-01251022￿

HAL Id: hal-01251022

https://inria.hal.science/hal-01251022

Submitted on 5 Jan 2016

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

A Modular Network Architecture Resolving Memory Interference through
Inhibition

Randa Kassab and Fr´ed´eric Alexandre

Institut des Maladies Neurod´eg´en´eratives, Universit´e de Bordeaux,
CNRS, UMR 5293, Bordeaux, France LaBRI, Universit´e de Bordeaux,
Bordeaux INP, CNRS, UMR 5800, Talence, France Inria Bordeaux Sud-Ouest,
200 Avenue de la Vieille Tour, 33405 Talence, France
{randa.kassab,frederic.alexandre}@inria.fr

Abstract. In real learning paradigms like pavlovian conditioning, sev-
eral modes of learning are associated, including generalization from
cues and integration of speciﬁc cases in context. Associative memories
have been shown to be interesting neuronal models to learn quickly
speciﬁc cases but they are hardly used in realistic applications because
of their limited storage capacities resulting in interferences when too
many exam- ples are considered. Inspired by biological considerations,
we propose a modular model of associative memory including mechanisms to
manipu- late properly multimodal inputs and to detect and manage
interferences. This paper reports experiments that demonstrate the good
behavior of the model in a wide series of simulations and discusses its
impact both in machine learning and in biological modeling.

Keywords: Associative memory, interference, inhibition, biological sys-
tems

1

Introduction

In the domain of machine learning, models of neural networks are
classiﬁed along their architecture and their mode of learning [6],
speciﬁcally corresponding to su- pervised and unsupervised modes. In
contrast, in the domain of cognitive science, a natural learning
paradigm considered in a realistic behavioral and ecological environment
often associates several neuronal architectures and learning modes. This
is for example the case with pavlovian conditioning that has been shown
to require learning a variety of invariants and to modify the neuronal
circuitry in several brain regions including the amygdala, hippocampus
and cortex [11]. Consequently, in addition to developing eﬃcient models
of neural networks de- signed for their speciﬁc characteristics, there
is also a need for a more systemic view of learning, considered at the
global cognitive level.

Such an approach was already proposed twenty years ago in [12] arguing
that the brain exploits complementary learning systems, with a slow and
procedural learning in the cortex, able to extract structures and
regularities in the data

2

Randa Kassab and Fr´ed´eric Alexandre

and to generalize, compared with a quick learning in the hippocampus
able to retain the speciﬁcs of one’s life experiences. This paper, with
a very strong im- pact in both cognitive and machine learning
communities, proposes that these systems might be respectively
implemented with classical neural models of pat- tern matching like the
multilayer perceptron for the slow learning and models of associative
memory for the quick learning.

As an illustration, these models can be contrasted with the property of
gen- eralization. Generalization is often reported as a desirable
property of artiﬁcial neural networks. This phenomenon occurs if, when a
network is presented with an example it has never seen before, it is
able to interpolate a satisfactory response from the combination of
close previously learned examples. Such a response can be judged
satisfactory not only because from a limited learning phase the net-
work behaves well in a wider domain but also because in some sense
learning went beyond speciﬁc cases and was able to extract some general
structures or regularities in the example space. In some cases, however,
this property might be considered a ﬂaw. This is the case for example
when there is no useful topogra- phy in the example space or when the
goal is to learn some arbitrary association. Consider for example
learning to associate a phone number with a name: there is nothing to
learn from the euclidean distance between two such numbers and you can
in no way discover an association if it was not instructed to you
before. This contrasts the cases of learning a general rule from a set
of examples, as it is for example studied with layered architectures
like the multilayer perceptron, versus learning by heart speciﬁc cases
like in associative memories.

Neural models of associative memories have been proposed with recurrent
networks like the Hopﬁeld model [7] and the Willshaw model [19]. Based
on classical connectionist characteristics (like units with non linear
activation func- tions and hebbian learning), the recurrent architecture
of these networks indi- cates that learning is mainly focused on the
inner characteristics of an example to be memorized and not on the
elaboration of abstract representations in in- termediate layers.
Nevertheless, some problems can appear if too close examples are
learned. In such a case, the network might elaborate an answer from the
combination of several learned examples; what would be called
generalization in other circumstances is called here interference.

As a consequence, models of associative memories are generally used as
con- tent addressable memories, where few prototypes are stored as
stable states of the network and noisy or incomplete patterns are
presented as inputs and reconstructed to the closest stored example.
Beyond this use as an autoassocia- tive memory (where initial input and
ﬁnal result have the same dimension), the adaptation to
heteroassociative memory is straightforward: just virtually split the
recurrent network in two sets of neurons A and B. The recurrent connec-
tivity includes connections within A and within B (seen as two
autoassociative memories) and between A and B (heteroassociative memory
between the two sets of diﬀerent dimension A and B). As conﬁgurations of
A+B are learned as prototypes, proposing an incomplete pattern A (B
neurons being set to 0) will result in the reconstruction of A+B,
yielding the answer B. The main acknowl-

A Modular Network Architecture Resolving Memory Interference

3

edged weakness of these models is about their limited capacity of
storage and the associated risk of catastrophic interference when this
capacity is exceeded or when too close prototypes are stored [4, 9]. The
best solution to this problem is to require a sparse coding, which
intrinsically also limits the maximum number of stored prototypes. An
associated strategy is to orthogonalize the inputs and project their
encoding in higher dimensions, which results in larger weight ma- trices
to manipulate [13]. In both cases, this might prevent associative
memories from being applied to large scale realistic problems and can
accordingly explain why they didn’t have the same expansion in the
machine learning community than layered networks. It is consequently
highly desirable to develop scalable models of associative memory.

In previous work, we have proposed a modular network model of
associative memory [8] grounded on biological data [1, 17]. These data
report heterogeneities in the hippocampal structure that might support
the coexistence of autoassocia- tive and heteroassociative networks in
this region. Speciﬁcally, the hippocampus is a neuronal structure known
to be involved in episodic memory [18], correspond- ing to the storage
of speciﬁc episodes including their context and their emotional or
motivational signiﬁcance. For example, the hippocampus is involved in
con- textual learning of pavlovian conditioning [3], linking neutral
stimuli and their context to biologically signiﬁcant events (reward and
punishment). Though pri- marily oriented toward biological modeling, we
have also explained in [8] the interest of such a segregation from an
information processing point of view (cf. the concluding section for a
summary). In addition, we have also postulated an additional mechanism
for the association of autoassociative memories, that might result in a
more robust system, particularly more resistant to interfer- ence. The
goal of this paper is to evaluate more precisely the performances of
this mechanism from an information processing point of view.

In the next section, we will present this model together with its
formalism based on the associative memory initially proposed by Willshaw
[19]. Then we will report the experiments that were conducted to
evaluate its resistance to interference and the associated results. We
will conclude by explaining the in- terest of such a mechanism both in
neuroscience and in information processing domains.

2 Multiple Associative-Memory Model

The model is made up of two autoassociative networks that are
heteroasso- ciatively linked through a layer of intermediate cells (Fig.
1). The goal is to associate two multi-element patterns in such a way
that when at least some ele- ments of the ﬁrst pattern are presented
both patterns can be recalled as a whole. In the hippocampus, these two
patterns are considered to represent two impor- tant dimensions of
episodic memories: 1) The perceptual dimension arises from the
integration of diﬀerent kinds of signals coming from the perception of
the outer world: exteroception. 2) The emotional dimension reﬂects the
perception of internal cues of diﬀerent valences related to pain and
pleasure: interoception.

4

Randa Kassab and Fr´ed´eric Alexandre

Fig. 1. The architecture of the hippocampal model. Black lines denote
the basic cir- cuit of the model while blue lines denote changes in
circuitry mediated by one group of associated cells (blue) following the
detection of valence-overload interference (red ar- row).
Autoassociative and heteroassociative connectivities between hippocampal
cells are denoted respectively by bidirectional dashed lines and simple
dashed lines without arrows. Inhibitory connections between valence
cells are denoted by lines ended with circles. Stable non-plastic
connections, both excitatory and inhibitory, are denoted by solid lines.

Then, the two autoassociative networks considered in the model receive
and store independently two types of input patterns, a(e) and a(i). The
layer of inter- mediate cells is organized into a small number of
ordered groups of valence cells that receive valence-related information
from the same interoceptive pathways as the interoceptive
autoassociative network. The cells in the ﬁrst group can be directly
activated by interoceptive inputs to the model and can therefore be
thought of as the primary valence cells. Interoceptive inputs on the
cells in the other groups, which are termed associated cells, are
conditional, that is, they can not evoke postsynaptic activity within
associated cells unless a concomitant signal, mk, related to the
activity pattern of a precedent group is applied.

The valence cells belonging to the same group of intermediate cells are
not interconnected. By contrast, inhibitory connections, Iij, exist
between cells be- longing to diﬀerent groups. The inhibitory connections
are not plastic. They are prewired such that an inhibitory connection
from cell i to cell j exists (Iij = 1) if the two cells belong
respectively to diﬀerent groups, k and l, and l precedes k (l < k).
Thus, each group of associated cells, once activated, silences excitable
cells in its preceding groups including the primary group of valence
cells. This means that at most valence cells in one group can be active
at a time.

The formation of extero-interoceptive associations is done at the level
of het- eroassociative links, w(e−v) , between the exteroceptive
autoassociative network and the groups of intermediate valence cells.
These latter provide direct exci- tatory input to the interoceptive
autoassociative network through non-plastic

ij

A Modular Network Architecture Resolving Memory Interference

5

connections, w(v−i) that are sensitive to the same kind of valence.

ij

. These connections are prewired only between valence cells

The classical binary version of the Willshaw network [19] is chosen as
the basis for the implementation of both auto- and heteroassociative
memory functions in the model. The neurons are simple McCulloch-Pitts
binary threshold units and learning begins with all the synaptic weights
set to zero. Synaptic plasticity is achieved according to a clipped
version of Hebbian learning: a single coincidence of presynaptic and
postsynaptic activity changes the synaptic weight wij from 0 to 1, while
further co-activations do not induce further changes. The recall process
is done by presenting a cue pattern ˜x and counting the dendritic sum
for each cell j (sj = (cid:80)n i=1 wij ˜xi) in one-time step. The
output cells that have a dendritic sum equal to or higher than the
number of active inputs are activated. The quality of a recalled pattern
can be assessed according to its Hamming distance (HD) from the
originally stored pattern (i.e. the number of elements that diﬀer
between the two patterns. For example, if x=(0 1 1 1 0) and y=(1 1 0 1
0) then HD(x,y)=2).

Similarly to cholinergic models of the hippocampus [5, 14], our model
operates in transition between two modes, storage and recall, depending
on a hyperpa- rameter ACh. This mechanism is inspired from biological
data describing mode switching under the dynamic regulation of the
levels of acetylcholine (ACh) re- leased from septal cholinergic
projections to the hippocampus. During recall, a retrieval cue, a(e), is
applied to the exteroceptive autoassociative network. The pattern of
activity obtained at the output, ˆa(e), drives retrieval in the
heteroas- sociative network. An intermediate valence cell, l, can ﬁre
only if the dendritic sum of its excitatory inputs exceeds the threshold
value and if it does not receive inhibitory inputs from other valence
cells that have already ﬁred. The activity of the intermediate valence
cells, ˜a(i), triggers recall in the interoceptive autoas- sociative
network yielding the valence prediction by the model, ˆa(i).

Just after delivery of the interoceptive information, two
novelty-detection processes take place to compare the retrieved patterns
to the actual patterns from extero- and interoception. The novelty
condition occurs when the Hamming distance between two patterns exceeds
pre-speciﬁed thresholds (HD(e) > e or HD(i) > v). Novelty induces ACh
dynamics that favor learning of new inputs, otherwise the model settles
in recall mode.

During learning, excitatory intrinsic synaptic transmission along the
recur- rent connections is removed and activity in the model is purely
driven by aﬀerent extero- and interoceptive inputs, a(e) and a(i). In
the model, two kinds of inter- ference can occur due to a saturation, or
overload of learning. The ﬁrst kind of interference occurs within the
autoassociative memories when too many or too close inputs are stored.
It is called pattern overload and can be much mitigated using sparse
patterns and low memory load conditions. The second kind of in-
terference is called valence overload and is more likely to occur when
elements making up the stored patterns become simultaneously associated
to diﬀerent valences. Consider for example learning AB+, AC- and BD-,
where A, B, C and D are exteroceptive patterns and + and - are
interoceptive valences. Since

6

Randa Kassab and Fr´ed´eric Alexandre

A and B are simultaneously associated to + and - valences, the recall of
AB would probably generate an interference (both responses produced).
The model deals with valence-overload interference by monitoring
activity of intermediate valence cells, y(v). If any activity is
observed among intermediate valence cells ((cid:80) i > 0) in response
to exteroceptive inputs a matching process takes place to determine
whether this activity matches interoceptive valence-speciﬁc inputs. A
mismatch (HD(v) > v) signals a potential interference to a successive
group of associated valence cells that become able to respond to
valence-related inputs and rapidly silence valence cells that were
active in preceding groups.

i y(v)

3 Experiments

The validity of the proposed model is examined through a series of
numerical experiments (cf. [8] for the description of other numerical
experiments with this model). The simulated model is conﬁgured with 150
cells in the exteroceptive autoassociative network and 3 cells in the
interoceptive autoassociative network. The intermediate valence cells
are organized into 5 groups of 3 cells each.

Inputs are provided to the model as two independent patterns of
activity. The exteroceptive inputs are generated as random 150-element
binary patterns with 6 elements being active (set to 1). The
interoceptive inputs are modeled by 3 binary cells to diﬀerentiate
positive, negative and neutral valence states. One of these cells
switches to its active state according to whether a pleasant (100),
unpleasant (010), or neutral (001) stimulus is present.

The performance is evaluated by comparing the output patterns recalled
by the model against the original representation of the input patterns
that were pre- sented to the model as new information to be stored.
Speciﬁcally, two kinds of recall errors are considered when evaluating
simulation results. Pattern comple- tion errors which reﬂect the Hamming
distance between the learned and retrieved activation for exteroceptive
patterns, and valence prediction errors which reﬂect the Hamming
distance between the correct and predicted valence. In both cases,
errors are scored when Hamming distance is greater than zero.

Two types of simulations are set out to test the model for its ability
to rapidly link exteroceptive patterns and their emotional valences
while avoiding valence overload interference. The ﬁrst set of
simulations examines the eﬀect of the num- ber of stored patterns on the
accuracy of valence prediction. The model is tested under full-cue and
partial-cue recall conditions. The number of stored patterns is kept low
enough that under full-cue conditions almost no pattern overload occurs
at the level of autoassociative memories. This is important to ensure
that any prediction errors might be detected arise directly from valence
overload at the level of heteroassociative links between exteroceptive
and interoceptive pat- terns. The second set of simulations focuses on
how to quantify the ability of associated valence units to orthogonalize
conﬂicting associations arising from a change in previously learned
valence values.

In all of the simulations, the performance of the proposed model, also
called the full model, is compared with that of a reduced model with the
groups of

A Modular Network Architecture Resolving Memory Interference

7

associated cells removed. A third model with a single autoassociative
memory in which both exteroceptive and interoceptive information are
merged into a single pattern is also considered to further delineate
beneﬁts of the proposed architecture under partial-cue conditions. All
results are averaged over 10 simu- lation runs and are displayed
throughout the ﬁgures as mean ± standard error of the mean. The
novelty-detection thresholds, e and v, are set to zero for all the
simulations.

4 Results

4.1 Storage capacity

Fig. 2. Inﬂuence of the number of stored patterns on the accuracy of
valence prediction. (A) Percentage of prediction errors of the model
without associated cells (W/O) and with associated cells after one block
(W (P1)) and two blocks (W (P2)) of training trials. (B) Rates of
interference detection during the ﬁrst (P1) and second (P2) training
trials. (C) Number of groups of associated cells needed to resolve
interference detected during training trials T1 and T2.

The ﬁrst set of simulations is run by varying the number of training
patterns and observing how valence prediction is aﬀected with and
without the groups of associated cells included in the model (Fig. 2).
Training patterns are presented randomly into blocks of N trials with N
varying from 10 to 100 in steps of 10. At the diﬀerent values of N, the
full and reduced models were able to recognize exteroceptive patterns
with pattern completion errors less than 0.3%. However, there was a
noticeable diﬀerence between the two models in terms of valence
prediction.

As illustrated in Fig. 2A, following the ﬁrst presentation of training
patterns, both models perform perfectly up to N=20, after which point
valence prediction errors begin to occur more frequently with increasing
size of the blocks of training trials. But as expected, adding the
associated cells decreases valence prediction errors at each value of N.
For instance, at N=100, the percentage of prediction errors is about 32%
for the reduced model but falls to about 20% for the full model. This
reduction results from the identiﬁcation of about 7% of the stored

8

Randa Kassab and Fr´ed´eric Alexandre

associations as interfering associations (Fig. 2B). Interference eﬀect
is accord- ingly reduced through the recruitment of one group of
associated cells (Fig. 2C). During the second presentation of training
patterns, the full model detects all the interfering associations that
remain and orthogonalizes them using the same group of associated cells
(Fig. 2C). Therefore, the performance of valence predic- tion diﬀers
signiﬁcantly between the two models after the second presentation of
training patterns: the reduced model continues to commit the same
prediction errors while the proposed model performs with no errors at
all.

4.2 Pattern completion

In the above simulations, it is pertinent to emphasize that no
diﬀerences were observed between the autoassociative model and the
heteroassociative model with the groups of associated cells removed.
This is of no surprise because in both cases valence prediction is
initiated by a complete set of exteroceptive cues. Under partial-cue
conditions, the proposed model as well as its reduced version are
expected to take advantage of the fact that pattern completion of
exteroceptive cues is performed prior to valence prediction. To test
this premise, the three models are trained in the same manner as in the
previous simulations except that recall is triggered by partial versions
of the original trained patterns. Speciﬁcally, the block size is set to
100 training patterns and the model is cued with partial versions with
either 1, 2 or 3 of the 6 active inputs turned oﬀ.

As shown in Fig. 3A and B, all the models perform similarly and reason-
ably well in terms of pattern completion of exteroceptive cues. The
accuracy of valence prediction of the autoassociative model is much
worse than that of the heteroassociative models and monotonously drops
as the number of deleted elements increases (Fig. 3C). On the contrary,
the heteroassociative models are much less sensitive to the percentage
of deleted elements. The accuracy of valence prediction with the 1/6
partial-cue condition is the same as that obtained with the full-cue
condition (Fig. 3D and E). This is because exteroceptive patterns are
almost perfectly reconstructed as shown in Fig. 3B. The removal of two
or three of the six active cues causes a proportional decrease in the
accuracy of pattern completion of exteroceptive patterns. Consequently,
the improvement in valence prediction by the proposed model is less
pronounced but still highly signiﬁcant as compared to the reduced model.
For all the percentages of removal simulated, the model makes use of one
group of associated cells to tackle valence-overload interference (Fig.
3G).

4.3 Discrimination

Here we investigate the functional signiﬁcance of the groups of
associated cells using numerical simulations with reversal learning
tasks. The task in the ﬁrst set of simulations involves two phases. In
the ﬁrst phase the model is presented re- peatedly with 50 training
patterns [e.g. A+, B-, C (neutral), etc.] over 4 blocks of trials and
the percentage of prediction errors made at the beginning of each trial
is measured and displayed in Fig. 4A. This is a simple discrimination
learning

A Modular Network Architecture Resolving Memory Interference

9

Fig. 3. Performance of the proposed model after training on 100 input
patterns. The model is tested using partial cues in which 1, 2, or 3 out
of 6 active elements in the original inputs are turned oﬀ. (A) Pattern
completion performance, deﬁned as the per- centage of retrieved patterns
that diﬀer at least by one element from the originally stored patterns.
(B) Pattern completion performance, deﬁned in terms of Hamming distance
between the stored and retrieved patterns. (C, D) Valence prediction
perfor- mance of the proposed model with (w) and without (w/o)
associated cells after one and two blocks of training trials. (E)
Maximal number of groups of associated cells needed to resolve
interference detected under all simulation conditions (one and two
blocks of training trials P1 and P2, and for 1/6, 2/6 and 3/6
partial-cue conditions).

10

Randa Kassab and Fr´ed´eric Alexandre

problem similar to those tested in the previous simulations. Thus as was
observed before, valence-overload interference occurs at the early
stages of learning and exhibits the recruitment of one group of
associated cells to tackle it. When the groups of associated cells are
removed the reduced model shows impaired per- formance that persists
over the repeated trials. In the second phase, emotional valences of the
training patterns are randomly changed to other value with a probability
of 50% [e.g. A-, B (neutral), C (neutral), etc.]. As shown in Fig. 4A
the proposed model quickly learns to reverse its behavior as all the
emotionally changed patterns are detected and learned on the ﬁrst
training trials after rever- sal. On the other hand, the reduced model
fails to acquire the new associations since the old ones have not been
unlearned.

4.4 Reversal learning

Further analysis of the model behavior is based on a cue-context
reversal learning task similar to that established by [10] to
investigate reversal learning in patients with mild amnesic cognitive
impairment. To simulate this task, three groups of 4 exteroceptive
patterns each are formed such that one of the 6 active elements is used
to encode the presence of a sensory cue and the others to encode
contextual cues. No overlap is allowed between cells encoding for
diﬀerent cues or contexts (cf. Table 1).

Table 1. The experimental design of the task of [10]. Note. A–H refer to
eight cue shapes, 1–8, eight contexts, + and – indicate respectively
positive and negative valences.

Training patterns

Task

Group 1

Group 2

Group 3

Phase 1

Phase 2

(original) (cue reversal) (context reversal)

(acquisition) (retention & reversal)

A1+ B2+ C3– D4–

E1– F2– G3+ H4+

A5– B6– C7+ D8+

Group1

Group1 Group2 Group3

In the ﬁrst phase of acquisition, the model is repeatedly presented with
the training patterns in the ﬁrst group and valence prediction is
evaluated over four blocks of training trials. Fig. 5 shows that both
full and reduced models make correct valence prediction after a single
exposure to the training patterns. Then, the reversal phase is
immediately followed by exposing the models to new train- ing patterns
from the second and third groups, in addition to the old ones. The
training patterns are also presented repeatedly four times in random
order. The results show that, in the ﬁrst block of trials, valence
prediction errors are made for both new and old patterns. This reﬂects
the fact that heteroassociative con- nections are irrelevantly
strengthened between the original patterns and valences

A Modular Network Architecture Resolving Memory Interference

11

Fig. 4. Discrimination reversal learning. (A) Percentage of prediction
errors of the model with (w) and without (w/o) associated cells. (B)
Rates of interference detection over each block of trials. (C) Number of
groups of associated cells needed to resolve interference across the
diﬀerent blocks of trials.

12

Randa Kassab and Fr´ed´eric Alexandre

of new patterns. When interference is detected, one group of
valence-associated cells is recruited and prediction errors fall to zero
rapidly on the third block of trials after reversal. In contrast, the
number of prediction errors the reduced model makes is still the same as
the blocks progress for the same reason stated above.

5 Discussion

The primary goal of this paper was to propose a new framework to
consider associative memories, particularly to make them eﬃcient even in
adverse con- ditions. Indeed, associative memories have powerful
properties for learning by heart speciﬁc patterns and recalling them
from partial information. They can learn quickly and recall patterns as
they were initially presented, without modiﬁ- cation nor generalization.
It has been shown [12] that such properties are highly desirable for
speciﬁc classes of cognitive functions and a class of models able to
emulate them should consequently be considered carefully. Nevertheless,
asso- ciative memories are little exploited in classical machine
learning because they suﬀer from limited storage capacities,
particularly when patterns to be stored are close, resulting in
interferences and catastrophic forgetting [6].

It has also been shown [6] that associative memories can be simply used
in auto-association for pattern retrieval but also in heteroassociation
between two diﬀerent classes of inputs. In the model presented here, we
exploit this heteroas- sociative view to propose a modular network. From
an information processing point of view, we explain in that paper that a
heteroassociation between two data spaces of diﬀerent size leads to more
robust retrieval than a simple au- toassociation with a ﬂat vector
concatenating both kinds of information because the evaluation of the
Hamming distance between stored and actual patterns would consider in
this latter case that one error in any dimension yields the same
penalty, which is obviously not the case. This is conﬁrmed in the paper,
considering comparison of performances between similar autoassociative
and het- eroassociative models.

A modular view of associative memories is also exploited to implement
an- other powerful property of our model, for managing interferences,
using another set of units called associated cells. When an association
is learned between a high dimensional data space and a smaller space
representing labels (valences in the present case), one central problem
is about the association of close patterns with diﬀerent labels or of
diﬀerent combinations of patterns with diﬀerent labels. This classical
problem has been termed conﬁgural learning [2]. With a fully automatic
algorithm to insert associated cells between the heteroassociative
modules, we have proposed in the present model a mechanism able to
detect interference at the heteroassociative level and to trigger new
learning accordingly. The exper- iments reported here, particularly
comparing performances of reduced and full heteroassociative models,
show that our model is very eﬃcient at performing such a learning. In
addition, this learning process is very quick, which preserves another
important speciﬁcity of episodic learning.

A Modular Network Architecture Resolving Memory Interference

13

Fig. 5. Cue-context reversal learning. (A) Percentage of prediction
errors of the model with (w) and without (w/o) associated cells. (B)
Rates of interference detection over each block of trials. (C) Number of
groups of associated cells needed to resolve inter- ference across the
diﬀerent blocks of trials.

14

Randa Kassab and Fr´ed´eric Alexandre

In this paper, we also propose to relate the very good properties of
modular heteroassociative memories to two diﬀerent frameworks. In the
framework of brain modeling, the model has been primarily built as a
biologically informed model of the hippocampus [8]. In addition to
proposing some evidences for the implementation of a modular network in
this cerebral structure, we also propose that heteroassociation could
take place between exteroception and interoception, corresponding to
diﬀerent kinds of hippocampl inputs. In further studies, this could be
extended to other classes of hippocampal inputs, particularly related to
the frontal cortex.

The focus was set here on interoception and exteroception because this
study was related to other studies in the team [3] related to pavlovian
conditioning. This learning paradigm is very interesting because it is
an excellent basis for a systemic view of learning in the brain, with
adaptive processing involving (at least) the amygdala, the hippocampus
and the cortex [11]. Extending the duality between procedural learning
in the cortex and speciﬁc cases learning in the hippocampus[12], we
explain in [3] that the amygdala is designed to learn pavlovian
associations from cues extracted by both structures with their own way
of learning and also report, in accordance to other authors [15], a
synergy between the three modes of learning, where an event in one
learning module (an error of prediction, the occurrence or the storage
of a speciﬁc case) can trigger or modify learning in another learning
module. Considering the importance of such a distributed learning
principle, better understanding its details deserves additional work.

Bio-inspiration was also a strong motivation for this work because, in
addi- tion to classical evaluation of performances, one of the
experiments we made was also designed to reproduce behavioral and
cognitive data in the medical domain for amnesic impairments [10].
Related medical data strongly suggest the central role of the
hippocampus in this memory process, giving additional interest to the
complementary learning system hypothesis [16]. The cognitive framework
initi- ated in [12] postulates how procedural learning in the cortex,
slowly learning and able of generalization, might be instructed by
speciﬁc cases learned quickly in the hippocampus avoiding interferences.
Adding emotional aspects with the dis- sociation between interoceptive
and exteroceptive cues, extends this framework of mnemonic synergy in
the brain, proposed for medical purposes.

In the framework of machine learning, we have also presented this work
as a new model of associative memory and its main results have been
described here mainly in the framework of information processing. This
is also the reason why we use simple binary units in the hippocampal
model, even if more complex functioning rules might be expected in the
framework of a biologically inspired model: Even if more complex units
might be considered in future works, par- ticularly to ﬁt with more
precise biological data, the main goal of the present work was to settle
the main computational principles of our modular model. Beyond the case
for pavlovian conditioning with interoceptive and exteroceptive cues, we
believe that it is not rare in the information processing domain to cope
with such associations between data of diﬀerent dimensions, as it is the
case for

A Modular Network Architecture Resolving Memory Interference

15

example with labeled data (high-dimensional data associated with a
symbolic label). In this case, we claim that combining autoassociation
and heteroassoci- ation as proposed here results in more robustness in
the retrieval phase. More generally and beyond associative memories,
heteroassociation using intermediate cells to reduce ambiguities is a
general class of approaches in machine learning and the criteria
proposed here to avoid interference and keep associations simple could
be extended to other models of machine learning in further works. This
could illustrate other cases where a primary biological inspiration
yields eﬃcient learning principles.

16

Randa Kassab and Fr´ed´eric Alexandre

References

1.  P. Andersen. The Hippocampus Book. Oxford Neuroscience Series.
    Oxford Univer-

sity Press, USA, 2007.

2.  Catalin V. Buhusi and Nestor A. Schmajuk. Attention, conﬁguration,
    and hip-

pocampal function. Hippocampus, 6(6):621–642, January 1996.

3.  Maxime Carrere and Frederic Alexandre. A pavlovian model of the
    amygdala and its inﬂuence within the medial temporal lobe. Frontiers
    in Systems Neuroscience, 9(41), 2015.

4.  B. Graham and D. Willshaw. Capacity and information eﬃciency of the
    associative

net. Network: Computation in Neural Systems, 8(1):35–54, 1997.

5.  Michael E. Hasselmo, Bradley P. Wyble, and Gene V. Wallenstein.
    Encoding and retrieval of episodic memories: Role of cholinergic and
    gabaergic modulation in the hippocampus. Hippocampus,
    6(6):693–708, 1996.

6.  J. Hertz, A. Krogh, and R. Palmer. Introduction to the theory of
    neural computa-

tion. Addison Wesley, 1991.

7.  J. J. Hopﬁeld. Neural networks and physical systems with emergent
    collective computational abilities. In Proceedings of the National
    Academy of Sciences, USA, pages 2554–2558, 1982.

8.  Randa Kassab and Frederic Alexandre. Integration of exteroceptive
    and intero- ceptive information within the hippocampus: a
    computational study. Frontiers in Systems Neuroscience, 9(87), 2015.

9.  A. Knoblauch, G. Palm, and F. T. Sommer. Memory capacities for
    synaptic and

structural plasticity. Neural Computation, 22(2):289–341, 2010.

10. E. Levy-Gigi, O. Kelemen, M. A. Gluck, and S. K´eri. Impaired
    context reversal learning, but not cue reversal learning, in
    patients with amnestic mild cognitive impairment. Neuropsychologia,
    49(12):3320–6, 2011.

11. Stephen Maren. Building and Burying Fear Memories in the Brain. The
    Neuro-

scientist, 11(1):89–99, February 2005.

12. J. L. McClelland, B. L. McNaughton, and R. C. O’Reilly. Why there
    are com- plementary learning systems in the hippocampus and
    neocortex: insights from the successes and failures of connectionist
    models of learning and memory. Psycholog- ical review,
    102(3):419–457, 1995.

13. B.L. McNaughton and L. Nadel. Hebb-marr networks and the
    neurobiological representation of action in space. In Neuroscience
    and Connectionist Theory, pages 1–63. Hillsdale, NJ: L.
    Erlbaum, 1990.

14. M. Meeter, J. M. Murre, and L. M. Talamini. Mode shifting between
    storage and recall based on novelty detection in oscillating
    hippocampal circuits. Hippocampus, 14(6):722–41, 2004.

15. Ahmed A. Moustafa, Mark W. Gilbertson, Scott P. Orr, Mohammad M.
    Herzallah, Richard J. Servatius, and Catherine E. Myers. A model of
    amygdala–hippocampal– prefrontal interaction in fear conditioning
    and extinction in animals. Brain and Cognition, 81(1):29–43,
    February 2013.

16. Randall C. O’Reilly, Rajan Bhattacharyya, Michael D. Howard, and
    Nicholas Ketz.

Complementary Learning Systems. Cognitive Science, December 2011.

17. T. Samura, M. Hattori, and S. Ishizaki. Sequence disambiguation and
    pattern com- pletion by cooperation between autoassociative and
    heteroassociative memories of functionally divided hippocampal CA3.
    Neurocomputing, 71(16–18):3176–183,
18. 

A Modular Network Architecture Resolving Memory Interference

17

18. Endel Tulving. Episodic and semantic memory. Organization of Memory.
    Academic

Press., 1972.

19. D. J. Willshaw, O. P. Buneman, and H. C. Longuet-Higgins.
    Non-holographic

associative memory. Nature, 222(5197):960–962, 1969.


