Knowledge extraction from the learning of sequences in a long short term
memory (LSTM) architecture Ikram Chraibi Kaadoud, Nicolas P. Rougier,
Frédéric Alexandre

To cite this version:

Ikram Chraibi Kaadoud, Nicolas P. Rougier, Frédéric Alexandre. Knowledge
extraction from the learning of sequences in a long short term memory
(LSTM) architecture. Knowledge-Based Systems, 2021, 235, pp.18.
￿10.1016/j.knosys.2021.107657￿. ￿hal-03437920￿

HAL Id: hal-03437920

https://inria.hal.science/hal-03437920

Submitted on 20 Nov 2021

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

Knowledge extraction from the learning of sequences in a long short
term memory (LSTM) architecture

Ikram Chraibi Kaadoud12

Nicolas P. Rougier123

Fr´ed´eric Alexandre123∗

1Inria Bordeaux Sud-Ouest, 200 avenue de la vieille tour, 33405 Talence,
France 2LaBRI, UMR 5800, CNRS, Universit´e de Bordeaux, Talence, France
3 Institut des Maladies Neurod´eg´en´eratives, UMR 5293, CNRS,
Universit´e de Bordeaux, Bordeaux, France ∗ Corresponding author:
{frederic.alexandre@inria.fr}

Abstract

Transparency and trust in machine learning algorithms have been deemed
to be funda- mental and yet, from a practical point of view, they remain
diﬃcult to implement. Partic- ularly, explainability and
interpretability are certainly among the most diﬃcult capabilities to be
addressed and imply to be able to understand a decision in terms of
simple cues and rules. In this article, we address this speciﬁc problem
in the context of sequence learning by recurrent neuronal models (and
more speciﬁcally Long Short Term Memory model). We introduce a general
method to extract knowledge from the latent space based on the cluster-
ing of the internal states. From these hidden states, we explain how to
build and validate an automaton that corresponds to the underlying
(unknown) grammar, and allows to predict if a given sequence is valid or
not. Finally, we show that it is possible for such complex recurrent
model, to extract the knowledge that is implicitly encoded in the
sequences and we report a high rate of recognition of the sequences
extracted from the original grammar. This method is illustrated on
artiﬁcial grammars (Reber grammar variants) as well as on a real
use-case in the electrical domain, whose underlying grammar is unknown.

Keywords: Sequence learning, Knowledge extraction, Long Short Term
Memory, Latent space, Implicit learning, Recurrent Neural Networks

1

Introduction

Eﬃcient numerical models are exploited as standard tools to process data
in many domains of the socio-economic world. The possibility to extract
knowledge from this kind of models becomes an increasingly important
demand, as it is frequently discussed in the context of ANNs (Artiﬁcial
Neural Networks). It is a valuable asset when one wants to compare such
models with more traditional and explicit algorithmic approaches. It is
even more fundamental, if not mandatory, in critical domains where the
functioning of an autonomous decision-making system must be assessed by
humans, in order to check that it relies on valid cues. Transparency is
often evoked to justify this requirement (Lipton, 2016), advocating that
the end-user of the model should have a level of understanding
comparable to that of the expert, designer of the model and owner of the
hidden knowledge. This is also true when the underlying knowledge is not
known a priori and when the assessment is not a simple veriﬁcation but
rather corresponds to the discovery of a hidden knowledge that might be
conﬁrmed a posteriori. This situation can be compared, in human
cognition, to the distinction between explicit and implicit memory
(Reber,

1

2013). In the case of explicit memory, the knowledge is conscious and
human decisions can be explained and expressed in a declarative way (you
can for example explain that you have ranked students by comparing their
marks following explicit rules). In the implicit case, you can acquire
skills without being conscious of what you have learned and without
being able to explain it. Consider, for example, typing on a keyboard a
series of symbols where repeated sequences are hidden (Pascual-Leone et
al., 1995). You improve your motor skills of reaching keys without being
conscious of the existence of regularities. Language is another typical
example of implicit learning acquired by children through practice
(repetition and imitation), not through explicit learning of valid rules
(Oudeyer, 2006). Implicit learning is pervasive in the socio-economic
world because experts in a domain tend to acquire their expertise
through practice and are, most of the time, unable (or only partly able)
to verbalize their knowledge. Such expertise can also be acquired by
models like ANNs by learning from examples produced by experts in order
to reproduce the corresponding skill. In this case, the importance for
knowledge extraction is high because this knowledge is hidden to all the
actors of the process and might bring valuable information. As reviewed
below, many approaches for knowledge extraction using ANNs have been
proposed. In this paper, we will consider the process of knowledge
extraction from the implicit learning of temporal behaviour. This means
that the numerical models to consider will have to be able to process
sequences (as it is the case with Recurrent Neural Networks (RNNs) for
example) and the data to be manipulated will be consequently corpuses of
sequences. Pre- liminary attempts for extracting knowledge from the
Simple Recurrent Network (SRN) model have already been proposed (Elman,
1990; Servan-Schreiber et al., 1988; Cleeremans et al., 1989;
Servan-Schreiber et al., 1991; Cleeremans and McClelland, 1991). But it
was established that due to the limited size of the context layer, the
SRN couldn’t handle long term dependencies : the old past is overwritten
by the recent past, which makes the development of a stable im- plicit
representation of grammar rules impossible, especially in the case of
long and ambiguous In the present paper, we propose to consider more
complex and powerful RNNs, sequences. namely LSTM (Long Short Term
Memory) networks (Hochreiter and Schmidhuber, 1997), since that model
showed great performance for learning sequences from diﬀerent kinds of
dataset (Greﬀ et al., 2017). Before diving into the methodology, we will
ﬁrst discuss some important aspects of the scientiﬁc context of
knowledge extraction in machine learning (section 1.1), in- troduce
experimental contexts that are classically considered in implicit
learning of sequences (section 1.2), describe a classical process of
knowledge extraction from RNNs (section 1.3) and report the current
level of performance of this process with diﬀerent kinds of RNNs
(section 1.3.3), thus motivating our positioning (section 1.4).

1.1

Interpretability in machine learning

The principles of learning algorithms associated to multi-layered
supervised neuronal ar- chitectures are known for more than thirty years
(Rumelhart and MacClelland, 1986). These architectures have recently
gained a renewed interest mainly because larger corpuses and the
corresponding computing power are now available. This led to the
development of innovative architectures such as for example
convolutional networks in the domain of image classiﬁcation (Goodfellow
et al., 2016) or Long Short Term Memory (LSTM) models for temporal
sequence processing (Hochreiter and Schmidhuber, 1997). These latter
models have outperformed sim- ple recurrent networks (SRN) in sequential
data processing and are able to deal with complex structured sequences.
However, and in spite of these major improvements, a major weakness
remains for these multi-layered architectures: They are considered as
black boxes and their ex- plainability, or put diﬀerently their capacity
to explain how they build their decision in a way intelligible for
humans, is very weak or even non existent.

Several notions have been proposed to describe the processes of
knowledge extraction from

2

these black boxes (Guidotti et al., 2018; Ayache et al., 2018). While
explainability is the ultimate step where the functioning of the black
box is fully understood and can be transferred to humans in
understandable terms, interpretability refers to the capacity of
breaking down all the inner mechanisms of the black box (without
necessarily understanding them). This might be useful for an expert but
not always for the layman. Interpretability is considered ”global” when
its purpose is to explain the whole logic of the model with rules
associating the output with the input variables (for example
approximating a non linear black box to a simple linear model). It is
deﬁned ”local” when the process focuses on understanding speciﬁc reasons
for a decision on a datum by extracting patterns critical to the
decision (for example extracting a surface of separation between
categories). These examples also indicate that interpretability can be
achieved by an approximation process and can give an intuition rather
than the full knowledge (Guidotti et al., 2018). These concepts
(i.e. explainability and interpretability) are often mixed up in the
literature, even if they are distinct since an interpretable model is a
requirement for having an explanation which in turn depends, among other
things, on the intended purpose, the decision-maker, and the context of
the situation. In this paper, we will remain on the technical side and
focus on the concept of interpretability and its dimensions.

In this speciﬁc ﬁeld, Remm and Alexandre (2002) have deﬁned two
approaches for extracting knowledge from neural networks, seen as black
boxes : the pedagogical method when rules can be extracted directly
during learning, and the decompositional method, when elements of the
rules are extracted from the hidden layers after learning before being
recombined globally (in this case, we evoke a post-hoc
interpretability).

Concerning RNNs, in the pedagogical case, some methods have proposed,
querying an RNN using examples and counter-examples. The resulting pairs
of (input, output) are used to confront the network against various
hypotheses and directly model its global behaviour (Weiss et al., 2017)
without considering what happens inside.

In the decompositional case, and speciﬁcally in post-hoc
interpretability, knowledge is ex- tracted a posteriori from the hidden
layers of the network (Lipton, 2016). Indeed, it is admitted that during
the learning phase, the network will extract automatically simple hints
or features (knowledge of features) or privileged relations expressing
simple causal relations between the input and the output spaces
(knowledge of rules) that are relevant to the targeted task. This
knowledge representing the implicit representation of the network is
encoded into its latent space (Abu-Mostafa, 1990; Bengio et al., 2013)
which is ”hidden-layer-dependant”. So by ex- tracting and analyzing the
activity of the hidden layers during the test phase (once the learning
is ﬁnished), it is possible to get hints at interpreting the network
behavior and its decisions.

Decompositional approaches have been proposed in the case of pattern
recognition (Setiono and Liu, 1996; Remm and Alexandre, 2002). They
include a step of pruning of the network, for the hidden layer to remain
in tractable dimension, and another step of simpliﬁcation by mapping the
activity of the hidden units onto a ﬁnite set of values, corresponding
to the features to be extracted. This allows to have a discrete and
limited set of combinations to consider, to link input states to the
resulting activation of the output, thus yielding rules. This approach
has led to sound feature and rule extractions in several application
domains. However, when dealing with recurrent networks processing
sequences, the extraction task becomes much more diﬃcult.

1.2 Implicit learning of sequences

The form of knowledge acquired by learning is a major question in
cognitive science and several experimental protocols have been designed
to study various paradigms in the case of implicit learning of sequences
(Perruchet and Pacton, 2006; Pothos, 2007). There are two major
approaches used in these studies, which could be contrasting to each
other.

In the ﬁrst one, it is believed that contingencies are extracted in a
bottom-up way by

3

a kind of associative learning, classical with ANNs and with simple
RNNs. In this case, the standard task is the Serial Reaction Time (SRT)
task : The subject is asked to reproduce sequences with hidden
regularities and improvements of performance in the reproduction of
sequences are measured (Perruchet and Pacton, 2006). It is believed that
a perceived symbol is associated to the next symbol to produce, and a
reduced reaction time is seen as a clue that the transition to be
reproduced is valid. The second approach is top-down and considers that
the knowledge is encoded in grammatical rules. The task of Artiﬁcial
Grammar Learning (AGL) is generally considered in this approach (Pothos,
2007). Grammatical sequences built with a ﬁnite state language are
presented and it is believed that the RNN is able to extract and build
an automaton encoding the corresponding language. This process is less
dependent on frequences of local transitions and in the test phase, new
strings can be classiﬁed as grammatical or not grammatical.

Two contexts for knowledge extraction can appear : When the grammar is
known a priori, and when it isn’t known or hidden. In the ﬁrst context,
it is possible to generate a learning corpus including positive and
negative (valid and non-valid) examples to learn the task as a cat-
egorization problem. In addition, the extracted rules or contingencies
can be directly compared to the original grammar for validation,
considering that valid examples should be recognized and non-valid
should be rejected. In the second context (i.e. when the grammar is not
known or hidden), only positive (valid) examples are available for
learning. The task then is predicting the next element (e.g. action,
decision, symbol) from the current element and the past ones. In this
context, assessing the quality of the extracted knowledge is a more
diﬃcult process, only possible by experimentation or by subjective
analysis by a human expert.

In our work, we will mainly consider the AGL approach. We will keep the
grammar hidden in the learning phase, thus corresponding to a prediction
task, but we will be able to evaluate directly the results against the
grammar in the validation phase. We will also present preliminary work
on a real data set, from the domain of electrical diagrams analysis
where no grammar is available, thus requiring human evaluation.

1.3 Major steps for knowledge extraction from sequences

Within the AGL approach, the extracted knowledge implicitly encoded in
an RNN can take the form of a ﬁnite state automaton (FSA). In the
deterministic case, it is called Deterministic Finite Automaton (DFA).
Automata extraction from RNNs has been described as a three-step process
(Wang et al., 2018), following a decompositional method (Remm and
Alexandre, 2002), for post-hoc interpretability. Considering that the
network is already trained with a learning corpus, the ﬁrst step
consists of, for each item in a test corpus (set of sequences),
recording the values of the hidden units in the shape of vectors. We
will refer to them as the hidden units’ activation patterns or hidden
patterns. During the second step, all the collected hidden patterns are
analyzed to quantize the hidden state space into a ﬁnite set of discrete
states that can be seen as important features extracted from the
network. The third step is the automaton construction followed by a
minimization process of the obtained automaton (Giles et al., 1992). In
addition, it is important to perform a validation process by submitting
novel valid and non- valid sequences to the automaton, to evaluate its
generalization capabilities or by requesting the subjective evaluation
of a human expert.

1.3.1 Hidden space quantization

Diﬀerent approaches were used to quantize the hidden space : 1)
considering that each hidden pattern is a state of the desired automaton
(Weiss et al., 2017), 2) cutting the hidden space into a limited number
of discrete states by a quantization process (Giles et al., 1991, 1992;

4

Omlin and Giles, 1996), and 3) using clustering techniques like k-means
(Zeng et al., 1993), self-organizing maps (Ti˜no and Sajda, 1995; Blanco
et al., 2000) and hierarchical clustering (Cleeremans et al., 1989;
Servan-Schreiber et al., 1989; Elman, 1990).

One characteristic shared by these studies is that they mostly use
positive and nega- tive examples generated from the Tomita grammars
(Tomita, 1982) that induce regular binary languages (sequence of 0 and
1), far from real life choices. Only Ti˜no and Sajda (1995) and
Schellhammer et al. (1998) proposed rule extraction processes from RNN
fed with only positive examples composed of non-binary choices.

1.3.2 Automata construction

Many techniques can be used for automata construction once the
partitioning is done. They all involve generating the states and outputs
(and the output classiﬁcation if necessary), while input patterns are
fed to the RNN. Indeed, by feeding the network with input patterns, it
is possible to ﬁnd the appropriate transitions between the states, and
thus the edges of the ﬁnal automaton (Omlin and Giles, 1996).

In the case of a quantization process, the hidden space is divided into
”macrospaces” that represent ”basically what the rule extraction
algorithm “sees” of the underlying RNN” (Jacobsson, 2005) and
transitions between macrostates will correspond to the edges of the ﬁnal
automaton.

Another technique is the sampling-based approach, the principle of which
is to replace the search in a quantized state space by the recorded
activity of the RNN interacting with its environment and the data
(Watrous and Kuhn, 1992). Ti˜no and Sajda (1995) proposed a similar
approach but only for the extraction of a Mealy machine (where the
output depends on the input at time t and the current state) as opposed
to a Moore machine (where the output depends only on the current state)
(Keller, 2001).

Another approach proposed by Schellhammer et al. (1998), whose dataset
was composed only of positive examples, involves using the k-means
algorithm for clustering the hidden units activation patterns and a
process based on frequency analysis for the automaton construction
phase. Only transitions whose frequency is above a speciﬁc threshold are
kept and displayed in the ﬁnal automaton.

1.3.3 Levels of performance

Even, if all these experiments use diﬀerent algorithms to analyze the
hidden space and build the automaton, they are all similar because they
all rely on the same three step process initially deﬁned in (Giles et
al., 1992; Omlin and Giles, 1996). A major question is subsequently to
wonder about the level of performance of this approach, depending on the
RNN analyzed and on the complexity of the underlying grammar.

Wang et al. (2018) have recently proposed an important study about
knowledge extraction from RNNs, using this kind of decompositional
methods. They consider several kinds of RNNs, namely Elman SRN,
Second-order RNN, Multiplicative Integration RNN, LSTM and Gated
Recurrent Unit (GRU) RNN. Even if these networks are theoretically
equally expressive, recent models have been designed to allow for more
eﬃcient learning, with Second-order RNN and Mul- tiplicative Integration
RNN introducing weights for second order and multiplicative integration
of internal representation and LSTM and GRU introducing gate units that
can keep a memory of past events. In both cases, learning can directly
apply on more elaborated information, at the price of more weight
parameters.

For this reason and to ensure a fair comparison of performances, authors
adapt the size of the hidden layers of each model, so that they all have
the same number of parameters. These

5

RNN models are trained on the seven Tomita grammars and, for each
model, knowledge is extracted with the classical approach of
decompositional methods (vector space of hidden layers partitioned into
ﬁnite parts, each treated as the states of a DFA subsequently
minimized).

The reader is referred to Wang et al. (2018) for the detailed analysis
of the results for each Tomita grammar but, in short, authors conclude
that ”second-order RNN provides the best and most stable performance of
DFA extraction on the seven Tomita grammars in general”. The authors
acknowledge that ”it is quite surprising that both LSTM and GRU fail on
this DFA extraction task with simple datasets, given the success of
these models”. They propose that this might be due to ”the fact that the
information representing the discrete states and their transitions are
not completely stored in the recurrent layer” [but also in the gates]
and conclude that ”extracting DFA from these more complicated models
remains an open question”.

1.4 Positioning and main contributions

The work reported here is concerned with the extraction of structured
knowledge acquired by implicit learning of sequences when the grammar is
unknown or hidden and only positive examples are available for learning
(Ti˜no and Sajda, 1995; Schellhammer et al., 1998). We propose to use
RNNs to learn corresponding sequences and exploit decompositional meth-
ods as deﬁned in Giles et al. (1992); Omlin and Giles (1996) to extract
knowledge from RNNs into FSA, following the three step process. Basic
RNN models have mostly been used for early attempts of automata
extraction. They often demonstrate some weaknesses in the complexity of
sequences they can assimilate. The recent study by Wang et al. (2018)
indicates that decom- positional methods can be successfully applied to
more recent RNNs, being able to cope with more complex grammars, except
LSTM. The main contribution of the present paper is to propose an
end-to-end eﬃcient methodology for knowledge extraction from LSTM. In
brief, by summarizing the vector space of hidden layers as only their
output activity, we solve both problems encountered in Wang et
al. (2018). On the one hand, instead of using all the hidden weights, we
only select one parameter per hidden unit, thus taming the combinatorial
increase of using more parameterized models. On the other hand, the
output activity of the hidden units is the result of all internal
computations within units, not only including hidden weights but also
gates. In the present study, we thus demonstrate experimentally two main
points :

• The small-sized hidden representation of LSTM, when associated to
classical decomposi- tional methods, carries enough information to allow
for the extraction of a fully accurate DFA

• The extracted automaton provides local interpretability about speciﬁc
predictions, and represents also a substitute of the neural network that
can mimic the behavior of the whole network for a speciﬁc dataset and
thus provides insights into the ﬁeld of global interpretabilty of RNNs

In the sequel of the paper, we ﬁrst introduce the grammars that have
been chosen for generating the corpus of sequences, derived from the
Reber grammar. We then present our methodology by describing the LSTM
recurrent network used, the knowledge extraction process and the val-
idation process of the extracted knowledge. Finally, we report
experimental results obtained on both artiﬁcial grammars and an
industrial application in the electrical domain, before discussing the
main lessons about this study and its importance in the ﬁeld of
knowledge extraction, and evoking prospective work in the last section.
Figure 1 depicts the global experimental approach followed in our work
and Table 1 lists the used acronyms.

6

Figure 1. The global experimental approach for implicit knowledge
extraction from RNN-LSTM in a task of prediction in three steps : 1) the
learning phase where valid sequences generated from a grammar are used
to train the network. 2) the knowledge extraction process, and 3) the
automata validation process where both valid and non-valid sequences are
used. In the text, step 2 and 3 are grouped, but presented visually here
separately to explain how sequences are used in the global approach.

Artiﬁcial Neural network Recurrent Neural network Long Short Term Memory

Acronym Meaning ANN RNN LSTM RNN-LSTM a RNN with a hidden layer composed
of LSTM units AGL RG ERG CERG FSA NFA DFA

Artiﬁcial Grammar Learning Reber Grammar Embedded Reber Grammar
Continious and Embedded Reber grammar Finite State Automaton
Non-deterministic Finite Automaton Deterministic Finite Automaton

Table 1. Acronyms used in this paper.

7

2 Methods

Figure 2. The three grammars used in the experiments, represented as a
Finite State Automaton including nodes representing states and bows
emitting symbols. From left to right : A - Reber Grammar (RG), B -
Embedded Reber Grammar (ERG), C - Continuous Embedded Reber Grammar
(CERG). B means ”Begin” and E means ”End”. See section 2.1 for details.

2.1 Reber grammar : Non-binary sequences closer to real life situations

Three grammars were used during this work (cf ﬁgure 2): the Reber
grammar (RG), a grammar originally used in cognitive psychology
experiments about implicit learning ability in humans (Reber, 1967;
Cleeremans and McClelland, 1991), as well as two of its variants, the
embeded Reber grammar (ERG) and the continuous and embedded Reber
grammar (CERG). Even if, as mentionned above, most works in AGL use
binary Tomita grammars, we found it more consistent with regard to
targeted industrial applications where symbols are electrical
components, to use RG, composed of 7 symbols (B,T,P,S,X,V,E) appearing
twice, 6 nodes and 12 transitions.

Each variant of RG was employed to test a particular aspect of the RNN,
namely perfor- mances during prediction, resistance to the size of the
sequences and good retention of informa- tion when long term
dependencies between elements are to be considered. The use of the RG
has allowed us to test out the ability of the model to learn and predict
in a context where the beginning and end of sequences are easily
identiﬁable. The ERG tests the ability of the network to handle
ambiguity for the begin and end symbols that are no longer only at ends
but may appear also in the middle of a sequence. Finally, the CERG
grammar eliminates the notion of beginning and end. The neural network
must learn a continuous ﬂow and be able to make the right predictions
based on the local context and the one further back in time. In all of
these grammars, transitions between symbols have to be learned in a more
or less extended temporal context. Similarly, in a real context such as
electrical diagrams analysis, electrical components can be used for
diﬀerent installations according to constraints like amperage, type of
power supply and a whole set of electrical technical characteristics,
which generate sequences with the same components but with diﬀerent
contexts (Chraibi Kaadoud et al., 2017).

2.2 The procedure of knowledge extraction from LSTM

Among the diﬀerent variants of the LSTM model (GRU, Vanilla LSTM, LSTM
with peephole connexions), we have chosen to implement the version
proposed by Gers et al. (2000) (with forget gates and no peephole
connection), that the authors applied in their study to the Reber
grammar. As presented in ﬁgure 3, the network architecture, also
inspired by the work of

8

Figure 3. The RNN-LSTM model with three layers. In addition to
classical input and output layers, hidden units are introduced : In the
picture, all white dots outside LSTM blocks are linked to all black
dots. There are skipped connections between input and output units. The
hidden layer provides a real-valued vector of size 8. Figure adapted
from Lapalme (2006)

four LSTM blocks with two cells and a CEC (Constant Error Carousel) in
each.

9

Gers et al. (2000), consists in three layers with recurrences limited
to the hidden layer composed of four LSTM blocks with two cells each.
For further technical details, B describes the network parameters and
characteristics.

The model is trained on the following task : considering a sequence, the
network receives as input at time t a symbol in the shape on a binary
vector (we use one-hot encoding strategy to encode each symbol) and
should predict the symbol at t + 1 in the sequence in the shape of a
numerical vector (encoding the estimated probability of each symbol).
Depending on the sessions, the network is trained and tested not only on
sequences but also ﬂows (sequences put end to end) generated by RG, ERG
and CERG.

During the learning phase, it is supposed that important knowledge
associated to the hidden rules is encoded in the processing part of the
network, at the level of the state space of the hidden layer units. This
knowledge should allow to deﬁne the rules of transition of the grammar
(set of rules) and the contexts in which they are valid. Transposing
existing approaches for rule extraction from SRNs (Servan-Schreiber et
al., 1988) onto LSTMs, at each time step, we record the activity of the
outputs of the LSTM cells, considered here as the activity of the hidden
layer. We extract implicitly learned features (corresponding to
important contexts for rule deﬁnition) by a clustering process onto this
hidden space and subsequently build an automaton by collecting sequences
of the extracted features when valid sequences are fed to the network.
The automaton is minimized in the ultimate step, before its evaluation.

2.2.1 Collecting activity patterns of the hidden layer

To extract the knowledge implicitly learned in the hidden space, we
generate a test corpus from the same grammar as provided for training
the network. To analyze information ﬂow as a sequence is presented, we
have created a list of activity patterns, each with a unique identiﬁer
(”id”) consisting of the current symbol and the time step. In the
sequence BTXSE for example, the id list of the activity patterns for
each symbol will be : B0, T1, X2, S3. We record the activity of the
outputs of the LSTM cells that are represented in numerical vectors of
dimension 8.

2.2.2 Feature extraction algorithm using k-means for clustering

We use the k-means algorithm for clustering. This algorithm consists of
partitioning the data collected from the hidden space into k groups by
minimizing the distance between samples within each partition. The
number of clusters in which the data must be partitioned is a parameter
of the model. For each value of k, each of the hidden activity patterns
ordered in a list that we call ”list-patterns” is assigned to 1 among k
clusters, thus yielding a list of same size called ”list-clusters”
indicating the cluster id of each pattern. Consider two examples of
lists: A list-patterns of 5 patterns: [h0, h1, h2, h3, h4] and a
list-clusters [0 3 2 2 0] from a clustering using k-means with k = 4. An
appropriate reading of these two lists is:

• the patterns h0 and h4 belong to cluster 0

• the pattern h1 belongs to cluster 3

• the patterns h2 and h3 belong to cluster 2

Matching both lists, it is possible to associate any pattern to a
cluster and thus to a state

(i.e. a node) in the FSA we wish to extract.

10

2.2.3 Average Silhouette coeﬃcient analysis for k-means algorithm

To analyze the impact of the clustering parameter, the k value, on the
distribution of clusters, we computed the mean silhouette coeﬃcient for
all samples for each value of k. The purpose of this method is to study
the distance between resulting clusters on a range of -1, 1. For a given
sample, if the silhouette coeﬃcient is close to -1, the sample is
assigned to the wrong cluster. If the measure is around 0, the sample is
on (or very close to) the decision boundary between two neighboring
clusters, and if the measure is close to 1, the sample is assigned to a
cluster and far from others. The best k value for clustering is reached
for the maximum value of the silhouette coeﬃcient. Computing this value
allows thus to objectively evaluate the quality of the clustering
computed for each value of k.

2.2.4 Automata generation

Figure 4. Example of an automaton generation with a list-patterns [h0,
h1, h2, h3, h4] and a list-clusters [0 3 2 2 0]. A - Time step t0,
creation of a new node (state). B - Time step t1, creation a second node
(state). C - After time step t3 and t4 : h2 and h3 belong to the same
cluster, which generates a loop. D - Time step t4 : h4 belongs to a
cluster already visited, which creates an edge between the current node
(C2) and the existing node (C0)

We adapted the algorithm described by Omlin and Giles (1996) for
automaton generation. Figure 4 presents an example of automaton
generation using the ﬁve patterns mentioned above and information given
by list-patterns and list-clusters. The generation of the FSA is
initiated by adding a node with the identiﬁer -1 as a starting point
(ﬁgure 4, label A).

The rule extraction process requires a simultaneous analysis of both
list-patterns and list- clusters : for each pattern, if the associated
cluster is a new one (i.e. not represented as a node in the FSA), a new
node is then added with its id as cluster number, together with a
directed edge from the previous node to the new node (ﬁgure 4, label A,
B, C). If the current pattern belongs to a cluster already represented
in the FSA, then a directed edge is added between the previous node and
the corresponding node (ﬁgure 4, label D, time t4). In the case where
two consecutive patterns belong to the same cluster, a recursive
connection is added to the node

11

representing the cluster (ﬁgure 4, label C, time t3). This process
yields an automaton with unlabeled transitions explaining the
arrangement of the clusters (states).

To improve the quality of the extracted automaton and give more
information on the processing of sequences in the hidden space, we have
modiﬁed the algorithm proposed in Omlin and Giles (1996), extending the
mechanism proposed in Schellhammer et al. (1998) associating nodes with
their frequency, to edges and the information they carry. In our
approach, each edge is assigned an identiﬁer during the automaton
generation phase, corresponding to the symbol that the LSTM processes at
the associated time step. In the previous example, if sequence BTXSE is
the ﬁrst to be analyzed by the network, the identiﬁer of each symbol
will be B0, T1, X2, S3. If an edge already exists between the two nodes,
the new symbol is added to its identiﬁer.

This original process allows to generate an FSA with long labels on
edges, that makes the temporal organization of the RNN patterns
explicit. Figure 6a shows the example of an automa- ton generated
without a label and ﬁgure 6b shows the same automaton with labels.
Algorithm 1 describes the process of extracting the rules in the form of
an FSA with long labels (ﬁgure 6c) using the hidden activity patterns of
an RNN-LSTM. Long labels are interesting because they give information
about the place of symbols in the sequence. They can also be synthesized
into short ones by just keeping the id of the symbol and removing
numerical information. The comparison with the original automaton of the
corresponding grammar becomes visually easier (for small automata).
Figure 6b and Figure 7b present examples of automata generated with a
simpliﬁed label.

12

Algorithm 1 Algorithm for extracting rules in the form of a FSA with
long labels, using the activity patterns of an RNN-LSTM Require: #
Learning and test of the RNN—–

RNN LSTM.learning(learning data set) # labels list : list of symbols
presented to the network during tests activity patterns list, labels
list = RNN LSTM.test(test data set)

Function rules extraction (activity patterns list, labels list, k) : #
Clustering —————————————————- clusters list = k means(k, activity
patterns list) # Generation of automaton A—————————————————- A = {} #
Dictionnary current node= -1 A[’nodes’].add(current node) A[’edges’] = [
] # list of dictionnaries for all pattern h of index i from activity
patterns list do

associated cluster = clusters list[i] if associated cluster (cid:54)∈
A[’nodes’] then A[’nodes’].add(associated cluster)

end if edge= {} # Dictionnary edge[’id’] = (current node, associated
cluster) if edge (cid:54)∈ GA[’edges’] then

new edge = edge new edge[’weight’] = 1 new edge[’label’] = labels
list[i] A[’arcs’].add(nouvel edge)

else

edge[’weight’] = edge[’weight’] +1 edge[’label’] = edge[’label’]+ labels
list[i] A[’edges’].update(edge)

end if # Update of the current node current node= associated cluster

end for

return A

End Function

2.2.5 Automaton minimization

The last step for automaton generation is the minimization process.
Minimization algo- rithms exist for deterministic ﬁnite automaton (DFA).
They involve transforming a DFA into a minimal version of that DFA with
the minimum number of states. The process starts by de- termining the
type of the FSA : non-deterministic ﬁnite automaton (NFA) or DFA. A DFA
has transitions that are uniquely determined by the input symbol from
each state. On the contrary, an NFA is an automaton where several
possibilities of transition can exist simultaneously for a state and a
given input symbol. In this case, a conversion into a complete DFA is
made.

In our work we use the table-ﬁlling method, derived from the
Myhill-Nerode theorem

(Nerode, 1958), for the minimization process.

This method involves ﬁrst building a transition table containing as many
columns as lines, where each of them represents a state of the
automaton. The second step is an iterative process that ﬁlls the table
according to the transitions of the automaton. The remaining unmarked
pairs are grouped as a single state. Algorithm 2 describes this method.

13

Algorithm 2 The Myhill-Nerode algorithm for DFA minimization

Function rules extraction (ﬁnal states, non ﬁnal states) : Draw a table
for all pairs of states (P, Q) Mark all pairs where only one state
belongs to the ﬁnal states Repeat until no more marking can be done :

if there is any unmarked pairs (P,Q) then

δ is the transition function that ”transform P into P’ according an input X

if [δ(P, X), δ(Q, X)] is marked then

Mark(P,Q)

end if

end if End Function

2.3 Validation procedure and analysis

The last step of the proposed methodology involves analyzing the
minimized DFA by feeding it with grammatical sequences in order to
conﬁrm if it recognizes the same language as the original grammar. For
this purpose, we apply the following process (ﬁgure 5) : for each
sequence, the starting node is the one containing -1 in its label. We
apply the input on it (corresponding to the ﬁrst symbol of the sequence)
to retrieve a new state. We establish then the neighbors list
(i.e. states) of this new state and their associated transitions. If
among these transitions, there is one corresponding to the next symbol
of the sequence, the new state becomes the current state. The process is
then repeated again, until the next symbol of the sequence is the last
symbol of the sequence (i.e. symbol E). In this case, we check if among
the successors there is the beginning symbol (i.e. symbol B, ﬁgure 6c).
If this condition is true, the minimized DFA not only recognizes the
full sequence, but also respects the long term dependencies of the
original grammar. If among the transitions of the neighbors, none of
them corresponds to the desired next symbol, the sequence is rejected.
For each k value of the clustering, we measure the average percentage of
accepted sequences for 10 simulations on diﬀerent grammatical sequences.
We emphasize here the diﬀerence between our validation approach and
those widely used in the literature. As our learning corpus only
involves positive examples, we cannot reproduce the validation process
used in Wang et al. (2018), which consists in testing positive and
negative examples on DFA. Here, we verify that the rules structuring the
Reber grammar are indeed those encoded by the network, i.e. that the
succession of symbols in a precise order as well as the sequential
dependencies are preserved. This implies that the local context of a
prediction is well learned and that the global representation of the
network behavior over time is adequate with the original grammar.
Adopting such an approach thus allows to validate the implicit encoding
power of LSTMs.

3 Results

All the simulations were run on a Macbook Pro using the Python scientiﬁc
stack, namely Numpy (van der Walt et al., 2011), Scipy (Jones et al.,
2001), Matplotlib (Hunter, 2007) and scikit-learn (Pedregosa et al.,
2011).

3.1 Artiﬁcial data set

Three diﬀerent LSTM-RNNs have been trained on data sets generated from
each of the three grammars. The training corpuses are described in Table
2 and A describes the sequence generation algorithm used to build the
data sets, for RG, ERG and CERG for reproducibility purpose. We have
ﬁrst checked that we could replicate the level of performance reported
in Gers

14

Figure 5. Schematic representation of the testing process of the
original sequence BPVVE from the Reber Grammar on the extracted and
minimized DFA. In black the selected path on the minimized DFA
corresponding to the sequence, in gray the ignored ones.

RG

ERG

200 000 1 397 109 7.98 3.35

Learning Corpus Size Number of samples Average length Standard deviation
Corpus of valid test sequences Size Average number of samples Average
length Standard deviation Corpus of non-valid test sequences 130 000
Size 140 538 Average number of samples 8.00 Average length 6.51 Standard
deviation

20 000 140 193 8.02 3.47

200 000 2 198 671 11.99 3.37

20 000 219 411 11.97 3.35

130 000 219 533 7.01 5.50

CERG

Learning Corpus Number of streams 30 000 Number of symbol per stream 100
000 Test corpus (after the learning of each training stream) Number of
streams Number of symbol per stream 100 000

10

Table 2. Characteristics of the datasets. On the left, for RG and ERG,
numbers concerning the test set are averages calculated on 10 corpuses.
On the right, for CERG, size of streams used for training and test
phase.

et al. (2000) for the RG and ERG grammars, with a rate of acceptance of
valid sequences and re- jection of non-valid sequences over 99% (test
corpuses also described in Table 2), thus conﬁrming that our LSTM-RNN
model successfully learned the grammars. This table also indicates that
10 simulations have been carried out for the test phase and for each
simulation, the hidden pat- terns for 20 000 valid sequences have been
recorded, to be used later on for automata extraction, for the RG and
ERG grammars (see below). Concerning CERG grammar, we couldn’t follow
the same procedure for the testing phase with percentage of sequences
accepted and rejected, because this grammar proposes a unique continuous
ﬂow (and no separated sequences). We consequently replicate the
criterion used in Gers et al. (2000), requiring in the testing phase 106
consecutive symbols without errors to validate the learning. To carry
out this test, we alternate learning and prediction: We make the network
learn a ﬂow of 100,000 successive symbols at the end of which we freeze
the weights. These weights are used for a test phase involving 10
streams of 100,000 symbols. The average number of prediction errors was
measured at each test phase. We succeeded in testing 30,000 training
ﬂows without error, thus reaching the Gers criterion. In the following
sections, we present FSA extraction results only on RG and ERG.

15

3.1.1 FSA Extraction : visual results for interpretability

Before evaluating more quantitatively the extracted FSA, we ﬁrst propose
here a more qualitative evaluation, showing their illustrative
capabilities. All ﬁgures in this section present FSA before
minimization, extracted from a small number of hidden patterns. In
ﬁgures 6a and 6b, we represent a simpliﬁed version of the graph, without
the loops labelled with the symbol B from the ﬁnal node to the starting
node.

In the RG context, by following the methodology described in section
2.2.4, we obtain, for k=10 clusters, the FSA without label represented
in ﬁgure 6a, the FSA with long labels represented in ﬁgure 6c and the
ﬁnal automaton graph in ﬁgure 6b. These results were extracted from the
analysis of the ﬁrst 33 time steps (i.e. the ﬁrst 33 hidden patterns
recorded) during the test phase. To help analyze results, note that it
was observed a posteriori that the data set was composed of several
occurrences of the following sequences : BPVVE, BTXSE, BPTVVE,
BTXXTTTTVVE.

(a) 
(b) 
(c) 

Figure 6. Extraction in RG context on 33 hidden patterns : An unlabeled
FSA (a), a ﬁnal FSA (b) a long-label FSA (c) obtained with a k-means
algorithm for clustering, where k=10. Final nodes, that indicate the end
of sequences (i.e. that the following symbol is E), are noted with red
double circles.

In the ERG context, an extraction process was realized on the ﬁrst 30
hidden patterns as presented in Figures 7. It was also observed during
the a posteriori analysis process that those patterns were related to 4
sequences: BPBTSXSEPE, BPBPTTVVEPE, BTBPVVETE, BPBPTVVEPE. We obtain,
for k=10 clusters, one FSA with 3 diﬀerent notation systems: the FSA
without label, represented in ﬁgure 7a, the FSA with long labels
represented in ﬁgure 7c and the ﬁnal FSA in ﬁgure 7b.

In the case of testing the model on a small volume of data, the
extracted FSA will not

16

(a)

(b) 
(c) 

Figure 7. Extraction in ERG context on 30 hidden patterns : An unlabeled
FSA (a), a ﬁnal FSA (b) a long-label FSA (c) obtained with a k-means
algorithm for clustering, where k=10. Final nodes are noted with red
double circle.

represent all the implicit and encoded representation of all the learned
data, but just the part of the representation that corresponds to those
inputs. This is why in ﬁgures 6, 7, 8, 9 and 12 some of the loops and
the transitions that are originally present on the RG and ERG may be
absent.

The main result to underline in this section is the visual
interpretability that the labels on transition provide. In ﬁgures 8 and
9, we present a comparison between the extracted FSA which represents
the implicit representation as encoded by the network and the portion of
the original grammar that generated the sequences that help build the
representation. On top of that, both ﬁgures 6c and 7c explicit the
temporal behavior of the model. In other words, these knowledge
representations allow to explain the reasons of the behavior of the
network at a speciﬁc time step. It is thus valuable for local
interpretability in RNNs.

Figure 8. Comparison of a portion of RG (a) and an extracted FSA for k=9
(b) for the 15 ﬁrst time steps related to occurrences of 2 sequences:
BPVVE and BTXSE. Final nodes are noted with red double circle on the
extracted FSA.

17

(a)

Figure 9. Comparison of a portion of ERG (a) and an extracted FSA for
k=10 (b) for the 30 ﬁrst time steps related to occurrences of 4
sequences : BPBTSXSEPE, BPBPTTVVEPE, BTBPVVETE, BPBPTVVEPE. Final nodes
are noted with red double circle on the extracted FSA.

(b) 

3.1.2 FSA Extraction : evaluation of the extracted FSA using valid
sequences

To evaluate the quality of the results, we analyze: 1) the evolution of
the average silhouette coeﬃcient for each k value (only the results of
analyses of the ﬁrst 5000 patterns are shown, unless stated otherwise)
and 2) the percentage of valid sequences recognized by each minimized
DFA for each k value.

Figures 10 and 11 represent, for each value of k, the evolution of the
average silhouette co- eﬃcient and the percentage of recognized valid
sequences in RG and ERG contexts respectively. The maximal value for the
average silhouette coeﬃcient is for k close to 400 in both contexts.
Nevertheless, the minimized DFA recognizes more than 70% of the valid
sequences from k > 50. In other words, when it comes to select the best
k value as indicated by the maximal value of the average silhouette
coeﬃcient according to ﬁgures 10 and 11, k should be close to 400 which
will require a lot of computation. But it seems also possible to choose
k=50 to observe good results with much less computation.

A related important information discussed below is that in the former
case, the computing time counts in days not to say weeks, whereas in the
latter case, some minutes are suﬃcient. If we compare the evolution of
the percentage of accepted sequences to the silhouette coeﬃcient
analysis of k in both grammars, it appears that the higher the
silhouette coeﬃcient is, the more it is possible to get an extracted
minimized DFA that can recognize the original sequences and accordingly
the original rules (and thus the original grammar) hidden in those
sequences.

We repeated the extraction process on 5000 diﬀerent patterns 10 times,
and we obtained

the following results (average values) :

For RG context :

18

Figure 10. Analysis of extraction process of 5000 patterns for k in
[6,500] : Evolution of the average silhouette coeﬃcient in RG context in
red and ERG context in blue .

Figure 11. Analysis of extraction process of 5000 patterns for k in
[6,500] : Evolution of the percent of accepted sequences in RG context
in red and ERG context in blue.

19

(a)

Figure 12. Comparison of extracted FSAs for k=6 for RG (a) and for k=14
for ERG (b). The presented FSAs are not valid for both context, but show
here that the selection of the adequate parameter k is independent of
the number of nodes present in the original grammars. Choosing k equal
to that number does not guarantee that the FSA will be valid.

(b) 

20

• for k > 50, average value of silhouette coeﬃcient is > 0.80 and the
percentage of accepted

sequences is between [70; 100]

• for k > 100, average value of silhouette coeﬃcient is > 0.875 and the
percentage of accepted

sequences is between [85; 100]

For ERG context :

• for k > 50, average value of silhouette coeﬃcient is > 0.750 and the
percentage of accepted

sequences is between [85;100 ]

• for k > 100, average value of silhouette coeﬃcient is > 0.825 and the
percentage of accepted

sequences is between [87;100 ]

These results show that it is possible, using the hidden representation
of the hidden layer of an RNN-LSTM, to extract a knowledge
representation of the hidden rules that ﬁts the original grammar by 80%
in the worst case.

On top of that, the analysis of our results shows that our algorithm
converges. In the cases of RG and ERG, an increase in k makes it
possible to extract automata that recognize a larger number of
sequences. Let us underline the importance of this result because it
implies that the choice of k is not a limitation of our algorithm, but a
compromise to be achieved according to the precision sought after.
Indeed, to have more precise results, it is suﬃcient to increase k (the
number of clusters), at the cost of a greater need for computing
resources. We also investigated the rather intuitive idea that the k
value might equal the number of node in the original grammar for RG and
ERG, but both ﬁgures 10 and 11 reject this simple idea, since the
average silhouette coeﬃcient and the percentage of recognized sequences
are at their minimal values in both cases. Figure 12 presents non valid
FSA for k=6 for RG and k=14 for ERG.

The results contribute to interpret the k value in the k-means
clustering as an adjusting parameter : when k increases, it induces more
precise and extended knowledge representation at the cost of lower
generalization in the representation. In other words, what is earned in
accuracy is lost in generality.

k is therefore a cursor to be adjusted according to the level of
interpretability we want for a situation. The methodology we propose
here provides ﬂexible results depending on the value of k and the
available computing power.

3.2 Real data set (electrical diagrams)

To test the applicability of our approach on real data, we applied our
approach on se- quences generated from electrical diagrams. Diagrams
considered here are large and complex plans of the wiring of industrial
installations, that can be represented on documents of hundreds of pages
long. Previous works shed light on the existence of hidden knowledge
inside sequences of electrical components (Chraibi Kaadoud et al.,
2017). For this initial exploration, with the help of experts from the
electrical domain, we selected 3 diagrams of similar installations
including 25 diﬀerent electrical components. Following the wiring, we
could extract several sequences of such components and, thanks to the
equivalence between some components mentioned by the experts, we could
automatically generate 200 000 sequences for a learning phase and 20 000
sequences for a test phase.

Following the same procedure as described above, we could train an
RNN-LSTM network and could verify an accuracy over 99% in the test
phase, thus conﬁrming the good performances of LSTM in sequence learning
and prediction. More interestingly, we generated the correspond- ing FSA
and proposed its subjective analysis by the experts. For illustration,
the extracted FSA is presented in ﬁgure 13, with 25 symbols (electrical
components) and 40 states. Although the

21

Figure 13. Result of the interpretability approach on electrical
sequences from real word electrical diagrams : A long-label FSA obtained
with a k-means algorithm for clustering, where k=41 and from an RNN-LSTM
having learned sequences of electrical components. Extraction was
performed on the ﬁrst 93 time steps.

22

evaluation is subjective, it can be reported that the FSA is meaningful
for the experts. They found in the automaton a large part of their
business knowledge at the level of electrical connec- tivity rules. An
electrical diagram of several hundred pages could therefore be
summarized in a single graph on a single page. On our side, these
preliminary results showed that our approach transposed on a real
context where the grammar is more complex is completely functional and
that it especially allows to explain predictions related to an
electrical component as a function of the components used before.
Nevertheless, it is necessary to put these preliminary experiments into
perspective because in practice, the choice of an electrical component
is not only due to several characteristics such as its model or its
amperage, but also to the habits of the expert and other explicit
constraints. We have not had the opportunity to explore whether our
approach allows to take into account dependencies of this type, but we
believe that this is an interesting line of research to pursue.

4 Discussion

In this paper, we tackle the problem of knowledge extraction acquired by
implicit learning of sequences when the grammar is unknown or hidden and
only positive examples are available for learning. The main contribution
of our work is to propose an end-to-end methodology for knowledge
extraction from LSTM. We also underline that this approach could be
directly applied to peephole and GRU networks, since it is also possible
with these networks to easily get the output activity of the hidden
layers, thus extending the interest of our work. At the experimental
level, we were able to demonstrate that it is possible to learn complex
grammars with LSTM models and to extract knowledge, in the form of a
graph that represents rules, acquired by learning from such models, for
their interpretability.

Since our work is a multidisciplinary one at the crossroad of several
domains, we will implicit learning of temporal behaviour, neural

discuss our contributions for each domain : network interpretability and
knowledge extraction.

First, let us discuss the capabilities of implicit learning of temporal
behaviour with LSTM: we could verify that, thanks to their unique
characteristics, such as short-term memory (activity patterns at each
time step), long-term memory (weights), and intermediate memory (CEC of
LSTM units), RNN-LSTM networks are able to learn and extract a stable
representation of the encoded rules hidden in sequences, which can be of
variable length and include long temporal dependencies between
non-binary symbols in three diﬀerent contexts where the ambiguity of the
sequences is increasing.

Then, at the interpretability level, it is important to highlight that
no precise method- ology to extract acquired rules in LSTM models was
available despite the fact that these models have demonstrated their
learning capabilities. Our work thus allows to ﬁll this gap. To do so,
we assumed that by implicit learning, 1) important knowledge about
grammatical rules corre- sponding to the valid transitions and their
context in sequences is encoded in the hidden layer of the RNN-LSTM and
2) that it can be extracted and explicitly represented as an automaton.
We adapted for the LSTM models the classical three step process already
described for simple and more advanced RNNs, by proposing several
extensions and improvements, including the use of non-binary grammars
and of continuous ﬂows in addition to limited sequences. By only
processing the output activity patterns of the hidden layer, we were
able to extract a repre- sentation in the form of graphs, with diﬀerent
notation systems (cf ﬁgures 6, 7), each carrying information on the
internal functioning of the RNN-LSTM. The representation without nota-
tion informs about the arrangement of states and transitions between
them. The notation with long labels informs about the temporal
arrangement of patterns between the diﬀerent states, and oﬀers a
contextual explanation regarding the management of patterns by the
RNN-LSTM.

23

Finally, the representation with simpliﬁed notation provides a
synthetic and explicit represen- tation of the grammatical rules
learned, governing the predictions of the RNN-LSTM, to be directly
compared with the original grammars. Figure 8 shows a comparison between
the Reber grammar and the extracted automaton with simpliﬁed notation,
and ﬁgure 9 the same in an ERG context. The ﬁnal automaton with
simpliﬁed labels, once minimized, has been fed with valid sequences for
each considered grammar. Over 10 consecutive simulations, the percentage
of recognition of valid sequences is above 80% for k > 50 for RG and
ERG. In addition, thanks to the use of the silhouette coeﬃcient to
monitor the k value, we have shown that this percent- age is not a limit
in itself of our algorithm, but a compromise to be made between the
degree of precision desired during the extraction process and the
computing power allocated. The k value in the k-means algorithm is thus
an important parameter for the interpretability : rather than accuracy
of the extracted automaton, the important point is to determine ”what is
a good level of representation”, which is a context and a human
dependent question. Since the use of non-binary grammars was mainly
motivated by the exploration of more realistic problems, we have also
shown through preliminary results, that our approach could be transposed
to more concrete ﬁelds, and particularly here the analysis of electrical
diagrams, where the grammar is probably more complex and not explicitly
known by the experts.

Next, in the knowledge extraction ﬁeld, this work oﬀers a methodology
for implicit rule extraction in the shape of automaton when only
positive examples are provided. Indeed, this methodology was applied to
an industrial context where sequences of electrical components on
electrical diagrams that represent industrial installations were
available. The ﬁrst results of our methodology to extract knowledge was
submitted to experts and showed that the extracted rules hidden in those
sequences did indeed contain not only electrical connectivity rules but
also habits of the engineers that draw electrical diagrams (Chraibi
Kaadoud et al., 2017; Chraibi Kaadoud, 2018). This makes us propose
that, more generally, this process of knowledge extraction could be 1)
interesting to study expertise in many ﬁelds, which represents a very
precious knowledge in many companies but is most of the time implicit
and consequently very diﬃcult to transfer to other people, and 2)
applied to diﬀerent domains where sequences are available and the
grammar unknown or not completely understood. This latter point is
especially important since, we found few works like ours or the ones
reported in Ti˜no and Sajda (1995) and Schellhammer et al. (1998),
proposing a process for extracting rules from an RNN fed only with
positive sequences composed of non-binary elements. Although this is
probably more typical of real-life phenomena, this topic is rarely
studied in the recent literature.

To conclude and more globally, all this work addresses the problem of
interpretability in ANNs seen as black boxes and especially in RNNs,
which are particularly diﬃcult to understand as they process sequences
and not just patterns. We have just evoked that our techniques can
provide valuable information at diﬀerent levels of details, for local as
well as global interpretabil- ity. It is important to stress the fact
that the present work allows to extract a representation of the rules
expressed by the activity of the network, i.e. the portion of the set of
acquired implicit rules used to make a prediction. In other words the
presented work can be used to zoom in on the network behavior for a
given sub-corpus of data and thus compare the representations of several
sub-corpuses. Again, the goal here is not to evaluate the behavior of
the network, but to extract the implicit rules present in the sequences
analysed by the network after the learning phase in the shape of a graph
as illustrated in all ﬁgures presented in this work. An important
perspective is to work on information about the hidden space richer than
the activity of the output of the hidden layer. In particular, to
explain how LSTMs support sequential dependen- cies, it would be
interesting to explore the hidden representation at the level of
activity patterns of LSTM gates, cells, as well as CECs. Note that this
question also arises if we apply our approach to variations of standard
LSTMs, such as Gated Recurrent Units (GRU) or LSTMs

24

with peepholes connections. A network with additional hidden layers
could also be considered to study the abstraction of the implicit
representation encoded by the network from one layer to another. All
these characteristics and perspectives may thus participate to improve
trust in machine learning algorithms by making them more accessible to
as many people as possible.

5 Acknowledgement

This work was initiated as part of a thesis in collaboration with Inria
and the Algo’Tech company (Bidart, France), specialized in computer
design and drawing software in the ﬁeld of electrical diagrams. We would
like to thank them, especially the company Algo’Tech who ﬁnanced the
thesis, and all the actors who made this work possible.

References

Abu-Mostafa, Y. S. (1990). Learning from hints in neural networks.
Journal of Complexity, 6(2):192–198.

Ayache, S., Eyraud, R., and Goudian, N. (2018). Explaining black boxes
on sequential data using weighted au- tomata. In Unold, O., Dyrka, W.,
and Wieczorek, W., editors, Proceedings of the 14th International
Conference on Grammatical Inference, ICGI 2018, Wroc(cid:32)law, Poland,
September 5-7, 2018, volume 93 of Proceedings of Machine Learning
Research, pages 81–103. PMLR.

Bengio, Y., Courville, A. C., and Vincent, P. (2013). Representation
learning: A review and new perspectives.

IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798–1828.

Blanco, A., Delgado, M., and Pegalajar, M. (2000). Extracting rules from
a (fuzzy/crisp) recurrent neural network

using a self-organizing map. International Journal of Intelligent
Systems, 15(7):595–621.

Chraibi Kaadoud, I. (2018). Apprentissage de s´equences et extraction de
r`egles de r´eseaux r´ecurrents : application au tra¸cage de sch´emas
techniques. (Sequence learning and rules extraction from recurrent
neural networks : application to the drawing of technical diagrams).
Theses, Universit´e de Bordeaux.

Chraibi Kaadoud, I., Rougier, N., and Alexandre, F. (2017). Implicit
knowledge extraction and structuration from electrical diagrams. In
Benferhat, S., Tabia, K., and Ali, M., editors, Advances in Artiﬁcial
Intelligence: From Theory to Practice - 30th International Conference on
Industrial Engineering and Other Applications of Applied Intelligent
Systems, IEA/AIE 2017, Arras, France, June 27-30, 2017, Proceedings,
Part I, volume 10350 of Lecture Notes in Computer Science, pages
235–241. Springer.

Cleeremans, A. and McClelland, J. L. (1991). Learning the structure of
event sequences. Journal of Experimental

Psychology: General, 120(3):235.

Cleeremans, A., Servan-Schreiber, D., and McClelland, J. L. (1989).
Finite state automata and simple recurrent

networks. Neural Comput., 1(3):372–381.

Elman, J. L. (1990). Finding structure in time. Cogn. Sci.,
14(2):179–211.

Gers, F. A. and Schmidhuber, J. (2001). LSTM recurrent networks learn
simple context-free and context-sensitive

languages. IEEE Trans. Neural Networks, 12(6):1333–1340.

Gers, F. A., Schmidhuber, J., and Cummins, F. A. (2000). Learning to
forget: Continual prediction with LSTM.

Neural Comput., 12(10):2451–2471.

Giles, C., Chen, D., Miller, C., Chen, H., Sun, G., and Lee, Y. (1991).
Second-order recurrent neural networks for grammatical inference. In
IJCNN-91-Seattle International Joint Conference on Neural Networks,
volume ii, pages 273–281 vol.2.

Giles, C. L., Miller, C. B., Chen, D., Chen, H., Sun, G., and Lee, Y.
(1992). Learning and extracting ﬁnite state

automata with second-order recurrent neural networks. Neural Comput.,
4(3):393–405.

Goodfellow, I., Bengio, Y., and Courville, A. (2016). Convolutional
networks. In Deep Learning, chapter 9, pages

330–372. MIT Press.

25

Greﬀ, K., Srivastava, R. K., Koutn´ık, J., Steunebrink, B. R., and
Schmidhuber, J. (2017). LSTM: A search space

odyssey. IEEE Trans. Neural Networks Learn. Syst., 28(10):2222–2232.

Guidotti, R., Monreale, A., Turini, F., Pedreschi, D., and Giannotti, F.
(2018). A survey of methods for explaining

black box models. CoRR, abs/1802.01933.

Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory.
Neural Comput., 9(8):1735–1780.

Hunter, J. D. (2007). Matplotlib: A 2d graphics environment. Comput.
Sci. Eng., 9(3):90–95.

Jacobsson, H. (2005). Rule extraction from recurrent neural networks: A
taxonomy and review. Neural Comput.,

17(6):1223–1263.

Jaeger, H. (2002). Tutorial on training recurrent neural networks,
covering BPPT, RTRL, EKF and the” echo

state network” approach, volume 5. GMD-Forschungszentrum
Informationstechnik.

Jones, E., Oliphant, T., and Peterson, P. (2001). SciPy: Open source
scientiﬁc tools for Python.

Keller, R. M. (2001). Classiﬁers, acceptors, transducers, and
sequencers. Computer Science: Abstraction to

Implementation. Harvey Mudd College, page 480.

Lapalme, J. (2006). Composition automatique de musique `a l’aide de
r´eseaux de neurones r´ecurrents et de la

structure m´etrique. PhD thesis, Universit´e de Montr´eal.

Lipton, Z. (2016). The mythos of model interpretability.

In Proceedings of the ICML Workshop on Human

Interpretability in Machine Learning, pages 1–6.

Nerode, A. (1958). Linear automaton transformations. Proceedings of the
American Mathematical Society,

9(4):541–544.

Omlin, C. W. and Giles, C. L. (1996). Extraction of rules from
discrete-time recurrent neural networks. Neural

Networks, 9(1):41–52.

Oudeyer, P. (2006). Self-organization in the evolution of speech.

Pascual-Leone, A., Grafman, J., and Hallett, M. (1995). Procedural
learning and prefrontal cortex. Annals of the

New York Academy of Sciences, 769(1):61–70.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B.,
Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V.,
Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and
Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.

Perruchet, P. and Pacton, S. (2006). Implicit learning and statistical
learning: one phenomenon, two approaches.

Trends in Cognitive Sciences, 10(5):233–238.

Pothos, E. M. (2007). Theories of artiﬁcial grammar learning.
Psychological Bulletin, 133(2):227–244.

Reber, A. S. (1967). 6(6):855–863.

Implicit learning of artiﬁcial grammars. Journal of verbal learning and
verbal behavior,

Reber, P. J. (2013). The neural basis of implicit learning and memory: A
review of neuropsychological and

neuroimaging research. Neuropsychologia, 51(10):2026–2042.

Remm, J. and Alexandre, F. (2002). Knowledge extraction using artiﬁcial
neural networks: application to radar

target identiﬁcation. Signal Process., 82(1):117–120.

Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the
interpretation and validation of cluster analysis.

Journal of computational and applied mathematics, 20:53–65.

Rumelhart, D. and MacClelland, J., editors (1986). Parallel distributed
processing. MIT Press, Cambridge.

Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1985). Learning
internal representations by error propa-

gation. Technical report, California Univ San Diego La Jolla Inst for
Cognitive Science.

26

Schellhammer, I., Diederich, J., Towsey, M., and Brugman, C. (1998).
Knowledge extraction and recurrent neural networks: An analysis of an
elman network trained on a natural language learning task. In
Proceedings of the joint conferences on new methods in language
processing and computational natural language learning, pages 73–78.
Association for Computational Linguistics.

Servan-Schreiber, D., Cleeremans, A., and McClelland, J. L. (1989).
Learning sequential structure in simple

recurrent networks. In Advances in neural information processing
systems, pages 643–652.

Servan-Schreiber, D., Cleeremans, A., and McClelland, J. L. (1991).
Graded state machines: The representation

of temporal contingencies in simple recurrent networks. Mach. Learn.,
7:161–193.

Servan-Schreiber, D., Cleermans, A., and McClelland, J. (1988). Encoding
sequential structure in simple re- current networks. School of Computer
Science, Carnegie-Mellon University, Pittsburgh, PA, CMU-CS-88-
183.(November, 1988).

Setiono, R. and Liu, H. (1996). Symbolic representation of neural
networks. Computer, 29(3):71–77.

Ti˜no, P. and Sajda, J. (1995). Learning and extracting initial mealy
automata with a modular neural network

model. Neural Comput., 7(4):822–844.

Tomita, M. (1982). Dynamic construction of ﬁnite-state automata from
examples using hill-climbing. In Proceed-

ings of the Fourth Annual Conference of the Cognitive Science Society,
pages 105–108.

van der Walt, S., Colbert, S. C., and Varoquaux, G. (2011). The NumPy
array: A structure for eﬃcient numerical

computation. Computing in Science & Engineering, 13(2):22–30.

Wang, Q., Zhang, K., Ororbia, I., Alexander, G., Xing, X., Liu, X., and
Giles, C. L. (2018). A comparison of rule extraction for diﬀerent
recurrent neural network models and grammatical complexity. arXiv
preprint arXiv:1801.05420.

Watrous, R. L. and Kuhn, G. M. (1992). Induction of ﬁnite-state automata
using second-order recurrent networks.

In Advances in neural information processing systems, pages 309–317.

Weiss, G., Goldberg, Y., and Yahav, E. (2017). Extracting automata from
recurrent neural networks using queries

and counterexamples. CoRR, abs/1711.09576.

Werbos, P. J. (1990). Backpropagation through time: what it does and how
to do it. Proceedings of the IEEE,

78(10):1550–1560.

Williams, R. J. and Zipser, D. (1989). Experimental analysis of the
real-time recurrent learning algorithm.

Connection science, 1(1):87–111.

Zeng, Z., Goodman, R. M., and Smyth, P. (1993). Learning ﬁnite state
machines with self-clustering recurrent

networks. Neural Comput., 5(6):976–990.

A Sequence generation algorithm

We present in this section the sequence generation algorithm used to
build the datasets.

Reber Grammar (RG) The sequences are generated as follows: each sequence
(empty at the beginning) is initialized with a B, and node #0 is
designated as the initial node. The directed arcs (arcs with a
direction) having a probability of 0.5 to be chosen, the next node is
selected randomly among the successors of the current node. Once the
node is selected, the label of the chosen arc is added to the sequence.
At this point, the result is either BT or BP. Once the next node is
selected, the random selection process is repeated among the successors
of the current node, and so on until node #5 is reached. When this
happens, the sequence is considered ﬁnished since node #5 has no arcs to
other nodes. The symbol E is then added to the sequence, thus concluding
it. Figure 14 reminds the RG structure and Table 3 provides an example
of sequences generated from that grammar.

27

Embedded Reber Grammar (ERG) The algorithm used for ERG sequence
generation is the same as for RG. However it is important to highlight
that if the sequence starts with ”BT” or ”BP” then it automatically ends
with ”TE” and ”PE” respectively. Figure 15.B reminds the ERG structure
and Table 16 provides an example of sequences generated from that
grammar. Continuous and embedded Reber Grammar (CERG) CERG allows to
generate a continuous ﬂow of symbols. The sequences are generated as in
RG and ERG at the beginning, i.e. each sequence (empty at the beginning)
is initialized with a B, and node #0 is designated as the initial node.
However, in this case, the algorithm does not stop at node #5. It
continues until the sequence reaches a size of 100 000 symbols. The
dataset for CERG is thus composed of sequences of 100 000 symbols.
Figure 15.C reminds the CERG structure and Table 4 provides an example
of sequences generated from that grammar.

Sequences BTXSE BPVVE BTSXXVPSE BTSSSSSSSSSSSXSE

Figure 14. RG

Table 3. Examples of RG sequences

Sequences BTBTXSETE BPBTXSEPE BTBPVVETE BPBPVVEPE

Figure 15. ERG (B) and CERG (C)

Figure 16. Examples of ERG sequences

Sequences BPBTXSEPEBTBTXSETE BTBPVVETE BPBPVVEPE…
BPBTSSSSSSSSXSEPEBTBPVVETEBTBPTTTTTTVPSETE…

Table 4. Examples of portions of CERG sequences

B RNN-LSTM network and learning algorithms

In our work, we implemented a network of three layers and initialized it
as described in Gers et al. (2000). In this appendix, we provide ﬁrst,
the technical details related to the network architecture and
parameters, and second the algorithms used.

B.1 RNN-LSTM architecture and parameters

The RNN-LSTM consists of 3 layers: an input layer, a hidden layer and an
output layer. The input and output layers are both composed of 7 units,
due to the fact that the grammars

28

used have 7 symbols. The hidden layer consists of 4 LSTM blocks each
with 2 LSTM cells and 3 gates, i.e. 8 cells and 12 gates in total.
Figure 3 presents the RNN-LSTM architecture. All the units in the input
layer are connected to all the units in the output layer, to each of the
8 cells, and to each of the gates in the 4 blocks. All the cells have
connections to each of the output units, but also to the input of all
the cells (a cell thus has a recurrent connection to itself) and ﬁnally,
to each of the gates of each block. For learning, we use a combination
of BackPropagation Through Time (Werbos, 1990) and Real Time Recurrent
Learning (Williams and Zipser, 1989), that will be deﬁned and explained
in the following section. Our network has 424 weights in total. The bias
weights of the input and output gates are initialized to -0.5 for the
ﬁrst block, then decremented by -0.5 for the next ones (-1, -1.5, -2).
For the forget gates in the 4 blocks, we implemented the reverse : for
the ﬁrst block, the bias of the gate is initialized to 0.5, then is
incremented by 0.5 for the next gates in the other blocks (1, 1.5, 2).
All other weights, as well as the output layer bias, are initialized
randomly between [-0.2, 0.2]. For this implementation, the learning rate
is initialized to 0.5 and then multiplied by 0.99 every 100 time steps.
During learning phase and prediction, at the beginning of each sequence,
the previous states st−1, as well as the input at the previous time
steps are reset to zero. The context is thus reinitialized at the
beginning of each sequence.

B.2 RNN-LSTM learning algorithms

As originally proposed by Gers et al. (2000), BackPropagation Through
Time (BPTT) (Werbos, 1990) and Real Time Recurrent Learning
(RTRL)(Williams and Zipser, 1989) are two algorithms used for the
learning phase.

The central idea of BPTT is the unfolding of the recurrent neural
network in time, so that at each instant it can be assimilated to a
feed-forward neural network as the perceptron (Rumelhart et al., 1985).
Conceptually, at each time step, the network is copied into a new
instance, so there are as many instances of the network as time steps
processed in a sequence. For each instance, the network receives an
input and the internal state of the previous time step, then provides an
output. The errors are computed and the weights are updated using
standard backpropagation, with one restriction however: the equivalent
weights in all instances must be the same. Spatially, each time step of
the unrolled RNN can be seen as an additional layer since the network
depends on the order of the presented data and the internal state of the
previous time step is taken as input at each time step.

RTRL is a learning algorithm that calculates the exact error gradient at
each time step. In other words, this algorithm computes the derivatives
of states and outputs with respect to all weights during forward
propagation at each time step. It is therefore suitable for online
learning tasks. Mathematically simpler than BPTT, it is nevertheless
very slow: the more units the network has, the more weights it has, and
the longer and more complex the computations are. This drawback makes
that the algorithm can only be used on very small networks. A detailed
version of RTRL is provided by Giles et al. (1991). For a detailed
description of the diﬀerent learning algorithms of RNN, the reader can
refer to Jaeger (2002).

In our work we used BPTT and RTRL, and their truncated versions. A
truncation implies that the outgoing errors from a cell or gate will be
cut oﬀ, even though they serve to modify the incoming weights. Thus, the
CEC of a cell becomes the only part of the system through which the
error can be backpropagated and maintained forever. This makes LSTM
updates eﬃcient without signiﬁcantly aﬀecting the learning power. To do
this: (i)the output units use BPTT , (ii)the output gates use a
truncated version of BPTT, (iii)the weights going to the cells, the
input gates and the forget gates of the blocks are updated via a
truncated version of RTRL

We include below the learning algorithm of the RNN-LSTM inspired from
Gers and

29

Schmidhuber (2001).

Notations For LSTM blocks and cells :

• j : block of a given hidden layer

• v : cell in a given block

• cv

j : cell v of block j

• inj : the input gate of the block j

• φj : the forget gate of block j

• outj : the output gate of block j

• scv

j

(t)) : state s of the cell v of block j

For the output layer of the network :

• k : an output layer neuron of the network

• ek : error calculated at unit k of the output layer of the network

• tk : expected output (value) at unit k

Other notations :

• m : neuron with a link to an LSTM unit

• netx : activation received by the computational unit x (be it a gate
or a cell or an artiﬁcial

neuron)

• ym : activity of the calculation unit m

• w(l,m) : weight from neuron m to the computational unit l

Algorithm 3 Learning algorithm of a RNN-LSTM Require: Lets consider the
following activation functions : f (x) = 1 1+e−x # sigmoid function
between [0,1] h(x) = 2 ∗ f (x) − 1 # sigmoid function between [-1,1]
g(x) = 4 ∗ f (x) − 2 # sigmoid function between [-2,2] and their
respective derivatives f (cid:48), h(cid:48) and g(cid:48) for all size
sequence n do for i = 0 `a (n − 1) do

Step 1 : forward propagation

network entry = sequence[i] expected output = sequence[i + 1] obtained
output = RNN LSTM.forward propagation(network entry) error RNN LSTM =
expected output - obtained output # Step 2 : back propagation RNN
LSTM.back propagation(error RNN LSTM)

end for

end for

30

Algorithm 4 Learning a RNN-LSTM: forward propagation

Function forward propagation (network entry) # Forward propagation from
the input layer to the hidden layer for all bloc j do for all cell cv

j do

#Calculation of the received activations

(1) 
(2) 
(3) 
(4) 
(5) 
(6) 
(7) 
(8) 
(9) 
(10) 
(11) 
(12) 

netl(t) =

(cid:88)

(w(l,m) · ym(t)) +

m

(w(l,p) · yp(t − 1))

(cid:88)

p

with l ∈ {cv # Calculation of activations according to the activation
functions

j , inj, φj, outj}

yinj (t) = finj (netinj (t)) yφj (t) = fφj (netφj (t)) youtj (t) = foutj
(netoutj (t)) (t − 1) · yφj (t)

(t)) · yinj (t) + scv

j

scv

j

(t) = g(netcv

j

Calculation of partial derivatives dSjv for cell c, input gate in and forget gate φ

ycv

j (t) = h(scv

j

(t)) · youtj (t)

dSjv

in,m(t) = dSjv

dSjv

c,m(t) = dSjv

cm(t − 1) ∗ yφj (t) + g(cid:48)(netcv (t)) ∗ f (cid:48)

j

in,m(t − 1) ∗ yφj (t) + g(netcv φ,m(t − 1) ∗ yφj (t) + h(scv

j

j

inj (t)) ∗ f (cid:48) φj

)(t) ∗ yinj (t) ∗ ym(t − 1) (netinj (t)) ∗ ym(t − 1) (netφj (t)) ∗ ym(t
− 1)

dSjv

φ,m(t) = dSjv

end for

end for # forward propagation from the hidden layer to the output layer
error RNN LSTM = vector of size k for all unit k of the output layer do

Calculation of the network output

netk(t) =

(cid:88)

m

(w(k,m) · ym(t)) +

(cid:88)

(w(k,p) · yp(t − 1))

p

yk(t) = fk(netk(t)) error RN N LST M [k] = yk(t)

end for return error RNN LSTM End function

31

Algorithm 5 Learning algorithm of a RNN-LSTM: back propagation

Function back propagation (error RNN LSTM) : # Back propagation for all
unit k of the output layer do

ek(t) = error RN N LST M [k] k(netk(t)) ∗ (ek(t))

δk(t) = f (cid:48)

(13) 
(14) 

end for for all bloc j do for all cell cv

j do

Between the k units of the output layer and the output gates of the j blocks

δoutj (t) = f (cid:48)

outj (netoutj (t)) ∗ (

Sj (cid:88)

v=1

h(scv

j

(t)) ∗ (

(cid:88)

m

wkcv

j

∗ δk(t)))

(15) 

Between the units k of the output layer and the gates inputs, the gates forgets, as

well as the cells of the blocks j

(16) 
(17) 
(18) 
(19) 
(20) 
(21) 

ecv

j

(t) = youtj ∗ h(cid:48)(scv

j

(t)) ∗ (

(cid:88)

m

wkcv

j

∗ δk(t)))

end for

end for # Updating the weights of a RNN-LSTM network

∆lm(t) = α ∗ (δl(t)) ∗ ym(t), l ∈ {k, i, out} (t) ∗ dSjvc,m(t)

∆cv

j ,m(t) = α ∗ ecv Sj (cid:88)

j

∆in,m(t) = α ∗ (

ecv

j

(t) ∗ dSjvin,m(t))

v=1 Sj (cid:88)

∆φ,m(t) = α ∗ (

v=1

ecv

j

(t) ∗ dSjvφ,m(t))

Wzm(t) = Wzm(t) + ∆zm(t), z ∈ {k, i, out, cv

j , in, φ}

Fin Fonction

32


