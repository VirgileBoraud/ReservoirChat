L’approche systémique : simuler moins pour modéliser plus en
neurosciences Frédéric Alexandre

To cite this version:

Frédéric Alexandre. L’approche systémique : simuler moins pour modéliser
plus en neurosciences. Actes du colloque ”Modélisation: succès et
limites”, CNRS et Académie des Technologies, CNRS - Académie des
Technologies, pp.12, 2018. ￿hal-01933185￿

HAL Id: hal-01933185

https://inria.hal.science/hal-01933185

Submitted on 23 Nov 2018

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

L’approche systémique : simuler moins pour modéliser plus en
neurosciences

Frédéric ALEXANDRE

Inria Bordeaux Sud-Ouest LaBRI, UMR 5800 Institut des Maladies
Neurodégénératives F-33076 Bordeaux

Frederic.Alexandre@inria.fr

RÉSUMÉ. Le recours à la modélisation et à la simulation permet
aujourd’hui des performances considérables pour les prévisions
météorologiques ou pour la conception d’objets technolo- giques très
complexes. Il est tentant de poursuivre ces efforts et de les orienter
vers d’autres sujets particulièrement complexes comme l’étude du
cerveau. Il est cependant très important de bien analyser les principes
de la démarche de modélisation et de simulation pour les appliquer au
mieux dans un cadre systémique, le plus adapté pour étudier le cerveau,
et de se rendre compte ainsi qu’il ne s’agit pas de construire les
modèles les plus précis et les plus lourds mais les plus adaptés à la
question que l’on se pose.

ABSTRACT. The use of modeling and simulation now allows considerable
performance for mete- orological forecasting or for the design of very
complex technological objects. It is tempting to pursue these efforts
and direct them to other particularly complex subjects such as the study
of the brain. However, it is very important to analyze the principles of
the modeling and simulation approach in order to best apply them in a
systemic framework, the most adapted to study the brain, and to realize
that it is not a question to build the most precise and heaviest models
but the most adapted to the question that one asks.

MOTS-CLÉS : modélisation, simulation, approche systémique,
neurosciences.

KEYWORDS: modeling, simulation, systemic approach, neurosciences.

2

F. Alexandre

1.  Introduction

Avec le développement de l’informatique, son ancrage solide dans les
mathéma- tiques et les progrès technologiques associés, le recours à la
modélisation est devenu une pratique très courante pour rendre compte de
phénomènes de plus en plus com- plexes, et ceci d’autant plus que les
moyens de calcul actuels permettent d’exploi- ter plus facilement ces
modèles. En plus de réalisations impressionnantes dans de nombreux
domaines de la physique, l’approche modélisatrice s’attaque également à
différents domaines du vivant, avec des succès également notables et une
notoriété croissante. C’est le cas en particulier pour les neurosciences
computationnelles qui ont acquis une popularité importante aujourd’hui
pour l’étude du cerveau.

En première analyse, le domaine des neurosciences computationnelles
pourrait évoquer un oxymore. D’une part, les neurosciences peuvent être
perçues comme des sciences éminemment descriptives qui, par observation,
expérimentation et recueil de données, visent à décrire le cerveau (ou
plus généralement le système nerveux) dans sa réalité. D’autre part, le
computationnel évoque plutôt des sciences normatives qui, à l’aide de
modèles et de simulations informatiques, se proposent de représenter un
objet d’étude (ici le cerveau) à travers un certain prisme, en suivant
des formalismes mathématiques ou informatiques, ce qui semble l’éloigner
de sa réalité vivante (hu- mide, diraient les biologistes).

Il est donc légitime de se demander comment des approches issues de
sciences dites exactes peuvent être utilisées dans les sciences du
vivant, domaines plutôt tradi- tionnellement associés à l’exploitation
des données. Nous nous proposons de creuser un peu plus ici les rapports
entre ces deux formes d’étude du cerveau, de montrer qu’elles sont au
contraire très compatibles et d’observer que cette discussion fait éga-
lement émerger un certain nombre de recommandations pour leur meilleure
associa- tion. Cette analyse sera également l’occasion de rappeler les
règles et les fondements de la modélisation, ce qui permettra aussi
d’aborder un autre sujet important dans ce contexte, le développement de
modèles de plus en plus complexes et le recours à des moyens de calcul
énormes pour les simuler, avec en ﬁligrane la question de savoir si
cette escalade dans la complexité et la puissance des calculs est
nécessaire ou si d’autres voies sont possibles, voire préférables.

2.  Les neurosciences : une approche descriptive ?

John von Neumann faisait déjà remarquer que la vérité est beaucoup trop
complexe pour permettre autre chose que des approximations [NEU 47].
Cette remarque s’ap- plique particulièrement bien au cerveau et les
biologistes ont toujours été confrontés à la difﬁculté de l’observer
sans biais. On peut bien sûr être admiratif devant l’explo- sion de la
puissance des moyens technologiques développés pour mieux observer le
cerveau, de Ramon y Cajal qui travaillait au début du vingtième siècle
avec une colo- ration de Golgi et un simple microscope et dessinait ses
observations à la plume et à l’encre, jusqu’aux techniques plus récentes
de microscopie biphotonique et d’expres-

Simuler moins pour modéliser plus

3

sion de protéines ﬂuorescentes (technique Brainbow) ou plus récemment
encore la technique Clarity [CHU 13] qui rend les tissus cérébraux
transparents pour permettre une encore plus grande précision dans les
détails. Mais dans tous les cas, des biais importants subsistent. Les
techniques de coloration sont sélectives, les observations concernent
des animaux sacriﬁés et les tissus sont transformés par la préparation.

Comme le suggère l’évocation de ces limites technologiques, le cerveau
est un objet d’étude particulièrement délicat à observer pour de
multiples raisons. D’une part, par sa structure, il est fragile,
difﬁcile d’accès, multi-structures et multi-échelles. D’autre part, par
ses fonctions, il est vivant et difﬁcilement dissociable d’autres
entités qui l’hébergent, comme le corps, ou interagissent avec lui,
comme son environnement immédiat mais aussi plus largement son histoire
ou son contexte social, sans oublier le fait que des considérations
éthiques peuvent également limiter son exploration. Enﬁn, pour le rendre
encore plus particulier, on peut aussi noter que le cerveau peut être vu
tout aussi bien comme une machine physico-chimique que comme un système
de traitement de l’information et de communication.

On peut donc considérer que les neuroscientiﬁques ont toujours été
confrontés à l’intérêt majeur de cet objet d’étude mais aussi aux
difﬁcultés, intrinsèques à sa struc- ture et à sa fonction, de
l’observer sans y introduire de biais. Ils ont donc dès l’origine dû
mener une réﬂexion élaborée pour développer des expériences (des
techniques ou des protocoles) permettant de se rapprocher le plus
possible de ce souhait de décrire la réalité du cerveau et, ce faisant,
ont emprunté une démarche similaire à celle de la modélisation que l’on
évoquera dans la section suivante.

On peut ainsi mentionner la découverte d’animaux modèles, comme par
exemple des rongeurs qui semblent développer naturellement des maladies
neurodégénératives [ARD 12] et que l’on pourra étudier plus simplement
ou plus invasivement que des humains, ainsi que la mise au point de
modèles animaux, par exemple relativement à l’observation que
l’injection d’une neurotoxine, le MPTP, peut induire chez le singe les
symptômes de la maladie de Parkinson [POR 12]. On parlera ici de modèle
non parce que le système à étudier est plus simple mais parce qu’il
permet un accès faci- lité à l’étude d’une question, ce qui est tout à
fait compatible avec la déﬁnition d’un modèle qu’on reprendra
ci-dessous.

C’est aussi dans cette même perspective que des procédés expérimentaux
ont dû être développés dans les neurosciences, en particulier par
électrophysiologie ou par imagerie, pour observer des phénomènes
autrement inaccessibles. Mais le dévelop- pement de ces technologies
s’est également accompagné d’un débat sur leurs limites et sur
l’interprétation de leurs résultats. Quelles sont par exemple les
limites de réso- lutions spatiales et temporelles en IRM fonctionnelle
où le signal BOLD mesuré est relatif au niveau d’oxygénation local des
tissus et pas (directement) à l’activité neuro- nale ? On voit à travers
ces exemples que les neurosciences, considérant la complexité de leur
objet d’étude, doivent recourir massivement à des médiations entre cet
objet et les connaissances qu’elles veulent en extraire, suivant en cela
une démarche similaire à la modélisation.

4

F. Alexandre

Il est notable en particulier qu’aujourd’hui la plupart des avancées
récentes en neurosciences reposent sur des plateformes technologiques de
plus en plus impres- sionnantes mais aussi parfois dont il est de plus
en plus difﬁcile de maîtriser les biais potentiels et dont on peut
parfois penser, comme on le notera aussi plus bas pour les neurosciences
computationnelles, qu’elles sont uniquement élaborées pour le plaisir de
complexiﬁer. Inversement, on peut aussi noter que sur certains sujets
anatomiques, les dessins plus que centenaires de Cajal restent une
référence, probablement car, plus qu’une observation, ils incluent
l’intuition du Maître sur ce qu’il fallait observer…

3.  Modélisation et simulation

La théorisation est probablement la plus aboutie des sciences normatives
et vise à décrire un objet d’étude en fournissant des explications ou
des connaissances, sous forme, par exemple, de relations entre ses
variables d’état (comme la loi d’Ohm, par exemple). Cependant, et
particulièrement pour un objet complexe, une théorie com- plète est
généralement hors d’atteinte ou nécessite au mieux une mise au point par
démarche itérative, en attaquant successivement différents aspects de
cet objet, en ré- pondant à des séries de questions. C’est ainsi que
l’on peut déﬁnir cette autre approche normative qu’est la modélisation,
comme une médiation entre expérience et théorie, qui va pouvoir
faciliter certains aspects de ce passage, en particulier au niveau de
l’ex- périmentation (par exemple le modèle animal) ou de la formulation
(voir les modèles phénoménologiques évoqués plus bas) [VAR 13]. C’est
aussi dans cette perspective que d’autres auteurs indiquent qu’un modèle
est avant tout fait pour répondre à une question [MIN 65] et qu’il a
cette vertu analogique par rapport à l’objet qu’il repré- sente, pour ce
qui concerne cette question [THO 72].

Ainsi, selon R. Thom, faire fonctionner un modèle, c’est le questionner
sur le sujet pour lequel il a été conçu. On peut voir que ceci
s’applique particulièrement bien à un modèle animal. Dans une vision
positiviste, où tout peut être expliqué par des phé- nomènes
physico-chimiques et décrit par des équations mathématiques [BUL 99], on
pourra aussi construire des modèles dits de connaissance, utilisant
souvent l’algèbre et les systèmes dynamiques et ainsi tenter d’expliquer
certaines propriétés de l’objet d’étude (c’est le rôle de justiﬁcation
théorique du modèle). Cette approche a connu un essor extraordinaire
dans la seconde moitié du XXème siècle, avec le développe- ment de
l’informatique et de l’analyse numérique, en particulier pour rendre
compte de phénomènes naturels (océans, météorologie) ou pour développer
des dispositifs technologiques complexes (aviation, industrie
nucléaire).

Dans une vision plus moderne, où l’on est capable de rassembler de
grandes col- lections de données (Big Data) comme traces de
fonctionnement d’un phénomène, et de développer des approches
statistiques adaptatives (Machine Learning), on parlera plutôt de
modèles de représentation ou de modèles phénoménologiques qui, n’étant
pas fondés sur une analyse structurelle, n’auront pas de vertu
explicative mais plu- tôt un pouvoir prédictif. On parlera alors de
l’efﬁcacité pragmatique d’un modèle. Ce type d’approches a connu un
regain d’intérêt spectaculaire récemment, en particulier

Simuler moins pour modéliser plus

5

grâce au développement de l’Internet permettant un meilleur accès aux
données, et des capacités de calcul permettant de calculer des modèles
de plus grande taille, au point où dans certains domaines où la théorie
est difﬁcile (fortement non-linéaire par exemple), il est plus efﬁcace
d’approximer les équations principales par apprentis- sage à partir de
données [BRU 16]. On pourra aussi constater que dans le domaine du
traitement du langage naturel, après des décennies de théorisation, les
meilleurs systèmes de traduction automatique sont aujourd’hui basés sur
des statistiques et sont donc phénoménologiques…

La simulation, qui s’attache à la mise en œuvre numérique de modèles de
connais- sance, peut, d’un certain point de vue, être également
considérée comme un modèle phénoménologique. Alors que l’étape de
modélisation proprement dite vise à déﬁnir la représentation des
connaissances et le formalisme de calcul qui seront les plus adap- tés à
la question posée, la simulation a pour but de mettre effectivement en
œuvre le modèle calculatoire pour répondre à des interrogations de type
“Qu’est-ce qui se passe si … ?” (What if), en construisant des scénarios
permettant par exemple de considérer l’effet des paramètres choisis.
Depuis de nombreuses années, des domaines entiers de l’informatique et
des mathématiques ont été développés pour étudier la mise au point de
schémas numériques efﬁcaces et pour permettre leur mise en œuvre perfor-
mante sur des architectures de calcul distribuées, au point que dans
certains domaines de l’algèbre linéaire, les progrès de la simulation
sont autant dus à l’algorithmique qu’à l’accroissement des puissances de
calcul. La simulation peut être effectivement considérée comme un modèle
phénoménologique dans la mesure où cette étape de calcul, malgré sa
puissance, ne reste qu’un moyen sans vertu explicative et qu’il faut
ensuite procéder à une étape d’exploitation des résultats obtenus, le
plus souvent par visualisation mais aussi par d’autres moyens
d’évaluation ou de mesure.

Comme il a été mentionné plus haut, à terme, l’aboutissement de telles
approches de modélisation pourrait être de construire une théorie ou en
tout cas de contribuer à son établissement progressif. Ceci se traduit
par l’expression de trois étapes princi- pales lors de la réalisation de
modèles. Il s’agit premièrement de choisir la question de connaissance à
laquelle on souhaite répondre, typiquement une question sur laquelle les
théories courantes ne sont pas satisfaisantes, et de construire le
modèle qui sera le plus adapté pour y répondre, en choisissant en
particulier la structure du système de représentation et son état
initial ainsi que le formalisme de calcul le plus adapté pour rendre
compte des variables importantes et de leurs relations (il est ainsi
superﬂu d’in- troduire des aspects de l’objet d’étude qui ne sont pas
concernés par ces dimensions). Dans un deuxième temps, si on ne peut pas
répondre analytiquement à la question ou si le modèle échappe à la
compréhension car il devient trop complexe pour être considéré dans son
ensemble, on pourra exécuter une simulation jusqu’à l’étape où les
résultats générés peuvent être interprétés pour permettre de répondre à
la question et, le cas échéant, de fournir des explications.

Dans un troisième temps, on peut participer à la réfutation du modèle,
en sui- vant la démarche proposée par Karl Popper [POP 34] et en
comparant les productions ultérieures du modèle (et de ses simulations)
avec la réalité ou en provoquant cette

6

F. Alexandre

comparaison, en proposant des prédictions sur des sujets qui n’étaient
pas a priori prévus lors de l’établissement du modèle. Une comparaison
défavorable va sufﬁre à mettre en cause le modèle (à le réfuter) et
conduira à le modiﬁer plus ou moins radica- lement pour lui permettre
d’intégrer ce nouveau cas alors qu’une comparaison favo- rable ne
permettra de rien conclure d’autre que le fait que le modèle courant
reste la meilleure explication à notre disposition (en attendant une
prochaine réfutation éven- tuelle), puisqu’il faudrait pouvoir tester le
modèle dans toutes les circonstances pour l’adopter déﬁnitivement.

Cette démarche itérative a été utilisée pour le rafﬁnement de nombreux
modèles, avec ses bons et ses moins bons aspects. Si elle a permis de
construire des modèles complexes en considérant successivement
différentes facettes d’une question assez générale, elle a aussi donné
lieu à des dérives, en créant des modèles exagérèment complexes,
constitués d’excroissances et de rustines destinées à rendre compte de
cas particuliers ou de questions relativement annexes au problème
considéré, alors qu’il ne faut pas oublier que la vocation d’un modèle
n’est pas de rendre compte de la réalité d’un objet d’étude sous tous
ses aspects, mais seulement de fournir un substrat pour répondre à une
question particulière. On évoquera plus bas de tels exemples dans le
champ des neurosciences computationnelles. On observera également dans
ce cas que le problème principal de mise au point d’un modèle n’est pas
tant de réduire progres- sivement un écart de précision que de savoir y
intégrer des connaissances hétérogènes, ce qui reste un des points les
plus délicats non abordés ici car relevant souvent plutôt d’un savoir
faire : le passage de la question à la forme du modèle le mieux adapté
pour y répondre. On évoquera seulement pour conclure la loi de
l’instrument proposée par A. Maslow [MAS 66] : “Il est tentant, si le
seul outil que vous avez est un marteau, de tout traiter comme si
c’était un clou”.

4.  Les Neurosciences Computationnelles

Les neurosciences computationnelles peuvent être déﬁnies comme le
domaine scientiﬁque visant à étudier les relations entre les structures
et les fonctions du cer- veau par le moyen de techniques de traitement
de l’information [SCH 90, DAY 01]. Il a été évoqué plus haut la grande
efﬁcacité de l’approche modélisatrice pour rendre compte, dans une
vision très positiviste, de phénomènes physico-chimiques à l’aide
d’équations différentielles. Il est notable à ce propos que l’histoire
des neurosciences computationnelles trouve son origine dans une
description du fonctionnement d’un neurone sous le couvert des lois
physiques de l’électricité. Le premier modèle histo- rique de neurone
[BRU 07] et, plus tard, celui sur lequel une grande partie des neuros-
ciences computationnelles s’est construite, le modèle de Hodgkin-Huxley
[NEL 95], écrivent l’équation du potentiel de membrane d’un neurone en
appliquant simplement les lois d’Ohm et de Kirchoff (lois de
l’électricité). De façon intéressante, on notera que le modèle de
Hodgkin-Huxley, plus complexe que son ancètre, ajoute à ce modèle de
connaissance d’autres équations phénoménologiques qui ont été
déterminées expé-

Simuler moins pour modéliser plus

7

rimentalement par Hodgkin et Huxley en 1952 sur l’axone de calamar
géant, rendant compte de phénomènes comme la probabilité d’ouverture de
canaux ioniques.

On pourra donc noter qu’à un certain niveau de description, il n’y a
rien à com- prendre de ce modèle, sauf qu’il traduit des observations de
biologistes, mais il n’en reste pas moins vrai qu’il a connu un succès
retentissant (valant en particulier le prix Nobel à ses auteurs), car il
pouvait permettre de mimer avec une grande précision le comportement
électrique d’un neurone isolé soumis à des créneaux de courant en en-
trée. Cette précision s’est améliorée ultérieurement en étendant le
modèle spatialement mais aussi en descendant progressivement dans les
niveaux de description des boîtes phénoménologiques ou en ajoutant
d’autres détails, relatifs aux synapses par exemple. Outre les premiers
essais pour étudier des assemblages de tels modèles de neurones,
parallèlement, des modèles plus intégrés ont été développés [AMA 77, WIL
73], choi- sissant comme niveau de description l’activité électrique
moyenne d’une population de neurones et visant à rendre compte de
phénomènes plus globaux de propagation de cette activité électrique [COO
05].

Alors qu’il est important de mentionner que ces modèles élémentaires
n’ont été confrontés à la biologie que dans des cas artiﬁciels de petits
nombres de neurones soumis à des stimulations externes tout aussi
artiﬁcielles, et qu’ils étaient essentiel- lement dédiés à répondre à
des questions du type “comment est-ce que les neurones calculent ?”, ces
approches de modélisation, très fructueuses dans leur domaine d’in-
vestigation initial, ont également suscité des attentes énormes et ont
été réinterprétées dans des approches ascendantes visant à simuler des
morceaux importants de cerveau (simuler alors qu’avant on parlait de
modélisation). C’est en particulier le cas du Blue Brain Project,
ancêtre de l’actuel Human Brain Project.

Le Blue Brain Project, dont on peut aujourd’hui commencer à tirer des
bilans [MAR 15], se proposait d’assembler des modèles de neurones très
détaillés, en agré- geant des données de neuroanatomie correspondant à
trente mille neurones et quarante millions de synapses du cortex
sensoriel du rat, sans mécanisme de plasticité céré- brale et sans
questionnement sur un éventuel calcul sous-jacent. Comme ses modèles
ancêtres, il inclut également des parties phénoménologiques, par exemple
concernant l’activité électrique des neurones, extraite par observation
statistique, à coté d’autres aspects très détaillés, plutôt basés sur
des connaissances. Ce processus de rétroin- génierie consistant à
assembler un grand nombre de briques élémentaires pour mieux comprendre
l’objet global a pu faire évoquer de la complexité pour le plaisir de la
com- plexité [CHI 16] et en tout cas a conduit à se demander si ce type
de projet ascendant est sufﬁsamment contraint pour permettre de passer
ainsi de niveaux sub-neuronaux à des niveaux à bien plus large échelle
tout en retrouvant, juste par agrégation de don- nées, les propriétés
des niveaux macroscopiques [FR˜14]. Parmi ces propriétés que l’on
voudrait voir ainsi émerger, il y a en particulier des aspects
cognitifs, car c’est ce qui est évidemment une justiﬁcation majeure
d’une telle démarche intégratrice, pour ne pas mentionner l’Human Brain
Project qui, lui, vise le cerveau humain dans son en- semble,
c’est-à-dire un réseau d’une taille deux millions de fois supérieure au
modèle précédent.

8

F. Alexandre

Même si c’est également un aspect très important de ces projets et si
cet aspect a généré des questions scientiﬁques très intéressantes, nous
ne discuterons pas ici en détails les travaux relatifs à l’implantation
matérielle et à la réalisation concrète de tous les calculs
sous-jacents. On donnera seulement des ordres de grandeurs en in-
diquant que calculer de tels modèles peut impliquer des milliers de
processeurs et des dizaines voire des centaines de téraﬂops (milliers de
milliards d’opérations par seconde).Toujours pour rester dans les ordres
de grandeur, on remarquera que de tels calculs peuvent générer une
consommation électrique de l’ordre du Méga Watt, à com- parer avec les
vingt Watts consommés par notre cerveau.

Les neurosciences computationnelles semblent donc proposer des modèles
inté- ressants lorsque l’on considère des petits systèmes de neurones,
vus comme des ma- chines physico-chimiques stimulées artiﬁciellement.
Ceci peut être précieux pour ré- pondre à des questions et faire des
prédictions dans un certain nombre de situations relatives par exemple
aux modes de fonctionnement ou d’apprentissage de neurones isolés, mais
semble plus difﬁcile à transposer au cerveau dans son ensemble ou à des
fonctions cognitives complexes. Nous pensons que cette difﬁculté est due
à plusieurs raisons que nous résumons brièvement ici. Tout d’abord, le
cerveau est un système ouvert, la cognition est incarnée dans un corps
et le cerveau se construit et fonctionne en interaction avec
l’environnement. Ensuite, le cerveau est un système adaptatif et
changeant. La cognition résulte de différentes formes d’apprentissage et
d’interaction entre différentes formes de mémoires et s’élabore de façon
plus ou moins autonome tout au long de la vie. Enﬁn, le cerveau est un
système multimodal et multi-niveaux et l’on pourra donc poser des
questions totalement différentes selon que l’on considère des sensations
élémentaires comme plaisir et douleur ou des perceptions beaucoup plus
structurées comme la vision et l’audition et selon que l’on s’intéresse
au rôle des hormones ou à celui du langage dans le fonctionnement du
cerveau. Il n’est donc pas évident que l’on puisse s’attaquer à tous et
à chacun de ces sujets en agrégeant sim- plement des modèles de neurones
à l’aide d’un simple plan de connexion, en suivant un simple processus
de rétroingénierie. Ce tableau du cerveau, vu comme un système complexe,
dépendant potentiellement d’un nombre astronomique de variables et de
paramètres, impliqué dans des boucles d’interaction avec son
environnement incluant le corps et dépendant de son histoire récente et
ancienne semble donc disqualiﬁer la méthode de modélisation
traditionnellement utilisée pour d’autres objets complexes comme un
avion ou un océan, quand il s’agit d’aborder des afﬁrmations telles que
celle formulée par P. Cabanis au XVIIIème siècle : “Le cerveau sécrète
la pensée comme le foie sécrète la bile” …

D’autres chercheurs, intéressés par mettre les neurosciences
computationnelles au service de l’exploration de fonctions cognitives,
ont fait ce constat. Ils ont également bien compris que faire des
modèles, si précis soient-ils, n’est pas simuler la réalité ni tout en
expliquer, qu’il reste des approximations et des aspects purement
phénoméno- logiques et que faire un modèle, c’est construire un cadre,
éventuellement simpliﬁé et comportant des a priori, dans le but
d’explorer une question précise. C’est dans cette perspective que, pour
modéliser le cerveau et ses fonctions cognitives, des forma- lismes de
calcul ont été proposés [ROU 12, O’R 00, STE 11], adaptés aux questions

Simuler moins pour modéliser plus

9

que ces chercheurs se posent, et qui permettent d’explorer ensuite
certaines fonctions cognitives comme les phénomènes attentionnels [FIX
11], la décision [O’R 06] ou la coordination sensorimotrice pour
manipuler de différentes façons des séquences per- ceptives [ELI 12].
Ces modèles ne sont pas purement ascendants mais introduisent également
des a priori et des hypothèses concernant des niveaux de description
inter- médiaires ou même parfois, de façon descendante, des cadres
conceptuels. Il ne s’agit donc pas de comparer ou d’évaluer ces modèles
en fonction de la quantité de détails qu’ils ont pu agréger, mais plutôt
de se demander s’ils reposent effectivement sur des a priori ou des
hypothèses cohérentes par rapport à ce que l’on sait aujourd’hui du
cerveau, s’ils arrivent effectivement à apporter des éléments de réponse
pertinents par rapport aux questions qui avaient été choisies, si les
méthodes d’évaluation sont so- lides et éventuellement s’ils proposent
des prédictions qui pourraient permettre de les réfuter ou de les faire
évoluer.

Dans le cadre d’une démarche de modélisation de fonctions cognitives,
ajouter la contrainte de la prise en compte du substrat neuronal peut
également avoir des effets bénéﬁques tout au long de la démarche, pour
aider à formuler la question, construire le cadre et le formalisme et
évaluer le modèle dans un contexte plus connu, plus classique à décrire
et plus facile à expérimenter. Si l’on considère par exemple la
compréhension du conditionnement répondant, une série de modèles
purement comportementaux ont été proposés [LEP 04], chaque modèle se
traduisant par la complexiﬁcation du précé- dent en ajoutant un terme
dans une équation phénoménologique pour rendre compte de résultats
nouveaux ayant réfuté le précédent. De façon contrastée, considérer ce
même paradigme de conditionnement en le faisant reposer sur son
implantation neu- ronale décrite dans le lobe temporal médial [CAR 15]
permet de proposer une solution plus simple où les comportements
complexes reposent simplement sur une compé- tition entre plusieurs
voies neuronales élémentaires. Une telle approche de modéli- sation
cognitive par les neurosciences computationnelles permet de plus de
générer des prédictions pouvant être vériﬁées expérimentalement à des
niveaux différents, par exemple pharmacologiques [CAL 06].

5.  Discussion

Dans ce chapitre, nous avons tout d’abord rappelé le recours croissant à
l’utilisa- tion de modèles dans de nombreux domaines de la physique et
dans l’industrie. Ces modèles rendent compte de phénomènes trop
complexes pour être directement décrits par une théorie mais pour
lesquels la connaissance de leurs mécanismes élémentaires par la théorie
ou l’observation phénoménologique permet de construire un modèle et de
l’utiliser ensuite lors de campagnes de simulation. Le succès de telles
entreprises est dû aux progrès considérables réalisés dernièrement en
mathématiques et en infor- matique pour construire ces modèles et pour
les calculer efﬁcacement. Mais il repose également sur une utilisation
réﬂéchie et maitrisée de ces outils très puissants. Nous avons identiﬁé
deux types de risques associés à une mauvaise compréhension et à un
mauvais usage des modèles et des simulations.

10

F. Alexandre

De façon générale, il est important de se rappeler qu’un modèle n’est
pas une des- cription de la réalité qu’il conviendrait de rendre de plus
en plus précise mais un outil de médiation construit entre l’expérience
et la théorie pour répondre à une question particulière. Si nous avons
insisté sur cette déﬁnition de la modélisation, c’est pour rappeler que
la qualité première d’un modèle n’est pas, dans l’absolu, la ﬁnesse du
niveau de description utilisé mais sa capacité à répondre à la question
qui était posée. Il y a de toutes façons le plus souvent des aspects
phénoménologiques inclus dans les modèles qui empêchent de descendre
dans la ﬁnesse du niveau d’explication et le plus important est donc de
vériﬁer que les hypothèses retenues sont cohérentes avec le cadre choisi
et donc la question à traiter. Modéliser plus ﬁnement et donc devoir
simuler plus n’est pas un but en soi.

De façon plus particulière, concernant les neurosciences (ou d’autres
sciences étu- diant des systèmes complexes), nous avons mis en garde
contre l’extension de la seule vision positiviste dans un contexte
systémique. Il n’est pas satisfaisant de décrire le cerveau comme une
simple machine physico-chimique à modéliser de façon ascen- dante mais
il convient plutôt de le remettre dans un cadre systémique en
considérant des boucles d’interaction avec son corps, son environnement,
ses niveaux d’échelle ou encore son histoire à différentes constantes de
temps. La complexité de cette des- cription est une raison de plus pour
disqualiﬁer une modélisation reposant trop sur l’afﬁnement du niveau de
description mais va plutôt inciter à réﬂéchir ﬁnement aux questions à
poser et aux hypothèses et aux formalismes à retenir. Pour ces raisons,
le choix d’un cadre général bio-inspiré et d’hypothèses reposant sur des
niveaux de des- cription intermédiaires est une voie intéressante pour
tenter de rejoindre expériences et théories. Pour autant, considérant la
complexité de l’objet d’étude et les effets d’émer- gence associés à ces
boucles et à ces niveaux d’échelle et de temps, le recours à la
simulation reste un outil important pour explorer ce cadre de
modélisation.

6.  Bibliographie

[AMA 77] AMARI S., « Dynamics of pattern formation in lateral-inhibition
type neural

ﬁelds », Biological Cybernetics, vol. 27, no 2, 1977, p. 77–87.

[ARD 12] ARDILES A. O., TAPIA-ROJAS C. C., MANDAL M., ALEXANDRE F.,
KIRKWOOD A., INESTROSA N. C., PALACIOS A. G., « Postsynaptic dysfunction
is associated with spatial and object recognition memory loss in a
natural model of Alzheimer’s disease. », Proceedings of the National
Academy of Sciences, vol. 109, no 34, 2012, p. 13835-40, National
Academy of Science.

[BRU 07] BRUNEL N., VAN ROSSUM M. C. W., « Lapicque’s 1907 paper : from
frogs to

integrate-and-ﬁre », Biological Cybernetics, vol. 97, no 5, 2007,
p. 337–339.

[BRU 16] BRUNTON S. L., PROCTOR J. L., KUTZ J. N., « Discovering
governing equations from data by sparse identiﬁcation of nonlinear
dynamical systems », Proceedings of the National Academy of Sciences,
vol. 113, no 15, 2016, p. 3932–3937, National Academy of Sciences.

[BUL 99] BULLOCK A., TROMBLEY S., The Fontana Dictionary of Modern
Thought, Lon-

don : Harper-Collins, 1999.

Simuler moins pour modéliser plus

11

[CAL 06] CALANDREAU L., TRIFILIEFF P., MONS N., COSTES L., MARIEN M.,
MARI- GHETTO A., MICHEAU J., JAFFARD R., DESMEDT A., « Extracellular
hippocampal ace- tylcholine level controls amygdala function and
promotes adaptive conditioned emotional response. », The Journal of
neuroscience : the ofﬁcial journal of the Society for Neuros- cience,
vol. 26, no 52, 2006, p. 13556–13566.

[CAR 15] CARRERE M., ALEXANDRE F., « A pavlovian model of the amygdala
and its in- ﬂuence within the medial temporal lobe », Frontiers in
Systems Neuroscience, vol. 9, no 41, 2015.

[CHI 16] CHI K. R., « Neural modelling : Abstractions of the mind »,
Nature, vol. 531, no

7592, 2016, p. S16–17.

[CHU 13] CHUNG K., DEISSEROTH K., « CLARITY for mapping the nervous
system », Nat.

Methods, vol. 10, no 6, 2013, p. 508–513.

[COO 05] COOMBES S., « Waves, bumps and patterns in neural ﬁeld theories
», Biol. Cybern.,

vol. 93, 2005, p. 91-108.

[DAY 01] DAYAN P., ABBOTT L. F., Theoretical neuroscience :
computational and mathema-

tical modeling of neural systems, Cambridge, MA : MIT Press, 2001.

[ELI 12] ELIASMITH C., STEWART T. C., CHOO X., BEKOLAY T., DEWOLF T.,
TANG Y., TANG C., RASMUSSEN D., « A large-scale model of the functioning
brain. », Science (New York, N.Y.), vol. 338, no 6111, 2012,
p. 1202–1205, American Association for the Advancement of Science.

[FIX 11] FIX J., ROUGIER N. P., ALEXANDRE F., « A Dynamic Neural Field
Approach to the Covert and Overt Deployment of Spatial Attention »,
Cognitive Computation, vol. 3, 2011, p. 279–293.

[FR˜14] FRÉGNAC Y., LAURENT G., « Where is the brain in the Human Brain
Project ? »,

Nature, vol. 513, 2014.

[LEP 04] LE PELLEY M. E., « The role of associative history in models of
associative lear- ning : a selective review and a hybrid model. », The
Quarterly Journal of Experimental Psychology, vol. 57, no 3, 2004,
p. 193–243.

[MAR 15] MARKRAM H., MULLER E., RAMASWAMY S., REIMANN M. W., ABDELLAH
M., SANCHEZ C. A., AILAMAKI A., ALONSO-NANCLARES L., ANTILLE N., ARSEVER
S., KAHOU G. A., BERGER T. K., BILGILI A., BUNCIC N., CHALIMOURDA A.,
CHIN- DEMI G., COURCOL J.-D., DELALONDRE F., DELATTRE V., DRUCKMANN S.,
DUMUSC R., DYNES J., EILEMANN S., GAL E., GEVAERT M. E., GHOBRIL J.-P.,
GIDON A., GRA- HAM J. W., GUPTA A., HAENEL V., HAY E., HEINIS T.,
HERNANDO J. B., HINES M., KANARI L., KELLER D., KENYON J., KHAZEN G.,
KIM Y., KING J. G., KISVARDAY Z., KUMBHAR P., LASSERRE S., LE BÉ J.-V.,
MAGALHÃES B. R. C., MERCHÁN-PÉREZ A., MEYSTRE J., MORRICE B. R., MULLER
J., MUÑOZ CÉSPEDES A., MURALIDHAR S., MUTHURASA K., NACHBAUR D., NEWTON
T. H., NOLTE M., OVCHARENKO A., PA- LACIOS J., PASTOR L., PERIN R.,
RANJAN R., RIACHI I., RODRÍGUEZ J.-R., RIQUELME J. L., RÖSSERT C.,
SFYRAKIS K., SHI Y., SHILLCOCK J. C., SILBERBERG G., SILVA R., TAUHEED
F., TELEFONT M., TOLEDO-RODRIGUEZ M., TRÄNKLER T., VAN GEIT W., DÍAZ J.
V., WALKER R., WANG Y., ZANINETTA S. M., DEFELIPE J., HILL S. L., SE-
GEV I., SCHÜRMANN F., « Reconstruction and Simulation of Neocortical
Microcircuitry », Cell, vol. 163, no 2, 2015, p. 456–492, Elsevier.

[MAS 66] MASLOW A., The Psychology of Science, New York : Harper and
Row, 1966.

12

F. Alexandre

[MIN 65] MINSKY M., « Matter, Mind and Models », Proc. International
Federation of

Information Processing Congress, 1965, p. 45-49.

[NEL 95] NELSON M., RINZEL J., « The Hodgkin-Huxley model. », chapitre
4, p. 27-51,

Bower and Beeman. The book of Genesis, Springer, New York, 1995.

[NEU 47] VON NEUMANN J., « The Mathematician », HEYWOOD R., Ed., The
works of the

mind, University of Chicago Press, 1947.

[O’R 00] O’REILLY R., MUNAKATA Y., Computational Explorations in
Cognitive Neuros- cience : Understanding the Mind by Simulating the
Brain, MIT Press, Cambridge, MA, USA, 2000.

[O’R 06] O’REILLY R. C., FRANK M. J., « Making Working Memory Work : A
Computatio- nal Model of Learning in the Prefrontal Cortex and Basal
Ganglia », Neural Computation, vol. 18, no 2, 2006, p. 283–328.

[POP 34] POPPER K., The Logic of Scientiﬁc Discovery, Routledge, Edition
2002, 1934.

[POR 12] PORRAS G., LI Q., BEZARD E., « Modeling Parkinson’s disease in
primates : The MPTP model », Cold Spring Harb Perspect Med, vol. 2, no
3, 2012, page a009308. [ROU 12] ROUGIER N. P., FIX J., « DANA :
Distributed numerical and adaptive modelling framework », Network :
Computation in Neural Systems, vol. 23, no 4, 2012, p. 237–253. [SCH 90]
SCHWARTZ E. L., Computational Neuroscience, MIT Press, Cambridge, MA,
USA,

1990. 

[STE 11] STEWART T. C., BEKOLAY T., ELIASMITH C., « Neural
representations of com- positional structures : representing and
manipulating vector spaces with spiking neurons », Connection Science,
vol. 23, no 2, 2011, p. 145–153, Taylor and Francis, Inc.

[THO 72] THOM R., Stabilité structurelle et morphogénèse : essai d’une
théorie générale des

modèles, W. A. Benjamin Reading, Mass, 1972.

[VAR 13] VARENNE F., « Modèles et simulations dans l’enquête scientiﬁque
: variétés tradi- tionnelles et mutations contemporaines », VARENNE F.,
SILBERSTEIN M., Eds., Modéli- ser et simuler. Epistémologies et
pratiques de la modélisation et de la simulation, p. 11-49, Editions
Matériologiques, 2013.

[WIL 73] WILSON H., COWAN J., « A mathematical theory of the functional
dynamics of

cortical and thalamic nervous tissue », Kybernetic, vol. 13, 1973,
p. 55–80.


