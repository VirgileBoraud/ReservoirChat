Pruning Self-Organizing Maps for Cellular Hardware Architectures Andres
Upegui, Bernard Girau, Nicolas P. Rougier, Fabien Vannel, Benoit

Miramond

To cite this version:

Andres Upegui, Bernard Girau, Nicolas P. Rougier, Fabien Vannel, Benoit
Miramond. Pruning Self-Organizing Maps for Cellular Hardware
Architectures. AHS 2018 - 12th NASA/ESA Con- ference on Adaptive
Hardware and Systems, Aug 2018, Edinburgh, United Kingdom. pp.272-279,
￿10.1109/AHS.2018.8541465￿. ￿hal-01826263￿

HAL Id: hal-01826263

https://hal.science/hal-01826263

Submitted on 30 Aug 2018

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

1

Pruning Self-Organizing Maps for Cellular Hardware Architectures Andres
Upegui∗, Bernard Girau†, Nicolas Rougier‡, Fabien Vannel∗ and Benoˆıt
Miramond§ ∗InIT, hepia, University of Applied Sciences of Western
Switzerland, Switzerland Email: andres.upegui@hesge.ch,
fabien.vannel@hesge.ch †Universit´e de Lorraine, CNRS, LORIA, F-54000
Nancy, France Email:bernard.girau@loria.fr ‡ Inria Bordeaux Sud-Ouest -
LaBRI / Universit´e de Bordeaux / CNRS UMR 5800, France Email:
nicolas.rougier@inria.fr § LEAT, Universit´e Cˆote d’Azur / UMR 7248
CNRS, France Email: benoit.miramond@unice.fr

Abstract—Self-organization is a bio-inspired feature that has been
poorly developed when it comes to talking about hardware architectures.
Cellular computing approaches have tackled it without considering input
data. This paper introduces the SOMA architecture, which proposes an
approach for self-organizing machine architectures. In order to achieve
the desirable features for such machine, we propose PCSOM, a
bio-inspired approach for self-organizing cellular hardware
architectures in function of input data. PCSOM is a vector quantization
algorithm deﬁned as a network of neurons interconnected through
synapses. Synapse pruning makes it possible to organize the cellular
system archi- tecture (i.e. topology and conﬁguration of computing
elements) in function of the content of input data. We present
performance results of the algorithm and we discuss the beneﬁts of PCSOM
compared to other existing algorithms.

I. INTRODUCTION Neuro-biological systems have been a source of
inspiration for computational science and engineering. The rapid
improve- ments of digital computing devices may soon reach their tech-
nological and intellectual limits. This has motivated the emer- gence of
alternative computing devices based on bio-inspired concepts. Moreover,
by evolving from a personal computing usage to an ubiquitous computing
paradigm computing and computers deserve now to be rethought : how to
represent complex information, how to handle this information, why
dissociating data and computation?

In front of such issues, the brain still remains our best source of
inspiration. It offers us a different perspective on the organization of
computing systems to meet the challenges of the increasing complexity of
current and future devices. Several current issues such as analysis and
classiﬁcation of major data sources (sensor fusion, big data, Internet
of things), and the need for adaptivity in many application areas
(autonomous drones, driving delegation in automotive systems, space
exploration…), lead us to study a desirable property from the brain that
encompasses all others: the cortical plasticity. This term refers to one
of the main developmental properties of the brain where the organization
of its structure (structural plasticity) and the learning of the
environment (synaptic plasticity) develop simultaneously toward an
optimal com- puting efﬁciency. Such developmental process is only made

possible by some key features: focus on relevant information,
representation of information in a sparse manner, distributed data
processing and organization ﬁtting the nature of data, leading to a
better efﬁciency and robustness.

The work presented in this paper makes part of the SOMA project
(Self-Organizing Machine Architecture), which aims to deﬁne an original
brain-inspired computing system to be prototyped onto FPGA devices. Its
architecture is organized as a decentralized neural network interleaved
into a set of many-core cellular computing resources. Neurons learn data
to develop the computing areas related to the incoming data in a
cortical way. In a previous project [1], we have studied how neural
self-organizing maps (SOMs) may control the devel- opment of these
computing areas in the manycore substrate, thus applying synaptic
plasticity to hardware conﬁguration. The central issue addressed by the
SOMA project is how the communications within and between the dynamic
computing areas self-organize by means of a particular type of dynami-
cally reconﬁgurable Network-on-Chip (NoC) controlled by the neural
network, thus transposing structural plasticity principles onto
hardware.

To reach this goal, the SOM that controls the computing resources must
be able to learn its underlying topology while learning to represent the
incoming data, and this underlying topology must reﬂect the hardware
constraints of the NoC. This paper presents the Pruning Cellular
Self-Organizing Maps (PCSOM) algorithm, which makes use of bio-inspired
mechanisms in order to better ﬁt application requirements, combining
synaptic and structural plasticity. Section II will introduce previous
works done towards self-organizing hard- ware approaches; section III
describes the cellular hardware architecture on which our algorithm will
evolve, it will im- pose the constraints for the PCSOM algorithm
presented in section IV. Finally, section V will describe how the
algorithm has been tested and the obtained results.

II. HARDWARE SELF-ORGANIZATION

Reconﬁgurable computing devices are the core implementa- tion platforms
for the so-called adaptive architectures. Several works have tackled
this issue. For example the NAPA adaptive

Author version - HAL-01826263 - CC-BY 4.0 International

architecture [2] provides an adaptive datapath in the form of a
coprocessor to be controlled by a processor. The PRISM- II processor [3]
includes new hardware instructions to the processor instruction set in
order to adapt to the code at hand. Other approaches handle the
adaptability at compilation- time [4] to generate a hardware
architectures that better ﬁts these a previously coded execution task.
Nevertheless, all approaches still depend on a previous speciﬁcation of
the problem at hand.

Though different from self-organization properties, self- replication
and self-reparation are two other bio-inspired prin- ciples that are
still far from being satisfactorily implemented in engineered systems.
They have been studied in projects such as Embryonics [5], POEtic [6],
and PERPLEXUS [7]. The Embryonics project is an emblematic project
giving birth to a whole new computing paradigm which borrows inspiration
from the genome interpretation done by each cell composing living
beings, thus enabling self-repair and self-replication in robust
integrated circuits that perform cellular computations. The POEtic and
Perplexus projects have pushed forward these initial ideas by including
dynamic routing and enhanced computation abilities. In spite of these
efforts, there are still many scalability problems concerning the
dynamic routing and the overall system synchronization.

The self-organizing mechanisms to be deployed on our architecture have
to extract meaningful representations from online data in order to guide
the deﬁnition of the hardware architecture. Such property is based on
vector quantization (VQ) capabilities. This must be done while avoiding
an over- ﬁtting of the architecture to the input data. Even though
different algorithms are able to partly deal with this issue1, the
self-organizing map (SOM) algorithm is certainly the most popular in the
ﬁeld of computational neurosciences because it gives a plausible account
on the organization of receptive ﬁelds in sensory areas where adjacent
neurons share similar representations. However, the stability and the
quality of such self-organization depends on a decreasing learning rate
as well as a decreasing neighbourhood function. SOMs have been and are
still used in a huge number of works in image and signal processing,
pattern recognition, speech processing, artiﬁcial intelligence, etc.
Hundreds of variants of the original algorithm exist today [13], [14]
but few of them have been deployed in hardware [15]–[17].

The major problems of most neural map algorithms is the necessity to
have a ﬁnite set of observations to perform adaptive learning starting
from a set of initial parameters (learning rate, neighbourhood or
temperature) at time ti down to a set of ﬁnal parameters at time tf . In
the framework of signal processing or data analysis, this may be
acceptable as long as we can generate a ﬁnite set of samples in order to
learn it off-line. However, from a more general point of view, it is not
always possible to have access to a ﬁnite set and we must face on-line
learning. The question is thus how to achieve both stability and
reactivity? We answered

1such as variations of the k-means method [8], Linde–Buzo–Gray (LBG)
algorithm [9] or neural network models such as the self-organizing map
(SOM, a priori topology) [10], neural gas (NG, no topology) [11] and
growing neural gas (GNG, a posteriori topology) [12]

2

this question by introducing a variant of the original SOM learning
algorithm (Dynamic SOM algorithm, DSOM) where the time dependency has
been removed [18]. Based on several experiments in both two-dimensional,
high-dimensional cases and dynamic cases, this new algorithm deﬁnes an
on-line and continuous learning that ensures anytime a tight coupling
with the environment that can be dynamic. It is to be noted that the
resulting codebook does not ﬁt data density as expected in most vector
quantiﬁcation algorithms. This could be a serious drawback in the
framework of signal processing or data compression but we rather think
this must be decided explicitly depending on the task.

III. THE SOMA ARCHITECTURE

The neural-based mechanism responsible of

the self- organization is integrated into a cellular processing archi-
tecture. It is decomposed in four distinct layers intended to provide
the hardware plasticity targeted by the SOMA project. These layers are:
(1) data acquisition, (2) pre-processing, which can be in the form of
feature extraction, (3) self- organization of computation and
communications, and (4) computation in the form of a reconﬁgurable
computation unit (FPGA or processor). These four architecture layers
have been presented in [19] and we already designed preliminary versions
of the three ﬁrst layers in [20], [21].

The proposed self-organizing mechanisms will be exploited by user-deﬁned
applications running on a multicore array in the form of a NoC-based
manycore system. The SOMA architecture will deﬁne the computing nodes of
the NoC as either General Purpose processors or application speciﬁc
computation nodes. Such a NoC will provide different features such that
computation nodes may:

• Communicate between neighboring and remote compu-

tation nodes in the manycore array.

• Communicate to the underlying self-organizing layer in order to
inﬂuence the self-organization. In this way the self-organization
mechanism will gather information from the computation in order to
self-adapt to the distributed computation requirements.

• Get inputs from the self-organization layer. The main goal of the
project is to adapt the computation execution on the diverse computation
nodes in order to adapt to the nature of the task at hand. This
mechanism will be able to drive the computation from the
self-organization and thus close the loop with the previous item.

The NoC architecture we currently use is based on the HER- MES routing
architecture [22] for inter-node communication, for which we previously
proposed an adapted version of the NoC architecture to support dynamic
reconﬁguration [21]. The main novelty concerns the coupling with the
self-organizing mechanism [23]. Figure 1 illustrates a 2-D version of
the SOMA cellular architecture composed of a reconﬁgurable computation
layer and a routing self-organizing layer. We only use two layers here
in order to highlight the role of the self- organizing layer which is
the central part of the work presented in this paper.

The self-organizing layer is responsible of coordinating with neighbor
nodes in order to decide which node must deal with

3

performed for every new input vector. The PCSOM algorithm is described
as follows:

Initialize the network as an array of neurons. Each neuron n is deﬁned
by a m-dimensional weight wn and a set of synapses deﬁning connections
to other neurons. for every new input vector v do

Compute winner s as the neuron n with wn closest to v. Update the weight
of the winner ws :

(a) Cellular array of pro- cessing elements

(b) A processing element composed of routing and computation layers

Fig. 1: 2-D SOMA architecture

ws = ws + α(v − ws)||v − ws||

(1) 

new incoming data in a completely decentralized manner. It will also be
able to reconﬁgure the computing layer in order to better ﬁt the
hardware to the nature of incoming data. The proposed architecture will
permit also 1-D and 3-D physical connectivity, increasing the
possibilities of the platform.

The detailed hardware mechanisms permitting to deal with the computation
reconﬁguration is out of the scope of this paper. Here, we will mainly
focus on the self-organizing mech- anism and the interactions between
such distributed nodes to coordinate themselves in order to permit them
to make further decisions about the type of computation to be done. This
functionality is provided in the form of a self-organizing map.

IV. THE PRUNING CELLULAR SELF-ORGANIZING MAP ALGORITHM - PCSOM

PCSOM is a vector quantization algorithm which aims to represent a
probability density function into a set of prototype vectors. Is is a
neural network, composed of an n-dimensional array of neurons. In the
particular case of the SOMA archi- tecture n refers to the
dimensionality of the NoC. A neuron will be thus associated to a
computation element and it will determine the computation to be executed
by this element. The probability density of input data will thus drive
the type of hardware to be deployed on the NoC computation elements. A
typical dimensionality would be a 2-dimensional processor array;
however, the SOMA architecture will also support 3- dimensional
connectivity. In the case of this paper we will only consider the case
of the 2-dimensional architecture.

Each neuron has an associated weight vector, or prototype vector,
representing a set of input vectors. This is an m- dimensional vector
where m is dissociated from n. It is deﬁned by the nature of input data.

Each neuron has also a number of associated synapses that deﬁne which
neuron will have an inﬂuence on which other. Synapses can be seen as
interconnection matrices. In the case of the SOMA architecture, we will
naturally assume that synapses are initially interconnecting every
neuron to its four physical neighbors. Afterwards, during the network
lifetime, some of these synapses will be pruned in order to allow the
prototype vectors to better ﬁt the density function. The goal here is to
remove useless synapses that will prevent the network to achieve optimal
performance. Moreover, removed connections can be further used for other
purposes or for creating new synapses. Such re-utilization is not
discussed learning is in this paper. Given its incremental approach,

where α is the learning rate. for every other neuron in the network do

Update weights as follows:

wn = wn + α(wi − wn)e(− 1

η

hops

||wn−wi||) )

(2) 

where wn is the weight of the neuron to be updated, hops is the number
of propagation hops from the winner, wi is the weight of the inﬂuential
neuron 2, η is the elasticity of the network.

end for for every synapse in the network do

Apply pruning following the probability:

Pij = e

(− 1 ω

1 ||wi−wj ||)titj

)

(3) 

where Pij is the probability of pruning the synapse interconnecting ni
and nj, ω is the pruning rate, and ti is the time from the last winning
of neuron ni

3.  

end for

end for The principle of the algorithm is that when a prototype vector,
represented as a weight ws of a neuron, is the closest to a new input
vector v, it is considered to be the winner and it will adapt its weight
according to equation 1. This means that the weight update will be
performed towards the new vector, and the magnitude of the update is
proportional to the difference between ws and v.

This winning neuron will inﬂuence lateral neurons directly through
synapses. Equation 2 updates the connected to it weight of neighboring
neurons wn by attracting their weight towards the winning neuron ws. In
this particular case the number of hops is 1 because they are directly
connected to the winner. After being updated, these neighboring neurons
will update their neighbor neurons as well by following the same
equation 2. This will result in a gradient permitting the overall
network weights to get inﬂuenced by every new input vector. A similar
principle can be found in other self- organizing map models like SOM and
DSOM. However, the main novelty of the PCSOM algorithm until this point
is the fact that the inﬂuence of one neuron to another depends on the
network connectivity: the network can be described as a graph with any
arbitrary topology. In SOM and DSOM, on

2 An inﬂuential neuron is deﬁned as the neuron from which wn received
the propagation signal. For a neuron with hops = h, the inﬂuential
neurons(s) will be every connected neuron with hops = h − 1.

3At initialization, it is assumed that all neurons won at iteration −1
in order

to avoid early synaptic prune

Routing Compute (ARM A9 or FPGA)4

Fig. 2: The three density functions used to test the performance of the
PCSOM algorithm.

the other hand, the inﬂuence is determined by a position of a neuron on
an n-dimensional space (usually 2-dimensional). This graph oriented
model permits us to think about modi- fying the network topology in
order to better ﬁt the incoming probability density function. Synaptic
pruning and sprouting are two biological mechanisms permitting
structural plasticity. In the case of the PCSOM algorithm presented
here, we are only using pruning through the equation 3, which deﬁnes the
probability of a synaptic connection to be removed. A useless synapse
connects two neurons whose activities are poorly correlated. Such poor
correlation is expressed in equation 3 as a high distance between
vectors wi and wj, and a very low activity (winning) of neurons ni and
nj which reﬂects a poor capability of the prototype vector to represent
the probability density function. All these factors are modulated by a
pruning rate and determine the probability of a given synapse to be
removed.

Neuron activity in this context is represented by ti and tj, which
represent the time from the last winning of neurons i and j. Time-scale
is an important issue in on-line learning because data arrive according
to the time-scale of the physical system. In the case of the experiments
presented in this paper, time represents the number of iterations of the
algorithm, in other words it is increased by 1 at every new input
vector. In a real embedded system it may use a real notion of time. In
any case, it is the pruning parameter ω that will modulate the
time-scale issue as well as the pruning intensity.

V. EXPERIMENTAL SETUP AND RESULTS

A. Quantization of 2D distributions

We have tested our algorithm by presenting a set of input vectors in 2
dimensions. We have analyzed the behavior and the performance of our
approach on 3 different types of density functions. These functions are
illustrated on ﬁgure 2, they rep- resent three different scenarios where
pruning may be useful. The ﬁrst function, generated as a uniform random
sampling bounded between 1 and 7 for each of the components of input
vectors, represents a function where pruning does not provide any
advantage. For the second function a slight pruning may be useful for
better ﬁtting the probability function. And ﬁnally, the third function
may require a much higher level of pruning. 1) Experiments: In order to
perform our experiments, sev- eral parameters have been ﬁxed in an
arbitrary manner. The network topology has been initially deﬁned as a
2-D array mesh of 8x8 neurons. Each neuron has 4 synapses connected to
neighbor neurons. Weights have been uniformly initialized

Fig. 3: Neuron’s weights linked through synapses

with values from (0,0) to (8,8). The graph in ﬁgure 3 illustrates such
initial network, weights are represented by the position of nodes, and
lateral synapses by the links between these nodes. Parameters like
learning rate and elasticity have been ﬁxed to α = 0.2 and η = 0.6 while
pruning rate ω has been modiﬁed in order to analyze the effects of
pruning.

The performance is measured by the average quantization error (AQE)4.
This error represents the loss of information induced by the vector
quantization on the overall vector sampling. It is usually computed as
the mean quadratic error:

AQE =

1 p

p (cid:88)

k=1

min 1≤i≤N

(dist(vk, wi))2

(4) 

Where p is a number of input vectors used as reference for computing the
AQE, vk is one of the p vectors to be used for computing the AQE, and N
is the total number of neurons. dist(vk, wi) is the distance between the
weight of neuron i and the input vector vk. Only the minimum distance is
considered for AQE computation because this is the prototype vector
representing the incoming data. p can be the complete vector training
set or a subset of it. Using a subset is mandatory when the number of
input vectors not known in advance or when the system runs online. In
the experiments presented here, p has been ﬁxed to 500.

Two different experiments have been carried out on these functions. The
ﬁrst one analyzes the effect of pruning on the worst case scenario (the
third density function), and how AQE behaves under different pruning
conditions. The second experiment aims to analyze the behavior of the
best network of the ﬁrst experiment when applied to two other
probability functions.

2)  The effect of pruning: The ﬁrst experiment aims to observe how a
    network evolves under different values of ω. Figure 4 illustrates
    the behavior of PCSOM with 8 different values of ω applied on the
    third density function of ﬁgure 2. Each curve in the ﬁgure
    represents the average of 48 runs of the algorithm with the same
    parameters. In this ﬁgure it can be observed the evolution of the
    network in terms of the total

4Another error measure for self-organizing maps is often used, the dis-
tortion measure. For each input, a gaussian-weighted average of
quantization errors is computed around the best matching unit. The
distortion is the average over all inputs of all these computed weighted
averages. This error measure reﬂects how much neighbouring neurons have
learned similar prototypes. Since our model modiﬁes the underlying
topology, this error measure can not be satisfactorily applied to
compare the performance of SOMs and PCSOMs.

01234567801234567801234567801234567801234567801234567820246810202468105

Fig. 4: Number of synapses vs. time for different pruning rates

Fig. 7: Results after training with ω = 1e − 05

(a) Cellular architecture con- nections

(b) Weights and probability den- sity functions

Fig. 5: AQE vs. time for different pruning rates

Fig. 6: Probability density function and weights after training the
network with ω = 0

number of synapses over time. Figure 5 illustrates the AQE measured
during the network’s lifetime. The color code of the two ﬁgures
corresponds to the same evaluated pruning rates. It can be observed that
when ω = 0, meaning that no pruning is performed, the number of synapses
remains constant at 112. It can also be observed that with ω = 0 the AQE
converges quickly to a suboptimal value. Figure 6 illustrates a typical
resulting network when pruning is disabled (ω = 0). It can be observed
that learning has optimized the AQE by ﬁtting the probability function.
However, the network elasticity (the inﬂuence of lateral neurons) has
prevented the network to correctly ﬁt the function. Of course, a
solution could be to reduce the elasticity, reducing in this manner
lateral neuron inﬂuence. However, this solution would have other
undesirable side effects like slowing down learning.

We observe in the other extreme case, with ω = 3e − 05, a network with a
very high probability of pruning synapses. It results in a network that
gets unconnected very quickly, and has not enough time to take advantage
of the elasticity mechanism proposed by the algorithm and the measured
AQE converges quickly to a very poor value. This is an extreme case of
over-pruning. Another case of overpruning is with (ω =

(a) Cellular architecture con- nections

(b) Weights and probability den- sity functions

Fig. 8: Results after training with ω = 3e − 07

1e−05) the network is unable to converge to a stable structure and the
over-pruning ends up eventually by disconnecting the whole network.
Figure 7b shows a network suffering of over- pruning. The very early
synapse removal makes the network converge to a sub-optimal solution.
Elasticity becomes useless and lateral inﬂuence is not present anymore.

The best solution is obtained with ω = 3e−07. The number of neurons
converges to the optimal value achieving the best possible AQE for this
probability function. It can also be observed that networks tested with
the values ω = 3e−08 and ω = 1e − 07 converge to almost the same number
of neurons and almost the same AQE, the ﬁnal result is thus as good as
the best solution; however, they require a much higher number of
iterations, making the learning unnecessarily slower.

An example of a resulting network using the pruning rate achieving the
best performance can be observed in Figure 8b. In this network the
useless synapses highlighted in ﬁgure 6 have been removed, and only
useful connections are main- tained. The formed sub-network naturally
represents the four data clusters on the input data-set. Other pruning
rate values like ω = 1e − 06 and ω = 3e − 06 converge to sub-optimal AQE
values. In spite of the over-pruning, the network manages to stabilize
its structure. However, given early synapse removal it is impossible to
achieve the best performance.

In conclusion, the pruning mechanism is able to reduce AQE when used
with the correct pruning rate parameter. If the pruning rate results to
be smaller than the optimal value, it still converges to an optimal
solution but learning is slower. On the other hand, if the pruning rate
is higher, the network will fail to converge to the optimal solution.
Excessive removal of synapses reduces the inﬂuence of a neuron on other
neurons and reduces the capability of the SOM to work as a collaborative
network. The other extreme -

05000100001500020000iterations020406080100120number of
synapsesω=1e-07ω=3e-07ω=1e-06ω=3e-06ω=1e-05ω=0ω=3e-08ω=3e-0505000100001500020000iterations0.050.060.070.080.090.10AQEω=1e-07ω=3e-07ω=1e-06ω=3e-06ω=1e-05ω=0ω=3e-08ω=3e-0501234567801234567802468024680123456780123456786

(a) Cellular Architecture

(b) Weights

(c) Cellular Architecture

(d) Weights

Fig. 9: Resulting neural network with the 1st probability density
function

Fig. 11: Resulting neural network with the 2nd PDF

(a) Number of synapses

(b) Average quantization error

Fig. 10: Network evolution for the 1st Probability Density Function
(PDF) and Average Quantization Error (AQE)

without pruning - will maintain the original network with even useless
connections, that may prevent the network to converge to optimal
solutions.

3)  Validating on other probability density functions: In order to
    validate the robustness of our approach, we have tested the same
    network with the same parameters on different probability density
    functions. We kept thus the same param- eters described in the
    previous subsection and we only used three different values for the
    pruning rate: the optimal value of ω = 3e − 07, a case of
    over-pruning with ω = 1e − 05, and without pruning ω = 0, in order
    to compare their performance. Only the resulting networks with ω =
    3e − 07 are represented in ﬁgures 9 and 11 .

Figure 9b shows that almost no pruning is performed in this function.
Pruning does not contribute to minimize AQE and the network dynamics is
able to adapt to the function characteristics. Nevertheless, given the
stochastic nature of it can still be possible to prune the pruning
mechanism, a connection (bottom right neuron in the ﬁgure), without
negative consequences in this particular case.

Figure 10 illustrates the evolution of the network with three different
values for ω over 20000 iterations. The graph repre- sents the average
of 48 runs of the algorithm. From ﬁgure 10b it can be observed that the
same parameter ω = 3e − 07 found in the ﬁrst experiment as the optimal
value, achieves the same quantization error than the network without
pruning. It can also be observed that the network with very high pruning
exhibits a poor performance. From ﬁgure 10a it can be observed that the
over-pruning is even worse than the one observed in the previous
experiment (ﬁgure 4), and that the pruning performed on the network with
ω = 3e − 07 is negligible.

It is worth to comment the effect observed around to itera- tion 1000 of
ﬁgure 10b. After achieving a good quantization error, the network isn’t
able to maintain it. This is due to the elasticity of the network
regulated in equation 2 with the parameter η. This parameter regulates
the inﬂuence of the

(a) Number of synapses

(b) Average quantization error

Fig. 12: Network evolution for the 2nd PDF

weight of lateral neurons on the computation of each neuron weight. A
high value of η makes it possible to converge faster, but may prevent
the network to ﬁnd an optimal solution at long term (like in this case).
A solution may be to decrease over time the value of η in a similar way
than SOM. However, doing so reduces the further network capability to
adapt to different input data, making the algorithm unsuitable for
learning dynamic functions.

Figure 11 shows the result of a network with ω = 3e − 07 and the same
parameters, learning the 2nd probability density function of ﬁgure 2,
which still can beneﬁt from the pruning mechanism. From a qualitative
point of view, this ﬁgure permits to deduce that pruning has contributed
to obtain a good solution. For a quantitative assessment, ﬁgure 12 shows
the behavior of PCSOM for 3 values of ω over 20000 iterations (each
being the average of 48 runs). Again it can be observed that the network
with a correct pruning parameter value achieves a better performance
than the ones without pruning and with over-pruning.

From these two new probability density functions it can be observed that
parameters are rather independent of the function at hand. The same set
of parameters can be applied without knowing the function in advance.

B. Application to image compression

A well-known application of self-organizing maps is lossy image
compression [24]. The principle is to divide the image into small
sub-images, learn a good quantization of these sub-images, and then
replace each sub-image by the index of the closest prototype.
Uncompressing the image is then performed by replacing each index by the
corresponding sub- image prototype. Compared to other quantization
techniques, SOMs preserve topological properties, so that close parts of
an image that are often similar will be coded by neighbouring neurons.
It makes it possible to further reduce the size of the compressed ﬁle by
efﬁciently coding prototype indexes [25].

01234567801234567805000100001500020000iterations020406080100120number of
synapsesω=0ω=3e-07ω=1e-0505000100001500020000iterations0.100.110.120.130.140.150.160.17AQEω=0ω=3e-07ω=1e-0501234567801234567805000100001500020000iterations020406080100120number
of
synapsesω=0ω=3e-07ω=1e-0505000100001500020000iterations0.080.090.100.110.120.130.14AQEω=0ω=3e-07ω=1e-05In
order to evaluate how PCSOM behaves in real-world ap- plications, we
have decided to choose this image compression method as an example,
performing the following steps:

1)  The L × H picture is cut into I sub-images of l × h

pixels, thus I = (L/l) × (H/h).

2)  A SOM and a PCSOM of n × n neurons learn these sub-images. Each
    neuron has a l × h weight vector, that can be interpreted as a
    sub-image prototype.

3)  The compressed image starts with the size of the original image, the
    size of the sub-images and the number of neurons. Then it includes a
    palette of the n × n sub- image prototypes, as learned by the SOM or
    PCSOM. Then, the compressed image only contains a list of the
    indexes of the winning neurons for each sub-image.

Even without further compression of the index list thanks to topological
properties of self-organizing maps, the compres- sion ratio for a
grayscale image (1 byte per pixel) is thus:

L × H

20 + (n × n × l × h) + ( L

l × H

h × b)

where b is the minimal number of bits to code integers between 0 and n ×
n − 1.

As an example, we consider a 364 × 332 image subdivided into 7553
sub-images of 4 × 4 pixels. Using a 8 × 8 SOM or PCSOM (b = 6), the
compression ratio reaches a very high value of 18 (again, without the
further compression of the index list using differential and entropy
coding [25]). Based on 213 experiments using various sets of parameters,
PCSOM signiﬁcantly improves the AQE (average 3.277e-2, standard
deviation 3.16e-4) with respect to standard SOMs (average 3.464e-2,
standard deviation 1.49e-3). We also observe a signiﬁcant perceptual
improvement of the visual result that is partially illustrated by the
usual peak signal-to-noise ratio (PSNR) to be maximized: average 24.79
for PCSOM vs 23.54 for SOM. Figure 13 shows results obtained for 20
epochs of 7553 iterations, each one using the same pattern for SOM and
PCSOM learning though the ordering is random. Pruning is performed after
each epoch. The compression examples correspond to the SOM or PCSOM that
reaches the best PSNR. For this PCSOM, ω = 8.10−9, resulting in 51
pruned synapses. Most topological properties are maintained since this
pruning occurs progressively during learning. It must be mentioned that
PCSOM learning is very fast compared to SOMS, often reaching the SOM
ﬁnal performance after less than 10 epochs. Further studies need to be
carried out to analyze the precise effect of pruning in this
application.

An important amount of work still needs to be carried out so as to study
how the induced self-organization of the computing resources may take
advantage of the properties of PCSOM. In the speciﬁc case of this
application to lossy image compression, it lies in the way prototype
indexes asso- ciated to sub-images are further compressed. Following
[25], a differential coding is ﬁrst performed, computing differences
between the index associated to a sub-image and the index associated to
the neighbouring sub-image in the best local minimum gradient direction.
Then these index differences are sent to an universal entropy coder
using variable length

7

(a) Initial image

(b) compression

SOM-based

(c) PCSOM-based compression

(d) Jpeg compres- sion with ratio 18

(e) SOM: prototypes

learned

(f) PCSOM: learned prototypes, pruned synapses

Fig. 13: SOM vs PCSOM for lossy image compression

coding. Thanks to the topological preservation property of self-
organizing maps, the differential coding step mostly results in small
numbers for which entropic encoding is very compact. Uncompression uses
similar steps in a reverse order. In a parallelized version of the whole
process, computing resources that will handle differential coding will
have to communi- cate with other computing resources so as to compute
index differences and local minimum gradient directions between
”winners” of neighbouring sub-images. When handling several sub-images
in parallel, these communications will compete for NoC resources. The
self-organization of computing resources with respect to PCSOM will
minimize trafﬁc congestion in the NoC, since the nodes that will often
communicate will be very close according to the NoC constraints. Details
of this work still need to be speciﬁed.

VI. CONCLUSION AND FURTHER WORK

In this paper we have presented the SOMA architecture, a neural-based
processing architecture inspired by brain plastic- ity to tackle
hardware self-organization of massively parallel processing elements.
The bio-inspired process proposed by SOMA relies on an efﬁcient learning
of incoming data in order to dynamically adapt the size of processing
areas to stimulation coming from the external environment. The learning
process relies on a new neural model inspired by self-organizing maps.
It has been adapted for cellular architectures and integrates pruning
capabilities: it removes useless lateral connections in order to better
ﬁt the target probability density functions. The results of our
experiments show that the proposed pruning mechanisms may improve the
network performance by reduc- ing the average quantization error. We
have also analyzed the effect of different values for the pruning
parameter: very high values fail to achieve optimal AQE due to
over-pruning, very low values converge to optimal solutions, but may be
very slow.

8

[17] M. A. de Abreu de Sousa and E. Del-Moral-Hernandez, “Comparison of
three FPGA architectures for embedded multidimensional categorization
through Kohonen’s self-organizing maps,” in 2017 IEEE International
Symposium on Circuits and Systems (ISCAS), May 2017, pp. 1–4. [18] N. P.
Rougier and Y. Boniface, “Dynamic Self-Organising Map,” Neu-

rocomputing, vol. 74, no. 11, pp. 1840–1847, 2011.

[19] L. Rodriguez, B. Miramond, and B. Granado, “Toward a sparse self-
organizing map for neuromorphic architectures,” ACM Journal on Emerging
Technologies in Computing Systems, vol. 11, no. 4, 2015. [20] L.
Rodriguez, L. Fiack, and B. Miramond, “A neural model for hardware
plasticity in artiﬁcial vision systems,” in Proceedings of the
Conference on Design and Architectures for Signal and Image Processing,
2013.

[21] L. Fiack, B. Miramond, A. Upegui, and F. Vannel, “Dynamic parallel
reconﬁguration for self-adaptive hardware architectures,” in NASA/ESA
Conference on Adaptive Hardware and Systems (AHS-2014), 2014. [22] F.
Moraes, N. Calazans, A. Mello, L. M¨oller, and L. Ost, “Hermes: an
infrastructure for low area overhead packet-switching networks on chip,”
INTEGRATION, the VLSI journal, vol. 38, no. 1, pp. 69–93, 2004. [23] L.
Khacef, B. Girau, N. P. Rougier, A. Upegui, and B. Miramond,
“Neuromorphic hardware as a self-organizing computing system,” in WCCI
2018 - IEEE World Congress on Computational Intelligence, Workshop NHPU
: Neuromorphic Hardware In Practice and Use, IEEE, Ed., Jul. 2018,
pp. 1–4.

[24] Z. Huang, X. Zhang, L. Chen, Y. Zhu, F. An, H. Wang, and S. Feng,
“A hardware-efﬁcient vector quantizer based on self-organizing map for
high-speed image compression,” Applied Sciences, vol. 7, no. 11, 2017.
[25] C. Amerijckx, J. Legat, and M. Verleysen, “Image compression using

self-organizing maps,” Systems Analysis Modelling Simulation, 2003.

PCSOM has been designed taking into account the con- straints imposed by
the SOMA architecture, a cellular array of processing elements with
dynamic routing and enhanced reconﬁgurability capabilities. Future works
will implement PCSOM on this cellular architecture in order to drive the
type of hardware to be conﬁgured on each node.

Further work will focus on analyzing the behavior of PCSOM on dynamic
environments (i.e. probability density functions that change over time)
and on studying the interest of including sprouting mechanisms in order
to rebuild synapses or create connections with remote neurons. Again,
dynamic routing of the underlying NoC architecture should permit this by
still imposing physical constraints due to congestion probability.

ACKNOWLEDGMENT

The authors would like to thank the Swiss National Science Foundation
(SNSF) and the French National Agency (ANR) for funding the SOMA project
(ANR-17-CE24-0036).

REFERENCES

[1] A. SATURN, “The saturn project - self-adaptive technologies for up-
graded reconﬁgurable neural computing, french research agency (anr),
http://projet-saturn.ensea.fr,” 2011-2014.

[2] C. R. Rupp, M. Landguth, T. Garverick, E. Gomersall, H. Holt, J. M.
Arnold, and M. Gokhale, “The napa adaptive processing architecture,” in
Symp. on FPGAs for Custom Computing Machines, 1998, pp. 28–37. [3] M.
Wazlowski, L. Agarwal, T. Lee, A. Smith, E. Lam, P. Athanas, H.
Silverman, and S. Ghosh, “Prism-ii compiler and architecture,” in FPGAs
for Custom Computing Machines, 1993. Proceedings. IEEE Workshop on.

IEEE, 1993, pp. 9–16.

[4] P. M. Athanas and H. F. Silverman, “An adaptive hardware machine
architecture and compiler for dynamic processor reconﬁguration,” in
Computer Design: VLSI in Computers and Processors. ICCD’91. Pro-
ceedings, IEEE International Conference on. IEEE, 1991, pp. 397–400. [5]
D. Mange, M. Sipper, A. Stauffer, and G. Tempesti, “Toward robust
integrated circuits: The embryonics approach,” Proceedings of the IEEE,
vol. 88, no. 4, pp. 516–540, April 2000.

[6] Y. Thoma, G. Tempesti, E. Sanchez, and J. M. M. Arostegui, “POEtic:
an electronic tissue for bio-inspired cellular applications,”
Biosystems, vol. 76, no. 1-3, pp. 191–200, 2004.

[7] E. Sanchez, A. Perez-Uribe, A. Upegui, Y. Thoma, J. M. Moreno, A.
Napieralski, A. Villa, G. Sassatelli, H. Volken, and E. Lavarec, “Per-
plexus: Pervasive computing framework for modeling complex virtually-
unbounded systems,” in Adaptive Hardware and Systems (AHS). Second
NASA/ESA Conference on.

IEEE, 2007, pp. 587–591.

[8] J. B. Macqueen, “Some methods of classiﬁcation and analysis of mul-
tivariate observations,” in Proceedings of the Fifth Berkeley Symposium
on Mathematical Statistics and Probability, 1967, pp. 281–297.

[9] A. B. Y. Linde and R. Gray, “An algorithm for vector quantization

design,” IEEE Trans. on Communications, 1980.

[10] T. Kohonen, “Self-organized formation of topologically correct
feature

maps,” Biological Cybernetics, vol. 43, 1982.

[11] T. M. Martinetz, S. G. Berkovich, and K. J. Schulten, “Neural-gas
network for vector quantization and its application to time-series pre-
diction,” IEEE Trans. on Neural Networks, vol. 4, no. 4, 1993.

[12] B. Fritzke, “A growing neural gas network learns topologies,” in
Advances in Neural Information Processing Systems 7, G. Tesauro, D.
Touretzky, and T. Leen, Eds. MIT Press, 1995, pp. 625–632. [13] K. S.,
J. Jangas, and T. Kohonen, “Bibliography of self-organizing map

papers: 1981-1997,” Neural Computing Surveys, 1998.

[14] O. M., K. S., , and T. Kohonen, “Bibliography of self-organizing
map papers: 1998-2001 addendum,” Neural Computing Surveys, 2003. [15] J.
Misra and I. Saha, “Artiﬁcial neural networks in hardware: A survey of
two decades of progress,” Neurocomputing, vol. 74, no. 1–3, pp. 239 –
255, 2010, artiﬁcial Brains.

[16] W. Kurdthongmee, “A hardware centric algorithm for the best
matching unit searching stage of the som-based quantizer and its fpga
implemen- tation,” Journal of Real-Time Image Processing, vol. 12, no.
1, 2016.


