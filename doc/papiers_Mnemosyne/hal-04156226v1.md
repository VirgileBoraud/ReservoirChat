Comment fonctionne ChatGPT ? Décrypter son nom
pour comprendre les modèles de langage
Frédéric Alexandre

To cite this version:

Frédéric Alexandre. Comment fonctionne ChatGPT ? Décrypter son nom pour comprendre les modèles
de langage. The Conversation France, 2023. ￿hal-04156226￿

HAL Id: hal-04156226

https://inria.hal.science/hal-04156226

Submitted on 12 Jul 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Comment fonc\*onne ChatGPT ? Décrypter son nom pour comprendre les modèles 
de langage 

ALEXANDRE Frédéric, Centre Inria de l’université de Bordeaux, CNRS, Bordeaux INP 

On voit passer beaucoup d’avis sur des Intelligences ArDﬁcielles comme ChatGPT, sur leur 
dangerosité, leur niveau d’intelligence voire leur humanité. Mais ﬁnalement, que sait-on de 
ChatGPT ? Que c’est un réseau de neurones arDﬁciels composé de milliards de paramètres ; 
Qu’il est capable de tenir une discussion de haut niveau ; mais aussi qu’il peut tomber dans 
des pièges grossiers tendus par des internautes facéDeux. Bref, on nous parle beaucoup de 
lui mais on en sait ﬁnalement très peu sur son foncDonnement. Donc, avant d’en débaSre, je 
vous propose de présenter les mécanismes principaux sur lesquels ChatGPT repose, ce qui 
nous permeSra de comprendre que, si le résultat est parfois impressionnant, ses 
mécanismes élémentaires sont souvent astucieux mais pas vraiment nouveaux. Pour ce faire, 
passons en revue les diﬀérents termes du sigle ChatGPT. 

T comme Transformer 

Un Transformer est un réseau de neurones uDlisé principalement comme modèle de langage 
et qui bénéﬁcie du même algorithme d’apprenDssage que les réseaux profonds (Deep 
Networks), algorithme qui a donc déjà fait ses preuves pour l’entrainement de grosses 
architectures. Il bénéﬁcie également de deux caractérisDques éprouvées. D’une part, en tant 
que modèle de langage, il manipule des séquences de mots, qu’il code avec des techniques 
dites de [plongement lexical](hSps://fr.wikipedia.org/wiki/Plongement\_lexical), que l’on 
présente dans le [texte](« Comment ChatGPT comprend-il les mots qu’il uDlise ? »). 

D’autre part, l’autre caractérisDque intéressante concerne la façon dont l’aspect séquenDel 
des mots est traité. Il s’agit ici d’un problème majeur car ceSe séquenDalité est à prendre en 
considéraDon (pour interpréter le sens de certains mots dans le contexte plus général de la 
phrase) et les techniques proposées sont souvent très coûteuses en temps de calcul et 
relaDvement peu eﬃcaces. Ici aussi, la technique proposée par les Transformers privilégie 
une approche numérique et staDsDque, simple à calculer massivement et très eﬃcace. Il 
s’agit d’une technique dite aSenDonnelle qui consiste, pour interpréter le sens de chaque 
mot, de se demander à quelles parDes de la phrase il faut faire aSenDon pour associer un 
mot à son contexte. Le texte menDonné plus haut explique qu’avec le plongement lexical, 
chaque mot pouvait être remplacé par un descripDf numérique (un vecteur) de grande 
dimension. L’idée avec l’approche aSenDonnelle est d’apprendre comment chacun de ces 
vecteurs peut également être inﬂuencé par le vecteur (le descripDf) de certains autres mots 
de la phrase, ce qui permet d’accorder un mot ou de remplacer un pronom par le mot de la 
phrase qu’il représente. Et ici aussi, comme les textes sont, comme les humains, très 
prévisibles, il est impressionnant de constater comment ce type d’approches staDsDques 
appliquées à des grands corpus permet des interprétaDons de qualité. A Dtre d’illustraDon, 
voyez à quel point vous êtes capables de lire une BD des 
[Schtroumpfs](hSps://fr.wikipedia.org/wiki/Les\_Schtroumpfs) et de remplacer chaque 
‘schtroumpf’ par un mot issu de l’analyse aSenDonnelle des autres mots. 

G comme Généra\*f 

 
 
 
 
 
 
Ce terme renvoie au fait que ChatGPT est capable de générer du langage : on lui explique un 
problème, on lui pose une quesDon et, ayant assimilé ceSe interpellaDon, il nous répond 
avec du langage. C’est pour ça qu’on l’appelle ‘modèle de langage’, car pour ce faire, il doit 
voir appris un tel modèle. Ici aussi, la possibilité d’apprendre un modèle généraDf avec un 
réseau de neurones date de plus de trente ans et a été décrite sous la forme de modèles 
[d’auto-encodeurs](hSps://fr.wikipedia.org/wiki/Auto-encodeur), aussi appelés réseaux 
diabolo, faisant référence à la forme de ces réseaux. Prenons un exemple simple (illustré 
ﬁgure 1) avec un réseau ayant une couche d’entrée et une couche de sorDe de taille 
idenDque et une couche cachée de très peDte taille. 

Encodeur

Décodeur

Figure 1 : un auto-encodeur extrait les variables latentes et permet de créer un modèle 
généraDf 

On entraine le réseau en lui présentant des phénomènes similaires (par exemple des photos 
de visages) sur la couche d’entrée et en le supervisant pour qu’il reconstruise la même chose 
(le même visage) sur la couche de sorDe. C’est une tâche assez peu intéressante, sauf si on 
considère que, pour pouvoir faire ceSe reconstrucDon, le réseau doit passer par un codage 
plus compact dans la couche cachée. C’est l’obtenDon de ce codage compact qui sera la 
foncDon intéressante de ce réseau (ceSe parDe s’appelle l’encodeur), mais aussi sa capacité à 
reconstruire la forme d’origine à parDr du codage compact (ceSe parDe s’appelle le 
décodeur). Des études mathémaDques ont analysé les caractérisDques de ce codage 
compact (éliminant les détails superﬂus et rendant explicites les variables cachées 
principales du phénomène considéré, aussi appelées variables latentes) et d’autres ont 
étudié comment il était possible de sDmuler la couche cachée (de choisir certaines variables 
latentes) pour obtenir à travers le décodeur des nouveaux exemplaires du phénomène 
considéré. C’est par exemple en suivant ce procédé (pour des réseaux de grande taille 
comportant d’autres couches cachées intermédiaires avant et après la couche cachée 

 
 
 
 
centrale de peDte taille) que l’on est capable de créer des 
[deepfakes](hSps://fr.wikipedia.org/wiki/Deepfake), c’est-à-dire des trucages très réalistes. 

Si, au lieu de photos, on souhaite maintenant considérer des séquences (des vidéos ou des 
phrases), il faut en plus savoir prendre en compte l’aspect séquenDel du ﬂux d’entrée. Ceci 
peut être obtenu avec un phénomène aSenDonnel comme décrit plus haut, qui permet 
d’apprendre à produire chaque élément de la séquence en y ajoutant un contexte, 
représentant certains éléments précédents de la séquence, auxquels on aura appris à prêter 
aSenDon. La possibilité d’uDliser un simple mécanisme aSenDonnel pour traiter l’aspect 
séquenDel des entrées a été un constat majeur dans la mise au point des Transformers 
(« Vous n’avez besoin que d’aSenDon » Dtrait la publicaDon correspondante; « A.en0on is all 
you need »), car auparavant les méthodes privilégiées uDlisaient des réseaux plus complexes, 
dits récurrents, dont l’apprenDssage reste très lent et imparfait ; de plus ce mécanisme 
aSenDonnel se parallélise très bien (les mécanismes alternaDfs sont séquenDels !), ce qui 
accélère d’autant plus ceSe approche aSenDonnelle. 

P comme Pretrained 

Les mécanismes décrits plus haut consDtuent l’essenDel des méthodes uDlisées pour 
construire un Transformer et si beaucoup ont été surpris par leur eﬃcacité, c’est que ceSe 
dernière n’est pas seulement due à la puissance de ces méthodes, mais aussi (et surtout ?) à 
la taille de ces réseaux et des connaissances qu’ils ingurgitent pour s’entrainer. Les détails 
chiﬀrés sont diﬃciles à obtenir, mais on entend régulièrement parler pour des Transformers 
de milliards de paramètres (de poids dans les réseaux de neurones) ; pour être plus eﬃcaces, 
plusieurs mécanismes aSenDonnels (jusqu’à cent) sont construits en parallèles pour mieux 
explorer les possibles (on parle d’aSenDon « mulD-tête »), on peut avoir une succession 
d’une dizaine d’encodeurs et de décodeurs, etc. Rappelons que l’algorithme d’apprenDssage 
des Deep Networks est générique et s’applique aussi bien quelle que soit la profondeur (et la 
largeur) des réseaux ; il suﬃt juste d’avoir assez d’exemples pour entrainer pour ces poids, ce 
qui renvoie à une autre caractérisDque démesurée de ces réseaux, leur corpus 
d’apprenDssage. Ici aussi, peu d’informaDons oﬃcielles, mais il semble que des pans enDers 
d’internet soient aspirés pour parDciper à l’entrainement de ces modèles de langages, en 
parDculier l’ensemble de Wikipédia, les quelques millions de livres que l’on trouve sur 
internet (dont des versions traduites par des humains sont très uDles pour préparer des 
Transformers de traducDon), mais aussi très probablement les textes que l’on peut trouver 
sur nos réseaux sociaux favoris. Cet entrainement massif se déroule hors ligne, peut durer 
des semaines et uDliser des ressources calculatoires et énergéDques démesurées (chiﬀrées à 
plusieurs millions de dollars, sans parler des aspects environnementaux). 

Chat comme bavarder 

Nous sommes maintenant en meilleure posiDon pour présenter ChatGPT : il s’agit d’un agent 
conversaDonnel, bâD sur un modèle de langage, GPT, qui est un Transformer GénéraDf Pré-
entrainé. Les analyses staDsDques (avec approches aSenDonnelles) des très grands corpus 
uDlisés permeSent de créer des séquences de mots ayant une syntaxe de très bonne qualité. 
Les techniques de plongement lexical oﬀrent des propriétés de proximité sémanDque qui 
donnent des phrases dont le sens est souvent saDsfaisant. Outre ceSe capacité à savoir 

 
 
 
 
 
générer du langage de bonne qualité, un agent conversaDonnel doit aussi savoir converser, 
c’est-à-dire analyser les quesDons qu’on lui pose et y apporter des réponses perDnentes (ou 
détecter les pièges pour les éviter). C’est ce qui a été entrepris par une autre phase 
d’apprenDssage hors-ligne, avec un modèle appelé InstructGPT, qui a nécessité la 
parDcipaDon d’humains qui jouaient le jeu de faire l’agent conversaDonnel ou de pointer des 
sujets à éviter (en disant comment les éviter). Il s’agit d’apprenDssage dit par renforcement 
qui permet de sélecDonner des réponses selon les valeurs qu’on leur donne ; c’est une sorte 
de semi-supervision où les humains disent ce qu’ils auraient aimé entendre (ou pas). 

Ayant une meilleure compréhension des mécanismes de ChatGPT, on pourra discuter ailleurs 
plus en profondeur de sa possible dangerosité ou de sa similarité avec l’humain mais on peut 
conclure rapidement ici que ce n’est pas la peine de comprendre un sujet pour savoir en 
parler avec éloquence, sans donner forcément de garanDe sur la qualité de ses réponses 
(mais des humains aussi savent faire ça…). 

 
 
