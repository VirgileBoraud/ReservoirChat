Latent space exploration and functionalization of a gated working memory
model using conceptors Anthony Strock, Nicolas P. Rougier, Xavier Hinaut

To cite this version:

Anthony Strock, Nicolas P. Rougier, Xavier Hinaut. Latent space
exploration and functionalization of a gated working memory model using
conceptors. Cognitive Computation, 2022, ￿10.1007/s12559- 020-09797-3￿.
￿hal-02494493v2￿

HAL Id: hal-02494493

https://inria.hal.science/hal-02494493v2

Submitted on 13 Nov 2020

HAL is a multi-disciplinary open access archive for the deposit and
dissemination of sci- entific research documents, whether they are pub-
lished or not. The documents may come from teaching and research
institutions in France or abroad, or from public or private research
centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la
diffusion de documents scientifiques de niveau recherche, publiés ou
non, émanant des établissements d’enseignement et de recherche français
ou étrangers, des laboratoires publics ou privés.

Latent space exploration and functionalization of a gated working
memory model using conceptors

Anthony Strock2,1,3, Nicolas P. Rougier1,2,3,† and Xavier
Hinaut1,2,3,†,∗

1Inria Bordeaux Sud-Ouest, Talence, France 2LaBRI, Universit´e de
Bordeaux, CNRS UMR 5800, Talence, France 3IMN, Universit´e de Bordeaux,
CNRS UMR 5293, Bordeaux, France † Equal contribution, ∗Corresponding
author: xavier.hinaut()inria.fr

Abstract

Introduction. Working memory is the ability to maintain and manipulate
information. We introduce a method based on conceptors that allows us to
manipulate information stored in the dynamics (latent space) of a gated
working memory model. Methods. This latter model is based on a
reservoir: a random recurrent network with trainable readouts. It is
trained to hold a value in memory given an input stream when a gate
signal is on and to maintain this information when the gate is off. The
memorized information results in complex dynamics inside the reservoir
that can be faithfully captured by a conceptor. Results. Such conceptors
allow us to explicitly manipulate this information in order to perform
various, but not arbitrary, operations. In this work, we show (1) how
working memory can be stabilized or discretized using such conceptors,
(2) how such conceptors can be linearly combined to form new memories,
and (3) how these conceptors can be extended to a functional role.
Conclusion. These preliminary results suggest that conceptors can be
used to manipulate the latent space of the working memory even though
several results we introduce are not as intuitive as one would expect.

1

1 Introduction

A recent and major enhancement of the Reservoir Computing (RC) paradigm
has been proposed by Jaeger (2014) and Jaeger (2017) under the form of
conceptors which are able to capture the subspace of internal states of
a Recurrent Neural Network (RNN). In the case of Echo State Networks
(ESN), conceptors can be used to capture the trajectory of reservoir
states when the reservoir is fed with a particular input pattern (see
figure 1). These conceptors allow to extend the capacity of the original
ESNs by taking advantage of these new representations. For example,
(Jaeger, 2014; Bao et al., 2016; Bartlett et al., 2019; Gast et al.,
2017) showed how to use them for the recognition of temporal sequences
while using them for the storage and retrieval of multiple temporal
sequences. More recently, Mossakowski, Diaconescu, and Glauer (2019)
proposed an implementation of fuzzy logic based on conceptors, while
Liu, Ungar, and Sedoc (2019) used conceptors for online learning of
sentence representations, and He and Jaeger (2018) proposed a general
way to use conceptors during the learning of multiple tasks.

Conceptors yield several advantages when compared to classical
reservoirs since Jaeger (2014) demonstrated that conceptors can be used
for performing symbolic operations in the latent space of the different
input patterns. Such symbolic operations have been already exploited in
the framework of deep learning community and they provide impressive
results. For instance, in natural language pro- cessing (NLP), Mikolov
et al. (2013) showed that arithmetic operations such as “king − men +
woman” give a vector similar to “queen”. More recently, Brock et
al. (2016) proposed a method to edit global image features based on
operations performed on the latent space of generative adversarial
networks (GANs). Conceptors provide similar logical operations in the
framework of the reservoir computing paradigm. For instance, Jaeger
(2014) proposes an operator that quantifies if a stimulus is similar to
an already known conceptor. By associating one conceptor per class, it
is possible to measure if a stimulus belongs to a class (positive
evidence) or none (negative evidence). Beyond logical operations, linear
combinations of conceptors allow to implement continuous morphing
between set of states: they were used to create morphing between two
time series corresponding to the extended interpolation of the time
series (e.g. a morphing between two sine-waves with different
frequencies is a sine-wave with an intermediate frequency).

This capacity of performing operations in the latent space resonates
strongly with the notion of It is generally defined as the capacity to
hold working memory (WM) as found in neuroscience. information for a
short period of time as well as the capacity to manipulate this
information in order to achieve some task or to reach a specific goal.
In this context, we have introduced in (Strock, Hinaut, and Rougier,
2020) a reservoir with feedback connections that implements a gated
working memory, i.e. a generic mechanism to maintain information at a
given time (corresponding to when the gate is on, see figure 3). In this
model, the memory is encoded in the dynamics of the reservoir and
information can be maintained without any sustained activity. This
absence of sustained activity is precisely what makes it difficult to
manipulate the underlying information and this is also the reason why
some authors (Mongillo, Barak, and Tsodyks, 2008; Stokes, 2015; Masse et
al., 2019; Manohar et al., 2019) have suggested the existence of a
mechanism to temporarily store information in synaptic weights. In this
context, conceptors provide a plausible explanation for such a transfer
as well as an explicit method for manipulating information; even if the
conceptor mechanisms are for the moment not as biologically plausible as
the reservoirs themselves.

In this article, we explore the nature of operations carried on by such
conceptors and explore the dif- ferent ways to combine them such as to
explicitly manipulate memories. Even though the results we

2

introduce in this article are preliminary and to some extents,
counter-intuitive, this leads us to consider the notion of functional
conceptors that would allow to arbitrarily manipulate working memory in
the latent space.

2 Methods

2.1 Conceptors overview Considering an ESN R that has been trained1 to
produce the sequence O1 when presented with input sequence I1, Jaeger
(2014) demonstrated that it is possible to build an ESN R∗ that will
spontaneously produce the sequence O1, in the absence of any input (see
Figure 1). The activity of this new R∗ can be decoded using the read-out
weights of R. This is actually similar to the principle of the
full-FORCE method introduced in (DePasquale et al., 2018) where internal
weights are trained to match the internal activity of a teacher network
receiving the desired output as input. However, Jaeger (2014) principle
is applicable to multiple input patterns with the assumption that each
input pattern makes the reservoir evolve in a separable region of the
internal high dimensional space. In order to build R∗, he proposed to
approximate the activity of R when it receives any of these input
patterns. Then he equips R∗ with a set of recurrent connections
(i.e. the conceptors) that are specific to each couple of input/output
and that will project the internal state into the relevant sub-space.
Jaeger (2017) shows in particular how these conceptors can be considered
as long-term memories for temporal patterns. Conceptors can store
temporal patterns and reactivate them later with negligible loss of
recall/precision. More generally, conceptors can be considered as
long-term memories of internal states subspaces.

2.2 Models

2.2.1 Echo State Networks (ESN)

In this work we consider Echo State Networks (ESN) with feedback from
readout units to the reservoir (Jaeger, 2001). The system is described
by the following update equations:

x[n] = tanh (Winu[n] + W x[n − 1] + Wf by[n − 1]) + ξ y[n] = Woutx[n]

where u[n], x[n] and y[n] are respectively the input, the reservoir and
the output at time n. W , Win, Wf b and Wout are respectively the
recurrent, the input, the feedback, the output weight matrices and ξ is
a uniform white noise term added to reservoir units.

2.2.2 Controlling ESN dynamics using a conceptor

Following (Jaeger, 2014) notations, the equation for a conceptor C
enforcing some particular dynamics can be written as:

x[n] = C tanh (W x[n − 1] + b)

where C is the conceptor (possibly changing over time), x[n] is the
state of the model at time n, W is the recurrent matrix and b is a
constant bias. This can be extended to the general case where we also
have an input u[n] (with input matrix Win) (or similarly a feedback),
and writes:

x[n] = C tanh (W x[n − 1] + Winu[n] + Wf by[n − 1]) y[n] = Woutx[n]

1offline with ridge regression

3

Figure 1. Conceptors general idea A Considering an ESN R that outputs
the sequence O1 when an input I1 is presented, it is possible to build
an ESN R∗ that spontaneously outputs the sequence O1 in the absence of 4
any input. B Considering an ESN R that respectively outputs the
sequences O1 and O2 when input I1 and I2 are presented, it is not
possible to define R∗ as in A since we cannot define the expected output
in the absence of input. C Considering an ESN R that respectively
outputs the sequences O1 and O2 when input I1 and I2 are presented –
with the supplementary conditions that the inner representation
corresponding to inputs I1 and I2 are separable – it is possible to
build an ESN R∗ equipped with a set of feedback weights C1 or C2 such
that using C1, R∗ spontaneously outputs O1 and using C2, R∗
spontaneously outputs O2.

I1InputRR*I1InputI2InputO2OutputO1OutputO1OutputO1OutputA?OutputBI1InputI2InputO2OutputO1OutputCO2OutputO1OutputC1C2WRWRWRWRWRWRWRWRWRUsing
a conceptor C is similar to a change of W in ˜W = W C (and Wout in WoutC
if there is feedback). In our implementation, we thus consider:

x[n] = tanh ((W + Wf bWout) C x[n − 1] + Winu[n]) y[n] = Woutx[n]

2.2.3 Computing conceptors

In order to compute a conceptor for some given dynamics, it is necessary
to collect all the states of the reservoir and to concatenate them in a
matrix X. The conceptor C is then defined as:

C = XX T (cid:0)XX T + α−2I(cid:1)−1

= R (cid:0)R + α−2I(cid:1)−1

where R = XX T is similar to a covariance matrix, and α (a.k.a the
aperture) controls how close from the identity matrix C is.

2.2.4 Aperture adaptation

Intuitively, the aperture of a conceptor controls the precision of the
internal states representation. How- ever, no information on internal
states is lost, because it is possible to change the aperture of a
conceptor C without the need to recompute the conceptor from scratch. To
change the aperture, one only need to adapt the conceptor C as follows:

φ(C, γ) = C (cid:0)C + γ−2(I − C)(cid:1)−1

where φ(C, γ) represents the same states than C with a different
aperture, and γ is controlling how the aperture is modified.
Intuitively, φ(C, γ) modifies the aperture of C by a factor of γ.

2.2.5 Linear combination

Given two conceptors C and B and λ ∈ R, the linear combination of
conceptor C and B is defined as:

C = λC + (1 − λ)B

In the following when λ ∈ [0, 1] we will talk about interpolation, when
λ > 1 about right-extrapolation, and when λ < 0 about
left-extrapolation.

2.2.6 Boolean operations

Boolean operations can be written as:

(cid:16)

I + (cid:0)C(I − C)−1 + B(I − B)−1(cid:1)−1(cid:17)−1

C ∨ B = C ∧ B = (cid:0)C−1 + B−1 − I(cid:1)−1

¬C = I − C

However, as highlighted in (Mossakowski, Diaconescu, and Glauer, 2019),
∨ and ∧ are not idempotent (i.e. C ∨ C (cid:54)= C and C ∧ C (cid:54)=
C). More precisely if C (resp. B) is a conceptor built with the
covariance matrix R (resp. Q), Jaeger proposes to build C ∨ B using the
covariance matrix R + Q that is by design not idempotent. What we
propose here is to consider instead the matrix βR + (1 − β)Q with β ∈
[0, 1]

5

instead of R + Q, or if we want it to be symmetric (R + Q)/2. Similar
calculation gives the following new ∨β and ∧β.

(cid:16)

I + (cid:0)βC(I − C)−1 + (1 − β)B(I − B)−1(cid:1)−1(cid:17)−1

C ∨β B = C ∧β B = (cid:0)βC−1 + (1 − β)B−1(cid:1)−1

where n This way of building the OR operation also has a data driven
intuition. If we note β = n n+p (resp. p) is the number of data points
used to build R (resp. Q) then bR + (1 − b)Q is the ”correlation matrix”
obtained by taking the union of all the data points. Moreover, if we
choose β = 0.5 then there is a direct link between the two ways of
defining the OR operator: C ∨ B = φ(C ∨0.5 B, 2). In this study, the
aperture was mostly not influencing the results, thus we show only the
results for ∨.

2.3 Gating task

We consider the gating task described in (Strock, Hinaut, and Rougier,
2020). In this task the model receives an input V that is continuously
varying over time and another input being either 0 or 1 (trigger or gate
T ). To complete the task, the output has to be updated to the value of
the input when the trigger is active and to remain constant otherwise
(similarly to a line attractor). In other words, the trigger acts as a
gate that controls the entry of the value in the memory (the output).
Figure 2 describes this task.

Figure 2. Gating task. Each column represents a time step (time
increases from left to right), colored discs represent inputs (V and T )
and the output (M ).

2.3.1 Gated Working Memory Reservoir

We consider a reservoir with feedback from readout units to the
reservoir (see Figure 3). In (Strock, Hinaut, and Rougier, 2020) we
showed that this gated working memory reservoir is able to learn to
robustly gate information in presence of noise and of a distracting
input. The model behaves as a line attractor even if few values are used
for training (about 10 values is enough). We also provided a minimal
model version and showed an equivalence with GRU (Gated Recurrent Unit)
cells (Cho et al., 2014), which are a simplified version of Long-Short
Term Memory (LSTM) cells.

Figure 3. The Gated Working Memory Reservoir model The reservoir
receives a random signal V in [−1, +1] and a trigger signal T in {0, 1}.
The output M is fed back to the reservoir.

6

Value (V)Trigger (T)Output (M)VTMTriggerValueOutputWinWoutWfbWξ2.4
Implementation details

We consider a reservoir of 1000 neurons that has been trained to solve a
gating task described in Figure 2 A. The overall dynamics of the network
we consider are described by the following equations:

x[n] = tanh ((W + Wf bWout) C x[n − 1] + Winu[n]) + ξ y[n] = Woutx[n]

where u[n], x[n] and y[n] are respectively the input, the reservoir and
the output at time n. W , Win, Wf b, Wout and C are respectively the
recurrent, the input, the feedback, the output and the conceptor weight
matrices and ξ is a uniform white noise term added to reservoir units. W
, Win, Wf b are uni- formly sampled between −1 and 1, and left
untrained. Only W is scaled to have sparsity level equal to 0.5 and a
spectral radius of 0.1. If not stated otherwise, the noise is selected
uniformly between -10−4 and 10−4.

The major difference with Jaeger’s proposal in the way the patterns are
stored is that in our case pat- terns are stored implicitly when solving
the gating task. In other words, the patterns are stored by training
Wout and not by explicitly recomputing an internal weight matrix.
However as mentioned in (DePasquale et al., 2018), training Wout when
there is a feedback is equivalent to recomputing the internal weight
matrix W as W ∗ = W + Wf bWout. When Wout is computed to solve the
gating task, the conceptor C is considered to be fixed and equal to the
identity matrix (C = I). Wout is trained for 25,000 time steps. At each
time step there is a 0.01 probability of having a trigger and the input
value (V) is uniformly randomly sampled between -1 and 1. During
training, the average holding time of the value in memory is therefore
about 100 time steps.

After training, in normal mode, the conceptor C is equal to a conceptor
Cm that is generated and associated to a constant value m. In order to
compute this conceptor Cm, we impose a trigger (T = 1) as well as an
input value (V = m) at the first time step, such that the reservoir has
to maintain this value for 100 time steps. During these 100 time steps,
we use the identity matrix in place of the conceptor. The conceptor Cm
is then computed according to Cm = XX T (cid:0)XX T + I (cid:1)−1, where
X corresponds to the concatenation of all the 100 reservoir states
following the trigger, each row corresponding to a time step, I the
identity matrix and a the aperture. In all the experiments the aperture
has been fixed to a = 10. For the conceptors pre-computed in Figure 4
and 6, the reservoir have been initialised with its last training state.

a

3 Results

3.1 Constant-memory conceptors

The idea behind what we named constant-memory conceptors is to capture
explicitly the dynamics of a reservoir maintaining a value and to use
this conceptor to later constrain the dynamics of any reservoir,
inducing an alternative memory in the output as represented on figure 4.
In order to build a constant- memory conceptor Cm, we consider the gated
reservoir working memory model that receives a trigger and an input
value m at time t = 0. We collect the states of the reservoir for 100
iterations from which we build the conceptor Cm and apply it immediately
to the model. Results of this procedure is shown on figure 4B where five
conceptors are built (gray bands) and applied immediately (white bands)
without noticeable modification on the output of the reservoir since the
actual dynamics and the dynamics stored in the conceptor are congruent.
On figure 4A, we used the same procedure to build a set of 11 conceptors
whose captured dynamics correspond to 11 values uniformly spread between
- 1 and +1. On figure 4C, after 100 iterations following the
presentation of a new value, we apply the closest conceptor (Frobenius
norm, inducing a distance between conceptors) from the previous set of

7

11 constant-memory conceptors. One can see that on figure 4C that the
input value is first maintained (grey band) and jumps rapidly to the
closest discretized value when the conceptor is actually applied and
constrained the dynamics.

Figure 4. Approximation with conceptors and discrete conceptors. In A, B
and C the model receives the same inputs across time. Black lines:
Evolution of the reservoir readout y. Each black line in A. represents a
different trial where a different conceptor is applied. Gray lines: the
discrete value considered for each conceptor. Light gray areas: time
period when conceptors are computed for the current value. A. Different
trials showing various discrete conceptors applied. B-C. Conceptors Cm
are computed using the 100 time steps following a trigger while C = I
(light gray areas). B. The conceptor Cm is directly applied during the
400 following time steps. C. The closest conceptor among the discretized
conceptor is applied during the 400 following time steps. Dashed lines
represents the memory that should have been kept if not discretized.

These constant-memory conceptors also exhibit a nice property regarding
the long term maintenance of a memory. The initial gated working memory
model is already quite robust regarding the long term maintenance of
memory in the absence of internal noise (i.e. inside the reservoir).
When it is trained for a few hundreds of time steps, it is able to
maintain a memory for several thousands of time steps (see figure 5A)
before the memory starts to slowly degrade. When conceptors are applied,
this slow degradation vanishes (figure 5B): the RMSE without conceptor
is of 4.21e-02 ± 1.84e-02 (mean ± std) whereas with conceptors it is
1.02e-03 ± 6.95e-04. In the presence of internal noise (inside the
reservoir), the initial gated working memory model is much less robust
and memory starts to degrade after only a few thousand time steps (see
figure 5C). More precisely, a 10−4 noise (std(ξ) = 10−4) prevents the
model to maintain a value for a much longer time than the time that has
been used to train it (figure 5C). However, when conceptors are applied,
their benefit is even more obvious: after one hundred thousand time
steps, without conceptor the memory converge towards a few values,
whereas with conceptors the memory remains(figure 5D). The RMSE without
conceptor becomes 1.39e-01 ± 7.66e-02 (3.3 times greater than without
noise), whereas the RMSE with conceptors becomes 2.85e-03 ± 1.98e-03
(2.8 times greater than without noise).

8

050010001500200025001.00.50.00.51.0OutputA050010001500200025001.00.50.00.51.0OutputTicks:B05001000150020002500Time1.00.50.00.51.0OutputTicks:CFigure
5. Stability comparison with or without conceptor, with or without
noise. Black: Evolution of the readout y. lines: the discrete value
considered. Light areas: time when conceptors are computed for the
current value. A-B No noise. C-D 0.0001 noise. A and C No conceptor
used. B and D Discrete conceptor are applied.

9

0200004000060000800001000001.00.50.00.51.0OutputA0200004000060000800001000001.00.50.00.51.0OutputB0200004000060000800001000001.00.50.00.51.0OutputC0200004000060000800001000001.00.50.00.51.0OutputD3.2
Linear interpolation of constant-memory conceptors

In Figure 6, we show two main ideas: (1) how a linear interpolation
between two conceptors can allow to generalize the gating of other
values, and (2) a representation of the space in which lies the
conceptors and their link to the memory they encode. (1) Interpolation
and extrapolation C of conceptor C0.1 and conceptor C1.0 has been
computed as C = λC1.0 + (1 − λ)C0.1 with 31 λ values uniformly spread
between -1 and 2. Even though the interpolated (λ ∈ [0, 1]) conceptors
obtained are not exactly equivalent to Cm conceptors obtained in Figure
4, they seem to also correspond to a retrieved memory value that is
maintained. The mapping between λ and the value is non-linearly encoded.
For right- extrapolation (λ ∈ [1, 2]) the conceptor seems to be linked
to a noisy version of a Cm conceptor: the output activity is not
constant, but its moving average is constant. For left-extrapolation (λ
∈ [−1, 0]), the conceptor obtained does not seem to encode any
information anymore: all the output activities collapse to zero. (2)
Principal Component Analysis (PCA) have been performed using 201
pre-computed conceptors associated to values uniformly spread between -1
and 1. The first three components already explain approximately 85% of
the variance. The first component seems to non-linearly encode the
absolute value of the memory (Figure 6B) whereas the second component
seems to non-linearly encode the memory itself (Figure 6C). The curved
line composed of conceptors Cm (Figure 6E-G) shows visually why the
extrapolation does not work as we expected.

Figure 6. Generalisation of constant-memory conceptors Cm. Red: two
constant-memory conceptors: C0.1 and C1.0. Black: Inferred conceptors,
i.e. linear interpolation and extrapolation between C0.1 and C1.0. A
Temporal evolution of the readout for different conceptors. B-G :
constant-memory conceptors Cm for 201 values of m uniformly spread
between −1 and 1. B-D Link between principal components of the
conceptors and the memory they are encoding. For the interpolated
conceptors, the memory is considered as the mean in the last 1000 time
steps. E-G Representation of the conceptors in the three principal
components of the Cm conceptors.

3.3 Functional conceptors

In this section, we show three examples where conceptors have a
functional role: (1) constant- memory conceptors when triggers are
received, (2) a conceptor enforcing triggers, and (3) the con-

10

05001000150020002500Time0.00.51.01.5OutputA42024PC1 (53.79%)101Value
maintainedB1.00.50.00.51.0PC2 (22.08%)101Value
maintainedC0.50.00.51.0PC3 (8.97%)101Value
maintainedD42024PC1101PC2E1.00.50.00.51.0PC20.50.00.51.0PC3F0.50.00.51.0PC32.50.02.5PC1Gjunction/disjunction
of constant-memory conceptors. By functional we mean that conceptors do
not only store and reactivate a dynamical pattern. Conceptors can do
much more than constrain the dy- namics in an attractor. Conceptors can
also constrain the dynamics in a space where the behavior is
input-dependent, i.e. the outputs are some function of the inputs.

Constant-memory conceptors when triggers are received. When a
constant-memory conceptor Cm is applied to the gated working memory
model, it seems to quickly force the output y to match the value m,
making it insensitive to the input value. However, the actual behavior
is a bit different as illustrated on figure 7. On this figure, we can
observe that the readout value of the resevoir under the influence of a
conceptor C0.5 is destabilized when a trigger occurs. More precisely,
the trigger induce an instanteneous readout equal to the input until
quickly relaxing to the constant value of the conceptor (0.5). This
confirms that the combination of a reservoir and a constant-memory
conceptor remains sensitive to the input. Said differently, a
constant-memory conceptor Cm is not the mere storage of the value m but
rather a function that constrains the dynamics of the reservoir such
that when applied, the readout will eventually read m after some time.

Figure 7. Influence of trigger with constant-memory conceptors. A
constant-memory conceptor C0.5 is applied while receiving several
triggers. Gray: the input value (V). Black: Evolution of the readout y.
When a trigger occurs (indicated by dots on the Ticks line) the readout
transitorily goes to the current input value before going back to the
value memorized by the conceptor.

Conceptor enforcing triggers. This functional aspect of conceptor can be
further illustrated by building the following conceptor CT : during 100
time steps, the reservoir receives constant triggers (T = 1) and has
therefore to follow the value (V) it receives as input. Conceptor CT is
constructed from the states taken by the reservoir during these 100 time
steps. Figure 8 shows what happens when this conceptor is subsequently
applied: independently of the trigger signal, the output of the
reservoir follows the input. Everything happens as if the reservoir was
receiving a continuous trigger signal and the conceptor acts as a
pass-through filter that modifies the behavior of the gated working
memory as a whole (instead of modifying each individual value). This
result suggests that conceptors may probably be extended to store
arbitrary functions that act in the latent space of the reservoir. We do
not know yet how to specify such arbitrary functions inside a conceptor,
but the preliminary results we introduced are encouraging even though
more theoretical work is needed.

Conjunction/disjunction of constant-memory conceptors. The last example
where we show this functional aspect of conceptors is the disjunction of
conceptors. If the conceptors Cm and Cm(cid:48) represent the subspaces
when the value stored are respectively m and m(cid:48), the disjunction
of Cm and Cm(cid:48), i.e. Cm ∨ Cm(cid:48) would represent the union (or
the sum) of such subspaces. Applying Cm ∨ Cm(cid:48) might therefore
constrain the dynamics in one of these two subspaces, enforcing the
memory (i.e. the output y) to become either m or m(cid:48). Cm ∨
Cm(cid:48) could thus store a choice function between m and m(cid:48),
or in other words, a sort of conditional assignment that would store m
in some cases and m(cid:48) in some others. It is not exactly what Cm
∨Cm(cid:48) does but it still implements some conditional assignment.
When

11

050010001500200025001.00.50.00.51.0Input & OutputTicks:Figure 8.
Pass-through conceptors. This conceptor is able to modify the behavior
of the gated working model by letting all the values to pass through the
reservoir up to the output, independently of the gating signal (time
steps 1000 to 1500). Black: Evolution of the readout y. Gray: the input
value (V). Light gray areas: no conceptor is applied (i.e. C = I). White
areas: the conceptor simulating a constant trigger is applied (i.e. C =
CT ), thus the input is redirected to the output.

a trigger occurs the output jumps towards the value v to be maintained
and then relaxes to another value that depends on v.

In Figure 9, we show the values towards which the output relaxes
(i.e. relaxation values) when the disjunction of two constant-memory
conceptors is applied. First, as the disjunction of twice the same
conceptor Cm ∨ Cm is either equivalent to the same conceptor or to an
aperture adaptation of it (i.e. Cm ∨ Cm = φ(Cm, 2) and Cm ∨β Cm = Cm),
the value towards which the disjunction Cm ∨ Cm relaxes is the value of
the conceptor itself (i.e. m). Then, we realized that we could predict
what would be the relaxation values in different cases: in general the
relaxation value was mostly either almost zero or the maximum of the
absolute values of the two conceptors multiplied by the sign of the new
value to be maintained. We propose the following formula to predict the
value towards which Cm1 ∨ Cm2 relaxes:

vrelax(m1, m2, v) =

m1 sign(v) × max(|m1|, |m2|)



 

0

if m1 = m2 if min(|m1|, |m2|) < |v| or m1 = −m2 otherwise

(1) 

where v is the initial value (V ) proposed along with the trigger, m1
(resp. m2) is the constant associated to conceptor m1 (resp. m2),
vrelax(m1, m2, v) is the ultimate value reached while applying conceptor
implements a conditional assignment (modulo the Cm1 ∨Cm2 sign): if the
input value at time of trigger is bigger than the minimum between m1 and
m2 then it will converge towards the maximum of m1 and m2, otherwise it
will converge towards 0. The predictions made by the formula are less
accurate for extreme values such as for v = 1.00 (see Figure 9).

. Said differently, the conceptor Cm1 ∨Cm2

We hypothesize a similar formula for relaxation values of n
constant-memory conceptors:

vrelax(m1, …, mn, v) =

m1 sign(v) × max(|m1|, …, |mn|)



 

0

if m1 = … = mn if min(|m1|, …, |mn|) < |v| or (∀i, j |mi| = |mj| and ∃i,
j such that i > j and mi = −mj) otherwise

where v is the initial value (V ) given along with the trigger, mi is
the constant associated to conceptor Cmi

, vrelax(m1, m2, …, mn, v) is the ultimate value reached while applying
conceptor

Cmi

.

n (cid:87) i=1

12

050010001500200025001.00.50.00.51.0Input & OutputTicks:Figure 9.
Relaxation values (i.e. values towards which the output converges) when
applying the disjunction of two constant-memory conceptors. In other
words, it corresponds to the final values reached when applying the
conceptor Cm1 ∨ Cm2 . A-C Empirical results from experiments. D-F
Predictions based on equation 1.

We also studied how the conjunction constant-memory conceptors were
influencing the dynamics. If the disjunction of conceptors is similar to
a union (or sum), then the conjunction of conceptors is similar to an
intersection. Moreover, as the memory is represented as the output, the
memory cannot be simultaneously the value m and the value m(cid:48)
present. It is thus harder to predict what would be the behavior of such
conceptor. In practice, as for the disjunction, when a trigger occurs
the output jumps towards the value v to be maintained and then relaxes
to another value. However, in that case the value towards which it
relaxes is easy to describe, it is always almost zero. The conjunction
of constant-memory conceptors acts therefore as C0.

4 Discussion

Conceptors are powerful tools for performing explicit operations in the
latent space of a reservoir even though the composition of such
operations remains a difficult task. Using a reservoir model of gated
working memory, we have shown another way to enforce a particular memory
through the use of ad- hoc conceptors. These constant-memory conceptors
therefore provide a synaptic form to the memory, and we have shown how
they can be used to stabilize or discretize the memory. However, the
effect of conceptors composition is counter-intuitive and largely
differs from what we would naturally expect.

For instance, we have seen that the linear interpolation of
constant-memory conceptors does not create another constant-memory
conceptor, or at least it does not correspond to one we would have
computed. The reason being that the space of constant-memory conceptors
is not a straight line. Hence, they can- not be linearly interpolated: a
mere linear combination of constant-memory conceptors could not lead

13

1.00.80.60.40.20.00.20.40.60.81.0m11.00.80.60.40.20.00.20.40.60.81.0m2Av
=
0.251.00.80.60.40.20.00.20.40.60.81.0m11.00.80.60.40.20.00.20.40.60.81.0m2Dv
=
0.251.00.80.60.40.20.00.20.40.60.81.0m11.00.80.60.40.20.00.20.40.60.81.0Bv
=
0.501.00.80.60.40.20.00.20.40.60.81.0m11.00.80.60.40.20.00.20.40.60.81.0Ev
=
0.501.00.80.60.40.20.00.20.40.60.81.0m11.00.80.60.40.20.00.20.40.60.81.0Cv
=
1.001.00.80.60.40.20.00.20.40.60.81.0m11.00.80.60.40.20.00.20.40.60.81.0Fv
= 1.001.000.750.500.250.000.250.500.751.00vlim(m1,m2,v)to another
constant-memory conceptor. Nevertheless, we have shown empirically that
in all scenarios a linear combination of two constant-memory conceptors
lead to a value that is maintained. However, this new memory is slightly
oscillating around the combination of the constant values (see Figure
6). This oscillation being a direct consequence of the perturbation of
the system (i.e. the input).

Moreover, the disjunction of constant-memory conceptors gives an example
of functional conceptor. However, it does not implement what we
expected. In the case of two conceptors with two constant values v1 and
v2 such that 0 ≤ v1 ≤ v2, we would expect that the disjunction encodes
the two values simultaneously. More specifically, we expected such
disjunction to implement a choice function (i.e. a conditional
assignment) between the two values stored in each conceptor. Instead,
the disjunction implements another conditional assignment, that does not
converge towards v1 but only towards 0 or v2 depending on the given
input value. To some extent, v1 and v2 influence the disjunction with
different qualitative roles. Moreover, in the low rank case (i.e. when
the recurrent weights are the sum of a low rank matrix and a random
perturbation) only the largest fixed points can be stable (Schuessler et
al., 2020). Therefore, we can hypothesize that in the general case of a
disjunction of n > 2 constant- memory conceptors, only the extreme value
matter in the composite conceptor.

Even though our results are preliminary and will require more work to
fully characterize how oper- ations can be composed intuitively, this
work opens the door to another form of working memory: a procedural (or
functional) working memory. Instead of temporarily memorizing
declarative infor- mation, this kind of working memory would be able to
memorize procedural information (e.g. how a task should be performed,
which processes should be applied, etc.). For instance, imagine you are
given some instructions which are to sum up a series of numbers. In
order to complete this task, it is necessary to keep track of the
current sum (e.g. in a classical short-term declarative working memory)
that needs to be updated each time a new number is given. However, it is
also necessary to remember the preliminary instruction (i.e summing up)
in another form of working memory that needs to span the whole
experiment and which is procedural in nature. This procedural nature
makes this working memory quite peculiar because instead of memorizing a
given information, it needs to memorize a pro- cedure – here, a sequence
of operations depending on the context – that needs to be applied each
time an input is given. It is not yet clear how such memory could be
encoded in the brain (e.g. sustained activity, dynamic activity,
transient weights) and we think conceptors might be key in answering
such a question, but more experimental and theoretical work will be
needed. Similar conceptors than the ones we computed with our gated
working memory reservoir model are likely to be computed with other
working memory models (Lim and Goldman, 2013; Nachstedt and Tetzlaff,
2017; Bouchacourt and Buschman, 2019): it would be interesting to see
whether the functional conceptors obtained are similar (i.e. our results
would be generally applicable), or on the contrary, if differences
occur.

Compliance with Ethical Standards

Authors declare they have no conflict of interest. Ethical approval:
This article does not contain any studies with human participants or
animals performed by any of the authors.

References Bao, Jiao et al. (2016). “Action recognition based on
conceptors of skeleton joint trajectories.” In: Rev. Fac. Ing 31.4,
pp. 11–22. Bartlett, Madeleine et al. (2019). Recognizing Human Internal
States: A Conceptor-Based Approach. arXiv: 1909.04747 [cs.HC].
Bouchacourt, Flora and Timothy J. Buschman (July 2019). “A Flexible
Model of Working Memory.” In: Neuron 103.1, 147–

160.e8. doi: 10.1016/j.neuron.2019.04.020.

Brock, Andrew et al. (2016). “Neural photo editing with introspective
adversarial networks.” In: arXiv preprint arXiv:1609.07093. Cho,
Kyunghyun et al. (Oct. 2014). “Learning Phrase Representations using RNN
Encoder–Decoder for Statistical Machine Translation.” In: Proceedings of
the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP). Doha, Qatar: Association for Computational Linguistics,
pp. 1724–1734. url: http://www.aclweb.org/anthology/D14-1179.

14

DePasquale, Brian et al. (Feb. 2018). “full-FORCE: A target-based
method for training recurrent networks.” In: PLOS ONE 13.2. Ed. by
Maurice J. Chacron, e0191527. doi: 10.1371/journal.pone.0191527. url:
https://doi.org/10.1371/journal. pone.0191527.

Gast, Richard et al. (Apr. 2017). “Encoding and Decoding Dynamic Sensory
Signals with Recurrent Neural Networks: An Application of Conceptors to
Birdsongs.” In: BioRxiv. doi: 10.1101/131052. url:
https://doi.org/10.1101/131052. He, Xu and Herbert Jaeger (2018).
“Overcoming Catastrophic Interference using Conceptor-Aided
Backpropagation.” In: In-

ternational Conference on Learning Representations. url:
https://openreview.net/forum?id=B1al7jg0b.

Jaeger, Herbert (Jan. 2001). The ”echo state” approach to analysing and
training recurrent neural networks. Tech. rep. 148. Bonn,

Germany: German National Research Center for Information Technology GMD.

– (2014). “Controlling recurrent neural networks by conceptors.” In:
arXiv preprint arXiv:1403.3369. – (2017). “Using Conceptors to Manage
Neural Long-Term Memories for Temporal Patterns.” In: Journal of Machine
Learning

Research 18.13, pp. 1–43.

Lim, Sukbin and Mark S Goldman (Aug. 2013). “Balanced cortical
microcircuitry for maintaining information in working

memory.” In: Nature Neuroscience 16.9, pp. 1306–1314. doi:
10.1038/nn.3492.

Liu, Tianlin, Lyle Ungar, and Jo˜ao Sedoc (2019). “Continual Learning
for Sentence Representations Using Conceptors.” In:

CoRR abs/1904.09187. arXiv: 1904.09187. url:
http://arxiv.org/abs/1904.09187.

Manohar, Sanjay G. et al. (June 2019). “Neural mechanisms of attending
to items in working memory.” In: Neuroscience &

Biobehavioral Reviews 101, pp. 1–12. doi:
10.1016/j.neubiorev.2019.03.017.

Masse, Nicolas Y. et al. (June 2019). “Circuit mechanisms for the
maintenance and manipulation of information in working

memory.” In: Nature Neuroscience 22.7, pp. 1159–1167. doi:
10.1038/s41593-019-0414-3.

Mikolov, T. et al. (2013). “Distributed representations of words and
phrases and their compositionality.” In: Proc. of NIPS,

pp. 3111–3119.

Mongillo, G., O. Barak, and M. Tsodyks (Mar. 2008). “Synaptic Theory of
Working Memory.” In: Science 319.5869, pp. 1543–

1546. doi: 10.1126/science.1150769.

Mossakowski, T., R. Diaconescu, and M. Glauer (2019). “Towards logics
for neural conceptors.” In: J of Applied Logics 6.4,

pp. 725–744.

Nachstedt, Timo and Christian Tetzlaff (May 2017). “Working Memory
Requires a Combination of Transient and Attractor- Dominated Dynamics to
Process Unreliably Timed Inputs.” In: Scientific Reports 7.1. doi:
10.1038/s41598-017-02471-z. Schuessler, Friedrich et al. (2020).
“Dynamics of random recurrent networks with correlated low-rank
structure.” In: Physical

Review Research 2.1, p. 013111.

Stokes, Mark G. (July 2015). “‘Activity-silent’ working memory in
prefrontal cortex: a dynamic coding framework.” In: Trends

in Cognitive Sciences 19.7, pp. 394–405. doi:
10.1016/j.tics.2015.05.004.

Strock, Anthony, Xavier Hinaut, and Nicolas P. Rougier (Jan. 2020). “A
Robust Model of Gated Working Memory.” In: Neural

Computation 32.1, pp. 153–181. doi: 10.1162/neco a 01249.

15


