Visio-Linguistic Brain Encoding
Subba Reddy Oota, Jashn Arora, Vijay Rowtula, Manish Gupta, Raju S. Bapi

To cite this version:

Subba Reddy Oota, Jashn Arora, Vijay Rowtula, Manish Gupta, Raju S. Bapi. Visio-Linguistic Brain
Encoding. COLING 2022 - the 29th International Conference on Computational Linguistics, Oct 2022,
Gyeongju, South Korea. pp.116-133. ￿hal-03946675￿

HAL Id: hal-03946675

https://hal.science/hal-03946675

Submitted on 19 Jan 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Visio-Linguistic Brain Encoding

Subba Reddy Oota1,2, Jashn Arora2, Vijay Rowtula2, Manish Gupta2,3, Raju S. Bapi2
1Inria Bordeaux, France, 2IIIT Hyderabad, India, 3Microsoft, Hyderabad, India
subba-reddy.oota@inria.fr, {jashn.arora, vijay.rowtula}@research.iiit.ac.in,
gmanish@microsoft.com, raju.bapi@iiit.ac.in

Abstract

Brain encoding aims at reconstructing fMRI
brain activity given a stimulus. Earlier neu-
ral encoding models focused on brain encod-
ing for single-mode stimuli: visual (pretrained
CNNs) or text (pretrained language models).
Few recent papers have also obtained sepa-
rate visual and text representation models and
performed late-fusion using simple heuristics.
However, the human brain perceives the en-
vironment using information from multiple
modalities, and previous works have not ex-
plored the co-attentive multi-modal encoding
for visual and text reasoning. This paper sys-
tematically explores image and multi-modal
Transformers’ efficacy for brain encoding. Ex-
tensive experiments on two popular datasets,
BOLD5000 and Pereira, provide the follow-
ing insights. (1) We find that VisualBERT, a
multi-modal Transformer, significantly outper-
forms previously proposed single-mode CNNs,
image Transformers, and other previously pro-
posed multi-modal models, thereby establish-
ing new state-of-the-art. (2) The regions such
as LPTG, LMTG, LIFG, and STS, which have
dual functionalities for language and vision,
have a higher correlation with multi-modal
models, which reinforces the fact that these
models are good at mimicking the human brain
behavior. (3) The supremacy of visio-linguistic
models raises the question of whether the re-
sponses elicited in the visual regions are af-
fected implicitly by linguistic processing even
when passively viewing images. Future fMRI
tasks can verify this computational insight in
an appropriate experimental setting. We make
our code publicly available1.

1

Introduction

Brain encoding aims at constructing neural brain
activity recordings given an input stimulus. The
two most studied forms of stimuli include vision
and language. Since discovering of the relation-
ship between language/visual stimuli and functions

1https://tinyurl.com/VLBEncoding

of brain networks (Constable et al., 2004; Thirion
et al., 2006), researchers have been interested in
understanding how the neural encoding models pre-
dict the fMRI (functional magnetic resonance imag-
ing) brain activity. Recently, several brain encod-
ing models were developed to (i) understand the
ventral stream in biological vision (Yamins et al.,
2014; Kietzmann et al., 2019; Bao et al., 2020)
and (ii) study higher-level cognition like language
processing (Gauthier and Levy, 2019; Schrimpf
et al., 2020a; Schwartz et al., 2019). Previous work
has mainly focused on independently understand-
ing vision and text stimuli. However, the biologi-
cal systems perceive the world by simultaneously
processing high-dimensional inputs from diverse
modalities such as vision, auditory, touch, and pro-
prioception (Jaegle et al., 2021). In particular, how
the brain effectively processes and provides its vi-
sual understanding through natural language and
vice versa is still an open question in neuroscience.

Earlier studies mainly were related to neural
encoding models that predict brain activity using
representations of single-mode stimuli: visual or
text. Convolutional neural networks (CNNs) were
known to encode semantics from visual stimuli ef-
fectively. Interestingly, intermediate layers in deep
CNNs trained on the ImageNet (Deng et al., 2009)
categorization task can partially account for how
neurons in intermediate layers of the visual system
respond to any given image (Yamins et al., 2013,
2014; Güçlü and van Gerven, 2015; Yamins and
DiCarlo, 2016; Wang et al., 2019). However, more
recent and deeper CNNs did not further improve
on measures of brain-likeness, even though their
ImageNet performance has vastly increased (Rus-
sakovsky et al., 2015). Recently, Kubilius et al.
(2019) proposed a shallow recurrent anatomical
network, CORnet, which provided state-of-the-
art results on the Brain-score (Schrimpf et al.,
2020b) benchmark. Similar to CNN based vi-
sual encoding models, various studies leveraged

Proceedingsofthe29thInternationalConferenceonComputationalLinguistics,pages116–133October12–17,2022.116Fig. 1: Logical architecture of the proposed approach: We use features from image/multi-modal Transformers (like
ViT, VisualBERT, and LXMERT) as input to the regression model to predict the fMRI activations for different brain
regions. We evaluate the brain encoding results by computing 2V2 accuracy and Pearson correlation between actual
and predicted activations. We also perform layer-wise correlation analysis between transformer layers and brain
regions.

neural models like deep recurrent neural networks
(RNNs), Transformer (Vaswani et al., 2017) based
language models such as BERT (Devlin et al.,
2019), RoBERTa (Liu et al., 2019), and GPT-
2 (Radford et al., 2019) to predict the brain activ-
ity corresponding to semantic vectors of linguistic
items, including words, phrases, sentences, and
paragraphs (Gauthier and Levy, 2019; Schrimpf
et al., 2020a).

Brain encoding for more brain regions is vital
since input stimuli elicit diverse and distributed
representations in the brain. These activation re-
sponses could be internally repurposed for several
other brain tasks. Although previous neural encod-
ing models have demonstrated promising results
for processing one of the two brain regions (visual
cortex V4 and prefrontal cortex IT), more efforts
are needed to improve brain encoding for other
parts of the brain. Further, previous works manu-
ally choose2 particular CNN layers, whose activa-
tions were used for predicting brain activity spe-
cific to the datasets they work with (Kubilius et al.,
2019). Applying such methods to other datasets
will need dataset-specific, time-consuming manual
identification of the best layer. We observe in our
experiments that using last layer activations from
VisualBERT leads to the best accuracy.

Unlike previous studies, which focus on single-
modality (visual or language stimuli), some authors
demonstrated that multi-modal models formed by
combining text-based distributional information
with visual representations provide a better proxy

2Quoting from (Kubilius et al., 2019): “After testing every
layer on both V4 and IT, we report the model’s score as the
score of the best layer per region.”

for human-like intelligence (Anderson et al., 2015;
Oota et al., 2019). However, these methods ex-
tract representations from each mode separately
(image features from CNNs and text features from
pretrained embeddings) and then perform a simple
late-fusion. Thus, they cannot effectively exploit se-
mantic correspondence across the two modes at dif-
ferent levels. Such late-fusion-based multi-modal
models are the closest to our work, and our experi-
ments show that our models outperform them.

Recently, Transformer-based models were found
to be very effective than CNNs, in all language and
image-related tasks (Devlin et al., 2019). Image-
based transformer models like ViT (Dosovitskiy
et al., 2020), DEiT (Touvron et al., 2021), and
BEiT (Bao et al., 2021) have been shown to pro-
vide excellent results compared to traditional CNNs
on image classification tasks. Also, multi-modal
Transformers like VisualBERT (Li et al., 2019),
LXMERT (Tan and Bansal, 2019), and CLIP (Rad-
ford et al., 2021) have shown excellent results on
visio-linguistic tasks like visual question answer-
ing, visual common-sense reasoning. Inspired by
the success of language, image, and multi-modal
Transformers, we build multi-modal transformer
models to learn the joint representations of image
content and natural language and use them for brain
encoding. Overall, in this work, we investigate
whether image-based and multi-modal Transform-
ers can accurately perform fMRI encoding on the
whole brain. Fig. 1 illustrates our method for brain
encoding.

Specifically, we make the following contribu-
tions to this paper. (1) We present state-of-the-art

117SAFFNImageTransformerSAFFNTransformer LayersSAFFN2V2 Accuracy / PearsonCorrelationPredictedActualRidge RegressionL12L6L1Faster R-CNN Region Features[CLS] A tennisplayer swinging hisracket on a courtVisualBERTLXMERTTransformer (12 Layers)Cross-ModalityTransformerObject RelationshipTransformer (9 Layers)LanguageTransformer (12Layers)Ridge RegressionRidge RegressionFaster R-CNN Region Features[CLS] A tennisplayer swinging hisracket on a courtPredictions Image TransformerPredictions Predictions brain encoding results using multi-modal Trans-
formers. We also study the effectiveness of our
models in a cross-data setting. (2) Our approach
generalizes the use of Transformer-based architec-
tures, removing the need to manually select specific
layers as in existing CNN-based fMRI encoding
architectures. (3) We uncover several cognitive in-
sights about the association between fMRI voxels
and representations of multi-modal/image Trans-
formers and CNNs.

2 Brain Imaging Datasets

The following datasets are popularly used in the
literature for studying brain encoding: Vim-1 (Kay
et al., 2008), Harry Potter (Wehbe et al., 2014;
Pereira et al., 2018), BOLD5000 (Chang et al.,
2019), Algonauts (Cichy et al., 2019), and SS-
fMRI (Beliy et al., 2019). Vim-1 has only black
and white images, is only related to object recog-
nition, and is subsumed by BOLD5000. SS-fMRI
is smaller and very similar to BOLD5000. The
Harry Potter dataset does not have images. Lastly,
fMRIs have not been made publicly available for
the Algonauts dataset. Hence, we experiment
with BOLD5000 and Pereira Pereira et al. (2018)
datasets in this work.
BOLD5000: BOLD5000 dataset was collected
from four subjects where three subjects viewed
5254 natural images (ImageNet: 2051, COCO:
2135, Scenes: 1068) while fMRIs were acquired.
The fourth subject was shown 3108 images only.
Details of the visual stimuli and fMRI protocols of
the dataset have been discussed in (Chang et al.,
2019). We briefly summarize the details of the
dataset in Table 1. The data covers five visual areas
in the human visual cortex, i.e., the early visual area
(EarlyVis); object-related areas such as the lateral
occipital complex (LOC); and scene related areas
such as the occipital place area (OPA), the parahip-
pocampal place area (PPA), and the retrosplenial
complex (RSC). Each image also has correspond-
ing text labels: ImageNet has a few out of 1000
possible tags per image, COCO has five captions
per image, and Scenes have one out of 250 possible
categories per image.
Pereira: For the Pereira dataset Pereira et al.
(2018), participants were shown a concept word
and a picture to observe brain activation when par-
ticipants retrieved relevant meaning using visual in-
formation. Sixteen subjects were presented images
(six per concept) corresponding to 180 concepts

(abstract + concrete), while fMRIs were acquired.
Out of 180 concepts, 116 are concrete, and oth-
ers are abstract. Here, we augmented the image
captions using the concept word associated with
each image in the picture view. As in (Pereira
et al., 2018), we focused on nine brain regions cor-
responding to four brain networks: Default Mode
Network (DMN) (linked to the functionality of se-
mantic processing), Language Network (related to
language processing, understanding, word mean-
ing, and sentence comprehension, Task Positive
Network (related to attention, salience informa-
tion), and Visual Network (related to the processing
of visual objects, object recognition).

We show number of instances and voxel dis-
tribution across various brain regions for the
BOLD5000 and Pereira datasets in Tables 1 and 2
respectively.

Number of Voxels in Each ROI

PPA

LOC

ROIs→
↓Subjects #Instances LH RH LH RH LH RH LH RH LH RH
131 200 152 190 210 285 101 187 86 143
Subject-1
172 198 327 561 254 241 85
Subject-2
95 59 278
112 161 430 597 522 696 187 205 78 116
Subject-3
157 187 455 417 408 356 279 335 51 142
Subject-4

5254
5254
5254
3108

EarlyVis OPA

RSC

Table 1: BOLD5000 Dataset Statistics. LH=Left Hemi-
sphere. RH=Right Hemisphere.

Number of Voxels in Each ROI

Vision

DMN Task Positive

ROIs→ Language
↓Subj
P01
M01
M02
M03
M04
M05
M06
M07
M08
M09
M10
M13
M15
M16
M17

LH RH Body Face Object Scene Vision RH
5265 6172 3774 4963 8085
5716 5561 3934 4246 7357
4930 5861 3873 4782 7552
3616 4247 2838 3459 5956
5906 5401 3867 4803 7812
4607 4837 2961 4023 6609
4993 5099 3424 4374 7300
5629 5001 4190 4993 8617
5083 5062 2624 4082 6463
3513 3650 2876 3343 5992
5458 5581 3232 4844 7445
4963 4811 2675 4008 5809
5315 6141 4112 4941 8323
4726 5534 4141 4669 8060
5854 5698 4416 4801 8831

4141 12829 17190
3606 12075 17000
3173 11729 15070
9074 12555
2822
3602 12278 18011
3135 10417 14096
4058 11986 16289
3721 12454 17020
3503 10439 14950
2815
9003 12469
3474 11530 16424
3323
9848 14489
3496 12383 15995
4142 12503 15104
4521 13829 16764

LH
35120
34582
30594
24486
34024
28642
30109
30408
29972
25167
29400
30608
31610
31758
37463

Table 2: Pereira Dataset Statistics. LH=Left Hemi-
sphere. RH=Right Hemisphere.

3 Task Descriptions

We train fMRI encoding models using Ridge re-
gression on stimuli representations obtained using
various models for both datasets, as shown in Fig. 1.
The main goal of each fMRI encoder model is to
predict fMRI voxel values for each brain region
given stimuli. In all cases, we train a model per sub-
ject separately. Different brain regions are involved
in processing stimuli involving objects and scenes.
Similarly, some regions specialize in understand-

118ing vision inputs while others interpret linguistic
stimuli better.

To evaluate the generalizability of our models
across objects vs. scenes understanding, we also
perform cross-data experiments where the train im-
ages belong to one sub-dataset, and the test images
belong to the other sub-dataset. Thus, for each sub-
ject, we perform (1) three same-sub-dataset train-
test experiments and (2) six cross-sub-dataset train-
test experiments.
Full dataset fMRI Encoding: Whenever we train
and test on the same dataset, we follow K-fold
(K=10) cross-validation. All the data samples from
K-1 folds were used for training, and the model
was tested on samples of the left-out fold.
Cross-data fMRI Encoding: In the BOLD5000
dataset, we have three sub-datasets: COCO, Im-
ageNet, and Scenes.
ImageNet images mainly
contain objects. Scenes images are about natural
scenes, while COCO images relate to both objects
and scenes. For each of the three sub-datasets, we
perform K-fold (K=10) cross-validation within the
sub-dataset.

4 Methodology

We trained a ridge regression-based encoding
model to predict the fMRI brain activity associ-
ated with the stimuli representation for each brain
region. Each voxel value is predicted using a sepa-
rate ridge regression model. Formally, we encode
the stimuli as X ∈ RN ×D and brain region voxels
Y ∈ RN ×V , where N denotes the number of train-
ing examples, D denotes the dimension of input
stimuli representation, and V denotes the number
of voxels in a particular region. Although ridge
regression is a very naïve way of modeling, it has
been the most popular brain encoding technique in
this line of work. We plan to experiment with other
forms of regression methods in the future.

The input stimuli representation can be obtained
using any of the following models: (i) pretrained
CNNs, (ii) pretrained text Transformers (iii) im-
age Transformers, (iv) late-fusion models, or (v)
multi-modal Transformers. The ridge regression
objective function for the ith example is given as
follows.

F + λ∥W ∥2
F

f (Xi) = min
W

∥Yi − XiW ∥2
Here, W are the learnable weight parameters, ∥.∥F
denotes the Frobenius norm, and λ > 0 is a tun-
able hyper-parameter representing the regulariza-
tion weight. λ was tuned on a small disjoint valida-

tion set obtained from the training.

the layer-wise features

Next, we discuss different input stimuli repre-
sentation methods. Pretrained CNNs and Image
Transformers encode image stimuli only, while Pre-
trained text Transformers encode text stimuli only.
Late fusion models and Multi-modal Transformers
encode both text and image stimuli.
Pretrained CNNs:
Inspired by the Algo-
nauts challenge (Cichy et al., 2019), we
from differ-
extract
ent pretrained CNN models
such as VG-
GNet19 (Simonyan and Zisserman, 2014) (Max-
Pool1, MaxPool2, MaxPool3, MaxPool4, Max-
Pool5, FC6, FC7, FC8), ResNet50 (He et al.,
2016) (Block1, Block2, Block3, Block4, FC), In-
ceptionV2ResNet (Szegedy et al., 2017) (Conv2D5,
Conv2D50, Conv2D100, Conv2D150, Conv2D200,
and EfficientNetB5 (Tan and
Conv2D\_7b),
(Conv2D2, Conv2D8, Conv2D16,
Le, 2019)
Conv2D24, FC), and use them for predicting fMRI
brain activity. We use adaptive average pooling on
each layer to get features for each image.
Pretrained text Transformers: RoBERTa (Liu
et al., 2019) builds on BERT’s language masking
strategy and has been shown to outperform several
other text models on the popular GLUE NLP bench-
mark. We use the average-pooled representation3
from RoBERTa to encode text stimuli.
Image Transformers: We used three image Trans-
formers: Vision Transformer (ViT), Data Effi-
cient Image Transformer (DEiT), and Bidirectional
Encoder representation from Image Transformer
(BEiT). Given an image, image Transformers out-
put two representations: pooled and patches. We
experiment with both representations.
Late-fusion models:
the
stimuli representation is obtained as a concate-
nation of image stimuli encoding obtained from
pretrained CNNs and text stimuli encoding
obtained from pretrained text Transformers. Thus,
we experiment with these late-fusion models:
ResNet50+RoBERTa,
VGGNet19+RoBERTa,
InceptionV2ResNet+RoBERTa
and Efficient-
NetB5+RoBERTa.
These models do not
information fusion but do
incorporate real
concatenation across modalities.
Multi-modal Transformers: We experiment
with these multi-modal Transformer models:
Contrastive Language-Image Pre-training (CLIP),

In these models,

3Average-pooled representation gave us better results com-

pared to using the CLS representation.

119Fig. 2: BOLD5000 Results: Pearson correlation coefficient (top figure) and 2V2 (bottom figure) between predicted
and actual responses across different brain regions using various models. Results are averaged across all participants.
VisualBERT performs the best.

Learning Cross-Modality Encoder Representations
from Transformers (LXMERT), and VisualBERT.
These Transformers take both image and text stim-
uli as input and output a joint visio-linguistic
representation. Specifically, the image input for
these models comprises region proposals as well
as bounding box regression features extracted from
Faster R-CNN (Ren et al., 2015) as input fea-
tures. These models incorporate information fusion
across modalities at different levels of processing
using co-attention and hence are expected to result
in high-quality visio-linguistic representations.
Hyper-parameter Settings: We used sklearn’s
ridge-regression with default parameters, 10-fold
cross-validation, Stochastic-Average-Gradient De-
scent Optimizer, Huggingface for Transformer
models, MSE loss function, and L2-decay (λ) as
1.0. We used Word-Piece tokenizer for the lin-
guistic Transformer input and Faster-RCNN (Ren
et al., 2015) for extracting region proposals. All
experiments were conducted on a machine with 1
NVIDIA GEFORCE-GTX GPU with 16GB GPU
RAM. We make our code publicly available1.

5 Experiments

5.1 Evaluation Metrics

We evaluate our models using popular brain encod-
ing evaluation metrics described in the following.

Given a subject and a brain region, let N be the
number of samples. Let {Yi}N
i=1 de-
note the actual and predicted voxel value vectors for
the ith sample. Thus, Y ∈ RN ×V and ˆY ∈ RN ×V
where V is the number of voxels in that region.

i=1 and { ˆYi}N

2V2 Accuracy =

N −1
(cid:88)

N
(cid:88)

i=1

j=i+1

I[{cosD(Yi, ˆYi) + cosD(Yj, ˆYj)}

1
NC2
< {cosD(Yi, ˆYj) + cosD(Yj, ˆYi)}]
where cosD is the cosine distance function. I[c] is
an indicator function such that I[c] = 1 if c is true,
else it is 0. The higher the 2V2 accuracy, the better.
computed as
Pearson Correlation (PC)
PC= 1
the
is
corr
N
correlation function.

is
i=1 corr[Yi, ˆYi] where

(cid:80)n

5.2 Do multi-modal Transformers outperform

other models?

Unfortunately, no previous work uses image Trans-
formers or multi-modal Transformers for brain en-
coding. StepEnCog (Oota et al., 2019) is a late-
fusion method, but it has a different setting where
the model expects voxel values per brain slice
rather than per brain region. Besides performing ex-
tensive evaluation using a large variety of models,
we also compare our results with those obtained
by two previously proposed baselines that lever-

120+++++\*\*\*\*\*xxxxx+++++\*\*\*\*\*xxxxx&&&&&$$$$$#####+++++\*\*\*\*\*xxxxx&&&&&+++++\*\*\*\*\*xxxxx&&&&&+++++PPALOCEarlyVisOPARSC00.10.2Multi-modal Transformers: + CLIP, \* LXMERT, x VisualBERTImage Transformers: + ViT+PooL, \* ViT+Patch, x DEiT+Pool, & DEiT+Patch, $ BEiT+Pool, # BEiT+PatchPretrained CNNs: + InceptionV2ResNet (Conv2D100), \* EﬃcientNetB5 (Conv2D16), x ResNet50 (Block3), & VGGNet19 (MaxPool5)Late-fusion models: + InceptionV2ResNet (Conv2D150)+RoBERTa, \* EﬃcientB5 (Conv2D8)+RoBERTa, x ResNet50 (Block3)+RoBERTa, & VGGNet19 (MaxPool1)+RoBERTaPretrained text Transformers: + RoBERTaPearson Correlation+++++\*\*\*\*\*xxxxx+++++\*\*\*\*\*xxxxx&&&&&$$$$$#####+++++\*\*\*\*\*xxxxx&&&&&+++++\*\*\*\*\*xxxxx&&&&&+++++PPALOCEarlyVisOPARSC0.60.72V2 AccuracyFig. 3: Pereira Results: Pearson correlation coefficient (top figure) and 2V2 (bottom figure) between predicted and
actual responses across different brain regions using a variety of models. Results are averaged across all participants.
VisualBERT performs the best.

age pretrained CNN models: (Blauch et al., 2019)
and (Wang et al., 2019) which use VGGNet.

We present the 2V2 accuracy and Pearson cor-
relation results for models trained with differ-
ent input representations (extracted from the best-
performing layer of every pretrained CNN model
and the last output layer of the Transformer model)
on the two datasets: BOLD5000 and Pereira in
Figs. 2 and 3, respectively. We also compare the re-
sults using many intermediate layer activations (not
just the best) for CNN models and the last layer of
Transformer models in Figs. 9 and 10 in the Ap-
pendix. Further, we also compare the results using
all intermediate layer activations for Transformer
models in Figs. 11 and 12 in the Appendix.
BOLD5000: We make the following observations
from Fig. 2: (1) On both 2V2 accuracy and Pear-
son correlation, VisualBERT is better across all
the models. (2) Other multi-modal Transformers
such as LXMERT and CLIP perform as good as
pretrained CNNs. We observed that image Trans-
formers perform worse than pretrained CNNs. Late
fusion models and RoBERTa has the least perfor-
mance. (3) Late visual areas such as OPA (scene-
related) and LOC (object-related) display a higher
Pearson correlation with multi-modal Transform-
ers, which is in line with the visual processing hier-
archy. A higher correlation with all the visual brain
ROIs with multi-modal Transformers demonstrates

the power of jointly encoding visual and language
information. (4) The patch representation of image
Transformers shows an improved 2V2 accuracy
and Pearson correlation compared to the Pooled
representation. (5) Both InceptionV2ResNet and
ResNet-50 have better performance among uni-
modality models.

In order to estimate the statistical significance
of the performance differences, we performed post
hoc pairwise comparisons for all the subjects across
the five brain ROIs. We found that VisualBERT
is significantly better than LXMERT (second-best
multi-modal Transformer) and InceptionV2ResNet
(best pretrained CNN) for all ROIs except EarlyVis.
Lastly, InceptionV2ResNet is significantly better
than BEiT (best image Transformer) for all ROIs.
Detailed p-values are mentioned in Table 3.

Pereira: We make the following observations from
Fig. 3: (1) Similar to BOLD5000, multi-modal
Transformers such as VisualBERT and LXMERT
perform better. (2) Lateral visual areas such as

Models compared
VisualBERT
LXMERT
VisualBERT vs.
tionV2ResNet
InceptionV2ResNet
BEiT

PPA

vs.

0.044\* 0.004\*

LOC EarlyVis OPA
0.049\*
0.076

RSC
0.029\*

Incep-

0.049\* 0.032\*

0.521

0.041\* 0.0354\*

vs.

0.041\* 0.003\*

0.014\*

0.188

0.203

Table 3: p-values for post hoc pairwise comparisons for
BOLD5000 dataset

121+++++++++\*\*\*\*\*\*\*\*\*xxxxxxxxx+++++++++\*\*\*\*\*\*\*\*\*xxxxxxxxx&&&&&&&&&$$$$$$$$$#########+++++++++\*\*\*\*\*\*\*\*\*xxxxxxxxx&&&&&&&&&+++++++++\*\*\*\*\*\*\*\*\*xxxxxxxxx&&&&&&&&&+++++++++Language\_LHLanguage\_RHVision\_BodyVision\_FaceVision\_ObjectVision\_SceneVisionDMNTP0.20.40.60.8Multi-modal Transformers: + CLIP, \* LXMERT, x VisualBERTImage Transformers: + ViT+PooL, \* ViT+Patch, x DEiT+Pool, & DEiT+Patch, x BEiT+Pool, & BEiT+PatchPretrained CNNs: + InceptionV2ResNet (Conv2D150), \* EﬃcientNetB5 (FC), x ResNet50 (Block3), & VGGNet19 (FC6)Late-fusion models: + InceptionV2ResNet (Conv2D150)+RoBERTa, \* EﬃcientB5 (FC)+RoBERTa, x ResNet50 (Block3)+RoBERTa, & VGGNet19 (MaxPool1)+RoBERTaPretrained text Transformers: + RoBERTaPearson Correlation+++++++++\*\*\*\*\*\*\*\*\*xxxxxxxxx+++++++++\*\*\*\*\*\*\*\*\*xxxxxxxxx&&&&&&&&&$$$$$$$$$#########+++++++++\*\*\*\*\*\*\*\*\*xxxxxxxxx&&&&&&&&&+++++++++\*\*\*\*\*\*\*\*\*xxxxxxxxx&&&&&&&&&+++++++++Language\_LHLanguage\_RHVision\_BodyVision\_FaceVision\_ObjectVision\_SceneVisionDMNTP0.50.60.70.82V2 AccuracyFig. 4: MAE between actual and predicted voxels: (a) left figure is zoomed on V2 and V3 brain areas for VisualBERT
on BOLD5000 subject 1. Note that V1 and V2 are also called EarlyVis areas, while V3 is also called LOC area. (b)
the right figure is for VisualBERT on the Pereira dataset subject 2.

Models compared
VisualBERT
LXMERT
VisualBERT vs. ResNet
ResNet vs. ViT

vs.

Language\_LH Language\_RH Vision\_Body Vision\_Face Vision\_Object Vision\_Scene Vision DMN
0.048\*

0.046\*

0.045\*

0.039\*

0.046\*

0.052

0.047\* 0.040\* 0.035\*

TP

0.049\*
0.009\*

0.038\*
0.043\*

0.048\*
0.041\*

0.048\*
0.047\*

0.078
0.046\*

0.217
0.042\*

0.048\* 0.046\* 0.049\*
0.038\* 0.022\* 0.023\*

Table 4: p-values for post hoc pairwise comparisons for Pereira dataset

Vision\_Object, Vision\_Body, Vision\_Face, and Vi-
sion areas display higher correlation with multi-
modal Transformers. A higher correlation with all
the visual brain regions, language regions, DMN,
and TP with multi-modal Transformers, demon-
strates that the alignment of visual-language under-
standing helps.

In order to estimate the statistical significance
of the performance differences, we performed
post hoc pairwise comparisons for all the subjects
across the nine brain ROIs. We found that Vi-
sualBERT is statistically significantly better than
LXMERT (second-best multi-modal Transformer)
for all ROIs except Vision\_Body. Further, Vi-
sualBERT is statistically significantly better than
ResNet (best pretrained CNN) for all ROIs except
Vision\_Object and Vision\_Scene. Lastly, ResNet
is statistically significantly better than ViT (best
image Transformer) for all ROIs. Detailed p-values
are mentioned in Table 4.

As further analysis, in Fig. 4, we show the mean
absolute error (MAE) between the actual and pre-
dicted voxels across brain regions using Visual-
BERT. Comparing with similar brain charts for
other models (shown in Figs. 13 and 14 in the Ap-
pendix), we notice that the error magnitudes are
very small for the majority of the voxels. We ob-
serve that MAE values are relatively higher for
EarlyVis areas and lowest for OPA for BOLD5000.

5.3 Model size vs. Efficacy Comparison

We plot a comparison of model size with Pear-
son Correlation averaged across all subjects for
BOLD5000 in Fig. 5. Compared to LXMERT, Vi-
sualBERT is not just more accurate but also much
smaller. VisualBERT is much more accurate than
image Transformers while being almost the same
size. Lastly, pretrained CNNs are smaller than
VisualBERT but are less accurate even when the
particular layer activations are cherry-picked. We
observe similar trends for the Pereira dataset, as
shown in Fig. 6. We hope that smaller models can
be helpful for faster fine-tuning of new datasets.

5.4 Single Stream vs. Dual Stream Models

Since single stream (VisualBERT) and dual-stream
(CLIP, LXMERT, and ViLBERT) models fuse lan-
guage and images at different times. We report

Fig. 5: BOLD5000: #Parameters vs. Avg Pearson Corr.

122ers (VisualBERT, LXMERT, and CLIP). We also
show results for a baseline method (Blauch et al.,
2019). We observe that (1) multi-modal Transform-
ers outperform the baseline results across all the
five brain regions for cross-data tasks. (2) PC score
is higher for the model trained on COCO and tested
on ImageNet in the object-selective visual area
LOC (lateral occipital cortex), which makes sense
since COCO has many objects. (3) Similarly, the
scene-selective brain areas such as RSC and OPA
have a higher correlation for the COCO-Scenes,
ImageNet-Scenes, and Scenes-Scenes tasks. (4)
EarlyVisual areas have a lower correlation than
other brain regions across the three tasks. (5) Over-
all, the models trained on COCO or ImageNet
report a higher correlation than those trained on
Scenes.

6 Cognitive Insights: Does Language

Influence Vision?

BOLD5000 dataset comprises brain responses from
visual areas (early visual, scene-related, and object-
related) when visual stimuli are presented to the
subjects. Although only visual information is
present in the stimuli, it is conceivable that par-
ticipants implicitly invoke appropriate linguistic
representations that, in turn, influence visual pro-
cessing (Lupyan et al., 2020). Thus, it is not surpris-
ing that computational models such as multi-modal
Transformers (VisualBERT, and LXMERT) that
learn a joint representation of language and vision
show superior performance on the ‘purely’ visual
response data in BOLD5000 (see Figs. 2 and 4(a)).
Further, the performance of these models is
naturally good in the case when text and image
are shown to the participants, and whole-brain re-
sponses are captured as in the case of the Pereira
dataset (see Figs. 3 and 4(b)). We further investi-
gate the role of different sub-ROIs of the language
and visual networks. For this, we compare the pre-
dicted responses of the best encoding model, i.e.,
VisualBERT, with the ground truth (observed) re-
sponses of various language and visual sub-regions
(see Fig 8). We notice that the classical language ar-
eas in the temporal gyrus (LMTG and LPTG) and
the inferior frontal gyrus (LIFG) are more accu-
rately predicted than the other sub ROIs of the lan-
guage network. These sub-ROIs (LMTG, LPTG,
and LIFG) are highly involved in language compre-
hension and semantic processing. Interestingly, the
second-best correlations are seen for multi-modal

Fig. 6: Pereira: #Parameters vs. Avg Pearson Corr.

the comparison of single-stream vs. dual-stream
with Pearson Correlation (PC) averaged across all
subjects for BOLD5000 in Table 5 (top). Com-
pared to dual-stream models (CLIP, LXMERT, and
ViLBERT), VisualBERT showcases much better
performance.

5.5

Is Linguistic Information Important in
Multi-Modal Transformers?

Is the improvement in prediction performance of
vision+language models over vision-only models
due to the added linguistic information? For exam-
ple, what happens if we randomize the language
captions in BOLD5000, feed the model the correct
image with a wrong caption, and train the encoding
model to predict the correct image-elicited brain
recording? We report the comparison of multi-
modal transformers with correct caption vs. ran-
dom caption using Pearson Correlation averaged
across all subjects for BOLD5000 in Table 5. We
observe that linguistic information is crucial to bet-
ter performance with multi-modal Transformers.

5.6 Cross-Data fMRI Encoding

Fig. 7 illustrates PC for cross-data encoding on
BOLD5000 using three multi-modal Transform-

PPA LOC EarlyVis OPA RSC
Models compared
0.139 0.082
0.083
0.095 0.134
CLIP
0.146 0.087
0.102
0.106 0.142
LXMERT
0.188
0.128
0.141 0.187
0.12
VisualBERT
0.087 0.045
0.052
0.057 0.078
ViLBERT
0.031 0.002
0.033
0.020 0.024
CLIP-Random
0.049 0.029
0.035
LXMERT-Random
0.035 0.041
0.109 0.060
0.062
VisualBERT-Random 0.072 0.102
0.017 0.017
0.013
0.018 0.011
ViLBERT-Random

Table 5: Single stream (VisualBERT) vs. Dual
stream (CLIP, LXMERT, and ViLBERT) models with
BOLD5000: Pearson correlation computed between
predicted and actual responses across different brain
regions. Results are averaged across all participants.
VisualBERT performs the best. The bottom four rows
display the model performance when a random-caption
is provided with the correct image as input.

123Fig. 7: Cross-Data Results for BOLD5000 dataset. VB=VisualBERT, LX=LXMERT, CL=CLIP, B=Baseline (Blauch
et al., 2019), INC=InceptionV2ResNet. CC=Train and test on COCO, CI=Train on COCO and test on ImageNet,
CS=Train on COCO and test on Scenes.)

integration areas in the temporo-parietal regions
(LAngG, LFus, LPar) and higher-order processing
and attention-related areas in the middle frontal
region (MFG).

In the visual sub-ROIs (Fig. 8), we observe that
the superior temporal sulcus (bilaterally but more
in the left: LSTS) is more accurately predicted than
other sub-ROIs. Surprisingly, LSTS is implicated
in various social processes, ranging from language
perception to simulating the mental processes of
others. Also, the sub-ROIs such as LLOC, LFFA,
LOFA, and LEBA have a higher correlation. These
areas are involved in more visual-related functions
such as object recognition, face perception, face
recognition, and body recognition.

Based on the intuition from the computational
experiments, we make the following testable pre-
diction for future fMRI experiments. Instead of a
passive viewing task, if participants were to per-
form a naming task/decision-making task on the
objects/scenes, we expect to see more pronounced
and focused activation in the visual areas during the
language-based task compared to passive viewing.

Fig. 8: Pearson correlation results across the Language
Sub ROIs and Visual Sub ROIs for Pereira dataset, be-
tween predicted and true responses across different brain
regions using a variety of models. Results are averaged
across all participants and are obtained using Visual-
BERT.

7 Conclusion

We studied the effectiveness of multi-modal mod-
eling for brain encoding. We found that Visual-
BERT, which jointly encodes text and visual input
using cross-modal attention at multiple levels, per-
forms the best. Our experiments on BOLD5000
and Pereira datasets lead to interesting cognitive
insights. These insights indicate that fMRIs reveal
reliable responses in scenes and object selection
visual brain areas, which shows that cross-view
decoding tasks like image captioning or image tag-
ging are practically possible with reasonable accu-
racy. We plan to explore this as part of future work.
We also plan to explore correlations between brain
voxel space and representational feature space in
the future. Finally, the combined strength of joint
(audio, vision, and text) modalities remains to be
investigated.

8 Ethical Statement

We reused publicly available datasets for this
work: BOLD5000 and Pereira. We did not col-
lect any new dataset. BOLD5000 dataset, ex-
cept the stimulus images and their original anno-
tations, is licensed under a Creative Commons 0
License. Please read their terms of use4 for more
details. Pereira dataset can be downloaded from
https://osf.io/crwz7/. Please read their
terms of use5 for more details. We do not foresee
any harmful uses of this technology.

4https://bold5000-dataset.github.io/

website/terms.html

5https://github.com/

CenterForOpenScience/cos.io/blob/master/
TERMS\_OF\_USE.md

124VB\_CCLX\_CCCL\_CCB\_CCVB\_CILX\_CICL\_CIB\_CIVB\_CSLX\_CSCL\_CSB\_CSVB\_IILX\_IICL\_IIB\_IIVB\_ICLX\_ICCL\_ICB\_ICVB\_ISLX\_ISCL\_ISB\_ISVB\_SSLX\_SSCL\_SSB\_SSVB\_SCLX\_SCCL\_SCB\_SCVB\_SILX\_SICL\_SIB\_SI0.000.020.040.060.080.100.120.140.160.18Pearson CorrelationPPALOCEarlyVisOPARSC0.30.40.5LmMFGLpMFGLaMFGLIFGLIFGorbLAngGLParLFusLATGLMTGLPTGPearson CorrelationLanguage0.40.50.60.7REBALEBARSTSLOFARFFALFFAROFALSTSLLOCRLOCRPPALRSCRRSCLPPARTOSLTOSPearson CorrelationVisualReferences

Andrew James Anderson, Elia Bruni, Alessandro
Lopopolo, Massimo Poesio, and Marco Baroni. 2015.
Reading visually embodied meaning from the brain:
Visually grounded computational models decode
visual-object mental imagery induced by written text.
NeuroImage, 120:309–322.

Hangbo Bao, Li Dong, and Furu Wei. 2021. Beit: Bert
pre-training of image transformers. arXiv preprint
arXiv:2106.08254.

Pinglei Bao, Liang She, Mason McGill, and Doris Y
Tsao. 2020. A map of object space in primate infer-
otemporal cortex. Nature, 583(7814):103–108.

Roman Beliy, Guy Gaziv, Assaf Hoogi, Francesca Strap-
pini, Tal Golan, and Michal Irani. 2019. From voxels
to pixels and back: self-supervision in natural-image
reconstruction from fmri. In Proceedings of the 33rd
International Conference on Neural Information Pro-
cessing Systems, pages 6517–6527.

Nicholas M Blauch, Filipe De Avila Belbute Peres, Juhi
Farooqui, Alireza Chaman Zar, David Plaut, and Mar-
lene Behrmann. 2019. Assessing the similarity of cor-
tical object and scene representations through cross-
validated voxel encoding models. Journal of Vision,
19(10):188d–188d.

Nadine Chang, John A Pyles, Austin Marcus, Abhinav
Gupta, Michael J Tarr, and Elissa M Aminoff. 2019.
Bold5000, a public fmri dataset while viewing 5000
visual images. Scientific data, 6(1):1–18.

Radoslaw Martin Cichy, Gemma Roig, Alex Andonian,
Kshitij Dwivedi, Benjamin Lahner, Alex Lascelles,
Yalda Mohsenzadeh, Kandan Ramakrishnan, and
Aude Oliva. 2019. The algonauts project: A platform
for communication between the sciences of biologi-
cal and artificial intelligence. arXiv e-prints, pages
arXiv–1905.

R Todd Constable, Kenneth R Pugh, Ella Berroya,
W Einar Mencl, Michael Westerveld, Weijia Ni, and
Donald Shankweiler. 2004. Sentence complexity and
input modality effects in sentence comprehension: an
fmri study. NeuroImage, 22(1):11–21.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE conference
on computer vision and pattern recognition, pages
248–255. Ieee.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers), pages 4171–
4186.

Alexey Dosovitskiy,

Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, et al. 2020.
An image is worth 16x16 words: Transformers
In International
for image recognition at scale.
Conference on Learning Representations.

Jon Gauthier and Roger Levy. 2019. Linking artificial
and human neural representations of language. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP), pages 529–539.

Umut Güçlü and Marcel AJ van Gerven. 2015. Deep
neural networks reveal a gradient in the complexity
of neural representations across the ventral stream.
Journal of Neuroscience, 35(27):10005–10014.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew
Zisserman, Oriol Vinyals, and Joao Carreira. 2021.
Perceiver: General perception with iterative attention.
arXiv preprint arXiv:2103.03206.

Kendrick N Kay, Thomas Naselaris, Ryan J Prenger,
and Jack L Gallant. 2008. Identifying natural images
from human brain activity. Nature, 452(7185):352–
355.

Tim C Kietzmann, Courtney J Spoerer, Lynn KA
Sörensen, Radoslaw M Cichy, Olaf Hauk, and Niko-
laus Kriegeskorte. 2019. Recurrence is required to
capture the representational dynamics of the human
visual system. Proceedings of the National Academy
of Sciences, 116(43):21854–21863.

Jonas Kubilius, Martin Schrimpf, Kohitij Kar, Rishi Ra-
jalingham, Ha Hong, Najib Majaj, Elias Issa, Pouya
Bashivan, Jonathan Prescott-Roy, Kailyn Schmidt,
et al. 2019. Brain-like object recognition with high-
performing shallow recurrent anns. Advances in
Neural Information Processing Systems, 32:12805–
12816.

Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui
Hsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-
ple and performant baseline for vision and language.
arXiv preprint arXiv:1908.03557.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692.

Gary Lupyan, Rasha Abdel Rahman, Lera Boroditsky,
and Andy Clark. 2020. Effects of language on visual
perception. Trends in cognitive sciences.

125Subba Reddy Oota, Vijay Rowtula, Manish Gupta, and
Raju S Bapi. 2019. Stepencog: A convolutional lstm
autoencoder for near-perfect fmri encoding. In 2019
International Joint Conference on Neural Networks
(IJCNN), pages 1–8. IEEE.

Francisco Pereira, Bin Lou, Brianna Pritchett, Samuel
Ritter, Samuel J Gershman, Nancy Kanwisher,
Matthew Botvinick, and Evelina Fedorenko. 2018.
Toward a universal decoder of linguistic meaning
from brain activation. Nature communications,
9(1):1–13.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from
natural language supervision. Image, 2:T2.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners.

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
Sun. 2015. Faster r-cnn: Towards real-time object
detection with region proposal networks. Advances
in neural information processing systems, 28:91–99.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-
drej Karpathy, Aditya Khosla, Michael Bernstein,
et al. 2015. Imagenet large scale visual recognition
challenge. International journal of computer vision,
115(3):211–252.

Martin Schrimpf, Idan Blank, Greta Tuckute, Carina
Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua
Tenenbaum, and Evelina Fedorenko. 2020a. The
neural architecture of language: Integrative reverse-
engineering converges on a model for predictive pro-
cessing. BioRxiv.

Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J
Majaj, Rishi Rajalingham, Elias B Issa, Kohitij Kar,
Pouya Bashivan, Jonathan Prescott-Roy, Franziska
Geiger, et al. 2020b. Brain-score: Which artificial
neural network for object recognition is most brain-
like? BioRxiv, page 407007.

Dan Schwartz, Mariya Toneva, and Leila Wehbe. 2019.
Inducing brain-relevant bias in natural language pro-
cessing models. Advances in Neural Information
Processing Systems, 32:14123–14133.

Karen Simonyan and Andrew Zisserman. 2014. Very
deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556.

Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke,
Inception-v4,
and Alexander A Alemi. 2017.
inception-resnet and the impact of residual connec-
tions on learning. In Thirty-first AAAI conference on
artificial intelligence.

Hao Tan and Mohit Bansal. 2019. Lxmert: Learning
cross-modality encoder representations from trans-
formers. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
5100–5111.

Mingxing Tan and Quoc Le. 2019. Efficientnet: Re-
thinking model scaling for convolutional neural net-
In International Conference on Machine
works.
Learning, pages 6105–6114. PMLR.

Bertrand Thirion, Edouard Duchesnay, Edward Hub-
bard, Jessica Dubois, Jean-Baptiste Poline, De-
nis Lebihan, and Stanislas Dehaene. 2006.
In-
verse retinotopy: inferring the visual content of im-
ages from brain activation patterns. Neuroimage,
33(4):1104–1116.

Hugo Touvron, Matthieu Cord, Matthijs Douze, Fran-
cisco Massa, Alexandre Sablayrolles, and Hervé
Jégou. 2021. Training data-efficient image trans-
In Inter-
formers & distillation through attention.
national Conference on Machine Learning, pages
10347–10357. PMLR.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998–6008.

Aria Wang, Michael Tarr, and Leila Wehbe. 2019.
Neural taskonomy: Inferring the similarity of task-
derived representations from brain activity. Advances
in Neural Information Processing Systems, 32:15501–
15511.

Leila Wehbe, Brian Murphy, Partha Talukdar, Alona
Fyshe, Aaditya Ramdas, and Tom Mitchell. 2014. Si-
multaneously uncovering the patterns of brain regions
involved in different story reading subprocesses. in
press.

Daniel Yamins, Ha Hong, Charles Cadieu, and James J
DiCarlo. 2013. Hierarchical modular optimization
of convolutional networks achieves representations
similar to macaque it and human ventral stream.

Daniel LK Yamins and James J DiCarlo. 2016. Us-
ing goal-driven deep learning models to understand
sensory cortex. Nature neuroscience, 19(3):356–365.

Daniel LK Yamins, Ha Hong, Charles F Cadieu,
Ethan A Solomon, Darren Seibert, and James J Di-
Carlo. 2014. Performance-optimized hierarchical
models predict neural responses in higher visual cor-
tex. Proceedings of the national academy of sciences,
111(23):8619–8624.

126for early visual areas in the lower layers whereas
higher visual areas such as LOC, OPA, and PPA
have an increasing correlation in higher layers. (iv)
This clearly indicates that the hierarchy of process-
ing of visual stimulus in the human brain is similar
to image Transformer layers.

We make the following observations from
Fig. 12: (i) The multi-modal Transformers, Visu-
alBERT, have consistent performance across the
layers from 1 to 12. (ii) The LXMERT model have
marginal decreasing performance from lower to
higher layers. (iii) The image Transformer, ViT,
has higher Pearson correlation for early visual ar-
eas in the lower layers whereas higher visual areas
such as Vision\_Body, Vision\_Face, and Vision\_Obj
have an increasing correlation in higher layers.

C Brain Maps for various models for

BOLD5000 Dataset

Fig. 13 shows mean absolute errors (MAE) be-
tween actual and predicted voxels for various mod-
els on the BOLD5000 dataset. Notice that the
magnitude of errors is much higher for a major-
ity of voxels, compared to that with the Visual-
BERT model as shown in Fig. 4(a). Also, the multi-
modal Transformers, VisualBERT (MAE range:
0 to 0.0181) and LXMERT (MAE range: 0 to
0.0188), have lower MAE compared to both im-
age Transformers (MAE range: 0 to 0.02) and pre-
trained CNNs (MAE range: 0 to 0.0236).

D Brain Maps for various models for

Pereira Dataset

Fig. 14 shows mean absolute errors (MAE) be-
tween actual and predicted voxels for various mod-
els on the Pereira dataset. Notice that the magni-
tude of errors is much higher for a majority of vox-
els, compared to that with the VisualBERT model
as shown in Fig. 14(a). Also, the multi-modal
Transformers, VisualBERT and LXMERT, and In-
ceptionV2ResNet+Conv2D150 have lower MAE
compared to both image Transformers and other
pretrained CNNs.

A Do multi-modal Transformers perform

better encoding compared to
intermediate layer representations
from pretrained CNNs?

We present the 2V2 accuracy and Pearson corre-
lation for models trained with representations ex-
tracted from the last layer of multi-modal Trans-
formers and all the lower to higher-level represen-
tations from pretrained CNNs on the two datasets:
BOLD5000 and Pereira in Figs. 9 and 10, respec-
tively.

We make the following observations from Fig. 9:
(1) With respect to 2V2 and Pearson correlation, the
multi-modal Transformer, VisualBERT, performs
better than all the internal representations of pre-
trained CNNs. (2) In the pretrained CNNs, inter-
mediate blocks have better correlation scores as
compared to lower or higher level layer representa-
tions. (3) Other multi-modal Transformers, CLIP,
and LXMERT, have marginal improvements over
all the models except intermediate blocks such as
Conv2D150 in InceptionV2ResNet.

We make the following observations from
Fig. 10:
(1) With respect to 2V2 and Pearson
correlation, the multi-modal Transformer, Visual-
BERT, performs better than all the internal rep-
resentations of pretrained CNNs. (2) Similar to
BOLD5000, the intermediate blocks have better
correlation scores as compared to lower or higher
level layer representations in the pretrained CNNs
on Pereira Dataset. (3) Other multi-modal Trans-
former, LXMERT, have equal performance with
intermediate blocks of each pretrained CNN model.

B Do multi-modal Transformers perform

better encoding in their layers?

Given the hierarchical processing of visual or
visual-language information across the Trans-
former layers, we further examine how these Trans-
former layers encode fMRI brain activity using
image and mulit-modal Transformers. We present
the layer-wise encoding performance results on
two datasets: BOLD5000 and Pereira in Figs. 11
and 12, respectively.

We make the following observations from
Fig. 11: (i) The multi-modal Transformer, Visu-
alBERT, have consistent performance across the
(ii) The LXMERT model
layers from 1 to 12.
have marginal decreasing performance from inter-
mediate layer (L7) to higher layers. (iii) The im-
age Transformers have higher Pearson correlation

127Fig. 9: BOLD5000: 2V2 (top Fig.) and Pearson correlation coefficient (bottom Fig.) between predicted and
true responses across different brain regions using variety of models. Results are averaged across all participants.
Pretrained CNN results are shown for all layers while multi-modal Transformer results are shown for last layers
only.

128Fig. 10: Pereira dataset: 2V2 (top Fig.) and Pearson correlation coefficient (bottom Fig.) between predicted and
true responses across different brain regions using variety of models. Results are averaged across all participants.
Pretrained CNN results are shown for all layers while multi-modal Transformer results are shown for last layers
only.

129Fig. 11: BOLD5000: 2V2 (left) and Pearson correlation coefficient (right) between predicted and true responses
across different brain regions using Transformer models. Results are averaged across all participants. The results
are shown for all layers of image and multi-modal Transformers. Note that LXMERT has only 9 layers.

130Fig. 12: Pereira: 2V2 (left) and Pearson correlation coefficient (right) between predicted and true responses across
different brain regions using Transformer models. Results are averaged across all participants. The results are
shown for all layers of image and multi-modal Transformers. Note that LXMERT has only 9 layers.

131Fig. 13: MAE between actual and predicted voxels zoomed on V2 and V3 brain areas for various models. Note that
V1 and V2 are also called EarlyVis area, while V3 is also called LOC area.

132LXMERTCLIPDEiTBEiTInceptionV2ResNetViTEfficientNetB5VGGNetFig. 14: MAE between actual and predicted voxels zoomed on V2 and V3 brain areas for various models. Note that
V1 and V2 are also called EarlyVis area, while V3 is also called LOC area.

133