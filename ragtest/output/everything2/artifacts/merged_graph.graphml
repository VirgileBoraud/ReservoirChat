<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d6" for="edge" attr.name="source_id" attr.type="string" />
  <key id="d5" for="edge" attr.name="description" attr.type="string" />
  <key id="d4" for="edge" attr.name="weight" attr.type="double" />
  <key id="d3" for="node" attr.name="entity_type" attr.type="string" />
  <key id="d2" for="node" attr.name="source_id" attr.type="string" />
  <key id="d1" for="node" attr.name="description" attr.type="string" />
  <key id="d0" for="node" attr.name="type" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="&quot;RESERVOIR COMPUTING&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Reservoir Computing is a method used for both regression and classification tasks."
"Reservoir Computing is a type of recurrent neural network architecture that includes techniques such as Echo State Networks and Liquid State Machines."
"Reservoir Computing is a neural network architecture that uses a reservoir of neurons to process input signals, with feedback connections helping to stabilize and control neuron activities."
"Reservoir Computing is a mechanism used in neural networks to ensure reliable and consistent outputs by balancing sensitivity to input signals and robustness against noise."
"Reservoir Computing is a type of recurrent neural network used for time series prediction and data analysis."
"Reservoir Computing is a method used for processing and analyzing data, involving a reservoir of neurons that update their state based on input data."
"Reservoir Computing is a field that focuses on the design and analysis of recurrent neural networks with a large reservoir of neurons."
"Reservoir Computing is a type of machine learning model that uses a reservoir of randomly connected nodes to process input data. The output is then computed using a simple linear regression model, making reservoir computing efficient and effective for various tasks."
"Reservoir Computing is a method used for processing data, involving nodes that can handle multiple inputs or outputs."
"Reservoir Computing is a field that focuses on the design and analysis of recurrent neural networks with a large number of interconnected processing nodes, known as reservoirs."
"Reservoir Computing is a type of recurrent neural network that efficiently handles temporal and sequential data, making it suitable for both regression and classification tasks."
"Reservoir Computing is a type of technology used for both regression and classification tasks, efficiently handling temporal and sequential data."
"Reservoir computing is a type of machine learning algorithm that uses a reservoir of neurons to process data."
"Reservoir Computing is a type of technology used for hyperparameter exploration and optimization."
"Reservoir Computing is a field of study that focuses on the use of reservoirs, such as Echo State Networks, for various applications."
"Reservoir Computing is a technique used for training connections, in this case using linear regression with a regularization coefficient of 10^-5."
"Reservoir Computing is a method used for processing and analyzing time-series data, involving a reservoir of neurons that evolve over time."
"Reservoir Computing is a method used for processing time-series data, involving a reservoir of units that evolve over time."
"Reservoir Computing is a temporal processing technique mentioned in the text, which uses random, untrained recurrent networks as excitable media."
"Reservoir Computing is a type of artificial neural network that uses a recurrent network structure to process input data."
"Reservoir Computing is a term that encompasses various recurrent neural network architectures, including Echo State Networks and Liquid State Machines."
"Reservoir Computing is a concept in the field of neural networks, involving the use of a reservoir of nodes with fixed connections and adaptive output weights."
"Reservoir Computing is a method used in modeling systems, which demands reservoirs of inordinate size for complex systems but offers alternatives for simpler systems."
"Reservoir Computing is a type of machine learning model used for time series prediction and data analysis."
"Reservoir Computing is a Machine Learning paradigm for training Recurrent Neural Networks, mentioned in the text."
"Reservoir Computing is a Machine Learning paradigm that focuses on training Recurrent Neural Networks while keeping the recurrent layer untrained."
"Reservoir Computing is a concept that involves chaining models together, using delayed connections, and incorporating external signals. It is a method used in the construction of RC models."
"Reservoir Computing is a method used for chaotic timeseries forecasting, as mentioned in the text."
"Reservoir Computing is a method used for time series prediction, which involves training a reservoir of neurons to learn the dynamics of the data."
"Reservoir Computing is a method used for time series prediction, involving a reservoir of neurons and connections."
"Reservoir Computing is a method that uses randomly sampled matrices to define the underlying recurrent neural network, requiring optimization of multiple meta-parameters."
"Reservoir Computing is a method that takes arrays of shape (timesteps, features) as input and returns an array of shape (timesteps, states). It allows for resetting or modifying reservoir state and feeding states to a node anytime."
"Reservoir Computing is a method used for training a readout node to predict the next value in a sequence."
"Reservoir Computing is a type of recurrent neural network that uses a sparse, randomly generated matrix to store information."
"Reservoir Computing is a type of machine learning model used in the code."
"Reservoir Computing is a method used for processing and analyzing time-series data, involving a reservoir of neurons."
"Reservoir Computing is a concept that summarizes Echo State Networks, Liquid State Machines, and the Backpropagation Decorrelation learning rule for RNNs, as they all share the idea of operating a random, large, fixed, recurring neural network with the input signal."

"Reservoir Computing is an idea that uses a randomly configured ensemble of spiking neural oscillators to obtain a desired target output from a recurrent neural network."</data>
      <data key="d2">069ae9388dfd52fec9c184c7168f64dd,136559fd2a1fbef4cc8a6b11abcb3eef,158f53cd85edbb4f2e4c77b78c5e7acc,1c462a6eef00aac37dc1ab33a689b930,22499cd4a0b7216dad5b05eb109fcb73,2386633041e820b604fc4457264b5a33,244217eb4738aae272df8949bdaaf131,257d4cf08ffc32b99856b6e31fa4221e,26d78bc91458f47d4053954505c45f92,2f4c992d69812866e6fce6dbb52d8612,4a9f33fa18891b67267b7615d61caaac,4d87a0d12ce76c7a493a24e1c4b06a83,56cde5dc9d350498c1544cd57733ca8f,6de297d888d10db4c987b5eafc6398b2,716940af834825642e01a3cb59a7e006,804bd76fa6f4950ef9a5cf8f0025fc1c,82a734e7c7ada95b1c99783140dd7168,83ded1f13bd74b00694092be69a83870,8553a88d9aaf4f71d359c721a1f6fa70,86a136730a696f1a817bd530dbff778d,87757855658e1d198ec49a3290760dd5,88ff8a7687e01f40b2c9d151b6e83d64,8e16fc97c32c39d7961b52e21b99dc53,8e1f4f13f617b982d272175296ec99d3,8ecf03267c90a64376f5040307d98195,9e84667b4aeb0789808517f0912043ce,a16039f06e545c915f8e7668c39c3e5c,a3368f9cab1f65643dba089af5a1f95e,b32958d42199d47252887dc7be40ab5a,b3361508c3e49b5bb3089f10e31d2c81,b5b73413fbe4ab8b61c4a939fe6c6a2b,baeb61b8c35e75d37a338fafd6a417fa,d622f95153798af8bb6f485db54aaea3,e9f7bc2274e59b0767e1172a848ddca9,ef85a7b1ca82dc1446ea71964d607a73,ef9bf350e25daa8f123b0b5c4d60de5f,f2d5625f36aa4cb036089ce89ec607eb,f730c6800099724052a2d061f3cd8c2e,fac681bdc38ae5829173c747ee6240fa</data>
    </node>
    <node id="&quot;JAPANESE VOWEL DATASET&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Japanese Vowel Dataset is composed of utterances of the Japanese vowel &#230;, from 9 different male speakers, used for classification tasks."</data>
      <data key="d2">86a136730a696f1a817bd530dbff778d</data>
    </node>
    <node id="&quot;MALE SPEAKERS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Male Speakers are the individuals who contributed utterances to the Japanese Vowel Dataset."</data>
      <data key="d2">86a136730a696f1a817bd530dbff778d</data>
    </node>
    <node id="&quot;M. KUDO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"M. Kudo is a reference mentioned in the text, likely an author or contributor."
"M. Kudo is a co-author of a reference mentioned in the text, contributing to the research on multidimensional curve classification."</data>
      <data key="d2">7cd25eb11825d9b9c2978d248997c3fe,86a136730a696f1a817bd530dbff778d</data>
    </node>
    <node id="&quot;J. TOYAMA&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"J. Toyama is a reference mentioned in the text, likely an author or contributor."
"J. Toyama is a co-author of a reference mentioned in the text, contributing to the research on multidimensional curve classification."</data>
      <data key="d2">7cd25eb11825d9b9c2978d248997c3fe,86a136730a696f1a817bd530dbff778d</data>
    </node>
    <node id="&quot;M. SHIMBO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"M. Shimbo is a reference mentioned in the text, likely an author or contributor."
"M. Shimbo is a co-author of a reference mentioned in the text, contributing to the research on multidimensional curve classification."</data>
      <data key="d2">7cd25eb11825d9b9c2978d248997c3fe,86a136730a696f1a817bd530dbff778d</data>
    </node>
    <node id="&quot;CLASSIFICATION TASK&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Classification Task is the goal of assigning to each utterance the label of its speaker in the Japanese Vowel Dataset."
"A Classification Task is a type of machine learning problem where the goal is to categorize input data into discrete classes."
"Classification Task is a machine learning task that involves predicting a categorical output variable based on input features."
"Classification Task is the machine learning task being performed, which involves categorizing data into different classes."</data>
      <data key="d2">35631fbf2ad11c53d75cb9b42e2c39b4,716940af834825642e01a3cb59a7e006,86a136730a696f1a817bd530dbff778d,f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </node>
    <node id="&quot;UCI MACHINE LEARNING REPOSITORY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"UCI Machine Learning Repository is the source of the Japanese Vowels dataset, which provides the audio signals for analysis."</data>
      <data key="d2">7cd25eb11825d9b9c2978d248997c3fe</data>
    </node>
    <node id="&quot;JAPANESE VOWELS DATASET&quot;">
      <data key="d0">"DATASET"</data>
      <data key="d1">"Japanese Vowels Dataset is a collection of audio signals used for analysis, with each signal represented as a 12-dimensional vector of Linear Prediction Coefficients (LPC)."
"The Japanese Vowels dataset is a collection of spoken utterances used for classification tasks, where the goal is to assign each utterance to one of nine speakers."
"The Japanese Vowels dataset is a collection of spoken utterances used for classification tasks, with the goal of assigning each utterance to one of nine speakers."
"Japanese Vowels Dataset is a dataset used for demonstrating machine learning models, such as ScikitLearnNode."
"The Japanese Vowels Dataset is used for classification with reservoir computing, mentioned in the text."</data>
      <data key="d2">35631fbf2ad11c53d75cb9b42e2c39b4,7cd25eb11825d9b9c2978d248997c3fe,870f29520f7a1c42eecb0c4ff855f09e,9f7337ee2d87543ced3b99dcae344b13,c1ba6d7a4f4bd16c4fd25baf07c9747c</data>
    </node>
    <node id="&quot;MULTIDIMENSIONAL CURVE CLASSIFICATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Multidimensional Curve Classification is a technique mentioned in a reference, used to categorize data points based on their passing-through regions."</data>
      <data key="d2">7cd25eb11825d9b9c2978d248997c3fe</data>
    </node>
    <node id="&quot;PATTERN RECOGNITION LETTERS&quot;">
      <data key="d0">"PUBLICATION"</data>
      <data key="d1">"Pattern Recognition Letters is a publication where a reference is mentioned, contributing to the research on multidimensional curve classification."</data>
      <data key="d2">7cd25eb11825d9b9c2978d248997c3fe</data>
    </node>
    <node id="&quot;CEPSTRA&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"cepstra is a feature extraction technique used in audio processing to convert a signal into a sequence of cepstral coefficients."</data>
      <data key="d2">79d5959f3f6471cad55498ab4a8a3176</data>
    </node>
    <node id="&quot;RESERVOIRPY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ReservoirPy is a library used for building and training reservoir computing models, which are designed to work on sequences."
"ReservoirPy is a library used to train a simple Echo State Network for solving a task."
"ReservoirPy is a library used for creating reservoir computing models, which are used for data analysis and prediction."
"ReservoirPy is a library used for creating and training reservoir computing models, which are used for classification of sequential patterns."
"ReservoirPy is a library for reservoir computing, which includes the development of Echo State Networks."
"ReservoirPy is a tool used for creating Echo State Networks, mentioned in the text."
"ReservoirPy is a platform that requires timeseries data to be formatted as NumPy arrays of shape (timesteps, features) to avoid unexpected results or errors."
"ReservoirPy is a system that uses Numpy and Scipy for all computations, storing data in Numpy arrays."
"ReservoirPy is a library mentioned in the text, which uses only Numpy and Scipy for all computations."
"ReservoirPy is a Python library for reservoir computing, providing tools for designing and training Echo State Networks."
"ReservoirPy is a Python library used for creating and analyzing reservoir computing networks, such as Echo State Networks (ESNs)."
"ReservoirPy is a library or framework that includes concepts such as 'from_state' and 'with_state' parameters, which are used to specify the initial state of a reservoir in different contexts."
"ReservoirPy is a library used for creating and working with reservoir computing networks."
"ReservoirPy is a software library or tool that supports the training and running of multiple reservoirs or nodes simultaneously, enhancing the overall computational efficiency of the model."
"ReservoirPy is a library used for creating and working with Echo State Networks, which includes the implementation of input-to-readout connections."
"ReservoirPy is a software library used for creating and working with Echo State Networks (ESNs)."
"ReservoirPy is a software library or module that provides tools for creating and working with reservoir computing networks."
"ReservoirPy is a library for creating and working with reservoir computing networks, including Echo State Networks."
"ReservoirPy is a Python library that provides tools for working with Echo State Networks (ESNs), including utilities for optimizing hyperparameters."
"ReservoirPy is a software library used for analyzing and processing data, mentioned in the text."
"ReservoirPy is a tool used for optimization, which accepts objective functions that must respect certain conventions."
"ReservoirPy is a tool used for hyperparameter optimization, which accepts objective functions with certain conventions."
"ReservoirPy is a Python library used for hyperparameter optimization in reservoir computing."
"ReservoirPy is a library used for hyper-parameter exploration and visualization in the context of Reservoir Computing."
"ReservoirPy is a library that provides tools and utilities for various tasks, including the Japanese Vowels dataset and functions for sequence-to-sequence encoding."
"ReservoirPy is a framework used for creating and working with reservoir models, which includes the integration of Scikit-Learn models."
"ReservoirPy is a Python library used for creating and simulating reservoir computing networks, which are used for tasks such as function approximation, time series prediction, and more."
"ReservoirPy is a library that provides tools for creating and working with reservoir computing models, used in the code to create a reservoir node."
"ReservoirPy is a library used for creating and training reservoir models."
"ReservoirPy is a library used for creating reservoir computing models, which includes the Reservoir node and ScikitLearnNode."
"ReservoirPy is a library for implementing Reservoir Computing architectures, with a focus on Echo State Networks, and offers features such as offline and online training, parallel implementation, and sparse matrix computation."
"ReservoirPy is a Python library for creating and training reservoir computing networks, with features such as offline and online training, parallel implementation, and graphical tools for hyperparameter exploration."
"ReservoirPy is a library for creating and working with reservoir computing models, such as Echo State Networks."
"ReservoirPy is a Python library for reservoir computing, used for timeseries prediction and analysis."
"ReservoirPy is a Python library for reservoir computing, used for installing packages and running Python Notebooks."
"ReservoirPy is a software library used for exploring hyperparameters in reservoirs."
"ReservoirPy is a library mentioned in the text, which is cited in a paper by Trouvain et al."
"ReservoirPy is a library developed and supported by Inria at Bordeaux, France, in the Mnemosyne group, for designing echo state networks."
"ReservoirPy is a library developed and supported by Inria at Bordeaux, France in the Mnemosyne group, for designing Echo State Networks."
"ReservoirPy is a Python library used for creating and analyzing reservoir computing systems, such as Echo State Networks."
"Reservoirpy is a library used for creating and working with reservoir computing models."
"ReservoirPy is a library used for reservoir computing, which includes tools for optimizing hyperparameters."
"ReservoirPy is a tool that provides users with wrappers around other tools for data analysis and forecasting."
"ReservoirPy is a library used for time series forecasting and hyperparameter optimization."
"ReservoirPy is a library mentioned in the code snippet, which could be related to the study on improving reservoirs."
"reservoirpy is a library used for reservoir computing, which includes functions for generating matrices and defining nodes."
"ReservoirPy is a library mentioned in the text, which is used to implement the IP Rule."
"ReservoirPy is a library used for implementing reservoir computing algorithms, in which IPReservoir is a node."
"ReservoirPy is a library mentioned in the text, which is used for implementing the ES&#178;N model."
"reservoirpy is a library used for reservoir computing, which is mentioned in the code."
"ReservoirPy is a library used for creating and training reservoir computing models."
"reservoirpy is a Python library for Reservoir Computing (RC) models design and training, with a focus on Echo State Networks (ESNs)."
"reservoirpy is a Python library for Reservoir Computing models design and training, with a focus on Echo State Networks, mentioned in the text."
"ReservoirPy is a Python library for Reservoir Computing, providing tools and algorithms for training and analyzing Echo State Networks."
"Reservoirpy is a library written in Python 3, tailored for RC networks design, with a focus on Echo State Networks (ESNs)."
"reservoirpy is a Python library for designing recurrent neural networks, with a focus on Echo State Networks, and offers features such as online/offline learning, feedback connections, and parallelism."
"reservoirpy is a library that contains various implementations of Reservoir Computing tools, defined as Node sub-classes."
"reservoirpy is a library that contains various implementations of RC tools, such as Reservoir, Ridge, LMS, and RLS."
"ReservoirPy is an open-source library for reservoir computing, providing nodes for reservoirs and various utilities."
"Reservoirpy is an open-source library for reservoir computing, providing features such as initialization functions, activation functions, and observables."
"reservoirpy is a software library for Reservoir Computing, used for chaotic timeseries forecasting."
"reservoirpy is a library mentioned in the text, used for reservoir computing tasks and timeseries forecasting."
"reservoirpy is a toolbox for applying Reservoir Computing techniques to time-based data, offering high-level and low-level APIs, reservoir architectures, readouts, and learning rules."
"reservoirpy is an open-source toolbox for applying Reservoir Computing techniques to time-series data, under development for future features."
"reservoirpy is a library that supports the implementation of Liquid State Machines (LSMs) and other tools for machine learning."
"Reservoirpy is a library used for reservoir computing, as mentioned in the text."
"ReservoirPy is a library or toolkit mentioned in the text, potentially used for data analysis or modeling."
"ReservoirPy is a Python library used for building and training Echo State Networks."
"ReservoirPy is a library used for training and running Echo State Networks (ESNs), which is mentioned in the text."
"ReservoirPy is a library mentioned in the text, which provides various chaotic timeseries for analysis."
"ReservoirPy is a library used for training the Echo State Network (ESN) model, which is used for data analysis and prediction."
"ReservoirPy is a library for creating and working with Reservoir Computing architectures."
"ReservoirPy is a library for creating and training Echo State Networks (ESNs), which is used in the provided code."
"ReservoirPy is a library used for creating and training reservoir computing models."
"ReservoirPy is a library used for creating and working with reservoir computing nodes and models."
"Reservoirpy is a library used for creating reservoirs and readouts, which hold parameters stored as Numpy arrays or Scipy sparse matrices."
"reservoirpy is a module that contains functions for creating weights from any scipy.stats distribution."
"ReservoirPy is a library used for reservoir computing, which includes functions to generate matrices."
"ReservoirPy is a library that provides tools for working with reservoirs, which are used in various applications such as echo state networks."
"ReservoirPy is a library or framework that includes nodes such as ESN and Ridge, and is used for creating models."
"ReservoirPy is a library used for creating and training reservoir computing nodes, including the ESN node."
"ReservoirPy is a library used for creating and working with reservoir computing models, such as Deep ESN."
"ReservoirPy is a library used for implementing the NVAR node, as demonstrated in a provided notebook."
"ReservoirPy is a tool mentioned in the text that assists in computing outer products and selecting unique monomials."
"ReservoirPy is a software tool used to compute the outer product and find unique combinations of input features, and to compute the readout weights using Tikhonov regression."
"ReservoirPy is a Python tool for defining, training, and using Reservoir Computing architectures, such as Echo State Networks (ESNs)."
"ReservoirPy is a library that uses Numpy and Scipy for all computations, and it is used to create Echo State Networks (ESNs)."
"ReservoirPy is a library mentioned for creating and working with ESNs, including the Reservoir and Ridge nodes."
"ReservoirPy is a library used for creating and working with reservoirs, such as the Reservoir class mentioned in the text."
"ReservoirPy is a library used for creating and working with reservoirs, which are used for data processing and analysis."
"ReservoirPy is a library that provides various nodes such as Reservoir, which can be triggered on single timesteps or complete timeseries."
"ReservoirPy is a library used for creating and training ESNs, containing nodes such as Reservoir and Ridge."
"ReservoirPy is a library used for creating and working with reservoirs, which are used for processing time-series data."
"ReservoirPy is a library used for creating and training Echo State Networks (ESNs)."
"ReservoirPy is a library used for creating and training Echo State Networks (ESNs)."
"ReservoirPy is a library used for creating and working with reservoir computing models."
"ReservoirPy is a library used for creating and working with reservoir computing models, which are used for data processing and prediction."
"ReservoirPy is a library used for creating and working with reservoirs, which are used in the context of echo state networks."
"Reservoirpy is a library or toolkit used for generating matrices, such as the Reservoir matrix in the provided code."
"ReservoirPy is a library for generating matrices used in reservoir computing, which is a type of recurrent neural network."
"ReservoirPy is a library used for creating and working with reservoir computers, which are used for tasks such as time series prediction and data analysis."
"ReservoirPy is a library used for creating and training echo state networks (ESNs)."
"ReservoirPy is a library or framework mentioned in the text, used for creating and training reservoir computing models."
"ReservoirPy is a library used for creating and working with reservoir models, which are used for data processing and analysis."
"ReservoirPy is a library used for reservoir computing, mentioned in the text."
"ReservoirPy is a library used for data preprocessing and analysis, as demonstrated in the provided code."
"ReservoirPy is a library used for building and training Echo State Networks."
"ReservoirPy is a library used for creating and training reservoir computing networks."
"ReservoirPy is an open-source library for creating and training reservoir computing models, including the ESN used in the provided code."
"ReservoirPy is a library used for creating and working with reservoir computing models."
"ReservoirPy is a library used for creating and working with reservoirs, which are used in the provided code."
"ReservoirPy is a library used for creating and training reservoir computing models, which are used for time series prediction and analysis."
"ReservoirPy is a library used for reservoir computing, which is mentioned in the text."
"ReservoirPy is a library used for reservoir computing and time series analysis, mentioned in the text."
"ReservoirPy is a library used for training and running reservoir computing models, such as the Echo State Network."
"ReservoirPy is a library used for creating and working with reservoir computing models."
"ReservoirPy is a library used for creating reservoir computing models, which are used for data processing and analysis."</data>
      <data key="d2">0036fb6f489e13c0db0f1c02bf3323be,00d22666fe697ffb66c2392939f45b39,0113164912437e96423379cb9c039f56,01f8dd8235ba0d4cf0837b5ea958ec95,069ae9388dfd52fec9c184c7168f64dd,0982b8d1eb1e636b19fa2e9d9361e566,09ea760dd2f000c961d1cfd4ea795da5,0b6c69085074b2cf23267eb149068b9f,0e0afab060f214d46062c9886e762002,136559fd2a1fbef4cc8a6b11abcb3eef,15e969cdc81fd313d389558850d0c8ec,18910a60b2547ec3133340f42c45bb47,1ea13fb2c1fff4954b699a8e2377f99f,20b16c2e1cb8813ade96fea5f9591631,2336a57d055095c6ffa9d156ddee0096,238049de5f28dca3e857a46a8b1bed03,244217eb4738aae272df8949bdaaf131,280cbdf53022bbaed48ccb34ebe142bc,295606b4bc5d12929a913a3c79f93734,296bb6eb4ef7d170f7224efcccbbbaf7,29f9b2e5fa311519b18e7aef31c68d0a,2a197220a94bac0b44fc0b07712e45ba,2d8ea1123f365fb047b024022ba4fdc4,2f4c992d69812866e6fce6dbb52d8612,31ee481e47ac3a0b970199e72a0e0d31,324a8f3fb4d19b91457a99999e6d3d17,325bd631a690a34736918180b01f6917,34b9ce80a22112b32e063179511af6e0,37549a8af907ce182bd36eec43002a7d,399d166d41a7d3d575222d30699a29e4,3a3b7a67b23341dcd1b04ec5b61683f6,3a8ed31ef360d5587cc6411e9fce89d4,3ae4ccee74392bfe317d8132e99a3aa9,3b4d50c051c177770830f7c0a6b3dd69,3bee7b78d0ab9582cc9bffe9e305df2e,3ff318aebcb07ca141d0a40730d96c7c,4073cafddb73621f26061385c5570659,414aba25cd4916c4a9e916beef385e81,41fa16855df7da666dc6fc38d2f8ee53,4d87a0d12ce76c7a493a24e1c4b06a83,4da651284dbab3f68dc3cae41e6e0311,4f7b43545046f0e6f9b6fb3816da1d79,50d4e4aab1823b8df6573ccf227f24d0,5430aac4404b66ea20503e5cac4d4328,5732296d26c7a572dc90d4af1172626a,58330f62da357197950f63388e4ceaff,593080a95ef7640b3925b07cad1bedd4,64b0ff9558a0f4794c16619aa76354c4,688ebc7151bc148ac24dc7e2727d7afe,6be085e79e86abc5b1a7eaff6bda1ec5,6daefaa8fbd5c1492f2d832d79841463,6de297d888d10db4c987b5eafc6398b2,701ffa843b8d26f96c23dae69e683b58,716940af834825642e01a3cb59a7e006,71f966d00b6d0eceb580d00b9cb86b1e,73e81fd6509a2ba400a8435793ade3c5,74c073137c970e32982756d008532cb8,75c1234e634cf2c009a116e4ee6c053e,77c3759b4ed32509aaf1403c6fa8030f,79d5959f3f6471cad55498ab4a8a3176,7b9936d57ece8ba985947a7aca12e2c7,7f2d69f9a9baca70ffd25a6865189206,7f70879016c133fe58e4838172a69613,8294eed5fc10df1c118f9afa266910e4,82de30f43839f4985de20a981b524af1,83ded1f13bd74b00694092be69a83870,83fafb2423a01afae7e522917d79ace9,8648b5740b93d805f139d9745e1171e8,870f29520f7a1c42eecb0c4ff855f09e,8965403859beb43a6ab7e5c8c916b857,8c66981c9d2009113219bbf2681f664c,8f2f2cfd667a304a288723de779c9bee,91704ce63f9ba41247fdc452a7a62ba6,94fd1ebf256db17e4ac2255b89caa473,96c47d9b671ce319abe9c6ba2b8ae122,9abdbd696e340cb5dd8c66ac5cd30c67,9b360c6a33aafa6827417de5bd4faa82,9f7337ee2d87543ced3b99dcae344b13,a16039f06e545c915f8e7668c39c3e5c,a1adb5de4156f0a4a448caf79056e886,a2b183778107462d474c53e4ec0a9221,a3a74dc4754a8c8b0730f808285893e2,a58317c7e13f27d513fc7671fd187ecb,a8df60a94e25d863b436f47f4f8e6a6d,af2db1cc5ab6b16acae2c93d3facb668,b03e2cc6fe2648e792c1d5f1ec5773a3,b11a9f7777c0232bfa7323ae82ad139b,b2beacacc8c190393e4583a69518378c,b3c8de6f33c2ebb84f0d2797933d0cad,b483c6bbce54156c724905b340aa2e85,bc2d4d6bb706c3d06ffd2c9c2f362104,c05906c1f12c4edfc32a04aa9935067e,c53825a1ab5e01a794a428988435a7a7,c5413fef3b2d7e4d688c66e6046b56c7,c82c9d05b211ff65131f70eb8cb13513,ca59dc92d05a69002373e96e0a373215,cb71a9bc3b00e7abcd1a53004abdea69,cdc64af0dde941250d89b191d0666c9b,d0b9bbbd7257712eafd2eda5db1d0a8d,d1047d9e322054de394b880adaf6b536,d4684af3c445d312afe4d838abc45502,d5e39e29b61f6ea0ffe0c868ba7a4252,d622f95153798af8bb6f485db54aaea3,d7ac2f6fb13af389417785f2f3152c52,e39809b687cd044a7918eca37727a188,e94f386a2ed7de2156b4864797cc199e,ead6383a44acd8ebd17907b85a910455,eb7a223eeb120e3fcc45a96a6018707d,ed28ba3543e07641536ff1eb5e0749dd,eebc9d7d2b66e3898b7d068c38fd200f,ef85a7b1ca82dc1446ea71964d607a73,f5358a50d00a1cac02dd4ad8fcb167ee,f70c7d3d89baaabbeaad57b58e379e08,f838f4cbb7060f4409ba2d174a396fb1,fcac967511cf2b019fd856e23d2e91d9,fe90abb0dde126fafbf44782aeb6738c,ff860bc63e3d697a6183c0b850689048</data>
    </node>
    <node id="&quot;TRANSDUCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Transduction is a sequence-to-sequence encoding technique used in machine learning to convert an input sequence into an output sequence."
"Transduction is a method used in sequence-to-sequence models to generate a sequence of output labels from input data."
"Transduction refers to a sequence-to-sequence model used in machine learning."
"Transduction refers to the process of transforming input data into output data, which is a key aspect of the sequence-to-sequence modeling task."</data>
      <data key="d2">688ebc7151bc148ac24dc7e2727d7afe,79d5959f3f6471cad55498ab4a8a3176,9414efd266e7135a2cdd7461a888b045,c05906c1f12c4edfc32a04aa9935067e</data>
    </node>
    <node id="&quot;SEQUENCE-TO-SEQUENCE MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Sequence-to-Sequence Model is a type of machine learning model that takes a sequence of inputs and produces a sequence of outputs, with one output per input timestep."
"sequence-to-sequence model is a type of machine learning model that maps input sequences to output sequences."
"Sequence-to-Sequence Model is a type of machine learning model used for tasks such as translation and speech recognition."</data>
      <data key="d2">688ebc7151bc148ac24dc7e2727d7afe,79d5959f3f6471cad55498ab4a8a3176,9414efd266e7135a2cdd7461a888b045</data>
    </node>
    <node id="&quot;AUDIO PROCESSING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Audio Processing is the manipulation and analysis of audio signals, which is being performed in this context to convert audio signals into sequences of cepstral coefficients."</data>
      <data key="d2">79d5959f3f6471cad55498ab4a8a3176</data>
    </node>
    <node id="&quot;SIMPLE ECHO STATE NETWORK&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Simple Echo State Network is a model used for sequence-to-sequence encoding, also known as transduction."</data>
      <data key="d2">75c1234e634cf2c009a116e4ee6c053e</data>
    </node>
    <node id="&quot;SEQUENCE-TO-SEQUENCE ENCODING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Sequence-to-Sequence Encoding is a method used to solve tasks involving sequences, such as audio to label conversion."
"Sequence-to-sequence encoding, also known as transduction, is a method used to solve tasks where a model is trained to encode each vector of input sequence into a new vector in the output space."
"Sequence-to-Sequence Encoding, also known as transduction, is a method used by ReservoirPy Nodes to convert a sequence of input data into a sequence of output labels."</data>
      <data key="d2">75c1234e634cf2c009a116e4ee6c053e,9f7337ee2d87543ced3b99dcae344b13,a6f2502b5336ffc8606e1167b2813004</data>
    </node>
    <node id="&quot;INPUT SEQUENCE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input Sequence refers to the sequence of audio data used as input for the model."
"Input Sequence is a series of inputs used to drive the ESN during the state harvesting stage of training."</data>
      <data key="d2">75c1234e634cf2c009a116e4ee6c053e,f18a060e6d2bb1da70432cbc71378770</data>
    </node>
    <node id="&quot;OUTPUT SEQUENCE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Output Sequence refers to the sequence of labels produced by the model, with one label per timestep."
"Output Sequence is a series of outputs obtained from the ESN during the training stage, which may involve teacher forcing."</data>
      <data key="d2">75c1234e634cf2c009a116e4ee6c053e,f18a060e6d2bb1da70432cbc71378770</data>
    </node>
    <node id="&quot;TRAINING DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Training Data is the dataset used to train the model, consisting of input sequences and corresponding output sequences."
"Training Data is the historical data used to train an ESN model, allowing it to learn the underlying patterns and dynamics."
"Training Data is a set of data used to train a machine learning model, mentioned in the text."
"Training Data refers to the data used to train the Echo State Network (ESN) model."</data>
      <data key="d2">1365a36c76afc697ac626fd0f784804a,4b78fdc153f982e64291112395c316c7,75c1234e634cf2c009a116e4ee6c053e,af2db1cc5ab6b16acae2c93d3facb668</data>
    </node>
    <node id="&quot;TEST DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Test Data is the dataset used to evaluate the model's performance, consisting of input sequences and corresponding output sequences."</data>
      <data key="d2">75c1234e634cf2c009a116e4ee6c053e</data>
    </node>
    <node id="&quot;RIDGE REGRESSION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Ridge Regression is a method used for the readout of the model, with a ridge parameter of 1e-6."
"Ridge Regression is a statistical regularization technique used to prevent overfitting in machine learning models, particularly useful when dealing with multicollinearity."
"Ridge Regression is a statistical method used to address multicollinearity in data, adjusting coefficients to improve model generalization."
"Ridge Regression is a regularization technique used to prevent overfitting in machine learning models."
"Ridge Regression is a regularization technique used to prevent overfitting in statistical models by adding a penalty term to the loss function."
"Ridge Regression is a regularization technique used to prevent overfitting in statistical modeling."
"ridge regression is a method used for offline training of an Echo State Network (ESN)."
"Ridge Regression is a regularization technique used in the output layer of reservoir computing models to prevent overfitting."
"Ridge Regression is a type of linear regression model used to prevent overfitting and improve prediction accuracy."
"Ridge Regression is a technique used to estimate the coefficients of a linear regression model, which helps to prevent overfitting by adding a penalty term to the loss function."
"Ridge Regression is a machine learning technique used to estimate the coefficients of a linear regression model, helping to avoid overfitting."
"Ridge Regression is a statistical method used for regression analysis, with a parameter called ridge that controls the amount of regularization applied to the model."
"Ridge Regression is a simple linear regression algorithm used to train the connections between the readout and the reservoir in an Echo State Network."
"Ridge Regression is a method used for regression analysis that adds a penalty term to the loss function to prevent overfitting."
"Ridge Regression is a regularization technique used to prevent overfitting in the readout layer of the Echo State Network."</data>
      <data key="d2">18e4624a6da9e8e6d9b9b2ed260bf9b2,1db191f05801d40d5a346febd10d3352,29ce72a8f609c311ebb852cc96aee54d,4b0fe17444b892af954d2561fec36eb6,593080a95ef7640b3925b07cad1bedd4,688ebc7151bc148ac24dc7e2727d7afe,711ec1b4879d910d0df0a477c9e240ba,75c1234e634cf2c009a116e4ee6c053e,77c3759b4ed32509aaf1403c6fa8030f,b5b73413fbe4ab8b61c4a939fe6c6a2b,c53825a1ab5e01a794a428988435a7a7,c838b1b4744bc0f400abf85f791950cf,ed28ba3543e07641536ff1eb5e0749dd,ef9bf350e25daa8f123b0b5c4d60de5f,fe90abb0dde126fafbf44782aeb6738c</data>
    </node>
    <node id="&quot;SPEAKER LABELING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Speaker Labeling is the process of assigning a label to each input sequence in a sequence-to-vector model, which is used for classification of sequential patterns."</data>
      <data key="d2">64b0ff9558a0f4794c16619aa76354c4</data>
    </node>
    <node id="&quot;SEQUENCE-TO-VECTOR MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Sequence-to-Vector Model is a type of model where inference is performed only once on the whole input sequence, and it is used for assigning one label to each input sequence."
"Sequence-to-Vector Model is a more advanced method used for classifying sequential patterns, where inference is performed only once on the whole input sequence."</data>
      <data key="d2">64b0ff9558a0f4794c16619aa76354c4,a6f2502b5336ffc8606e1167b2813004</data>
    </node>
    <node id="&quot;DATA ANALYSIS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Data Analysis is the process of inspecting, cleaning, transforming, and modeling data to discover useful information, draw conclusions, and support decision-making."
"Data Analysis is the process of inspecting, cleaning, transforming, and modeling data to discover useful information, patterns, and insights."
"Data Analysis is the process of inspecting, cleaning, transforming, and modeling data to discover useful information, draw conclusions, and support decision-making."
"Data Analysis is the process of inspecting, cleaning, transforming, and modeling data to discover useful information, draw conclusions, and support decision-making."
"Data Analysis is the process of examining and interpreting data to gain insights and make decisions, which is facilitated by the trained ESN model."
"Data Analysis refers to the process of inspecting, cleaning, transforming, and modeling data to discover useful information, draw conclusions, and support decision-making."
"Data Analysis is the process of inspecting, cleaning, transforming, and modeling data to discover useful information, draw conclusions, and support decision-making."</data>
      <data key="d2">29f9b2e5fa311519b18e7aef31c68d0a,41fa16855df7da666dc6fc38d2f8ee53,64b0ff9558a0f4794c16619aa76354c4,7b9936d57ece8ba985947a7aca12e2c7,d0b9bbbd7257712eafd2eda5db1d0a8d,d15f6d075c072f0335b5332f11c00299,ef9bf350e25daa8f123b0b5c4d60de5f</data>
    </node>
    <node id="&quot;PREDICTION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Prediction is the process of forecasting future outcomes based on past data, using reservoir computing models."
"Prediction is a concept mentioned in the text, referring to forecasting or estimating future events or outcomes based on current data."
"Prediction is the process of generating future values of the timeseries based on the learned patterns and dynamics of the input data."
"Prediction is a part of statistical inference, which involves transferring knowledge about a sample to make predictions about the population or future data points."
"Prediction is the process of using a trained model to make predictions about future data, which is a common application of the Echo State Network (ESN) model."

"Prediction is the process of using a trained reservoir computing model to make predictions about future data."</data>
      <data key="d2">14bccd672d2f8dd2cd7300581c8844fb,64b0ff9558a0f4794c16619aa76354c4,693e4d1e43289f46866236c10207a17e,8c520b4037fe01ffce62d46b67175e67,9261efcc24379d9c0b2d35a2fde8275d,c82c9d05b211ff65131f70eb8cb13513,d0b9bbbd7257712eafd2eda5db1d0a8d</data>
    </node>
    <node id="&quot;RIDGE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Ridge is a type of model used in reservoir computing for training the readout, which is a part of the reservoir computing model."
"ridge is a parameter explored by hp_space that represents the ridge, but its configuration is not provided in the given text."
"ridge is a parameter explored by hp_space, log-uniformly distributed between 1e-8 and 1e1."
"Ridge is a type of readout layer in the ESN that uses linear regression to train connections from the reservoir to the output layer."
"Ridge is a component of the reservoir computing model used in the code."
"Ridge is a component used in the provided code, likely a type of regularization technique."
"ridge is a parameter that specifies a regularization term, which is log-uniformly distributed between 1e-8 and 1e1."
"Ridge is a regularization technique used to prevent overfitting in a model by adding a penalty term to the loss function."
"Ridge is a model used for regression analysis, which is used as a readout in combination with ES2N."
"Ridge is a model used for regression analysis, with a parameter ridge that controls the amount of regularization."
"Ridge is a machine learning model used in the provided code."
"Ridge is a model used for regularization in machine learning, which is mentioned in the code."
"Ridge is a model or algorithm used in the system, likely a component of the larger organization."
"Ridge is a regularization technique used in the machine learning model, likely a technology or method used to prevent overfitting."
"Ridge is a model mentioned in the text, potentially related to data processing or analysis."
"Ridge is a regularization technique used in the ESN and ES^2N models to prevent overfitting."
"Ridge is a regularization technique used in machine learning models to prevent overfitting."
"Ridge is a tool in reservoirpy that learns connections through Tikhonov linear regression for a readout layer of neurons."
"Ridge is a concept used in machine learning, referring to a type of linear regression that includes a penalty term to reduce overfitting."
"Ridge is a component of the ESN neural network, used for readout and training."
"Ridge is a regularization technique used in the ESN model to prevent overfitting and improve generalization."
"Ridge is a machine learning algorithm used for regression tasks."
"Ridge is a type of readout used in the ESN system for making predictions."
"Ridge is a type of readout layer used in ESNs, which estimates the output based on the reservoir's state."
"Ridge is a type of linear regression model that applies a regularization term to the loss function to prevent overfitting."
"Ridge is a type of regression used in the context of reservoir computing models."
"Ridge is a regularization technique used in machine learning models to prevent overfitting."
"Ridge is a type of regularization used in the context of predicting timeseries, with a ridge parameter of 1e-7."
"Ridge is a type of node in ReservoirPy, which is used as a component in creating an ESN node."
"Ridge is a component of the ESN node in ReservoirPy, used for training the readout weights."
"Ridge is a type of readout used in the ESN model for time series prediction."
"Ridge is a component mentioned in the text, which is used in the construction of Deep Echo State Networks."
"Ridge is a type of regression model used for making predictions based on input data."
"Ridge is a regularization method used in the model for linear regression."
"Ridge is a model or technique mentioned in the text, used for analyzing the dynamics of the Double Scroll Attractor."
"Ridge is a machine learning model that can be trained using its fit() method to create a mapping from inputs to targets."
"Ridge is a type of regularization used in machine learning to prevent overfitting, often used in the context of linear regression."
"Ridge is a node in the ESN Model used as a readout, which is trained to make predictions based on reservoir activations."
"Ridge is a regularization technique used in machine learning to prevent overfitting."
"Ridge is a class from the ReservoirPy library used for creating a readout in the ESN model."
"Ridge is a regularization technique used in the ESN Model to prevent overfitting."
"Ridge is a type of regression model used for data prediction and analysis."
"Ridge is a type of organization or system used in the provided code, likely a part of a larger machine learning framework."
"Ridge refers to a type of linear regression that uses a regularization term to prevent overfitting, which is a common issue in machine learning."
"Ridge refers to a type of linear regression used in the readout stage of an echo state network (ESN)."
"Ridge is a term used in the context of reservoir computing, referring to a type of readout or output layer."
"Ridge is a component in a reservoir model, used for data processing and analysis."
"Ridge is a type of linear regression used as the readout in the ESN."
"Ridge is a type of regression model used in the code."
"Ridge is a regularization technique used in the ESN model to prevent overfitting and improve generalization."
"Ridge is a concept used in the provided code to describe a type of regularization used in the code."
"Ridge is a regularization technique used in the model, with a parameter ridge."
"Ridge is a machine learning algorithm used for regression tasks, which is used in the provided code."
"Ridge is a parameter in the Hyperopt configuration, which is mentioned in the text."
"Ridge is a component of the model used for output prediction."</data>
      <data key="d2">00d22666fe697ffb66c2392939f45b39,069ae9388dfd52fec9c184c7168f64dd,0753d4e507badadd900c522ee03ad28d,09198e939639c229c2c97555f65b12a7,0982b8d1eb1e636b19fa2e9d9361e566,09ea760dd2f000c961d1cfd4ea795da5,1298c65a923053e1de35aacddc13832c,136d135c710f6cf78a4c536d43276fe1,1ea13fb2c1fff4954b699a8e2377f99f,2035514bf3ab5b7f12ae1321972551f1,2336a57d055095c6ffa9d156ddee0096,2bcc39da2ecef3011cc3da428fca5dd5,31ee481e47ac3a0b970199e72a0e0d31,36e4df75a46fb977f9516f2d2f1f9bc2,3a8ed31ef360d5587cc6411e9fce89d4,3ae4ccee74392bfe317d8132e99a3aa9,3bee7b78d0ab9582cc9bffe9e305df2e,3ff318aebcb07ca141d0a40730d96c7c,4073cafddb73621f26061385c5570659,46dcc47b4358d3895c1eeb1182c6f997,4a7ca13b3f869961817e2aa723e67d24,5366a81a025c098744b5d6f1432c2fbc,59b469bdd618b3f36b3547f4f2b8a862,5cea9edfd65fcfa25a081554300b28cc,684b1edf65b327cc06ceb69ca1279d74,6a7bea5f60347ea864c06adc327829dc,6daefaa8fbd5c1492f2d832d79841463,72e6eee633bcb5b1458c4cee3975cee1,751b176a8d6149a853e597c65a6fe0cf,7b9936d57ece8ba985947a7aca12e2c7,7f2d69f9a9baca70ffd25a6865189206,7f70879016c133fe58e4838172a69613,80033e741d8e10abdcfe20dd17192152,80c9f51870e239404ed671ef0374f191,8294eed5fc10df1c118f9afa266910e4,82ff270b1bbdfe0ee11e603de1e326c7,8648b5740b93d805f139d9745e1171e8,8ade7819a5f8d1ec26e9bdbd059142e6,8c520b4037fe01ffce62d46b67175e67,90c1a399dd410f75f1f4bb03fe1f5f33,94fd1ebf256db17e4ac2255b89caa473,96366d7c23d50de6294c54c3444eac86,97f5d2e9d34b3b50f8e922fc4bb7f824,993a69efae014a8f8d6ec0c235104d46,a0feae89e52a4291db0a512a3a102d8e,a614fa3f8de82d4078f220e5f373f03a,abd3ca2db22004a5de548dc22010c4d4,adfc38e9dc5e6fd0fe67ce83dfa1f154,b03e2cc6fe2648e792c1d5f1ec5773a3,bf4eaad93f89884d02cdad6a50f145a6,c82c9d05b211ff65131f70eb8cb13513,e39809b687cd044a7918eca37727a188,f0c8d4d322d73f46464e3e9f6914f2ee,f1fc6fbc8158d3da070d55544041a2ca,f7f7dbc1e69b3b0e801bc5ba9c0cabca</data>
    </node>
    <node id="&quot;RESERVOIR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Reservoir is a component of reservoir computing models used for generating internal states based on input sequences."
"reservoir is a component used in the model, potentially responsible for generating states based on input data."
"Reservoir is a pool of randomly connected neurons in an Echo State Network, forming a genuine recurrent neural network."
"The Reservoir is a component of Echo State Networks that receives input signals and transforms them into high-dimensional representations."
"Reservoir is a type of artificial neural network used in ReservoirPy, characterized by bi-directional flow of information."
"The Reservoir is a component in a machine learning model that processes input data and generates internal states."
"Reservoir is a component of an Echo State Network (ESN) that processes the input data and generates dynamic representations."
"Reservoir is a component in reservoir computing that processes input data in a specific way to enhance the model's ability to capture various aspects of the data."
"Reservoir is a component that receives feedback from the readout and uses the most recent output information for current processing."
"Reservoir is a component of Echo State Networks (ESNs) that stores and processes input data."
"The Reservoir in an ESN is a recurrently connected network of nodes that help preserve information and capture temporal dynamics."
"Reservoir is a component of a reservoir computing model, used to create a reservoir node in the code."
"Reservoir is a component used in the machine learning pipeline, likely a part of the scikit-learn library or a custom implementation."
"Reservoir is a concept used in reservoir computing, representing a recurrent network with a sparse, random connectivity structure."
"Reservoir is a component of the ESN that encodes inputs in a high-dimensional space using a random recurrent network."
"Reservoir is a component of the reservoir computing model used in the code."
"Reservoir is a component in a system that processes input data, with its dynamics influenced by the Spectral Radius and Echo State Property."
"The Reservoir is a system described in the text, with properties such as Spectral Radius and Echo State Property, and parameters such as Input Scaling, Units, and Leak Rate."
"Reservoir is a component used in the provided code, likely a part of a machine learning model."
"Reservoir is a component of the IPReservoir model, used for processing input data."
"Reservoir is a component of the ESN system that must have the echo state property for the system to function correctly."
"Reservoir is a component of the leakyESN model, likely a part of a larger system or organization."
"Reservoir is a component of the machine learning model, likely a technology or method used in its architecture."
"Reservoir is a model mentioned in the text, potentially related to data processing or analysis."
"Reservoir is a component of the ESN machine learning model, used for time series prediction."
"Reservoir is a tool in reservoirpy that performs high dimensional embedding of timeseries using a recurrent pool of leaky neurons."
"Reservoir is a concept used in Reservoir Computing, which is a type of machine learning algorithm."
"Reservoir is a component of the ESN neural network, responsible for processing input data."
"Reservoir refers to a component of the ESN network that stores and processes information."
"Reservoir is a component of the ESN model that processes input data and generates a high-dimensional state space."
"Reservoir is a component of Echo State Networks, responsible for processing and storing input data."
"Reservoir is a component of the ESN system used for processing input data."
"Reservoir is a component of ESNs that processes input data and generates a rich representation for the readout layer."
"Reservoir is a component of the model that processes the input data."
"Reservoir is a concept used in the model, which processes the input data and sends it to the readout node."
"Reservoir is a component of ESNs that processes input data and generates internal states."
"Reservoir is a component of a reservoir computing model, which stores and processes information."
"Reservoir is a component of a reservoir computing model, responsible for processing input data."
"Reservoir is a type of model used in the context of predicting timeseries, with a learning rate of 0.5 and a spectral radius of 0.9."
"Reservoir is a component of the Reservoirpy library, which holds parameters stored as Numpy arrays or Scipy sparse matrices."
"Reservoir is a type of node in ReservoirPy, which is used as a component in creating an ESN node."
"Reservoir is a component of the ESN node in ReservoirPy, used for creating the reservoir computing network."
"Reservoir is a component of the ESN model that stores and processes data."
"Reservoir is a component mentioned in the text, which is used in the construction of Deep Echo State Networks."
"Reservoir is a component of reservoir computing models, used for processing input data."
"The Reservoir is a pool of artificial rate neurons randomly connected to their inputs and to themselves, forming a recurrent neural network."
"Reservoir is a component of an ESN that processes input data and generates internal activations."
"Reservoir is a component mentioned in the text, which is part of the ReservoirPy library. It is used to create a reservoir for an ESN."
"Reservoir is a node provided by ReservoirPy, which can be triggered on single timesteps or complete timeseries."
"Reservoir is a recurrent neural network used to process a timeseries, with the ability to reset or modify its state."
"Reservoir is a type of network that can perform operations without erasing node memory, allowing for tracking of internal activations."
"The Reservoir is a component mentioned in the text, likely a part of a larger system."
"The reservoir is a component of the ESN model that processes input data."
"Reservoir is a component of an ESN that processes input data and generates a high-dimensional representation of the input."
"Reservoir is a node in the ESN Model that processes input data."
"Reservoir is a system or network used in the context of processing time series data, such as a sine wave."
"Reservoir is a class from the ReservoirPy library used for creating the Echo State Network (ESN) model."
"Reservoir is a component of the ESN Model, which stores and processes information."
"Reservoir is a component of a reservoir computing model, which stores and processes data."
"Reservoir is a component of an echo state network that stores and processes information."
"Reservoir is a type of organization or system used in the provided code, likely a part of a larger machine learning framework."
"Reservoir is a concept used in the provided code, likely referring to a reservoir computation or a reservoir matrix."
"Reservoir refers to the core component of an echo state network (ESN), which stores and processes information."
"Reservoir is a term used in the context of reservoir computing, referring to a component of the model."
"Reservoir is a component of the ESN that processes input data."
"Reservoir is a component in the Generative Mode method, with properties such as units, leak_rate, spectral_radius, input_scaling, connectivity, input_connectivity, and regularization."
"Reservoir is a component of the model used for training and generating data."
"Reservoir refers to a core component of reservoir computing networks, which processes input data."
"Reservoir is a component of the Echo State Network (ESN) model that stores and processes information from the input data."
"Reservoir is a component of the ESN model that processes input data and generates a high-dimensional state space."
"Reservoir is a concept used in the provided code to describe a type of system used for computing and processing data."
"Reservoir is a system or network used in the text, likely in a computational context."
"Reservoir is a term used in the context of a neural network, likely referring to a component that stores and processes information."
"Reservoir is an organization or system that runs simulations with different leaking rates."
"Reservoir is a component in a model used for time series prediction, with parameters such as units, sr, and lr."
"Reservoir is a component of an Echo State Network (ESN), a type of recurrent neural network, used in the provided code."
"Reservoir is a component of the model used for processing input data."
"Reservoir is a component used in the code to process input data."
"Reservoir is a component mentioned in the text, but its specific role or function is not clearly defined."</data>
      <data key="d2">00d22666fe697ffb66c2392939f45b39,0753d4e507badadd900c522ee03ad28d,09198e939639c229c2c97555f65b12a7,09ea760dd2f000c961d1cfd4ea795da5,0c5a253fb2bcebe8674581a5dc12fd96,0d922ae20673124fc4588949e3863ed0,0e0afab060f214d46062c9886e762002,0e6f0f7cd882a638ecb571ef36068868,1298c65a923053e1de35aacddc13832c,1365a36c76afc697ac626fd0f784804a,1ea13fb2c1fff4954b699a8e2377f99f,1f30b86a46d4819603edc730df816c49,2336a57d055095c6ffa9d156ddee0096,2bcc39da2ecef3011cc3da428fca5dd5,2cba60e2f36479613bb0243a19f3a3b4,31ee481e47ac3a0b970199e72a0e0d31,324a8f3fb4d19b91457a99999e6d3d17,333ecf478bfbd4291de9f193bbf0443a,36e4df75a46fb977f9516f2d2f1f9bc2,3a3b7a67b23341dcd1b04ec5b61683f6,3ae4ccee74392bfe317d8132e99a3aa9,3bee7b78d0ab9582cc9bffe9e305df2e,3ff318aebcb07ca141d0a40730d96c7c,4073cafddb73621f26061385c5570659,46dcc47b4358d3895c1eeb1182c6f997,4b78fdc153f982e64291112395c316c7,4da651284dbab3f68dc3cae41e6e0311,548c454b31f852543b600df173bd44ab,58115d5a63315a84d9c8d4e6ddc98ffd,59b469bdd618b3f36b3547f4f2b8a862,5d3baa9818a4e01fe1196c43378a2cea,6a4432cd530b28770e2b903fe242a0d1,6daefaa8fbd5c1492f2d832d79841463,70db98fabc82fc96ecf8cc2c023b586b,711ec1b4879d910d0df0a477c9e240ba,72e6eee633bcb5b1458c4cee3975cee1,751b176a8d6149a853e597c65a6fe0cf,7b294b788fe5ee385d08c4aabe2ca71d,7f2d69f9a9baca70ffd25a6865189206,7f70879016c133fe58e4838172a69613,80c9f51870e239404ed671ef0374f191,8294eed5fc10df1c118f9afa266910e4,82f7e4647b9da5d5063fe92613f4fbcb,82ff270b1bbdfe0ee11e603de1e326c7,83fafb2423a01afae7e522917d79ace9,8ade7819a5f8d1ec26e9bdbd059142e6,8c520b4037fe01ffce62d46b67175e67,94fd1ebf256db17e4ac2255b89caa473,96366d7c23d50de6294c54c3444eac86,97f5d2e9d34b3b50f8e922fc4bb7f824,993a69efae014a8f8d6ec0c235104d46,9b360c6a33aafa6827417de5bd4faa82,a0feae89e52a4291db0a512a3a102d8e,a35f6cae32a3d24b18ee17ec0471a9d4,abd3ca2db22004a5de548dc22010c4d4,b03e2cc6fe2648e792c1d5f1ec5773a3,b101b38a87b2fcac0ff450a4e3f22143,b11a9f7777c0232bfa7323ae82ad139b,b957e1bf5bf175c7630222ca742c7933,bba680a0a7dd439bd5b0fe1547ffe040,bf4eaad93f89884d02cdad6a50f145a6,c4f5a27caf9dd9c1d972492c1147efa0,c82c9d05b211ff65131f70eb8cb13513,cb71a9bc3b00e7abcd1a53004abdea69,cc1fb6ca5695434ad0279c2606e928af,cdc64af0dde941250d89b191d0666c9b,cf15a09e77b695a117e1cca05461aea2,d25cd385546ec6a033287e75d65a551a,d58662ee42c14a0787d839ebfd0a6e9b,dc3bd3697a140b64d70e0e3ac6db6c7e,e39809b687cd044a7918eca37727a188,eadceb9674dd1ce90473d99e0b58e141,ed28ba3543e07641536ff1eb5e0749dd,f0c8d4d322d73f46464e3e9f6914f2ee,f1fc6fbc8158d3da070d55544041a2ca,f5358a50d00a1cac02dd4ad8fcb167ee,f7f7dbc1e69b3b0e801bc5ba9c0cabca,fa082948fa919150e9c06c6f5c1b53b0,ff860bc63e3d697a6183c0b850689048</data>
    </node>
    <node id="&quot;INPUT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Input is a component of reservoir computing models used for feeding input data into the model."
"Input refers to the data or information that is processed and stored in STM."
"An input is a data point or example used in a model, such as an image in image classification or a sentence in machine translation."
"Input is a node in ESNs that represents the input data to be processed."
"Input refers to the data that is fed into the model."
"Input is a component of the ESN model that provides data to the reservoir."
"Input is a component mentioned in the text, which is used in the construction of Deep Echo State Networks."
"Input refers to the data or signals used as the source for processing in reservoir computing models."
"Input is a component of a reservoir computing model that provides data to be processed and analyzed."
"Input refers to the data or signals fed into an echo state network (ESN) for processing and prediction."
"Input is a term used in the context of reservoir computing, referring to the data or signals fed into the model."
"Input is a component in a reservoir model, used for data processing and analysis."</data>
      <data key="d2">00d22666fe697ffb66c2392939f45b39,1ea13fb2c1fff4954b699a8e2377f99f,2336a57d055095c6ffa9d156ddee0096,58115d5a63315a84d9c8d4e6ddc98ffd,59b469bdd618b3f36b3547f4f2b8a862,65a025a8610d85bf0d2b6c4979eb7439,8648b5740b93d805f139d9745e1171e8,c82c9d05b211ff65131f70eb8cb13513,dd41fca2f283c4f8c8d1cba5b836da45,e39809b687cd044a7918eca37727a188,f0c8d4d322d73f46464e3e9f6914f2ee,f1fc6fbc8158d3da070d55544041a2ca</data>
    </node>
    <node id="&quot;JAPANESE VOWELS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Japanese Vowels is a dataset used for training and testing the reservoir computing model."
"Japanese Vowels refers to the vowel sounds used in the Japanese language, which are used in a task involving sequence-to-sequence modeling."
"Japanese Vowels is a dataset used for training and testing the model."</data>
      <data key="d2">1ea13fb2c1fff4954b699a8e2377f99f,688ebc7151bc148ac24dc7e2727d7afe,a0feae89e52a4291db0a512a3a102d8e</data>
    </node>
    <node id="&quot;Y_TRAIN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Y_train is an array storing a single label for each utterance, potentially representing a training dataset."
"Y_train is a component of a machine learning model used for storing labels or targets for training data."
"y_train is a dataset used for training the machine learning model, containing the target values."
"Y_train is a subset of the Y variable used for training the machine learning model."
"y_train is the target data used for training machine learning models."
"y_train is a variable representing the training target data used to train the ESN system."
"Y_train is a variable that likely contains the training target data for the ESN model."
"Y_train is a variable representing the target output data used for training the reservoir computing model."
"Y_train is a variable representing the future values of the sine wave, which is used as target data for training the ESN model."
"Y_train is a concept or variable used in the provided code, likely representing the training data output."
"y_train is a subset of the output data used for training the ESN."
"y_train is a subset of the target data used for training the ESN model."
"Y_train is a variable used to store the training target data, which is mentioned in the text."
"Y_train is a set of target data used for training the model."
"Y_train is a variable in the code that represents the training output data."
"Y_train is a dataset used for training a machine learning model, containing target labels for the input features in X_train."
"Y_train is a dataset used for training the Reservoir Model, containing the target values."</data>
      <data key="d2">0036fb6f489e13c0db0f1c02bf3323be,0970cd32ce54f6ee1180ab237fdcefe1,0982b8d1eb1e636b19fa2e9d9361e566,09ea760dd2f000c961d1cfd4ea795da5,1298c65a923053e1de35aacddc13832c,3ff318aebcb07ca141d0a40730d96c7c,71366a4c7e791080872ba783d3787bd7,72e6eee633bcb5b1458c4cee3975cee1,751b176a8d6149a853e597c65a6fe0cf,75e530c1a04e30b373dc7cc68e3ad819,7f2d69f9a9baca70ffd25a6865189206,80c9f51870e239404ed671ef0374f191,a0feae89e52a4291db0a512a3a102d8e,b101b38a87b2fcac0ff450a4e3f22143,b3361508c3e49b5bb3089f10e31d2c81,dc3bd3697a140b64d70e0e3ac6db6c7e,f70c7d3d89baaabbeaad57b58e379e08</data>
    </node>
    <node id="&quot;STATES_TRAIN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"states_train is a variable used in the training process, potentially representing a set of training states."</data>
      <data key="d2">b101b38a87b2fcac0ff450a4e3f22143</data>
    </node>
    <node id="&quot;READOUT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"readout is a component used in the model, potentially responsible for generating predictions based on states."
"Readout is a single layer of neurons in an Echo State Network that decodes the reservoir's activations to perform a task."
"Readout is a component that sends its state to the reservoir for feedback, allowing the reservoir to remember and incorporate past decisions or predictions."
"Readout is a component of Echo State Networks (ESNs) that generates predictions based on the reservoir's output."
"Readout refers to the final stage of the ESN network that produces the output prediction."
"Readout is a component of Echo State Networks, responsible for transforming the internal state of the network into output predictions."
"Readout is a component of the model that produces the output data."
"Readout is a concept used in the model, which receives and processes the output from the data and reservoir nodes."
"readout is a variable used in the provided code, which could represent a function or a process."
"The Readout is a single layer of neurons in charge of decoding the reservoir activations to perform a task."
"Readout is a component of an ESN that takes the activations of the reservoir and produces output predictions."
"Readout is a type of network that works similarly to a reservoir but needs to be fitted with data to learn connections from the reservoir to the readout neurons."
"The readout is a component of the ESN model that makes predictions based on the output of the reservoir."
"Readout refers to the algorithm used to extract meaningful information from the reservoir's output."
"Readout is a component of the Echo State Network (ESN) model that maps the reservoir's output to the desired output."</data>
      <data key="d2">0d922ae20673124fc4588949e3863ed0,1365a36c76afc697ac626fd0f784804a,324a8f3fb4d19b91457a99999e6d3d17,333ecf478bfbd4291de9f193bbf0443a,58115d5a63315a84d9c8d4e6ddc98ffd,711ec1b4879d910d0df0a477c9e240ba,7b294b788fe5ee385d08c4aabe2ca71d,7b8e1f350eefb392053be12f35fe7daf,83fafb2423a01afae7e522917d79ace9,b101b38a87b2fcac0ff450a4e3f22143,bf4eaad93f89884d02cdad6a50f145a6,cc1fb6ca5695434ad0279c2606e928af,cdc64af0dde941250d89b191d0666c9b,cf15a09e77b695a117e1cca05461aea2,d25cd385546ec6a033287e75d65a551a</data>
    </node>
    <node id="&quot;X_TEST&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"X_test is a variable used in the testing process, potentially representing a set of test input data."
"X_test is a component of a machine learning model used for processing test data."
"x_test is a dataset used for testing the machine learning model."
"X_test is a subset of the X variable used for testing the performance of the trained machine learning model."
"x_test is a subset of the input data used for testing the model's performance."
"x_test is the input data used for testing machine learning models."
"X_test is a variable representing the testing input data used to evaluate the performance of the trained ESN system."
"X_test is a subset of the input data used for testing the ESN."
"X_test is a subset of the input data used for testing the trained ESN model."
"X_test is a variable used to store the testing input data, which is mentioned in the text."
"X_test is a set of input data used for testing the model's performance."
"X_test is a variable in the code that represents the testing input data."
"X_test is a dataset used for testing the performance of a trained machine learning model, containing input features."
"X_test is a dataset used for testing the performance of the Reservoir Model."</data>
      <data key="d2">0036fb6f489e13c0db0f1c02bf3323be,0970cd32ce54f6ee1180ab237fdcefe1,0982b8d1eb1e636b19fa2e9d9361e566,1298c65a923053e1de35aacddc13832c,71366a4c7e791080872ba783d3787bd7,72e6eee633bcb5b1458c4cee3975cee1,75e530c1a04e30b373dc7cc68e3ad819,80c9f51870e239404ed671ef0374f191,a0feae89e52a4291db0a512a3a102d8e,b101b38a87b2fcac0ff450a4e3f22143,b3361508c3e49b5bb3089f10e31d2c81,dc3bd3697a140b64d70e0e3ac6db6c7e,f3e58b69b1a93175e3094a2ba65c0429,f70c7d3d89baaabbeaad57b58e379e08</data>
    </node>
    <node id="&quot;Y_PRED&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Y_pred is a variable used to store predicted labels, potentially representing a set of model predictions."
"y_pred is a variable representing the predicted values of the output in a machine learning context."
"Y_pred is a variable containing the predicted values generated by a model."
"Y_pred is a variable representing the predicted future values of the sine wave, which is the output of the trained ESN model."
"Y_pred is a variable in the code that represents the predicted output data."
"Y_pred is a dataset containing the predicted labels for the input features in X_test, generated by a trained machine learning model."</data>
      <data key="d2">0036fb6f489e13c0db0f1c02bf3323be,00648b24263129fdae8652f1a3339041,09ea760dd2f000c961d1cfd4ea795da5,9078b0f36522f21a9e8e1aadac48ed9c,b101b38a87b2fcac0ff450a4e3f22143,dc3bd3697a140b64d70e0e3ac6db6c7e</data>
    </node>
    <node id="&quot;Y_TEST&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Y_test is a variable used to store true labels, potentially representing a set of actual test data labels."
"y_test is a variable representing the true values of the output in a machine learning context."
"Y_test is a component of a machine learning model used for storing labels or targets for test data."
"Y_test is a variable containing the actual values used for testing the performance of a model."
"y_test is a dataset used for testing the machine learning model, containing the actual target values."
"Y_test is a subset of the Y variable used for testing the performance of the trained machine learning model."
"y_test is a variable representing the actual test data, which is being compared to predictions made by the ES^2N and ESN models."
"y_test is a variable representing the testing target data used to evaluate the performance of the trained ESN system."
"y_test is a subset of the output data used for testing the ESN."
"y_test is a subset of the target data used for testing the trained ESN model."
"y_test is the actual target data used for testing the performance of a model."
"Y_test is a variable used to store the testing target data, which is mentioned in the text."
"Y_test is a set of target data used for testing the model's performance."
"Y_test is a variable in the code that represents the testing output data."
"Y_test is a dataset used for testing the performance of a trained machine learning model, containing target labels for the input features in X_test."
"Y_test is a dataset used for testing the performance of the Reservoir Model, containing the actual target values."</data>
      <data key="d2">0036fb6f489e13c0db0f1c02bf3323be,00648b24263129fdae8652f1a3339041,0970cd32ce54f6ee1180ab237fdcefe1,0982b8d1eb1e636b19fa2e9d9361e566,12d680622df43439e6de83058b734953,1db5e6cd356c6066227de5e273de1abe,71366a4c7e791080872ba783d3787bd7,72e6eee633bcb5b1458c4cee3975cee1,75e530c1a04e30b373dc7cc68e3ad819,80c9f51870e239404ed671ef0374f191,9078b0f36522f21a9e8e1aadac48ed9c,a0feae89e52a4291db0a512a3a102d8e,b101b38a87b2fcac0ff450a4e3f22143,b3361508c3e49b5bb3089f10e31d2c81,dc3bd3697a140b64d70e0e3ac6db6c7e,f70c7d3d89baaabbeaad57b58e379e08</data>
    </node>
    <node id="&quot;ACCURACY_SCORE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"accuracy_score is a function used to evaluate the performance of the model, potentially comparing predicted labels with true labels."
"accuracy_score is a function used to evaluate the performance of a model by comparing its predictions to the actual values."
"accuracy_score is a function used to evaluate the performance of the trained ESN model."
"accuracy_score is a metric used to evaluate the performance of a model, comparing predicted outputs to actual targets."
"accuracy_score is a metric used to evaluate the performance of machine learning models."
"accuracy_score is a metric used to evaluate the performance of a machine learning model, comparing the predicted labels in Y_pred to the actual labels in Y_test."</data>
      <data key="d2">0036fb6f489e13c0db0f1c02bf3323be,00648b24263129fdae8652f1a3339041,1db5e6cd356c6066227de5e273de1abe,72e6eee633bcb5b1458c4cee3975cee1,9414efd266e7135a2cdd7461a888b045,b101b38a87b2fcac0ff450a4e3f22143</data>
    </node>
    <node id="&quot;DR. STEPHEN GROSSBERG&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Dr. Stephen Grossberg is a researcher at Boston University, MA, who specializes in recurrent neural networks."</data>
      <data key="d2">5e20e3cd48e20db98711d9948014c2d8</data>
    </node>
    <node id="&quot;BOSTON UNIVERSITY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Boston University is an educational institution where Dr. Stephen Grossberg works."</data>
      <data key="d2">5e20e3cd48e20db98711d9948014c2d8</data>
    </node>
    <node id="&quot;MA&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"MA is a geographical location, likely referring to Massachusetts, where Boston University is located."</data>
      <data key="d2">5e20e3cd48e20db98711d9948014c2d8</data>
    </node>
    <node id="&quot;RECURRENT NEURAL NETWORKS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Recurrent Neural Networks is a topic of research and discussion, with Dr. Stephen Grossberg focusing on biological recurrent neural networks found in the brain."
"Recurrent Neural Networks is a concept mentioned in the text."
"Recurrent Neural Networks are mentioned in the text as a component of Reservoir Computing."
"Recurrent Neural Networks are a type of neural network that processes sequential data by maintaining an internal state, allowing them to capture and model long-term dependencies."
"Recurrent Neural Networks (RNNs) are a type of neural network that process sequential data, consisting of a network of recurrently connected neurons."

"Recurrent Neural Networks are a type of neural network that use a feedback loop to maintain an internal state over time, allowing them to process sequential data."
"Recurrent neural networks are a type of artificial neural network that have connections that allow information to persist, enabling them to capture and model sequential data."
"Recurrent Neural Networks are a type of neural network that use an internal memory to process sequential data."
"Recurrent Neural Networks are a type of neural network that operates on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step."
"Recurrent Neural Networks are a type of dynamic system used for training and prediction tasks, with learning algorithms such as backpropagation through time and real-time recurrent learning."</data>
      <data key="d2">0bb54b1de8d2297293defe94addb8076,136559fd2a1fbef4cc8a6b11abcb3eef,158f53cd85edbb4f2e4c77b78c5e7acc,2bdd28d9e151597072c8490db69b9941,2f1161d1f711d264529aa7bddf81959b,5e20e3cd48e20db98711d9948014c2d8,6de297d888d10db4c987b5eafc6398b2,9e61c22432d6984a19da5840f64d417d,bc2d4d6bb706c3d06ffd2c9c2f362104,dbca0570761b1698d32f0c0bfb593b1a,f59839daadfb1f3832bb9f8d201a7126</data>
    </node>
    <node id="&quot;MCCULLOCH-PITTS MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The McCulloch-Pitts Model is a classical model of threshold logic systems that describes the interaction of nodes in a network."
"The McCulloch-Pitts Model is a neural network model developed by McCulloch and Pitts, which had a significant influence on the field of neural networks and the development of digital computers."</data>
      <data key="d2">25eb64dbb12dee61a753085741ee91d4,5838adba6968ede203f6820ddc368bc4</data>
    </node>
    <node id="&quot;BINARY SYSTEMS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Binary Systems are a type of recurrent neural network that were inspired by neurophysiological observations showing that signals between many neurons are carried by all-or-none spikes."</data>
      <data key="d2">5838adba6968ede203f6820ddc368bc4</data>
    </node>
    <node id="&quot;LINEAR SYSTEMS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Linear Systems are a type of recurrent neural network that use linear operations to combine inputs and produce outputs."</data>
      <data key="d2">5838adba6968ede203f6820ddc368bc4</data>
    </node>
    <node id="&quot;CONTINUOUS-NONLINEAR SYSTEMS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Continuous-Nonlinear Systems are a type of recurrent neural network that use continuous and nonlinear operations to combine inputs and produce outputs."</data>
      <data key="d2">5838adba6968ede203f6820ddc368bc4</data>
    </node>
    <node id="&quot;GROSSBERG&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Grossberg is a researcher who has made contributions to the field of recurrent neural networks."
"Grossberg is a researcher who discovered laws of adaptive behavior in real time, leading to the foundation of biological neural network research."
"Grossberg is a prominent figure in neural network research, having made significant contributions to the development of the Additive Model."
"Grossberg is a researcher mentioned in the text, contributing to the development of neural network models and equations."
"Grossberg is a person mentioned in the text, who introduced the generalized STM equation and other neural models."
"Grossberg is a researcher mentioned in the text who has contributed to the development of neural models."
"Grossberg is a researcher mentioned in the text who has contributed to the understanding of neural networks and their functions."
"Grossberg is a person mentioned in the text as the author of various works related to MTM and its applications."
"Grossberg is mentioned as an author and a researcher, but their specific role or function in the context of the text is not explicitly described."
"Grossberg is a person mentioned in the text who has introduced variants of gated steepest descent learning, such as Outstar Learning and Instar Learning."
"Grossberg is a person mentioned in the text who has contributed to the development of neural network learning algorithms such as Outstar Learning and Instar Learning."
"Grossberg is a researcher who introduced ART, combining Instars and Outstars in his work, and also used them in his earlier research."
"Grossberg is a person mentioned in the text who has contributed to the development of ART and related concepts."
"A person who proposed the on-center off-surround network as a solution to the noise-saturation dilemma."
"Grossberg is a researcher who has generalized the feedforward on-center off-surround shunting network equations, generating many useful properties."
"Grossberg is a researcher mentioned in the text, known for his contributions to the shunting network equations and their physiological interpretation."
"Grossberg is a person mentioned in the text, possibly a researcher or author."
"Grossberg is a researcher mentioned in the text, contributing to the development of various concepts and theories."
"Grossberg is a researcher who proved theorems showing how the choice of feedback signal function transforms an input pattern before it is stored persistently in STM."
"Grossberg is an author who has developed the Competitive Learning (CL) and Adaptive Resonance Theory (ART) models, and has been mentioned in the context of shunting dynamics."
"Grossberg is a researcher mentioned in the text, known for his work on competitive dynamical systems and theorems for rate-based neurons."
"Grossberg is a researcher who developed a mathematical method to classify the dynamics of competitive systems and studied a general problem of competition, decision, and consensus."
"Grossberg is a scientist who introduced a class of bRNNs to study the problem of how complicated a system can be and still generate order."
"Grossberg is a researcher who posed questions about the trade-off between global consensus and local signals, and introduced a class of bRNNs that generate global consensus."
"Grossberg is a researcher who proved that all trajectories in a specific type of system are stored in STM, indicating a significant contribution to the field."
"Grossberg is a reference to a specific author who is mentioned in the context of the Theorem."
"Grossberg is a contributor to the Cohen-Grossberg Model and the Liapunov Function, known for their work on the adaptation level systems."
"Grossberg is mentioned in the text as an author of several works that are referenced."
"Grossberg is a researcher mentioned in the context of multiple models, including the LAMINART Family, LIST PARSE, and TELOS models."
"Grossberg is a co-author of a study that introduces the TELOS Model and its components."
"Grossberg is a researcher mentioned in the text who has published on spatial pattern learning and signal transmission between cells."
"Grossberg is a researcher mentioned in the context of the Unbiased Spatial Pattern Learning Theorem."
"Grossberg is a researcher who has made significant contributions to the field of associative learning and pattern recognition."
"Grossberg is a person mentioned in the text, who has developed the Competitive Learning or Self-Organizing Map Network and Adaptive Resonance Theory."
"Grossberg is a researcher who introduced Adaptive Resonance Theory and other learning models."
"Grossberg is a person who introduced the passive decay associative law in the 1960s."
"Grossberg is a person mentioned in the text who coined the term 'stability-plasticity dilemma' in relation to Adaptive Resonance Theory."
"Grossberg is a researcher and author known for his contributions to Adaptive Resonance Theory and other areas of cognitive science."
"Grossberg is a researcher who noted the importance of learning list chunks for the usefulness of working memory and predicted that all WMs are designed to solve the temporal chunking problem."
"Grossberg is a researcher who predicted two constraints for stable learning and memory of list chunks: the LTM Invariance Principle and the Normalization Rule."
"Grossberg is a researcher who predicted the LTM Invariance Principle and the Normalization Rule to support stable learning and memory of list chunks."
"Grossberg is a researcher mentioned in the text who has contributed to the analysis of working memories and long-term memory of list chunks."
"Grossberg is a researcher whose work has been cited in the context of Item-and-Order working memory."
"Grossberg is a researcher whose predictions have been supported by accumulating evidence that all working memories have a similar design to enable stable list chunks to be learned."
"Grossberg is a researcher whose work on working memory networks and list chunking networks is referenced in the text."
"Grossberg is a psychologist who developed a model of working memory and contributed to the understanding of serial verbal learning."
"Grossberg is a researcher mentioned in the text who has made contributions to the understanding of Working Memory and Short-Term Memory."
"Grossberg is a researcher who proved that the TMS is smaller than the IMS in 1978a."
"Grossberg is mentioned as an author of several studies on models for learning of temporal order in experimental psychology."
"Grossberg is a researcher who developed the concept of Avalanche."
"Grossberg is a researcher who proposed the inclusion of command cells in circuits to allow for sensitivity to environmental feedback."
"Grossberg is a person mentioned in the text who describes a series of mechanisms that modulate Avalanche performance."
"Grossberg is a researcher mentioned in the text, describing mechanisms that modulate performance and sensitivity to environmental feedback."
"Grossberg is a researcher who has made significant contributions to the development of the Cognitive-Emotional-Motor (CogEM) Theory and related models."
"Grossberg is a researcher mentioned in the text, known for his contributions to the study of brain mechanisms."
"Grossberg is a person who has made significant contributions to the field of Self-Organizing Avalanches, including mathematical analyses of serial learning."
"Grossberg is a researcher mentioned in the text, known for contributions in the field of data and neural networks."</data>
      <data key="d2">01e2a32da700813f593038a23a618e55,03fedd128d2ad4ec7e1e1a60d26ba4f5,0af3b52f2586c4e957aee493160223ba,0c9db6cd87deaca2e432c260d775349c,0dd75f3ca11854714bdbfc8a96ccf256,1976b19f768a8fdf37207b680c3b2b40,1c0f47f0b77faab56cbeba0e1e3e7e70,2061ae5039f379b5837d33a062f72ad1,24771832864aa38abd6aebec04b13a10,254fd1fbc0a719a86b8021fa759d22ea,2cbd29d6f0019f0c85bee43779ae8f4d,2e76149ff772441e6627913bc1df5000,2ea6b3379a87077d75e5c45024f4f3e2,3235445917507f02710bd66cc8368194,3281409fdcd18b09ca3109260ddb96d9,32b8b59687b8d1556ec90c99a090653c,3334f6dcd53b71cf3ceb7648ead24d5a,392028b79561bd7471cb68e7c9258b1e,3a64c8c26895f111f00a349dd69bb505,3d5b88f7f81ed9e14f07335bbef17020,4364aa6091e1966365fa889b34f5cf90,43cf5e32e2df964318d03574e6cd6cdc,47d1d12642cdab6e9a5d21c184f83c9c,4959d1559344e462a6a7463fd3273659,4c7e78f7237cb3420e70c7749bb259f9,554e8565591507441cecaa652cb926db,5838adba6968ede203f6820ddc368bc4,597668e07c7554bd2d0cb29399285a39,5d6b6e0d1a9ace28e21dce2cb0ac78c0,644602009dec8474bb5cd4702b391d3e,653b7986c4757bd5d0a251369187efa6,6648b18760b8b182e1097ad15c4df685,75495c1fc835d41adf5afcb01e8e520a,7aeda101aa8aba76f319932f0bd568f7,7ba0dfde8cc54bb1dcf66b46fcdd88f8,7beb44dd43aea0791fcb35806356ddb3,88a3f14024e29666891496bb6cd7d0e4,91f030f6c14c673e6d029c9bf1a66515,97a9ce754fa34aaf01d6cce57560b247,97ad8d6e6e3bf27ae6a7b457af9b312e,9b30fc06ca06f49c2faa238da7eddc6f,a9285aaa34de96a8fa62903437d2f3c4,b1636ec22c34ef50b57dec32239c6535,b286a9022774f24a400744b2a1b08bab,b69b23b14e0feccb488ba5412db0824c,bb19119aa74e1f624e402fcef651aac2,be0954a6263de67c84da3141d95de445,c3257facbf1b0a5da49d6a115f66df87,c486b91dc15a126174fe546094568aaa,c580fa74e3c36285cfae7df56340a990,dcd38cdc6195b2bbf41d936af0bf1f5f,df77d35da87a38cae0984a42b9a1d41c,eb6c9a7d24cc59ff93d554093a4360a4,f2468cda326d1ca11c98f2fbde186400,f290960776c5ec561653e90d2ac6751b,f373521b781482587f80fffc5623a1f9,fba20e559e6ecb61ebbf3a805e6d072c</data>
    </node>
    <node id="&quot;SHORT-TERM MEMORY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Short-Term Memory refers to the activities or traces of nodes in a network, which are described by the McCulloch-Pitts Model."
"Short-Term Memory, or STM, is a type of memory that retains information for a short period of time."
"Short-Term Memory is a component of Working Memory that holds information for brief periods, typically lasting around 30 seconds."
"Short-Term Memory (STM) is a cognitive system that temporarily stores information for immediate use."</data>
      <data key="d2">01e2a32da700813f593038a23a618e55,3d5b88f7f81ed9e14f07335bbef17020,5838adba6968ede203f6820ddc368bc4,5c4e24fc9bd10d0bd59a84d56f960cf9</data>
    </node>
    <node id="&quot;JOHN VON NEUMANN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"John von Neumann is a mathematician and computer scientist who contributed to the development of the digital computer, influenced by the McCulloch-Pitts Model."</data>
      <data key="d2">25eb64dbb12dee61a753085741ee91d4</data>
    </node>
    <node id="&quot;FRANK ROSENBLATT&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Frank Rosenblatt is a psychologist and neuroscientist who developed the continuous-time STM equation used in the classical Perceptron model."</data>
      <data key="d2">25eb64dbb12dee61a753085741ee91d4</data>
    </node>
    <node id="&quot;WARREN MCCULLOCH&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Warren McCulloch is a neurophysiologist who, along with Walter Pitts, developed the McCulloch-Pitts Model, a foundational model in neural networks."</data>
      <data key="d2">25eb64dbb12dee61a753085741ee91d4</data>
    </node>
    <node id="&quot;WALTER PITTS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Walter Pitts is a neurophysiologist who, along with Warren McCulloch, developed the McCulloch-Pitts Model, a foundational model in neural networks."</data>
      <data key="d2">25eb64dbb12dee61a753085741ee91d4</data>
    </node>
    <node id="&quot;FRANK CAIANIELLO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Frank Caianiello is a neuroscientist who developed a binary STM equation influenced by activities at multiple times in the past."</data>
      <data key="d2">25eb64dbb12dee61a753085741ee91d4</data>
    </node>
    <node id="&quot;CLASSICAL PERCEPTRON MODEL&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">25eb64dbb12dee61a753085741ee91d4</data>
    </node>
    <node id="&quot;BINARY STM EQUATION&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">25eb64dbb12dee61a753085741ee91d4</data>
    </node>
    <node id="&quot;CAIANIELLO&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Caianiello is an organization that introduced equations to change the weights in a learning model."</data>
      <data key="d2">9e901f71d0d2339294da133518f2162f</data>
    </node>
    <node id="&quot;ROSENBLATT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Rosenblatt is an organization that introduced equations to change the weights in a learning model."
"Rosenblatt is an organization or individual associated with the development of the LTM equations used for pattern classification."</data>
      <data key="d2">7c556574ea1f1f26ee3ad2a63d56b8e7,9e901f71d0d2339294da133518f2162f</data>
    </node>
    <node id="&quot;WIDROW&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Widrow is an organization that introduced the gradient descent Adeline adaptive pattern recognition machine."
"Widrow is an organization or individual associated with the development of the gradient descent Adeline adaptive pattern recognition machine."</data>
      <data key="d2">7c556574ea1f1f26ee3ad2a63d56b8e7,9e901f71d0d2339294da133518f2162f</data>
    </node>
    <node id="&quot;ANDERSON&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Anderson is an organization that described neural pattern recognition using a spatial cross-correlation function."
"Anderson is an organization or individual associated with the initial description of neural pattern recognition using a spatial cross-correlation function."</data>
      <data key="d2">7c556574ea1f1f26ee3ad2a63d56b8e7,9e901f71d0d2339294da133518f2162f</data>
    </node>
    <node id="&quot;STM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"STM is an organization or concept mentioned in the text, but its full name or description is not provided."
"STM is a type of memory system mentioned in the text, which includes terms such as LTM and MTM."
"STM (Short-Term Memory) is mentioned, but its specific role or function in the context of the text is not explicitly described."
"STM refers to Short-Term Memory, a component in neural networks that stores temporary patterns and activities."
"STM refers to Short-Term Memory, a component that temporarily stores and processes information."
"STM stands for Short-Term Memory, which is a component of the brain that enables it to process and learn from temporal patterns of information."
"STM is a concept mentioned in the text, possibly referring to a specific type of memory or storage mechanism."
"STM is an abbreviation used to refer to Short-Term Memory, a component mentioned in the text that stores input patterns persistently."
"STM refers to a persistent storage mechanism used to store input patterns."
"STM is a system or organization mentioned in the text, likely an acronym for Short-Term Memory."
"STM is a concept mentioned in the text, likely referring to a Short-Term Memory system or a similar entity."
"STM is a concept referring to Short-Term Memory, which is mentioned in the text as a component that needs to store more than one feature or category."
"STM is a memory system mentioned in the text, potentially used for storing signal functions and activities."
"STM is a component of a model or theory that stores partially contrast-enhanced patterns."
"STM is an abbreviation used in the text, but without further context, it's unclear what it stands for."
"STM refers to Short-Term Memory, a type of memory that holds information temporarily for immediate use."
"STM stands for Short-Term Memory, which is one of the types of memory interactions mentioned in the text."
"STM is a component mentioned in the text that interacts with LTM during neuronal learning."
"STM refers to a type of cell or neuron population mentioned in the text."
"STM refers to Short-Term Memory, a component of Working Memory that stores information temporarily for immediate use."
"STM is a type of memory mentioned in the text, which stores and updates patterns based on input."
"STM is a type of memory that can trigger learning and enable fluently recalled information at a future time."</data>
      <data key="d2">0d925896ce2bf9b73be90d8fa5ddb402,24771832864aa38abd6aebec04b13a10,2e76149ff772441e6627913bc1df5000,35551dc55b5522082b778171ff6d1bf9,3a64c8c26895f111f00a349dd69bb505,53f5bc3f4c71310c593a23aef01d1633,5812b5d4bcdfbf80de28dca56a6559b3,653b7986c4757bd5d0a251369187efa6,65a025a8610d85bf0d2b6c4979eb7439,6a47ed5881928d48cdcb74e40867a711,7ba0dfde8cc54bb1dcf66b46fcdd88f8,8da881a4f375e8a524fd0bf46ae2279e,8ee5dee5c6f3e89d8d8c20e3fe957583,91f030f6c14c673e6d029c9bf1a66515,97ad8d6e6e3bf27ae6a7b457af9b312e,9b30fc06ca06f49c2faa238da7eddc6f,9e901f71d0d2339294da133518f2162f,b40ff9b93414391d5e4b3c06dfe02bc9,b4f6256f3430f1aa72ca8092809ebba1,b881b9051ad24c6a16b468803fba51d3,c580fa74e3c36285cfae7df56340a990,fba20e559e6ecb61ebbf3a805e6d072c</data>
    </node>
    <node id="&quot;LTM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"LTM is an organization or concept mentioned in the text, but its full name or description is not provided."
"LTM is a type of long-term memory system mentioned in the text, which changes at a slower rate than STM."
"LTM stands for Long-Term Memory, a variable used in the STM Equation to describe the temporal evolution of inhibitory interneuronal activities."
"LTM (Long-Term Memory) is mentioned, and its learning mechanism is described as gated steepest descent learning, which allows adaptive weights to increase or decrease."
"LTM refers to Long-Term Memory, a component in neural networks that stores learned patterns and activities."
"LTM refers to Long-Term Memory, a component that stores and retrieves information over an extended period of time."
"LTM stands for Long-Term Memory, which is a component of the brain that enables it to learn and retain information over extended periods of time."
"LTM refers to Long-Term Memory, a type of memory that holds information for long-term storage and retrieval."
"LTM stands for Long-Term Memory, which is one of the types of memory interactions mentioned in the text."
"LTM is a component mentioned in the text that interacts with STM during neuronal learning."
"LTM refers to a type of cell or neuron population mentioned in the text."
"LTM refers to Long-Term Memory, a cognitive system that stores information for long-term retention and retrieval."
"LTM refers to a theoretical concept that biases working memory toward more primacy dominance."
"LTM is a type of memory that enables information to be fluently recalled at a future time."</data>
      <data key="d2">0d925896ce2bf9b73be90d8fa5ddb402,24771832864aa38abd6aebec04b13a10,2e76149ff772441e6627913bc1df5000,3281409fdcd18b09ca3109260ddb96d9,392028b79561bd7471cb68e7c9258b1e,53f5bc3f4c71310c593a23aef01d1633,5812b5d4bcdfbf80de28dca56a6559b3,653b7986c4757bd5d0a251369187efa6,97ad8d6e6e3bf27ae6a7b457af9b312e,9b30fc06ca06f49c2faa238da7eddc6f,9e901f71d0d2339294da133518f2162f,b40ff9b93414391d5e4b3c06dfe02bc9,b881b9051ad24c6a16b468803fba51d3,c580fa74e3c36285cfae7df56340a990</data>
    </node>
    <node id="&quot;ADELINE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Adeline is a pattern recognition machine introduced by Widrow."
</data>
      <data key="d2">7c556574ea1f1f26ee3ad2a63d56b8e7,9e901f71d0d2339294da133518f2162f</data>
    </node>
    <node id="&quot;PERCEPTRON&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Perceptron is a model mentioned in the text, but its full name or description is not provided."
"Perceptron is a simple machine learning algorithm used for binary classification tasks, which iteratively adjusts the weights of input features to separate data points into two classes."
"Perceptron is a model from scikit-learn used for classification tasks."
"Perceptron is a classifier implemented in the scikit-learn library, used for binary classification tasks."
"Perceptron is a model from Scikit-learn used for classification tasks."
"Perceptron is a machine learning algorithm used for classification tasks, provided by the Scikit-learn library."</data>
      <data key="d2">0036fb6f489e13c0db0f1c02bf3323be,35631fbf2ad11c53d75cb9b42e2c39b4,9e901f71d0d2339294da133518f2162f,d58662ee42c14a0787d839ebfd0a6e9b,dadca3c89b34dc48a60c53367ab55768,f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </node>
    <node id="&quot;NEURAL PATTERN RECOGNITION&quot;">
      <data key="d0" />
      <data key="d1">
</data>
      <data key="d2">7c556574ea1f1f26ee3ad2a63d56b8e7,9e901f71d0d2339294da133518f2162f</data>
    </node>
    <node id="&quot;KOHONEN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Kohonen is an organization or individual associated with the transition from linear algebra concepts to more biologically motivated studies in neural network research."
"Kohonen is a person mentioned in the text who has used Instar Learning in his applications of the Self-Organizing Map (SOM) model."
"Kohonen is a researcher who used Instar Learning in his applications of the SOM model."
"Kohonen is an author who has developed the SOM model, which utilizes shunting dynamics in some versions."</data>
      <data key="d2">2cbd29d6f0019f0c85bee43779ae8f4d,43cf5e32e2df964318d03574e6cd6cdc,7c556574ea1f1f26ee3ad2a63d56b8e7,b286a9022774f24a400744b2a1b08bab</data>
    </node>
    <node id="&quot;HARTLINE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Hartline is an organization or individual associated with neurophysiological experiments on the lateral eye of the Limulus, or horseshoe crab, leading to the development of the steady state Hartline-Ratliff model."</data>
      <data key="d2">7c556574ea1f1f26ee3ad2a63d56b8e7</data>
    </node>
    <node id="&quot;LTM EQUATIONS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">7c556574ea1f1f26ee3ad2a63d56b8e7</data>
    </node>
    <node id="&quot;NEURAL NETWORK RESEARCH&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">7c556574ea1f1f26ee3ad2a63d56b8e7</data>
    </node>
    <node id="&quot;STEADY STATE HARTLINE-RATLIFF MODEL&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">7c556574ea1f1f26ee3ad2a63d56b8e7</data>
    </node>
    <node id="&quot;HARTLINE-RATLIFF MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Hartline-Ratliff Model is a steady state model developed by H.K. Hartline and J.A. Ratliff in 1957, inspired by neurophysiological experiments on the lateral eye of the Limulus."</data>
      <data key="d2">48763056731d01884b6cf37bf0e0d0db</data>
    </node>
    <node id="&quot;H.K. HARTLINE&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"H.K. Hartline is a neurophysiologist who led the experiments on the lateral eye of the Limulus, for which he received the Nobel Prize in Physiology or Medicine in 1967."</data>
      <data key="d2">48763056731d01884b6cf37bf0e0d0db</data>
    </node>
    <node id="&quot;J.A. RATLIFF&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"J.A. Ratliff is a neurophysiologist who extended the steady-state Hartline-Ratliff model to a dynamical model in 1963."</data>
      <data key="d2">48763056731d01884b6cf37bf0e0d0db</data>
    </node>
    <node id="&quot;LIMULUS&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"Limulus is a species of horseshoe crab used in neurophysiological experiments."</data>
      <data key="d2">48763056731d01884b6cf37bf0e0d0db</data>
    </node>
    <node id="&quot;ADDITIVE MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Additive Model is a model described in the text, which is a precursor of the dynamical model developed by J.A. Ratliff."
"The Additive Model is an equation used to determine the rate of change of neuronal activities, discovered by Grossberg."
"The Additive Model is a computational framework that includes terms for passive decay, positive feedback, negative feedback, and input."
"The Additive Model is a mathematical framework used in neural networks for computational analyses in various applications."
"The Additive Model is a neural network concept mentioned in the text, with applications in decision-making and other areas."
"The Additive Model is a mathematical model that is used as an approximation of the Shunting Model when inputs are small and do not approach saturation values."
"Additive Model is another variant of the STM Equation, with parameters C and F equal to 0, used to describe the behavior of neural models."

"Additive Model is a model mentioned in the text for which Hopfield stated a Liapunov function."
"The Additive Model is a probabilistic decision-making model that does not exhibit self-normalization properties."
"The Additive Model is a type of network mentioned in the text, which is included in the Cohen-Grossberg Model systems."
"Additive Model is a component of the Liapunov function proposed by Cohen and Grossberg, which has been erroneously called the Hopfield network."
"The Additive Model is a model derived to explain associative learning of temporal order information in serial learning paradigms."</data>
      <data key="d2">0dd75f3ca11854714bdbfc8a96ccf256,2cbd29d6f0019f0c85bee43779ae8f4d,2e76149ff772441e6627913bc1df5000,3235445917507f02710bd66cc8368194,3281409fdcd18b09ca3109260ddb96d9,431b0938a2956f5d7d752462f3b71101,47d1d12642cdab6e9a5d21c184f83c9c,48763056731d01884b6cf37bf0e0d0db,98173c1c0fcd64ceb914e0dd6b366b30,9ed5e24bce2907e0ffa4acbe066dbfff,be0954a6263de67c84da3141d95de445,d4afac3b7aed3d6e11ff5eaf34589c2d,df77d35da87a38cae0984a42b9a1d41c</data>
    </node>
    <node id="&quot;HUGH EVERETT&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Hugh Everett is a physicist who extended a steady-state model to a dynamical model in 1963."</data>
      <data key="d2">caf44d8df044312829ae3fe56df0c440</data>
    </node>
    <node id="&quot;ANDREW HODGKIN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Andrew Hodgkin is a physicist who, along with Alan Huxley, studied the squid giant axon in 1952."</data>
      <data key="d2">caf44d8df044312829ae3fe56df0c440</data>
    </node>
    <node id="&quot;ALAN HUXLEY&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Alan Huxley is a physicist who, along with Andrew Hodgkin, studied the squid giant axon in 1952."</data>
      <data key="d2">caf44d8df044312829ae3fe56df0c440</data>
    </node>
    <node id="&quot;JOHN HOPFIELD&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"John Hopfield is a physicist who derived neural networks that form the foundation of most current biological neural network research in 1982."
"John Hopfield is not explicitly mentioned in the text, but the term 'infinite impulse response' is associated with his work on Hopfield networks, which are a type of recurrent neural network."</data>
      <data key="d2">caf44d8df044312829ae3fe56df0c440,f5b970cf7201f4a918d8bd6a1267657c</data>
    </node>
    <node id="&quot;NEURAL NETWORKS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Neural Networks are a focus of study, with examples from the work of Hugh Everett, Andrew Hodgkin, Alan Huxley, and John Hopfield."
"Neural Networks is a field of study that focuses on the structure and function of biological neural systems, and the design and application of artificial neural networks."
"Neural Networks are a focus of the text, with researchers and physicists contributing to their development and study."
"Neural Networks is a field of study mentioned in the text, focusing on the simulation of biological neural systems for information processing."
"Neural Networks are a type of machine learning model that use interconnected nodes or neurons to process information."

"Neural Networks are a type of artificial intelligence modeled after the human brain, used for tasks such as sequence prediction."</data>
      <data key="d2">47d1d12642cdab6e9a5d21c184f83c9c,4c7e78f7237cb3420e70c7749bb259f9,b31ca51b419f7270ee5f4910c90ea331,bf4dccb5096a917a6a71f0cc224e4d7c,caf44d8df044312829ae3fe56df0c440,cb7823dcc9852e6a6f9e3607cb55134f,df77d35da87a38cae0984a42b9a1d41c</data>
    </node>
    <node id="&quot;SQUID GIANT AXON&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Squid Giant Axon is a subject of study by Andrew Hodgkin and Alan Huxley in 1952."</data>
      <data key="d2">caf44d8df044312829ae3fe56df0c440</data>
    </node>
    <node id="&quot;ROCKEFELLER INSTITUTE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Rockefeller Institute is an organization where Grossberg was a student, and his research was published in a student monograph."</data>
      <data key="d2">3235445917507f02710bd66cc8368194</data>
    </node>
    <node id="&quot;EARLY APPLICATIONS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Early Applications refers to the initial use of the Additive Model in computational analyses."</data>
      <data key="d2">9ed5e24bce2907e0ffa4acbe066dbfff</data>
    </node>
    <node id="&quot;COMPUTATIONAL ANALYSES&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Computational Analyses are processes that involve the use of the Additive Model for analysis."</data>
      <data key="d2">9ed5e24bce2907e0ffa4acbe066dbfff</data>
    </node>
    <node id="&quot;HOPFIELD&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Hopfield is a physicist who independently developed a model similar to the Additive Model, which later became known as the Hopfield model."
"Hopfield is a researcher mentioned in the text, contributing to the development of the Hopfield model in neural networks."
"Hopfield is a researcher who stated a Liapunov function for the Additive Model."
"Hopfield is a researcher mentioned in the text who stated a Liapunov function for the Additive Model."
"Hopfield is a researcher who published a special case of the Additive Model and Liapunov function, asserting that trajectories approach equilibria."</data>
      <data key="d2">47d1d12642cdab6e9a5d21c184f83c9c,98173c1c0fcd64ceb914e0dd6b366b30,be0954a6263de67c84da3141d95de445,d4afac3b7aed3d6e11ff5eaf34589c2d,df77d35da87a38cae0984a42b9a1d41c</data>
    </node>
    <node id="&quot;VISION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Vision is one of the early applications of the Additive Model in neural networks, used for computational analyses of visual processing."</data>
      <data key="d2">df77d35da87a38cae0984a42b9a1d41c</data>
    </node>
    <node id="&quot;LEARNING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Learning is a common application of the Additive Model in neural networks, used for various learning tasks such as recognition and reinforcement learning."</data>
      <data key="d2">df77d35da87a38cae0984a42b9a1d41c</data>
    </node>
    <node id="&quot;SPEECH&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Speech is an application of the Additive Model in neural networks, used for learning the temporal order of speech signals."</data>
      <data key="d2">df77d35da87a38cae0984a42b9a1d41c</data>
    </node>
    <node id="&quot;LANGUAGE&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Language is an application of the Additive Model in neural networks, used for understanding and generating human language."</data>
      <data key="d2">df77d35da87a38cae0984a42b9a1d41c</data>
    </node>
    <node id="&quot;SENSORY-MOTOR CONTROL&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Sensory-Motor Control is an application of the Additive Model in neural networks, used for learning and coordinating sensory and motor activities."
"Sensory-Motor Control is mentioned as an event or phenomenon that MTM dynamics help to explain."</data>
      <data key="d2">254fd1fbc0a719a86b8021fa759d22ea,df77d35da87a38cae0984a42b9a1d41c</data>
    </node>
    <node id="&quot;DECISION-MAKING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Decision-Making is an application of the Additive Model in neural networks, used for modeling and understanding decision-making processes."</data>
      <data key="d2">df77d35da87a38cae0984a42b9a1d41c</data>
    </node>
    <node id="&quot;USHER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Usher is a researcher mentioned in the text, contributing to decision-making in neural networks."</data>
      <data key="d2">47d1d12642cdab6e9a5d21c184f83c9c</data>
    </node>
    <node id="&quot;MCCLELLAND&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"McClelland is a researcher mentioned in the text, contributing to decision-making in neural networks."</data>
      <data key="d2">47d1d12642cdab6e9a5d21c184f83c9c</data>
    </node>
    <node id="&quot;STM EQUATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The STM Equation is a neural network concept mentioned in the text, with applications in modeling individual neurons and maintaining sensitivity."
"STM Equation is a mathematical model used to describe neural models of different brain systems, including parameters such as A, B, C, E, and F."</data>
      <data key="d2">3281409fdcd18b09ca3109260ddb96d9,47d1d12642cdab6e9a5d21c184f83c9c</data>
    </node>
    <node id="&quot;HODGKIN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Hodgkin is a scientist who developed a mathematical model for the shunting dynamics of individual neurons."</data>
      <data key="d2">431b0938a2956f5d7d752462f3b71101</data>
    </node>
    <node id="&quot;STM TRACES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"STM traces are signals or activities that are bounded within an interval and interact with balanced positive and negative signals."
"STM Traces are short-term memory traces, representing activities in a neural system."
"STM Traces are a component of the Generalized Additive System, representing the activities of the system."</data>
      <data key="d2">18be9bfe53d3b9c1e15c1c8238674459,431b0938a2956f5d7d752462f3b71101,8bb0e63353e66a2c60a878028beff5f9</data>
    </node>
    <node id="&quot;SHUNTING MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Shunting Model is a mathematical model that approximates the Additive Model in certain conditions and uses a combination of shunting and additive terms."
"Shunting Model is a variant of the STM Equation, with parameters C and F, and E equal to 0 or a constant, describing silent or hyperpolarizing shunting inhibition."
"The Shunting Model is a concept used to describe the behavior of cells in the Feedforward On-Center Network."
"The Shunting Model is a type of network mentioned in the text, which is included in the Cohen-Grossberg Model systems."
"Shunting Model is a component of the Liapunov function proposed by Cohen and Grossberg."
"The Shunting Model is a model derived to explain associative learning of temporal order information in serial learning paradigms."</data>
      <data key="d2">0dd75f3ca11854714bdbfc8a96ccf256,2e76149ff772441e6627913bc1df5000,3281409fdcd18b09ca3109260ddb96d9,431b0938a2956f5d7d752462f3b71101,68b4b33f0da5edc9dcb301a08821b352,98173c1c0fcd64ceb914e0dd6b366b30</data>
    </node>
    <node id="&quot;WILSON-COWAN MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Wilson-Cowan model is a mathematical model developed by Wilson and Cowan that also uses a combination of shunting and additive terms, but in a different formulation."
"The Wilson-Cowan Model is mentioned in the text, which uses a sigmoid of sums that is multiplied by a shunting term."</data>
      <data key="d2">431b0938a2956f5d7d752462f3b71101,653b7986c4757bd5d0a251369187efa6</data>
    </node>
    <node id="&quot;WILSON&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Wilson is a scientist who developed the Wilson-Cowan model with Cowan."</data>
      <data key="d2">431b0938a2956f5d7d752462f3b71101</data>
    </node>
    <node id="&quot;COWAN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Cowan is a scientist who developed the Wilson-Cowan model with Wilson."
"Cowan is a researcher who reviewed experimental data supporting the existence of a four plus or minus one WM capacity limit when LTM and grouping influences are minimized."</data>
      <data key="d2">392028b79561bd7471cb68e7c9258b1e,431b0938a2956f5d7d752462f3b71101</data>
    </node>
    <node id="&quot;MTM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"MTM is a type of medium-term memory system mentioned in the text, which changes at a rate intermediate between STM and LTM."
"MTM stands for Medium-Term Memory, a variable used in the STM Equation to describe the temporal evolution of inhibitory interneuronal activities."
"MTM refers to a system or model that includes MTM traces or habituative transmitter gates, which are described in the text."
"MTM refers to Medium-Term Memory, a component that stores and retrieves information over a longer time frame than STM."
"MTM stands for Medium-Term Memory, which is not explicitly mentioned but could be inferred from the context of processing temporal patterns."</data>
      <data key="d2">0d925896ce2bf9b73be90d8fa5ddb402,254fd1fbc0a719a86b8021fa759d22ea,3281409fdcd18b09ca3109260ddb96d9,653b7986c4757bd5d0a251369187efa6,b40ff9b93414391d5e4b3c06dfe02bc9</data>
    </node>
    <node id="&quot;COHEN AND GROSSBERG&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Cohen and Grossberg are researchers who derived a Liapunov function for a generalization of the Additive and Shunting Models."
"Cohen and Grossberg are researchers who have contributed to the study of competitive systems and developed the Masking Field model."
"Cohen and Grossberg are researchers who developed the Cohen-Grossberg Liapunov function to prove the existence of global equilibria."
"Cohen and Grossberg are researchers who proposed a Liapunov function that includes the Additive Model and Shunting Model."</data>
      <data key="d2">4a78ff105fdd9a4b0d01ccf1e5816c74,4b48be5db14c2e681ef8f4ee7de4b847,98173c1c0fcd64ceb914e0dd6b366b30,d4afac3b7aed3d6e11ff5eaf34589c2d</data>
    </node>
    <node id="&quot;MEDIUM-TERM MEMORY (MTM)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Medium-term memory (MTM) is described as activity-dependent habituation, often called habituative transmitter gates, which plays multiple roles in the described system."</data>
      <data key="d2">d4afac3b7aed3d6e11ff5eaf34589c2d</data>
    </node>
    <node id="&quot;ADDITIVE AND SHUNTING MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Additive and Shunting Models are mentioned in the context of a generalization by Cohen and Grossberg."
"The Additive and Shunting Models are mentioned, but their specific role or function in the context of the text is not explicitly described."
"Additive and Shunting Models are mentioned as models that involve the learning of distributed patterns of LTM traces."</data>
      <data key="d2">24771832864aa38abd6aebec04b13a10,97ad8d6e6e3bf27ae6a7b457af9b312e,d4afac3b7aed3d6e11ff5eaf34589c2d</data>
    </node>
    <node id="&quot;LIAPUNOV FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Liapunov Function is a mathematical concept used to analyze the stability of dynamic systems, including the one mentioned in the text."
"A Liapunov Function is a mathematical concept used to analyze the stability of systems, such as the Cohen-Grossberg Model."
"Liapunov Function is a mathematical concept used to analyze the stability of dynamic systems."
"A Liapunov function is a mathematical concept used to prove the global convergence of a system."</data>
      <data key="d2">01e2a32da700813f593038a23a618e55,0dd75f3ca11854714bdbfc8a96ccf256,98173c1c0fcd64ceb914e0dd6b366b30,be0954a6263de67c84da3141d95de445</data>
    </node>
    <node id="&quot;COHEN-GROSSBERG MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Cohen-Grossberg Model is a model mentioned in the text that includes the Liapunov function."
"The Cohen-Grossberg Model is a system of equations used to study the behavior of networks, with a focus on symmetry in interaction coefficients."
"The Cohen-Grossberg Model is a model developed by Cohen and Grossberg, which is adapted by Kosko."
"The Cohen-Grossberg model is a neural network model developed by Cohen and Grossberg in 1983."</data>
      <data key="d2">01e2a32da700813f593038a23a618e55,0dd75f3ca11854714bdbfc8a96ccf256,644602009dec8474bb5cd4702b391d3e,be0954a6263de67c84da3141d95de445</data>
    </node>
    <node id="&quot;MEDIUM-TERM MEMORY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Medium-term Memory, or activity-dependent habituation, is a concept mentioned in the text that plays multiple roles in neural networks."</data>
      <data key="d2">be0954a6263de67c84da3141d95de445</data>
    </node>
    <node id="&quot;GATED DIPOLE OPPONENT PROCESSING NETWORK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Gated Dipole Opponent Processing Network is a type of neural network mentioned in the text that enables reset events and arousal bursts."</data>
      <data key="d2">be0954a6263de67c84da3141d95de445</data>
    </node>
    <node id="&quot;ADAPTIVE RESONANCE THEORY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Adaptive Resonance Theory is mentioned in the text as a related concept to MTM."
"Adaptive Resonance Theory is a concept mentioned in the text, likely a theoretical framework developed by Grossberg."
"Adaptive Resonance Theory is a concept mentioned in the text, which was introduced by Grossberg to propose how top-down learned expectations and attentional focusing could dynamically stabilize learning in a Competitive Learning or Self-Organizing Map model."
"Adaptive Resonance Theory is a learning model introduced by Grossberg in 1976 to stabilize learning in response to input patterns."
"Adaptive Resonance Theory, or ART, is a cognitive and bRNN theory that explains how the brain autonomously learns to categorize, recognize, and predict objects and events."
"Adaptive Resonance Theory is a cognitive and bRNN theory that explains how the brain autonomously learns to categorize, recognize, and predict objects and events in a changing world."
"Adaptive Resonance Theory is a neural network model developed by Grossberg, which emphasizes the role of attention in learning and recognition."</data>
      <data key="d2">01e2a32da700813f593038a23a618e55,2061ae5039f379b5837d33a062f72ad1,254fd1fbc0a719a86b8021fa759d22ea,32b8b59687b8d1556ec90c99a090653c,3d5b88f7f81ed9e14f07335bbef17020,554e8565591507441cecaa652cb926db,644602009dec8474bb5cd4702b391d3e</data>
    </node>
    <node id="&quot;VISUAL PERCEPTION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Visual Perception is mentioned as an event or phenomenon that MTM dynamics help to explain."
"Visual Perception is mentioned in the context of brightness constancy and brightness contrast, which are explained by the normalization rule."
"Visual Perception refers to the process by which the brain interprets and understands visual information from the environment."</data>
      <data key="d2">254fd1fbc0a719a86b8021fa759d22ea,8202e13f45a323970b361921f923c605,c8c573c11d0f29d207b3b639a9466518</data>
    </node>
    <node id="&quot;COGNITIVE-EMOTIONAL INTERACTIONS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Cognitive-Emotional Interactions is mentioned as an event or phenomenon that MTM dynamics help to explain."</data>
      <data key="d2">254fd1fbc0a719a86b8021fa759d22ea</data>
    </node>
    <node id="&quot;DECISION-MAKING UNDER RISK&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Decision-Making under Risk is mentioned as an event or phenomenon that MTM dynamics help to explain."</data>
      <data key="d2">254fd1fbc0a719a86b8021fa759d22ea</data>
    </node>
    <node id="&quot;GUTOWSKI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Gutowski is an author mentioned in the text, likely a researcher."</data>
      <data key="d2">c73b76e10166042ccaba5603ed67f380</data>
    </node>
    <node id="&quot;OGMEN AND GAGN&#201;&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Ogmen and Gagn&#233; are likely a research team or authors mentioned in the text."</data>
      <data key="d2">c73b76e10166042ccaba5603ed67f380</data>
    </node>
    <node id="&quot;ABBOTT ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Abbott et al. is a group of authors mentioned in the text, likely a research team."</data>
      <data key="d2">c73b76e10166042ccaba5603ed67f380</data>
    </node>
    <node id="&quot;TSODYKS AND MARKRAM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Tsodyks and Markram are likely a research team or authors mentioned in the text."</data>
      <data key="d2">c73b76e10166042ccaba5603ed67f380</data>
    </node>
    <node id="&quot;GAUDIANO AND GROSSBERG&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Gaudiano and Grossberg are likely a research team or authors mentioned in the text."
"Gaudiano and Grossberg are mentioned as authors of research papers, but their specific role or function in the context of the text is not explicitly described."</data>
      <data key="d2">24771832864aa38abd6aebec04b13a10,c73b76e10166042ccaba5603ed67f380</data>
    </node>
    <node id="&quot;GROSSBERG AND SEITZ&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Grossberg and Seitz are likely a research team or authors mentioned in the text."
"Grossberg and Seitz are mentioned as authors of a research paper, but their specific role or function in the context of the text is not explicitly described."</data>
      <data key="d2">24771832864aa38abd6aebec04b13a10,c73b76e10166042ccaba5603ed67f380</data>
    </node>
    <node id="&quot;MTM TRACE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"MTM Trace is a term used in the text, likely referring to a specific concept or technique in neurophysiology."</data>
      <data key="d2">c73b76e10166042ccaba5603ed67f380</data>
    </node>
    <node id="&quot;HABITUATIVE TRANSMITTER GATE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Habituative Transmitter Gate is a term used in the text, likely referring to a specific concept or technique in neurophysiology."</data>
      <data key="d2">c73b76e10166042ccaba5603ed67f380</data>
    </node>
    <node id="&quot;MASS ACTION INTERACTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Mass Action Interaction is a term used in the text, likely referring to a specific concept or technique in neurophysiology."</data>
      <data key="d2">c73b76e10166042ccaba5603ed67f380</data>
    </node>
    <node id="&quot;SOMATOSENSORY CORTEX&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Somatosensory Cortex is mentioned as a component in the text, but its role or function is not explicitly described."</data>
      <data key="d2">24771832864aa38abd6aebec04b13a10</data>
    </node>
    <node id="&quot;GATED STEEPEST DESCENT LEARNING&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Gated Steepest Descent Learning is a learning algorithm mentioned in the text, which has variants such as Outstar Learning and Instar Learning."</data>
      <data key="d2">97ad8d6e6e3bf27ae6a7b457af9b312e</data>
    </node>
    <node id="&quot;OUTSTAR LEARNING&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Outstar Learning is a variant of Gated Steepest Descent Learning introduced by Grossberg for spatial pattern learning."
"Outstar Learning is a variant of gated steepest descent learning introduced by Grossberg in 1968b for spatial pattern learning."</data>
      <data key="d2">97ad8d6e6e3bf27ae6a7b457af9b312e,b286a9022774f24a400744b2a1b08bab</data>
    </node>
    <node id="&quot;INSTAR LEARNING&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Instar Learning is another variant of Gated Steepest Descent Learning used in the text, introduced by Grossberg."
"Instar Learning is another variant of gated steepest descent learning used in Grossberg's work for learning bottom-up adaptive filters in Self-Organizing Map (SOM) models."</data>
      <data key="d2">97ad8d6e6e3bf27ae6a7b457af9b312e,b286a9022774f24a400744b2a1b08bab</data>
    </node>
    <node id="&quot;HEBBIAN TRACES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Hebbian Traces are a type of connection strength in neural networks that saturate at maximum values, according to the Hebb postulate."</data>
      <data key="d2">b286a9022774f24a400744b2a1b08bab</data>
    </node>
    <node id="&quot;SELF-ORGANIZING MAP (SOM)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Self-Organizing Map (SOM) is a type of artificial neural network that uses a recurrent on-center off-surround network for storage and learning of spatial patterns."
"Self-Organizing Map (SOM) is a model developed by Kohonen, which also utilizes shunting dynamics in some versions."</data>
      <data key="d2">2cbd29d6f0019f0c85bee43779ae8f4d,b286a9022774f24a400744b2a1b08bab</data>
    </node>
    <node id="&quot;LONG-TERM MEMORY (LTM)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Long-Term Memory (LTM) is a type of memory in neural networks that stores learned patterns and connections."</data>
      <data key="d2">b286a9022774f24a400744b2a1b08bab</data>
    </node>
    <node id="&quot;HECHT-NIELSEN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Hecht-Nielsen is a researcher who referred to a network with Instars and Outstars as a counterpropagation network."</data>
      <data key="d2">43cf5e32e2df964318d03574e6cd6cdc</data>
    </node>
    <node id="&quot;SOM MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"SOM model is a neural network model used for data analysis and visualization."</data>
      <data key="d2">43cf5e32e2df964318d03574e6cd6cdc</data>
    </node>
    <node id="&quot;ART&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ART is a neural network model introduced by Grossberg, which uses Instars and Outstars for learning."
"ART is a concept introduced in the text, likely an organization or system, but no specific details are provided."</data>
      <data key="d2">2061ae5039f379b5837d33a062f72ad1,43cf5e32e2df964318d03574e6cd6cdc</data>
    </node>
    <node id="&quot;SOM MODELS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"SOM Models are mentioned in the text, likely referring to Self-Organizing Maps, which are a type of artificial neural network."</data>
      <data key="d2">2061ae5039f379b5837d33a062f72ad1</data>
    </node>
    <node id="&quot;INSTAR-OUTSTAR NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Instar-Outstar Network is a type of network mentioned in the text, likely a combination of Instar and Outstar learning systems."</data>
      <data key="d2">2061ae5039f379b5837d33a062f72ad1</data>
    </node>
    <node id="&quot;O&#8217;REILLY&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"O&#8217;Reilly is a person mentioned in the context of the Leabra model, which uses STM, MTM, and LTM equations."</data>
      <data key="d2">b40ff9b93414391d5e4b3c06dfe02bc9</data>
    </node>
    <node id="&quot;MUNAKATA&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Munakata is a person mentioned in the context of the Leabra model, which uses STM, MTM, and LTM equations."</data>
      <data key="d2">b40ff9b93414391d5e4b3c06dfe02bc9</data>
    </node>
    <node id="&quot;LEABRA MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Leabra model is a neural network model developed by O&#8217;Reilly and Munakata that uses STM, MTM, and LTM equations."</data>
      <data key="d2">b40ff9b93414391d5e4b3c06dfe02bc9</data>
    </node>
    <node id="&quot;O&#8217;REILLY AND MUNAKATA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"O&#8217;Reilly and Munakata are mentioned as the authors of the Leabra model, which is used in the context of processing spatial patterns."</data>
      <data key="d2">0d925896ce2bf9b73be90d8fa5ddb402</data>
    </node>
    <node id="&quot;THE BRAIN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The brain is referred to as an organization that processes patterned information, learns from spatial and temporal patterns, and compensates for variable input intensities."</data>
      <data key="d2">0d925896ce2bf9b73be90d8fa5ddb402</data>
    </node>
    <node id="&quot;SPATIAL PATTERNS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Spatial patterns refer to the distribution of information across networks of neurons, such as in a picture or a scene."</data>
      <data key="d2">0d925896ce2bf9b73be90d8fa5ddb402</data>
    </node>
    <node id="&quot;TEMPORAL PATTERNS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Temporal patterns refer to the sequence of signals over time, such as in speech or language."</data>
      <data key="d2">0d925896ce2bf9b73be90d8fa5ddb402</data>
    </node>
    <node id="&quot;NEURONS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Neurons are biological cells that process information, evolving network designs to compensate for variable input intensities."
"Neurons are the processing units in the reservoir network."
"Neurons are the individual units in a reservoir, which can be triggered and activated."
"Neurons are the individual units within the Reservoir, which evolve in time following the evolution of the input timeseries."
"Neurons are a biological concept mentioned in the text, likely in the context of a neural network or similar technology."</data>
      <data key="d2">0e0afab060f214d46062c9886e762002,31080b985ce23ef751d488d0f7b9eff6,388cc054a99cc5cadff33147f95d6156,b957e1bf5bf175c7630222ca742c7933,c4f5a27caf9dd9c1d972492c1147efa0</data>
    </node>
    <node id="&quot;BRAINS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Brains are mentioned as having evolved network designs in neurons to process variable input intensities."</data>
      <data key="d2">31080b985ce23ef751d488d0f7b9eff6</data>
    </node>
    <node id="&quot;NOISE-SATURATION DILEMMA&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Noise-Saturation Dilemma refers to the challenge of maintaining neuronal sensitivity to input patterns while dealing with variable input intensities."
"The challenge of maintaining sensitivity to relative input sizes while avoiding saturation when the total input size is parametrically increased."</data>
      <data key="d2">31080b985ce23ef751d488d0f7b9eff6,7beb44dd43aea0791fcb35806356ddb3</data>
    </node>
    <node id="&quot;ON-CENTER OFF-SURROUND NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The On-center Off-Surround Network is a network design that enables neurons to retain sensitivity to the relative sizes of their inputs across the network."
"A type of network that consists of on-center and off-surround cells, which obey the membrane or shunting equations of neurophysiology."</data>
      <data key="d2">31080b985ce23ef751d488d0f7b9eff6,7beb44dd43aea0791fcb35806356ddb3</data>
    </node>
    <node id="&quot;MEMBRANE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Membrane, or shunting, is a concept mentioned in the context of neuronal network interactions."</data>
      <data key="d2">31080b985ce23ef751d488d0f7b9eff6</data>
    </node>
    <node id="&quot;SPATIAL PATTERN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A spatial pattern of inputs that is processed by a network of cells."</data>
      <data key="d2">7beb44dd43aea0791fcb35806356ddb3</data>
    </node>
    <node id="&quot;NETWORK OF CELLS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"A network of cells that processes a spatial pattern of inputs, with each cell maintaining sensitivity to the relative size of its inputs."</data>
      <data key="d2">7beb44dd43aea0791fcb35806356ddb3</data>
    </node>
    <node id="&quot;RELATIVE SIZE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The constant relative size, or reflectance, of each input in the spatial pattern."</data>
      <data key="d2">7beb44dd43aea0791fcb35806356ddb3</data>
    </node>
    <node id="&quot;TOTAL INPUT SIZE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The variable total input size that is the sum of the relative sizes of all inputs in the spatial pattern."</data>
      <data key="d2">7beb44dd43aea0791fcb35806356ddb3</data>
    </node>
    <node id="&quot;CELL (V_I)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Each cell in the network of cells that maintains sensitivity to the relative size of its inputs."
"Cell (v_i) is a biological unit that maintains sensitivity to its input size (I_i) and competes with other inputs (I_k) to activate itself."</data>
      <data key="d2">7beb44dd43aea0791fcb35806356ddb3,fe551b6e0c32ef0fb9ac0a07ff64d6ba</data>
    </node>
    <node id="&quot;INPUT (I_I)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input (I_i) is the total input size that each cell (v_i) receives, and increasing it increases the sensitivity of the cell."</data>
      <data key="d2">fe551b6e0c32ef0fb9ac0a07ff64d6ba</data>
    </node>
    <node id="&quot;INPUT (I_K)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input (I_k) are the additional inputs that compete with Input (I_i) to inhibit the activation of cell (v_i)."</data>
      <data key="d2">fe551b6e0c32ef0fb9ac0a07ff64d6ba</data>
    </node>
    <node id="&quot;KUFFLER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Kuffler is a researcher who reported on-center off-surround anatomies in the cat retina."
"Kuffler is a researcher who reported on-center off-surround anatomies in the cat retina in 1953."</data>
      <data key="d2">9d40ff29a29b7422b2b0c76b957c54f3,fe551b6e0c32ef0fb9ac0a07ff64d6ba</data>
    </node>
    <node id="&quot;ON-CENTER OFF-SURROUND ANATOMY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"On-Center Off-Surround Anatomy is a neural structure that activates and inhibits cells through competition among inputs."</data>
      <data key="d2">9d40ff29a29b7422b2b0c76b957c54f3</data>
    </node>
    <node id="&quot;EXCITED SITES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Excited Sites are parts of a cell that are activated by input signals."</data>
      <data key="d2">9d40ff29a29b7422b2b0c76b957c54f3</data>
    </node>
    <node id="&quot;UNEXCITED SITES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Unexcited Sites are parts of a cell that are not activated by input signals."</data>
      <data key="d2">9d40ff29a29b7422b2b0c76b957c54f3</data>
    </node>
    <node id="&quot;SPONTANEOUS DECAY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Spontaneous Decay is the natural decrease in excitation of a cell over time, allowing it to return to an equilibrium point."</data>
      <data key="d2">9d40ff29a29b7422b2b0c76b957c54f3</data>
    </node>
    <node id="&quot;FEEDFORWARD ON-CENTER NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"A network defined by equation (13) that consists of cells obeying a simple version of the Shunting Model."</data>
      <data key="d2">68b4b33f0da5edc9dcb301a08821b352</data>
    </node>
    <node id="&quot;EQUILIBRIUM VALUE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Equilibrium Value is a concept that represents the value at which each cell in the Feedforward On-Center Network approaches when a fixed spatial pattern is presented and the total input is held constant."</data>
      <data key="d2">68b4b33f0da5edc9dcb301a08821b352</data>
    </node>
    <node id="&quot;SATURATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Saturation is a concept that describes the behavior of cells in the Feedforward On-Center Network when the off-surround input is removed."</data>
      <data key="d2">68b4b33f0da5edc9dcb301a08821b352</data>
    </node>
    <node id="&quot;EQUATION (13)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Equation (13) is a mathematical expression used in the text to describe a process involving automatic gain control."</data>
      <data key="d2">04e132fa3a85e95e1e6164428852446e</data>
    </node>
    <node id="&quot;OFF-SURROUND&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Off-surround is a term used in the text to describe an inhibitory input that multiplies a variable in Equation (13)."</data>
      <data key="d2">04e132fa3a85e95e1e6164428852446e</data>
    </node>
    <node id="&quot;STEADY STATE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Steady state is a term used to describe the final value that a variable reaches after a change in conditions."</data>
      <data key="d2">04e132fa3a85e95e1e6164428852446e</data>
    </node>
    <node id="&quot;RATE OF CHANGE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Rate of change is a term used to describe how quickly a variable changes over time."</data>
      <data key="d2">04e132fa3a85e95e1e6164428852446e</data>
    </node>
    <node id="&quot;MASS ACTION NETWORKS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">04e132fa3a85e95e1e6164428852446e</data>
    </node>
    <node id="&quot;ACTIVITIES (X_I)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Activities (x_i) are described as variables in a mathematical model that depend on input strength (I) and approach a constant (B) as input increases."</data>
      <data key="d2">c8c573c11d0f29d207b3b639a9466518</data>
    </node>
    <node id="&quot;INPUT STRENGTH (I)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input Strength (I) is a variable in a mathematical model that affects the activities (x_i) and the total activity (x)."</data>
      <data key="d2">c8c573c11d0f29d207b3b639a9466518</data>
    </node>
    <node id="&quot;TOTAL ACTIVITY (X)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Total Activity (x) is the sum of all activities (x_i) and is independent of the number of active cells. It approaches a constant (B) as input strength (I) increases."</data>
      <data key="d2">c8c573c11d0f29d207b3b639a9466518</data>
    </node>
    <node id="&quot;NORMALIZATION RULE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Normalization Rule is a property of the mathematical model that ensures the total activity (x) remains constant, even as individual activities (x_i) change."
"The Normalization Rule is a constraint that ensures stable learning and memory of list chunks, likely through a specialized process."
"Normalization Rule is a rule that assumes working memory has a limited capacity and activity is redistributed, not just added, when new items are stored."
"The Normalization Rule is a principle that ensures the total activity of the working memory network has a maximum capacity, redistributing activity when new items are stored."
"Normalization Rule is a principle mentioned in the text, which follows from the tendency of RCFs to normalize total network activity."</data>
      <data key="d2">1976b19f768a8fdf37207b680c3b2b40,4364aa6091e1966365fa889b34f5cf90,c38beddeac1d3cac8282ad59bc835788,c8c573c11d0f29d207b3b639a9466518,dcd38cdc6195b2bbf41d936af0bf1f5f</data>
    </node>
    <node id="&quot;WEBER LAW&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Weber Law is a principle in psychophysics that describes the relationship between the perceived intensity of a stimulus and its physical intensity."</data>
      <data key="d2">8202e13f45a323970b361921f923c605</data>
    </node>
    <node id="&quot;SHIFT PROPERTY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Shift Property is a property of a system that causes the entire response curve to shift without a loss of sensitivity."</data>
      <data key="d2">8202e13f45a323970b361921f923c605</data>
    </node>
    <node id="&quot;BRIGHTNESS CONSTANCY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Brightness Constancy is a property of visual perception that ensures that objects appear the same brightness regardless of their surrounding luminance."</data>
      <data key="d2">8202e13f45a323970b361921f923c605</data>
    </node>
    <node id="&quot;BRIGHTNESS CONTRAST&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Brightness Contrast is a visual phenomenon that occurs when increasing the luminance of inputs to the off-surround makes the on-center look darker."</data>
      <data key="d2">8202e13f45a323970b361921f923c605</data>
    </node>
    <node id="&quot;NORMALIZATION PROPERTY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Normalization Property is a property that underlies many properties of limited capacity processing in the brain, such as visual perception and cognition."</data>
      <data key="d2">8202e13f45a323970b361921f923c605</data>
    </node>
    <node id="&quot;WORKING MEMORY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Working Memory is a limited capacity system in the brain that holds and manipulates information for immediate use in cognitive tasks."
"Working Memory refers to the cognitive system that temporarily stores and manipulates information needed for immediate use."
"Working Memory is a cognitive system that temporarily stores and manipulates information."
"Working Memory is a cognitive system that temporarily stores and manipulates information, such as items in a sequence, for processing and recall."
"Working Memory is a cognitive system that temporarily stores and manipulates information, with a limited capacity and activity redistribution when new items are stored."
"Working Memory is a model developed by Grossberg (1978a, 1978b) that explains the storage and retrieval of information in short-term memory."</data>
      <data key="d2">1976b19f768a8fdf37207b680c3b2b40,3d5b88f7f81ed9e14f07335bbef17020,5c4e24fc9bd10d0bd59a84d56f960cf9,8202e13f45a323970b361921f923c605,a9285aaa34de96a8fa62903437d2f3c4,b69b23b14e0feccb488ba5412db0824c</data>
    </node>
    <node id="&quot;SYSTEM&quot;">
      <data key="d0" />
      <data key="d1">
"System refers to the time series data being analyzed and predicted using the NVAR model."</data>
      <data key="d2">59c163f6fc13814d6ae0ff1b04d22653,8202e13f45a323970b361921f923c605</data>
    </node>
    <node id="&quot;LIMITED CAPACITY PROCESSING&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">8202e13f45a323970b361921f923c605</data>
    </node>
    <node id="&quot;SHUNTING NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Shunting Network is a model used to describe the activity of neurons, with properties such as Weber law processing, adaptation level processing, and edge and spatial frequency processing."</data>
      <data key="d2">f290960776c5ec561653e90d2ac6751b</data>
    </node>
    <node id="&quot;NEUROPHYSIOLOGY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Neurophysiology is the scientific discipline that studies the electrical and chemical properties of neurons and their role in information processing."</data>
      <data key="d2">f290960776c5ec561653e90d2ac6751b</data>
    </node>
    <node id="&quot;WERBLIN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Werblin is a researcher who has studied the retina of the mudpuppy Necturus, finding a shift property similar to that described in the shunting network equations."</data>
      <data key="d2">f290960776c5ec561653e90d2ac6751b</data>
    </node>
    <node id="&quot;MEMBRANE EQUATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Membrane Equation is a fundamental equation in neurophysiology that describes the electrical properties of neurons and their role in information processing."
"The Membrane Equation is a mathematical model mentioned in the text, which describes the voltage of a cell and is based on the work of Hodgkin and Huxley."
"The Membrane Equation is a mathematical model that describes the voltage of a cell based on various conductances and saturation voltages."</data>
      <data key="d2">768cec2f00889d1e375cb4955c58ad60,f2468cda326d1ca11c98f2fbde186400,f290960776c5ec561653e90d2ac6751b</data>
    </node>
    <node id="&quot;SHIFTING PROPERTY&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">f290960776c5ec561653e90d2ac6751b</data>
    </node>
    <node id="&quot;HODGKIN AND HUXLEY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Hodgkin and Huxley are a research team mentioned in the text, known for their work on the membrane equation in neurophysiology."
"Hodgkin and Huxley are researchers who proposed a model of signal propagation in axons."</data>
      <data key="d2">a94f07c842345c77af089558b0786bfe,f2468cda326d1ca11c98f2fbde186400</data>
    </node>
    <node id="&quot;SHUNTING NETWORK EQUATIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Shunting Network Equations are a mathematical model mentioned in the text, used to generate various properties such as Weber law processing and edge and spatial frequency processing."</data>
      <data key="d2">f2468cda326d1ca11c98f2fbde186400</data>
    </node>
    <node id="&quot;SODIUM CHANNEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Sodium Channel is a type of ion channel that contributes to the membrane equation, representing the excitatory saturation voltage."</data>
      <data key="d2">768cec2f00889d1e375cb4955c58ad60</data>
    </node>
    <node id="&quot;POTASSIUM CHANNEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Potassium Channel is a type of ion channel that contributes to the membrane equation, representing the inhibitory saturation voltage."</data>
      <data key="d2">768cec2f00889d1e375cb4955c58ad60</data>
    </node>
    <node id="&quot;ION CHANNEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Ion Channel is a general term for a type of protein that allows specific ions to pass through a membrane, such as Sodium Channel and Potassium Channel."</data>
      <data key="d2">768cec2f00889d1e375cb4955c58ad60</data>
    </node>
    <node id="&quot;(20)&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"(20) is an event mentioned in the text, possibly a reference to a specific process or condition."</data>
      <data key="d2">91f030f6c14c673e6d029c9bf1a66515</data>
    </node>
    <node id="&quot;(V^+)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"(V^+) is a concept mentioned in the text, possibly referring to a specific voltage or value."</data>
      <data key="d2">91f030f6c14c673e6d029c9bf1a66515</data>
    </node>
    <node id="&quot;(V^-)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"(V^-) is a concept mentioned in the text, possibly referring to a specific voltage or value."</data>
      <data key="d2">91f030f6c14c673e6d029c9bf1a66515</data>
    </node>
    <node id="&quot;(V^P)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"(V^p) is a concept mentioned in the text, possibly referring to a specific voltage or value."</data>
      <data key="d2">91f030f6c14c673e6d029c9bf1a66515</data>
    </node>
    <node id="&quot;(K^+)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"(K^+) is a concept mentioned in the text, possibly referring to a specific ion or chemical."</data>
      <data key="d2">91f030f6c14c673e6d029c9bf1a66515</data>
    </node>
    <node id="&quot;BRNN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"bRNN is a concept mentioned in the text, possibly referring to a specific type of neural network or model."
"bRNN is an abbreviation used to refer to a bidirectional Recurrent Neural Network, a concept mentioned in the text."</data>
      <data key="d2">3a64c8c26895f111f00a349dd69bb505,91f030f6c14c673e6d029c9bf1a66515</data>
    </node>
    <node id="&quot;RCF&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"RCF is an abbreviation used to refer to a Recurrent Competitive Field, a type of recurrent neural network mentioned in the text."
"RCF stands for Recurrent Cascade of Firing, a type of network that exhibits shunting dynamics."
"RCF is a type of network mentioned in the text, often used in the context of competitive dynamical systems."
"RCF is not explicitly defined in the text, but it is likely a reference to a Recurrent Competitive Filter, which is a type of neural network used for pattern recognition and classification."</data>
      <data key="d2">2cbd29d6f0019f0c85bee43779ae8f4d,3a64c8c26895f111f00a349dd69bb505,b1636ec22c34ef50b57dec32239c6535,be49e9beb2d7cc985fe9f6517fa0f4fe</data>
    </node>
    <node id="&quot;BUBBLE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Bubble refers to a self-normalizing process that generates a partial contrast-enhancement, or enhancement above a quenching threshold."</data>
      <data key="d2">7ba0dfde8cc54bb1dcf66b46fcdd88f8</data>
    </node>
    <node id="&quot;RECURRENT NONLINEAR DYNAMICAL SYSTEMS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Recurrent Nonlinear Dynamical Systems are systems that exhibit cooperative-competitive behavior and are applicable to various fields."</data>
      <data key="d2">7ba0dfde8cc54bb1dcf66b46fcdd88f8</data>
    </node>
    <node id="&quot;INPUTS I_I AND J_I&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Inputs I_i and J_i are events mentioned in the text, likely referring to specific input processes in the context of the STM system."</data>
      <data key="d2">b4f6256f3430f1aa72ca8092809ebba1</data>
    </node>
    <node id="&quot;FUNCTION F(W)&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Function f(w) is an event mentioned in the text, likely referring to a mathematical function used in the description of the STM system."</data>
      <data key="d2">b4f6256f3430f1aa72ca8092809ebba1</data>
    </node>
    <node id="&quot;FUNCTION H(W)&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Function h(w) is an event mentioned in the text, likely referring to a mathematical function that exhibits a 'hill' of activity under certain conditions."</data>
      <data key="d2">b4f6256f3430f1aa72ca8092809ebba1</data>
    </node>
    <node id="&quot;MATRIX A&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Matrix A is a concept mentioned in the text, likely referring to a mathematical matrix used in the description of the STM system."</data>
      <data key="d2">b4f6256f3430f1aa72ca8092809ebba1</data>
    </node>
    <node id="&quot;MATRIX B&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Matrix B is a concept mentioned in the text, likely referring to a mathematical matrix used in the description of the STM system."</data>
      <data key="d2">b4f6256f3430f1aa72ca8092809ebba1</data>
    </node>
    <node id="&quot;PATTERN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Pattern is a term used to describe input sequences or data structures that are stored in STM."</data>
      <data key="d2">8da881a4f375e8a524fd0bf46ae2279e</data>
    </node>
    <node id="&quot;NOISE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Noise is mentioned as a factor that can be amplified by signal functions, potentially causing interference or distortion in the data processing."</data>
      <data key="d2">8da881a4f375e8a524fd0bf46ae2279e</data>
    </node>
    <node id="&quot;SIGNAL FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Signal Function is a term used to describe mathematical functions that process input data, with properties such as linearity and monotonicity being discussed."
"A Signal Function is a mathematical function used to process information in the Network."
"Signal Function is a concept mentioned in the text that must suppress noise and be faster-than-linear at small activities."</data>
      <data key="d2">0ae1c3b9a183835b90295e9712b9656d,35551dc55b5522082b778171ff6d1bf9,8da881a4f375e8a524fd0bf46ae2279e</data>
    </node>
    <node id="&quot;HILL FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Hill Function is a term used to describe a specific mathematical function mentioned in the text, which is monotone decreasing."
"A Hill Function is a mathematical function used to analyze the behavior of the Network."</data>
      <data key="d2">0ae1c3b9a183835b90295e9712b9656d,8da881a4f375e8a524fd0bf46ae2279e</data>
    </node>
    <node id="&quot;NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Network is a system that processes information and makes choices based on input patterns."
"The Network is a system that chooses the population with the initial maximum of activity and inhibits activity in all other populations, behaving like a winner-take-all binary choice machine."</data>
      <data key="d2">0ae1c3b9a183835b90295e9712b9656d,d69baa85c856a1c0b5446a9c9fcd31b8</data>
    </node>
    <node id="&quot;LINEAR SIGNAL FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Linear Signal Function is a type of Signal Function that amplifies noise and eliminates differences in inputs."</data>
      <data key="d2">0ae1c3b9a183835b90295e9712b9656d</data>
    </node>
    <node id="&quot;SLOWER-THAN-LINEAR SIGNAL FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Slower-than-Linear Signal Function is a type of Signal Function that also amplifies noise and eliminates differences in inputs."</data>
      <data key="d2">0ae1c3b9a183835b90295e9712b9656d</data>
    </node>
    <node id="&quot;FASTER-THAN-LINEAR SIGNAL FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Faster-than-Linear Signal Function is a type of Signal Function that suppresses noise and enhances differences in inputs."</data>
      <data key="d2">0ae1c3b9a183835b90295e9712b9656d</data>
    </node>
    <node id="&quot;EQUILIBRIUM POINTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Equilibrium Points are the stable states of a system, which in this context are the solutions of an equation that describes the behavior of the Network."</data>
      <data key="d2">d69baa85c856a1c0b5446a9c9fcd31b8</data>
    </node>
    <node id="&quot;SIGNAL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Signal is a concept that is mentioned in the text, but its specific nature is not explicitly described."</data>
      <data key="d2">d69baa85c856a1c0b5446a9c9fcd31b8</data>
    </node>
    <node id="&quot;BIOLOGY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Biology is mentioned in the text as a field where signal functions are studied and must be bounded."</data>
      <data key="d2">35551dc55b5522082b778171ff6d1bf9</data>
    </node>
    <node id="&quot;NOISE SUPPRESSION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Noise Suppression is a technique used to reduce unwanted signals, allowing for the storage of specific features or categories."</data>
      <data key="d2">6a47ed5881928d48cdcb74e40867a711</data>
    </node>
    <node id="&quot;SIGMOID SIGNAL FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Sigmoid Signal Function is a mathematical function that combines faster-than-linear and slower-than-linear properties, used to suppress noise and enhance activity patterns."</data>
      <data key="d2">6a47ed5881928d48cdcb74e40867a711</data>
    </node>
    <node id="&quot;QUENCHING THRESHOLD&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Quenching Threshold is a value used in the Sigmoid Signal Function that determines when activity is quenched or contrast-enhanced."</data>
      <data key="d2">6a47ed5881928d48cdcb74e40867a711</data>
    </node>
    <node id="&quot;CORTICAL MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Cortical Models are mentioned in the text, potentially referring to neural network models used to study signal functions and dynamics."</data>
      <data key="d2">6a47ed5881928d48cdcb74e40867a711</data>
    </node>
    <node id="&quot;RCFS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"RCFs are Recurrent Cortical Fields, a type of model mentioned in the text, which have been generalized and studied."
"RCFs are a type of model or theory that has been studied to explain various data, such as visual perception and decision-making."
"RCFs are likely a type of specialized process mentioned in the text, potentially related to the Normalization Rule."
"RCFs are specialized units mentioned in the text, but their nature is not explicitly defined."
"RCFs, or Recurrent Connections with Feedback, are a type of network mentioned in the text that help to store inputs in short-term memory and obey the LTM Invariance Principle."
"RCFs are a type of network model mentioned in the text, which behaves like an Item-and-Order working memory model under certain conditions."</data>
      <data key="d2">1976b19f768a8fdf37207b680c3b2b40,4364aa6091e1966365fa889b34f5cf90,6a47ed5881928d48cdcb74e40867a711,8ee5dee5c6f3e89d8d8c20e3fe957583,c38beddeac1d3cac8282ad59bc835788,dcd38cdc6195b2bbf41d936af0bf1f5f</data>
    </node>
    <node id="&quot;QT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"QT is a component of a model or theory that converts a network into a tunable filter."</data>
      <data key="d2">8ee5dee5c6f3e89d8d8c20e3fe957583</data>
    </node>
    <node id="&quot;USHER AND MCCLELLAND&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Usher and McClelland are authors who have modeled probabilistic decision-making using an Additive Model."</data>
      <data key="d2">8ee5dee5c6f3e89d8d8c20e3fe957583</data>
    </node>
    <node id="&quot;DOUGLAS ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Douglas et al. are authors who have applied shunting properties to simulate data about the properties of cortical circuits that subserve visual perception."
"Douglas et al. are authors who have applied shunting properties to simulate data about the properties of the cortical circuits that subserve visual perception."</data>
      <data key="d2">2cbd29d6f0019f0c85bee43779ae8f4d,8ee5dee5c6f3e89d8d8c20e3fe957583</data>
    </node>
    <node id="&quot;GROSSBERG AND MINGOLLA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Grossberg and Mingolla are authors who have used shunting properties to simulate data about cortical circuits that subserve visual perception."
"Grossberg and Mingolla are authors who have applied shunting properties in their research."</data>
      <data key="d2">2cbd29d6f0019f0c85bee43779ae8f4d,8ee5dee5c6f3e89d8d8c20e3fe957583</data>
    </node>
    <node id="&quot;GROSSBERG AND TODOROVIC&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Grossberg and Todorovic are authors who have used shunting properties to simulate data about cortical circuits that subserve visual perception."
"Grossberg and Todorovic are authors who have applied shunting properties in their research."</data>
      <data key="d2">2cbd29d6f0019f0c85bee43779ae8f4d,8ee5dee5c6f3e89d8d8c20e3fe957583</data>
    </node>
    <node id="&quot;HEEGER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Heeger is an author who has applied shunting properties to simulate data about cortical circuits that subserve visual perception."
"Heeger is an author who has applied shunting properties in their research."</data>
      <data key="d2">2cbd29d6f0019f0c85bee43779ae8f4d,8ee5dee5c6f3e89d8d8c20e3fe957583</data>
    </node>
    <node id="&quot;CISEK&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Cisek is an author who has applied shunting properties to simulate data about the selection of commands for arm movement control."</data>
      <data key="d2">8ee5dee5c6f3e89d8d8c20e3fe957583</data>
    </node>
    <node id="&quot;GROSSBERG AND PILLY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Grossberg and Pilly are authors who have applied shunting properties to simulate data about the control of eye movements in response to probabilistically defined visual motion signals."</data>
      <data key="d2">8ee5dee5c6f3e89d8d8c20e3fe957583</data>
    </node>
    <node id="&quot;COMPETITIVE LEARNING (CL)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Competitive Learning (CL) is a model developed by Grossberg and others, which utilizes shunting dynamics."</data>
      <data key="d2">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </node>
    <node id="&quot;ADAPTIVE RESONANCE THEORY (ART)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Adaptive Resonance Theory (ART) is a model developed by Grossberg, which does not utilize shunting dynamics."</data>
      <data key="d2">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </node>
    <node id="&quot;MCLAUGHLIN ET AL.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"McLaughlin et al. are authors who have applied shunting properties in their research."</data>
      <data key="d2">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </node>
    <node id="&quot;VON DER MALSBURG&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"von der Malsburg is an author who has developed a version of the CL model that does not utilize shunting dynamics."
"von der Malsburg is a researcher mentioned in the text, known for her work in the field of neural networks."</data>
      <data key="d2">2cbd29d6f0019f0c85bee43779ae8f4d,eb6c9a7d24cc59ff93d554093a4360a4</data>
    </node>
    <node id="&quot;PALMA ET AL.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Palma et al. are authors who have shown that an RCF with spiking neurons can replicate key properties of the Grossberg (1973) theorems for rate-based neurons."</data>
      <data key="d2">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </node>
    <node id="&quot;COMPETITIVE DYNAMICAL SYSTEMS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Competitive Dynamical Systems is a concept mentioned in the text, defined by a system of differential equations with competitive interactions between populations."</data>
      <data key="d2">b1636ec22c34ef50b57dec32239c6535</data>
    </node>
    <node id="&quot;MAY AND LEONARD MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The May and Leonard Model is a mathematical model developed by May and Leonard to study the voting paradox, which is an example of a competitive system."</data>
      <data key="d2">3334f6dcd53b71cf3ceb7648ead24d5a</data>
    </node>
    <node id="&quot;COMPETITIVE SYSTEM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Competitive System refers to a system in which entities compete for resources or advantages."</data>
      <data key="d2">3334f6dcd53b71cf3ceb7648ead24d5a</data>
    </node>
    <node id="&quot;DECISION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Decision refers to the process of making choices or decisions in a competitive system."</data>
      <data key="d2">3334f6dcd53b71cf3ceb7648ead24d5a</data>
    </node>
    <node id="&quot;VOTING PARADOX&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Voting Paradox is a phenomenon in which the outcome of a vote can be paradoxical or counterintuitive."
"The Voting Paradox is a concept introduced by Grossberg in 1975, which is studied using a method of bRNNs."</data>
      <data key="d2">0af3b52f2586c4e957aee493160223ba,3334f6dcd53b71cf3ceb7648ead24d5a</data>
    </node>
    <node id="&quot;LIAPUNOV FUNCTIONAL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Liapunov Functional is a mathematical tool used to analyze the behavior of systems, as introduced by Grossberg."</data>
      <data key="d2">0af3b52f2586c4e957aee493160223ba</data>
    </node>
    <node id="&quot;SOCIAL CHAOS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Social Chaos is a problem that arises when arbitrarily many individuals, each obeying unique and personal laws, interact with each other, leading to the question of how to achieve global order or consensus."</data>
      <data key="d2">0af3b52f2586c4e957aee493160223ba</data>
    </node>
    <node id="&quot;ALLIGOOD ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Alligood et al. is a group of researchers mentioned in the text, focusing on the question of how simple a system can be to generate chaotic behavior."</data>
      <data key="d2">597668e07c7554bd2d0cb29399285a39</data>
    </node>
    <node id="&quot;SYSTEM (21)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"System (21) is a specific type of system mentioned in the text that generates globally-consistent decision-making."
"System (21) is a special case of a competitive network with a broad inhibitory surround, which is a part of the Adaptation Level Systems."</data>
      <data key="d2">597668e07c7554bd2d0cb29399285a39,edd10f4a8bda41294ef582dc7f048ad5</data>
    </node>
    <node id="&quot;ADAPTATION LEVEL SYSTEMS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Adaptation Level Systems is a class of systems that includes a special case called System (21), characterized by globally-consistent decision-making and a broad inhibitory surround."</data>
      <data key="d2">edd10f4a8bda41294ef582dc7f048ad5</data>
    </node>
    <node id="&quot;STATE-DEPENDENT AMPLIFICATION FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"State-dependent Amplification Function is a mathematical function used in the equations defining the behavior of Adaptation Level Systems."</data>
      <data key="d2">edd10f4a8bda41294ef582dc7f048ad5</data>
    </node>
    <node id="&quot;SELF-SIGNAL FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Self-signal Function is a mathematical function used in the equations defining the behavior of Adaptation Level Systems."</data>
      <data key="d2">edd10f4a8bda41294ef582dc7f048ad5</data>
    </node>
    <node id="&quot;STATE-DEPENDENT ADAPTATION LEVEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"State-dependent Adaptation Level is a mathematical function used in the equations defining the behavior of Adaptation Level Systems."</data>
      <data key="d2">edd10f4a8bda41294ef582dc7f048ad5</data>
    </node>
    <node id="&quot;THEOREM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Theorem is a mathematical result that proves the stability of a price in a competitive market with an arbitrary number of competing firms."
"The Theorem is a mathematical result that applies to the Cohen-Grossberg Model, proving the stability of the price in a market and the balancing of each firm's books."</data>
      <data key="d2">0c9db6cd87deaca2e432c260d775349c,0dd75f3ca11854714bdbfc8a96ccf256</data>
    </node>
    <node id="&quot;COMPETITIVE MARKET&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Competitive Market is a system where multiple firms operate, each choosing a production and savings strategy to maximize net profit based on a market price."</data>
      <data key="d2">0c9db6cd87deaca2e432c260d775349c</data>
    </node>
    <node id="&quot;FIRMS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Firms are the individual entities within the Competitive Market that make decisions based on market price and their own production and savings strategies."</data>
      <data key="d2">0c9db6cd87deaca2e432c260d775349c</data>
    </node>
    <node id="&quot;COHEN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Cohen is a contributor to the Cohen-Grossberg Model and the Liapunov Function, known for their work on the adaptation level systems."</data>
      <data key="d2">0dd75f3ca11854714bdbfc8a96ccf256</data>
    </node>
    <node id="&quot;BRAIN-STATE-IN-A-BOX MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Brain-State-in-a-Box Model is mentioned in the text, which is included in the Cohen-Grossberg Model systems."</data>
      <data key="d2">0dd75f3ca11854714bdbfc8a96ccf256</data>
    </node>
    <node id="&quot;COHEN-GROSSBERG SYSTEMS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Cohen-Grossberg Systems are a class of competitive systems developed by Cohen and Grossberg, which generate jump trees and are the subject of ongoing research."
"Cohen-Grossberg Systems are mathematical models developed by Cohen and Grossberg."</data>
      <data key="d2">4a78ff105fdd9a4b0d01ccf1e5816c74,4b48be5db14c2e681ef8f4ee7de4b847</data>
    </node>
    <node id="&quot;GLOBAL EQUILIBRIUM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Global Equilibrium is a theoretical concept that Cohen and Grossberg attempted to prove for their systems, which would have significant implications for the study of competitive systems."
"Global Equilibrium is a concept that Cohen and Grossberg attempted to prove by showing that all Cohen-Grossberg systems generate jump trees, and thus no jump cycles."</data>
      <data key="d2">4a78ff105fdd9a4b0d01ccf1e5816c74,4b48be5db14c2e681ef8f4ee7de4b847</data>
    </node>
    <node id="&quot;JUMP TREES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Jump Trees are a feature of Cohen-Grossberg Systems, which are hypothesized to not contain jump cycles, aiding in the proof of Global Equilibrium."</data>
      <data key="d2">4b48be5db14c2e681ef8f4ee7de4b847</data>
    </node>
    <node id="&quot;LIAPUNOV METHODS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Liapunov Methods are a mathematical technique used to analyze the stability of dynamic systems, which Cohen and Grossberg used as inspiration in their research."</data>
      <data key="d2">4b48be5db14c2e681ef8f4ee7de4b847</data>
    </node>
    <node id="&quot;COMPETITIVE SYSTEMS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Competitive Systems are a broader class of systems that Cohen and Grossberg's research contributes to, focusing on understanding their behavior and properties."</data>
      <data key="d2">4b48be5db14c2e681ef8f4ee7de4b847</data>
    </node>
    <node id="&quot;MASKING FIELD MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Masking Field Model is a specific model developed by Cohen and Grossberg, which has been studied in the context of Global Equilibrium and jump trees."</data>
      <data key="d2">4b48be5db14c2e681ef8f4ee7de4b847</data>
    </node>
    <node id="&quot;COHEN-GROSSBERG LIAPUNOV FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Cohen-Grossberg Liapunov Function is a mathematical tool developed by Cohen and Grossberg to prove the existence of global equilibria."</data>
      <data key="d2">4a78ff105fdd9a4b0d01ccf1e5816c74</data>
    </node>
    <node id="&quot;BURTON&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Burton is a researcher who has referred to the Cohen-Grossberg-Hopfield model in their work."</data>
      <data key="d2">98173c1c0fcd64ceb914e0dd6b366b30</data>
    </node>
    <node id="&quot;BURWICK&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Burwick is a researcher who has referred to the Cohen-Grossberg-Hopfield model in their work."</data>
      <data key="d2">98173c1c0fcd64ceb914e0dd6b366b30</data>
    </node>
    <node id="&quot;GUO ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Guo et al. is a group of researchers who have referred to the Cohen-Grossberg-Hopfield model in their work."</data>
      <data key="d2">98173c1c0fcd64ceb914e0dd6b366b30</data>
    </node>
    <node id="&quot;HOPFIELD NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Hopfield Network is a neural network model published in multiple articles since the 1960s, often misattributed to other investigators."
"Hopfield Network is a type of recurrent neural network developed by John Hopfield in 1982, which was also based on the work of Shun&#8217;ichi Amari."
"The Hopfield network is a type of neural network invented by John Hopfield in 1982."
"Hopfield Network is a type of recurrent neural network (RNN) that requires stationary inputs and guarantees convergence, but it is not a general RNN."
"Hopfield Network is a type of RNN with equally sized connections across layers, used for content-addressable memory and pattern recognition."</data>
      <data key="d2">643e65a5f4132289cfd1d5b954043642,8b12ccb4afc119b3357ceff10e04ce9f,a8c0edd2cdddb7d6d899284063b541f5,b31ca51b419f7270ee5f4910c90ea331,f59839daadfb1f3832bb9f8d201a7126</data>
    </node>
    <node id="&quot;COHEN-GROSSBERG-HOPFIELD MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Cohen-Grossberg-Hopfield Model is a more historically accurate name for the Hopfield Network, used in various articles."</data>
      <data key="d2">643e65a5f4132289cfd1d5b954043642</data>
    </node>
    <node id="&quot;JOHN J. HOPFIELD&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"John J. Hopfield is a researcher who published the Hopfield Network model in multiple articles since the 1960s."</data>
      <data key="d2">643e65a5f4132289cfd1d5b954043642</data>
    </node>
    <node id="&quot;DAVID COHEN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"David Cohen is a researcher who contributed to the development of the Hopfield Network model, often referred to in the Cohen-Grossberg-Hopfield Model."</data>
      <data key="d2">643e65a5f4132289cfd1d5b954043642</data>
    </node>
    <node id="&quot;MICHAEL I. GROSSBERG&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Michael I. Grossberg is a researcher who contributed to the development of the Hopfield Network model, often referred to in the Cohen-Grossberg-Hopfield Model."</data>
      <data key="d2">643e65a5f4132289cfd1d5b954043642</data>
    </node>
    <node id="&quot;SYNCHRONIZED OSCILLATIONS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Synchronized Oscillations is a phenomenon described in the text, where neural networks can persistently oscillate."
</data>
      <data key="d2">53f5bc3f4c71310c593a23aef01d1633,643e65a5f4132289cfd1d5b954043642</data>
    </node>
    <node id="&quot;EXCITATORY FEEDBACK SIGNALS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Excitatory Feedback Signals are signals that stimulate other populations in a neural network."</data>
      <data key="d2">53f5bc3f4c71310c593a23aef01d1633</data>
    </node>
    <node id="&quot;INHIBITORY INTERNEURONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Inhibitory Interneurons are neurons that produce inhibitory signals, which can slow down the activity of other neurons."</data>
      <data key="d2">53f5bc3f4c71310c593a23aef01d1633</data>
    </node>
    <node id="&quot;SHUNTING NETWORKS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Shunting Networks are neural networks that use slow inhibitory interneurons to persistently oscillate."</data>
      <data key="d2">53f5bc3f4c71310c593a23aef01d1633</data>
    </node>
    <node id="&quot;ORDER-PRESERVING LIMIT CYCLES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Order-Preserving Limit Cycles are a type of synchronized oscillation during attentive brain dynamics, first described by Grossberg in 1976b."</data>
      <data key="d2">53f5bc3f4c71310c593a23aef01d1633</data>
    </node>
    <node id="&quot;HABITUATIVE GATES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Habituative Gates are mechanisms that multiply recurrent signals in a neural network, allowing for persistent oscillations."
"Habituative Gates are mentioned in the text as a mechanism that multiplies recurrent signals."</data>
      <data key="d2">53f5bc3f4c71310c593a23aef01d1633,9b30fc06ca06f49c2faa238da7eddc6f</data>
    </node>
    <node id="&quot;BRNNS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"bRNNs refer to Biologically Realistic Neural Networks, which are neural networks that are embodied in highly differentiated anatomical structures in the brain."</data>
      <data key="d2">53f5bc3f4c71310c593a23aef01d1633</data>
    </node>
    <node id="&quot;PERSISTENT OSCILLATIONS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">53f5bc3f4c71310c593a23aef01d1633</data>
    </node>
    <node id="&quot;HIGHLY DIFFERENTIATED ANATOMICAL STRUCTURES&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">53f5bc3f4c71310c593a23aef01d1633</data>
    </node>
    <node id="&quot;SLOW INHIBITORY INTERNEURONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Slow Inhibitory Interneurons are a type of neuron that multiply recurrent signals, as mentioned in the text."</data>
      <data key="d2">9b30fc06ca06f49c2faa238da7eddc6f</data>
    </node>
    <node id="&quot;RECURRENT NEURAL NETWORKS (RNNS)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Recurrent Neural Networks (RNNs) are mentioned in the text, and their interaction terms are characterized in abstract mathematical terms."
"RNNs are a type of neural network that can process arbitrary sequences of inputs, making them suitable for tasks like handwriting recognition and speech recognition."
"RNNs are a type of artificial neural network that processes sequential data, such as time series or natural language."
"RNNs are a type of artificial neural network designed to process sequential or time-series data, utilizing their 'memory' to take information from previous inputs, influencing the current output."
"Recurrent Neural Networks (RNNs) are a type of neural network architecture designed to process sequential data by maintaining an internal state."
"Recurrent Neural Networks (RNNs) are a type of neural network that acts as a random, nonlinear medium whose dynamic response is used as a signal base in echo state networks."</data>
      <data key="d2">04b89ad6396cb78ca75689473c47a247,24a607f45ad989d81411fed4f2941884,4246748fef7001ea0bd03ac702565b0d,9b30fc06ca06f49c2faa238da7eddc6f,a3368f9cab1f65643dba089af5a1f95e,d0a69d653d08e58959dd8d0f2033e697</data>
    </node>
    <node id="&quot;BIOLOGICALLY REALISTIC NEURAL NETWORKS (BRNNS)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Biologically Realistic Neural Networks (bRNNs) are mentioned in the text, and they are embodied in architectures with highly differentiated anatomical circuits."</data>
      <data key="d2">9b30fc06ca06f49c2faa238da7eddc6f</data>
    </node>
    <node id="&quot;CEREBRAL CORTEX&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Cerebral Cortex is mentioned in the text, and models of how it works are defined by bRNNs that integrate bottom-up, horizontal, and top-down interactions in laminar circuits."</data>
      <data key="d2">9b30fc06ca06f49c2faa238da7eddc6f</data>
    </node>
    <node id="&quot;LAMINAR COMPUTING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Laminar Computing is a computational paradigm mentioned in the text, which has begun to classify how different behavioral functions may be realized by architectures that are all variations on a shared laminar design."</data>
      <data key="d2">9b30fc06ca06f49c2faa238da7eddc6f</data>
    </node>
    <node id="&quot;LAMINART FAMILY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The LAMINART family of models is mentioned in the text, and it includes models of how the visual cortex, notably cortical areas V1, V2, and V4, interact together to see."
"LAMINART Family is a model that explains how the visual cortex interacts to see, focusing on areas V1, V2, and V4."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685,9b30fc06ca06f49c2faa238da7eddc6f</data>
    </node>
    <node id="&quot;CARPENTER AND GROSSBERG&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Carpenter and Grossberg are mentioned in the text as authors of a work that is referenced."</data>
      <data key="d2">9b30fc06ca06f49c2faa238da7eddc6f</data>
    </node>
    <node id="&quot;CAO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Cao is a researcher mentioned in the context of the LAMINART Family model."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685</data>
    </node>
    <node id="&quot;RAIZADA&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Raizada is a researcher mentioned in the context of the LAMINART Family model."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685</data>
    </node>
    <node id="&quot;LIST PARSE MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"LIST PARSE Model is a model that explains how prefrontal cortical working memory and list chunk learning interact with volitional processes to generate motor trajectory commands."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685</data>
    </node>
    <node id="&quot;PEARSON&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Pearson is a researcher mentioned in the context of the LIST PARSE Model."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685</data>
    </node>
    <node id="&quot;CARTWORD MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"cARTWORD Model is a model that explains contextual interactions during speech perception by the auditory cortex, including backwards effects in time."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685</data>
    </node>
    <node id="&quot;KAZEROUNIAN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Kazerounian is a researcher mentioned in the context of the cARTWORD Model."
"Kazerounian is a co-author of a study that introduces the TELOS Model and its components."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685,75495c1fc835d41adf5afcb01e8e520a</data>
    </node>
    <node id="&quot;TELOS MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"TELOS Model is a model that explains learning and choice of saccadic eye movement commands by interactions between prefrontal cortex, frontal eye fields, posterior parietal cortex, and anterior and posterior inferotemporal cortex, as well as basal ganglia circuits."
"The TELOS Model is a model of learning and choice of saccadic eye movement commands, involving interactions between various brain regions."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685,75495c1fc835d41adf5afcb01e8e520a</data>
    </node>
    <node id="&quot;PFC&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"PFC is a part of the brain mentioned in the context of the TELOS Model."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685</data>
    </node>
    <node id="&quot;FEF&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"FEF is a part of the brain mentioned in the context of the TELOS Model."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685</data>
    </node>
    <node id="&quot;PPC&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"PPC is a part of the brain mentioned in the context of the TELOS Model."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685</data>
    </node>
    <node id="&quot;ITA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ITa is a part of the brain mentioned in the context of the TELOS Model."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685</data>
    </node>
    <node id="&quot;ITP&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ITp is a part of the brain mentioned in the context of the TELOS Model."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685</data>
    </node>
    <node id="&quot;BG&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"BG is a part of the brain mentioned in the context of the TELOS Model."</data>
      <data key="d2">6648b18760b8b182e1097ad15c4df685</data>
    </node>
    <node id="&quot;LISTELOS MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The lisTELOS Model is a model of learning and choice of sequences of saccadic eye movements, involving an Item-Order-Rank spatial working memory in the prefrontal cortex and interactions with other brain regions."</data>
      <data key="d2">75495c1fc835d41adf5afcb01e8e520a</data>
    </node>
    <node id="&quot;PREFRONTAL CORTEX&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Prefrontal Cortex is a brain region involved in both the TELOS and lisTELOS Models, playing a role in learning and choice of eye movement commands."</data>
      <data key="d2">75495c1fc835d41adf5afcb01e8e520a</data>
    </node>
    <node id="&quot;FRONTAL EYE FIELDS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Frontal Eye Fields are a brain region involved in both the TELOS and lisTELOS Models, playing a role in the generation of eye movement commands."</data>
      <data key="d2">75495c1fc835d41adf5afcb01e8e520a</data>
    </node>
    <node id="&quot;POSTERIOR PARIETAL CORTEX (PPC)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"PPC is a region of the brain that interacts with other regions to carry out specific operations."
"Posterior Parietal Cortex (PPC) is a region of the brain involved in spatial orientation, attention, and visual perception."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232,db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;FRONTAL EYE FIELDS (FEF)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"FEF is a region of the brain that interacts with other regions to carry out specific operations."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;BASAL GANGLIA (BG)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"BG is a region of the brain that interacts with other regions to carry out specific operations."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;SUPERIOR COLLICULUS (SC)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"SC is a region of the brain that interacts with other regions to carry out specific operations."
"Superior Colliculus (SC) is a region of the brainstem involved in visual processing and motor control."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232,db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;MOTIVATOR MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"MOTIVATOR Model is a model that focuses on valued goals and blocks learning of irrelevant events during reinforcement learning and motivated attention."
"MOTIVATOR Model is a brain mechanism model that emerged from the ideas and generalizations of the Cognitive-Emotional-Motor (CogEM) Theory."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232,f373521b781482587f80fffc5623a1f9</data>
    </node>
    <node id="&quot;INFEROTEMPORAL (IT) CORTEX&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"IT Cortex is a region of the brain that interacts with other regions in cognitive-emotional interactions."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;RHINAL (RHIN) CORTEX&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"RHIN Cortex is a region of the brain that interacts with other regions in cognitive-emotional interactions."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;LATERAL ORBITOFRONTAL CORTEX (ORBL)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ORBl is a region of the brain that interacts with other regions in cognitive-emotional interactions."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;MEDIAL ORBITOFRONTAL CORTEX (ORBM)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ORBm is a region of the brain that interacts with other regions in cognitive-emotional interactions."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;AMYGDALA (AMYGD)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"AMYGD is a region of the brain that interacts with other regions in cognitive-emotional interactions."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;LATERAL HYPOTHALAMUS (LH)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"LH is a region of the brain that interacts with other regions in cognitive-emotional interactions."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;BASAL GANGLIA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Basal Ganglia is a region of the brain that interacts with other regions in cognitive-emotional interactions."
"Basal Ganglia is a group of nuclei in the brain involved in movement, emotion, and motivation."
"Basal ganglia are a group of subcortical structures that modulate song performance in songbirds."
"Basal Ganglia is a brain region that modulates song performance in songbirds, indicating its role in flexible performance."</data>
      <data key="d2">44ddf121af4b66da2bfd6b2ac0637a23,7aeda101aa8aba76f319932f0bd568f7,89a24f37cf198d043ccd6b6b795dc232,db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;ARTSCAN MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ARTSCAN Model is a model that focuses on view-invariant object learning and visual search during unconstrained saccadic eye movements."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;VISUAL CORTEX V1&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Visual Cortex V1 is a region of the brain that interacts with other regions in the ARTSCAN Model."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;VISUAL CORTEX V2&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Visual Cortex V2 is a region of the brain that interacts with other regions in the ARTSCAN Model."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;VISUAL CORTEX V3A&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Visual Cortex V3A is a region of the brain that interacts with other regions in the ARTSCAN Model."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;VISUAL CORTEX V4&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Visual Cortex V4 is a region of the brain that interacts with other regions in the ARTSCAN Model."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;PREFRONTAL CORTEX (PFC)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Prefrontal Cortex (PFC) is a region of the brain that interacts with other regions in the ARTSCAN Model."
"Prefrontal Cortex (PFC) is a region of the brain involved in decision-making, planning, and cognitive control."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232,db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;POSTERIOR&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"Posterior is a region of the brain that interacts with other regions in the ARTSCAN Model."</data>
      <data key="d2">89a24f37cf198d043ccd6b6b795dc232</data>
    </node>
    <node id="&quot;AMYGDALA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Amygdala is a part of the brain involved in processing emotions and fear responses."</data>
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;LATERAL HYPOTHALAMUS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Lateral Hypothalamus is a part of the brain involved in regulating various bodily functions, including reward and motivation."</data>
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;REWARD EXPECTATION FILTER&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Reward Expectation Filter is a mechanism that modulates the reward value of stimuli based on previous experiences."</data>
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;ARTSCAN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ARTSCAN is a model that simulates view-invariant object learning and visual search during unconstrained saccadic eye movements."</data>
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;VISUAL CORTICES V1, V2, V3A, AND V4&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Visual Cortices V1, V2, V3A, and V4 are areas of the brain involved in processing visual information."</data>
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;LATERAL INTRAPARIETAL AREA (LIP)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Lateral Intraparietal Area (LIP) is a region of the brain involved in visual processing and spatial attention."</data>
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;POSTERIOR AND ANTERIOR INFEROTEMPORAL CORTEX (PIT, AIT)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Posterior and Anterior Inferotemporal Cortex (pIT, aIT) are regions of the brain involved in object recognition and visual perception."</data>
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;ARTSCENE SEARCH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ARTSCENE Search is a model that simulates object and spatial contextual cueing of visual search for desired objects in a scene."</data>
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;VENTRAL AND DORSOLATERAL PREFRONTAL CORTEX (VPFC, DLPFC)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Ventral and Dorsolateral Prefrontal Cortex (VPFC, DLPFC) are regions of the brain involved in decision-making, planning, and cognitive control."</data>
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;PERIRHINAL CORTEX (PRC)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Perirhinal Cortex (PRC) is a region of the brain involved in spatial navigation and memory."</data>
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;PARAHIPPOCAMPAL CORTEX (PHC)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Parahippocampal Cortex (PHC) is a region of the brain involved in spatial navigation and memory."</data>
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;GRIDPLACEMAP&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"GridPlaceMap is a model that simulates the formation of a grid cell representation of space."</data>
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;VISUAL CORTICES V1, V2, AND V4&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;POSTERIOR AND ANTERIOR INFEROTEMPORAL CORTEX (ITA, ITP)&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;BRAIN REGIONS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">db27e91f327c97c16df11a500cdeab4d</data>
    </node>
    <node id="&quot;NEURONAL LEARNING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Neuronal Learning is a process described in the text that involves the interaction of STM and LTM."</data>
      <data key="d2">b881b9051ad24c6a16b468803fba51d3</data>
    </node>
    <node id="&quot;SPATIAL PATTERN LEARNING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Spatial Pattern Learning is a process mentioned in the text that is achieved through the interaction of STM and LTM."
"Spatial Pattern Learning is a concept mentioned in the text, referring to the ability of certain anatomies to learn patterns."</data>
      <data key="d2">5d6b6e0d1a9ace28e21dce2cb0ac78c0,b881b9051ad24c6a16b468803fba51d3</data>
    </node>
    <node id="&quot;GENERALIZED ADDITIVE RNNS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Generalized Additive RNNs is a model mentioned in the text that is capable of unbiased spatial pattern learning."
"Generalized Additive RNNs is a type of architecture that includes interactions between STM and LTM, allowing it to learn from its environments."</data>
      <data key="d2">5812b5d4bcdfbf80de28dca56a6559b3,b881b9051ad24c6a16b468803fba51d3</data>
    </node>
    <node id="&quot;MATHEMATICAL THEOREMS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Mathematical Theorems are mentioned in the text as a foundation for the capability of certain models to learn spatial patterns unbiasedly."
"Mathematical Theorems are mentioned as a foundation for the learning capabilities of the architectures."</data>
      <data key="d2">5812b5d4bcdfbf80de28dca56a6559b3,b881b9051ad24c6a16b468803fba51d3</data>
    </node>
    <node id="&quot;SIGNAL TRANSMISSION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Signal Transmission is a concept mentioned in the text, referring to the process of transmitting signals between cells."</data>
      <data key="d2">5d6b6e0d1a9ace28e21dce2cb0ac78c0</data>
    </node>
    <node id="&quot;KATZ&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Katz is a researcher who contributed to the understanding of signal density and its effect on postsynaptic cells."</data>
      <data key="d2">a94f07c842345c77af089558b0786bfe</data>
    </node>
    <node id="&quot;SIGNAL PROPAGATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Signal Propagation refers to the transmission of signals along axons and their effect on postsynaptic cells."</data>
      <data key="d2">a94f07c842345c77af089558b0786bfe</data>
    </node>
    <node id="&quot;AXON&quot;">
      <data key="d0">"BIO"</data>
      <data key="d1">"Axon is a type of nerve cell that transmits signals from the cell body to other cells."</data>
      <data key="d2">a94f07c842345c77af089558b0786bfe</data>
    </node>
    <node id="&quot;SYNAPTIC KNOB&quot;">
      <data key="d0">"BIO"</data>
      <data key="d1">"Synaptic Knob is a structure on the axon where signals interact with postsynaptic cells."</data>
      <data key="d2">a94f07c842345c77af089558b0786bfe</data>
    </node>
    <node id="&quot;IONIC FLUXES&quot;">
      <data key="d0">"BIO"</data>
      <data key="d1">"Ionic Fluxes refer to the movement of ions in response to signals, contributing to signal transmission."</data>
      <data key="d2">a94f07c842345c77af089558b0786bfe</data>
    </node>
    <node id="&quot;CHEMICAL TRANSMITTER&quot;">
      <data key="d0">"BIO"</data>
      <data key="d1">"Chemical Transmitter is a substance released at synaptic knobs to communicate signals to postsynaptic cells."</data>
      <data key="d2">a94f07c842345c77af089558b0786bfe</data>
    </node>
    <node id="&quot;SIGNAL DENSITY&quot;">
      <data key="d0">"BIO"</data>
      <data key="d1">"Signal Density refers to the concentration of signals at synaptic knobs, influencing chemical transmitter release and postsynaptic cell effect."</data>
      <data key="d2">a94f07c842345c77af089558b0786bfe</data>
    </node>
    <node id="&quot;SIGNAL VELOCITY&quot;">
      <data key="d0">"BIO"</data>
      <data key="d1">"Signal Velocity refers to the speed at which signals propagate along axons, which should be proportional to axon length for unbiased learning."
"Signal Velocity refers to the speed at which signals are transmitted through axons."</data>
      <data key="d2">18be9bfe53d3b9c1e15c1c8238674459,a94f07c842345c77af089558b0786bfe</data>
    </node>
    <node id="&quot;AXON LENGTH&quot;">
      <data key="d0">"BIO"</data>
      <data key="d1">"Axon Length refers to the length of a nerve axon, which should be proportional to axon diameter for consistent signal velocity."
"Axon Length refers to the physical length of axons, which can influence signal transmission."</data>
      <data key="d2">18be9bfe53d3b9c1e15c1c8238674459,a94f07c842345c77af089558b0786bfe</data>
    </node>
    <node id="&quot;AXONS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Axons are part of a neural system, transmitting signals from source cells to target cells."</data>
      <data key="d2">18be9bfe53d3b9c1e15c1c8238674459</data>
    </node>
    <node id="&quot;SOURCE CELLS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Source Cells are the origin points of signals transmitted through axons."</data>
      <data key="d2">18be9bfe53d3b9c1e15c1c8238674459</data>
    </node>
    <node id="&quot;TARGET CELLS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Target Cells are the end points of signals transmitted through axons."</data>
      <data key="d2">18be9bfe53d3b9c1e15c1c8238674459</data>
    </node>
    <node id="&quot;AXON DIAMETER&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Axon Diameter refers to the width of axons, which can also influence signal transmission."</data>
      <data key="d2">18be9bfe53d3b9c1e15c1c8238674459</data>
    </node>
    <node id="&quot;LTM TRACES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"LTM Traces are long-term memory traces, representing adaptive weights in a neural system."
"LTM Traces are a component of the Generalized Additive System, representing the adaptive weights of the system."</data>
      <data key="d2">18be9bfe53d3b9c1e15c1c8238674459,8bb0e63353e66a2c60a878028beff5f9</data>
    </node>
    <node id="&quot;GENERALIZED ADDITIVE SYSTEM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Generalized Additive System is a model described in the text, with activities represented by STM traces and adaptive weights represented by LTM traces."</data>
      <data key="d2">8bb0e63353e66a2c60a878028beff5f9</data>
    </node>
    <node id="&quot;SAMPLED CELLS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Sampled Cells are a component of the Generalized Additive System, representing the cells that are being observed or sampled."</data>
      <data key="d2">8bb0e63353e66a2c60a878028beff5f9</data>
    </node>
    <node id="&quot;SAMPLING CELLS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Sampling Cells are a component of the Generalized Additive System, representing the cells that are actively sampling the system."</data>
      <data key="d2">8bb0e63353e66a2c60a878028beff5f9</data>
    </node>
    <node id="&quot;UNBIASED SPATIAL PATTERN LEARNING THEOREM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Unbiased Spatial Pattern Learning Theorem proves how unbiased learning may occur in response to correlated stimuli and spatial patterns."
"The Unbiased Spatial Pattern Learning Theorem is a mathematical concept mentioned in the text, which guarantees the learning of spatial patterns."</data>
      <data key="d2">4959d1559344e462a6a7463fd3273659,6e8f1f4e6c7865b14f3b5665aa62e12e</data>
    </node>
    <node id="&quot;CONDITIONED STIMULI (CS)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Conditioned Stimuli (CS) are signals that are correlated with particular spatial patterns in the context of the Unbiased Spatial Pattern Learning Theorem."</data>
      <data key="d2">4959d1559344e462a6a7463fd3273659</data>
    </node>
    <node id="&quot;UNCONDITIONED STIMULI (US)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Unconditioned Stimuli (US) are particular spatial patterns that are correlated with Conditioned Stimuli in the context of the Unbiased Spatial Pattern Learning Theorem."</data>
      <data key="d2">4959d1559344e462a6a7463fd3273659</data>
    </node>
    <node id="&quot;PAVLOVIAN CONDITIONING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Pavlovian Conditioning is a form of associative learning that involves pairing a stimulus with a response to create a conditioned response."</data>
      <data key="d2">c486b91dc15a126174fe546094568aaa</data>
    </node>
    <node id="&quot;CS AND US&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"CS and US are likely abbreviations for Conditioned Stimulus and Unconditioned Stimulus, which are components of Pavlovian Conditioning."</data>
      <data key="d2">c486b91dc15a126174fe546094568aaa</data>
    </node>
    <node id="&quot;PATTERN LEARNING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Pattern Learning refers to the ability of a system to recognize and reproduce patterns, which is discussed in the context of Grossberg's work."</data>
      <data key="d2">c486b91dc15a126174fe546094568aaa</data>
    </node>
    <node id="&quot;OUTSTAR LEARNING THEOREM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Outstar Learning Theorem is a theorem presented by Grossberg that discusses the conditions under which perfect pattern learning occurs in a network."
"The Outstar Learning Theorem is a learning theory proposed by Stanley Grossberg, which suggests how a series of Outstars can learn an arbitrary spatiotemporal pattern."</data>
      <data key="d2">be49e9beb2d7cc985fe9f6517fa0f4fe,c486b91dc15a126174fe546094568aaa</data>
    </node>
    <node id="&quot;GENERALIZED ADDITIVE MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Generalized Additive Model is a statistical model that allows for the flexible representation of relationships between variables."</data>
      <data key="d2">c486b91dc15a126174fe546094568aaa</data>
    </node>
    <node id="&quot;GROSSBERG AND SOMERS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Grossberg and Somers is a collaboration between Grossberg and Somers, which is mentioned in the context of resynchronizing activities in a network."</data>
      <data key="d2">c486b91dc15a126174fe546094568aaa</data>
    </node>
    <node id="&quot;GROSSBERG AND GRUNEWALD&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Grossberg and Grunewald is a collaboration between Grossberg and Grunewald, which is mentioned in the context of resynchronizing activities in a network."</data>
      <data key="d2">c486b91dc15a126174fe546094568aaa</data>
    </node>
    <node id="&quot;SOMERS AND KOPELL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Somers and Kopell is a collaboration between Somers and Kopell, which is mentioned in the context of resynchronizing activities in a network."</data>
      <data key="d2">c486b91dc15a126174fe546094568aaa</data>
    </node>
    <node id="&quot;YAZDANBAKHSH AND GROSSBERG&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Yazdanbakhsh and Grossberg is a collaboration between Yazdanbakhsh and Grossberg, which is mentioned in the context of resynchronizing activities in a network."</data>
      <data key="d2">c486b91dc15a126174fe546094568aaa</data>
    </node>
    <node id="&quot;STANLEY GROSSBERG&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Stanley Grossberg is a researcher known for his contributions to neural networks and learning theories, including the Outstar Learning Theorem and the Sparse Stable Category Learning Theorem."</data>
      <data key="d2">be49e9beb2d7cc985fe9f6517fa0f4fe</data>
    </node>
    <node id="&quot;SPARSE STABLE CATEGORY LEARNING THEOREM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Sparse Stable Category Learning Theorem is another learning theory proposed by Stanley Grossberg, which occurs using the dual network to the Outstar, namely the Instar. This theorem involves multiple Instars competing with each other via a RCF to form a Competitive Learning or Self-Organizing Map network."</data>
      <data key="d2">be49e9beb2d7cc985fe9f6517fa0f4fe</data>
    </node>
    <node id="&quot;INSTAR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Instar is the dual network to the Outstar, which is used in the Sparse Stable Category Learning Theorem."</data>
      <data key="d2">be49e9beb2d7cc985fe9f6517fa0f4fe</data>
    </node>
    <node id="&quot;COMPETITIVE LEARNING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Competitive Learning is a type of unsupervised learning in which a network of neurons competes for the right to respond to a given input pattern."
"Competitive Learning is a type of learning model that involves competition among neurons to respond to input patterns."
"Competitive Learning is a method mentioned in the text, used to allow source cells to self-organize as learned category cells."</data>
      <data key="d2">01e2a32da700813f593038a23a618e55,be49e9beb2d7cc985fe9f6517fa0f4fe,eb6c9a7d24cc59ff93d554093a4360a4</data>
    </node>
    <node id="&quot;SELF-ORGANIZING MAP&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Self-Organizing Map (SOM) is a type of artificial neural network that is trained using unsupervised learning to produce a low-dimensional representation of the input space of the training data."
"Self-Organizing Map is a type of learning model that dynamically organizes input data."</data>
      <data key="d2">01e2a32da700813f593038a23a618e55,be49e9beb2d7cc985fe9f6517fa0f4fe</data>
    </node>
    <node id="&quot;COMPETITIVE LEARNING OR SELF-ORGANIZING MAP NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Competitive Learning or Self-Organizing Map Network is a type of network mentioned in the text, which is formed by multiple Instars competing via a RCF."</data>
      <data key="d2">644602009dec8474bb5cd4702b391d3e</data>
    </node>
    <node id="&quot;KOSKO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Kosko is a person mentioned in the text, who has adapted the Cohen-Grossberg Model."
"Kosko is a researcher who adapted the Cohen-Grossberg model and Liapunov function to define a system that combines STM and LTM."
"Kosko is a person who referred to the equation in (39) as the signal Hebb law, although it does not fully obey Hebb's property."</data>
      <data key="d2">01e2a32da700813f593038a23a618e55,32b8b59687b8d1556ec90c99a090653c,644602009dec8474bb5cd4702b391d3e</data>
    </node>
    <node id="&quot;LONG-TERM MEMORY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Long-Term Memory, or LTM, is a type of memory that retains information for a longer period of time."
"Long-Term Memory is a cognitive system that stores information over extended periods, supporting stable learning and the retention of list chunks."</data>
      <data key="d2">01e2a32da700813f593038a23a618e55,1976b19f768a8fdf37207b680c3b2b40</data>
    </node>
    <node id="&quot;PASSIVE DECAY ASSOCIATIVE LAW&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Passive Decay Associative Law is a learning rule introduced by Grossberg in 1967 that describes how connections between neurons decay over time."</data>
      <data key="d2">01e2a32da700813f593038a23a618e55</data>
    </node>
    <node id="&quot;BAM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"BAM is a type of model that applies to learning laws, such as the passive decay associative law and the signal Hebb law."
"BAM is an organization mentioned in the text, which was inspired by Adaptive Resonance Theory."</data>
      <data key="d2">32b8b59687b8d1556ec90c99a090653c,554e8565591507441cecaa652cb926db</data>
    </node>
    <node id="&quot;HEBB&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Hebb is a person who ascribed the property of monotonely increasing learned weights to his law in the 1940s."</data>
      <data key="d2">32b8b59687b8d1556ec90c99a090653c</data>
    </node>
    <node id="&quot;CARPENTER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Carpenter is a person mentioned in the text who has discussed the problem of catastrophic forgetting in relation to learning new facts."</data>
      <data key="d2">554e8565591507441cecaa652cb926db</data>
    </node>
    <node id="&quot;FRENCH&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"French is a person mentioned in the text who has also discussed the problem of catastrophic forgetting in relation to learning new facts."</data>
      <data key="d2">554e8565591507441cecaa652cb926db</data>
    </node>
    <node id="&quot;PAGE&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Page is a person mentioned in the text who has discussed the problem of catastrophic forgetting in relation to learning new facts."</data>
      <data key="d2">554e8565591507441cecaa652cb926db</data>
    </node>
    <node id="&quot;DESIMONE&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Desimone is a person mentioned in the text who has discussed the operation of attention via a form of self-normalizing 'biased competition' in relation to Adaptive Resonance Theory."
"Desimone is a researcher mentioned in the text as having contributed to the concept of self-normalizing biased competition in the context of attention."</data>
      <data key="d2">3d5b88f7f81ed9e14f07335bbef17020,554e8565591507441cecaa652cb926db</data>
    </node>
    <node id="&quot;SCHOLARPEDIA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Scholarpedia is a peer-reviewed online encyclopedia that provides open access to scholarly articles."</data>
      <data key="d2">3d5b88f7f81ed9e14f07335bbef17020</data>
    </node>
    <node id="&quot;BADDELEY&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Baddeley is a psychologist and author known for his contributions to the study of Working Memory and Short-Term Memory."</data>
      <data key="d2">3d5b88f7f81ed9e14f07335bbef17020</data>
    </node>
    <node id="&quot;COGNITIVE SCIENTISTS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Cognitive Scientists are researchers studying the processes of the mind and cognition."</data>
      <data key="d2">5c4e24fc9bd10d0bd59a84d56f960cf9</data>
    </node>
    <node id="&quot;NEUROSCIENTISTS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Neuroscientists are researchers studying the brain and its functions."</data>
      <data key="d2">5c4e24fc9bd10d0bd59a84d56f960cf9</data>
    </node>
    <node id="&quot;EVENT SEQUENCES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Event Sequences are sequences of events that are temporarily stored in Working Memory."</data>
      <data key="d2">5c4e24fc9bd10d0bd59a84d56f960cf9</data>
    </node>
    <node id="&quot;LIST CHUNKS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"List Chunks are unitized plans that group events in Working Memory, allowing for later performance."
"List Chunks are sequences of items that can be learned and recognized as a single unit, enabling efficient storage and recall in working memory."
"List Chunks are units of learned sequences that can create context and control subsequent responses in verbal, spatial, and motor learning."</data>
      <data key="d2">5c4e24fc9bd10d0bd59a84d56f960cf9,a9285aaa34de96a8fa62903437d2f3c4,cdd2935776d71ef9fd3a33979af0b9b5</data>
    </node>
    <node id="&quot;ATKINSON AND SHIFFRIN MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Atkinson and Shiffrin Model is a popular model of Working Memory that proposes binary activations of a series of items."</data>
      <data key="d2">5c4e24fc9bd10d0bd59a84d56f960cf9</data>
    </node>
    <node id="&quot;GROSSBERG'S ITEM-AND-ORDER WM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Grossberg's Item-and-Order WM is a model of Working Memory that represents items and their order as a temporally evolving spatial pattern of activity across working memory cells."</data>
      <data key="d2">5c4e24fc9bd10d0bd59a84d56f960cf9</data>
    </node>
    <node id="&quot;ATKINSON AND SHIFFRIN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Atkinson and Shiffrin are researchers who proposed a binary activation model of working memory, which is contrasted with the Item-and-Order WM model."</data>
      <data key="d2">a9285aaa34de96a8fa62903437d2f3c4</data>
    </node>
    <node id="&quot;ITEM-AND-ORDER MODELS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Item-and-Order models are a type of model that Grossberg predicted to embody two constraints for stable learning and memory of list chunks."</data>
      <data key="d2">4364aa6091e1966365fa889b34f5cf90</data>
    </node>
    <node id="&quot;LTM INVARIANCE PRINCIPLE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The LTM Invariance Principle is a postulate that ensures stable learning and memory of list chunks, allowing new superset list chunks to be learned without distorting existing representations."
"LTM Invariance Principle is a postulate that ensures stable learning and memory of list chunks without catastrophic forgetting of familiar subset list chunks."
"LTM Invariance Principle is a principle mentioned in the text, which suggests that all working memories are specialized versions of the same underlying network design."</data>
      <data key="d2">4364aa6091e1966365fa889b34f5cf90,c38beddeac1d3cac8282ad59bc835788,dcd38cdc6195b2bbf41d936af0bf1f5f</data>
    </node>
    <node id="&quot;RECURRENT CONNECTIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Recurrent Connections are connections in a network that allow for the storage of inputs in short-term memory after the inputs shut off."</data>
      <data key="d2">1976b19f768a8fdf37207b680c3b2b40</data>
    </node>
    <node id="&quot;GROSSBERG AND PEARSON&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Grossberg and Pearson are mentioned as the authors of a model that generalizes Item-and-Order working memories to include rank information, permitting the temporary storage of repeated items in a list."</data>
      <data key="d2">c38beddeac1d3cac8282ad59bc835788</data>
    </node>
    <node id="&quot;VENTROLATERAL PREFRONTAL CORTEX&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"Ventrolateral Prefrontal Cortex is a brain region mentioned in the text, where the deeper layers are predicted to realize verbal, spatial, and motor working memories."</data>
      <data key="d2">c38beddeac1d3cac8282ad59bc835788</data>
    </node>
    <node id="&quot;FRONTAL CORTEX&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Frontal Cortex is a region of the brain involved in various cognitive functions, including working memory."
"Frontal cortex is a brain region that modulates song performance in higher species, such as songbirds."
"Frontal Cortex is a brain region that modulates song performance in songbirds, indicating its role in flexible performance."</data>
      <data key="d2">296fa3812e2c81f685d8cdc403cb03dd,44ddf121af4b66da2bfd6b2ac0637a23,7aeda101aa8aba76f319932f0bd568f7</data>
    </node>
    <node id="&quot;ITEM-AND-ORDER WORKING MEMORY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Item-and-Order Working Memory is a cognitive model that stores and recalls lists in a specific order, influenced by primacy, recency, and bowed activation gradients."</data>
      <data key="d2">296fa3812e2c81f685d8cdc403cb03dd</data>
    </node>
    <node id="&quot;ITEM-ORDER-RANK WORKING MEMORY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Item-Order-Rank Working Memory is a variant of Item-and-Order Working Memory that includes positional information, allowing the temporary storage of repeated items in a list."</data>
      <data key="d2">296fa3812e2c81f685d8cdc403cb03dd</data>
    </node>
    <node id="&quot;FREE RECALL&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Free Recall is a cognitive process where individuals try to recall a once-heard list in any order, typically showing patterns of primacy and recency."</data>
      <data key="d2">296fa3812e2c81f685d8cdc403cb03dd</data>
    </node>
    <node id="&quot;BRADSKI ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Bradski et al. is a group of researchers who mathematically proved the patterns of activation in an Item-and-Order working memory."
"Bradski et al. is a group of researchers who defined the STORE model, a family of Item-and-Order RNNs with mathematically provable primacy, recency, and bowed gradient properties."</data>
      <data key="d2">392028b79561bd7471cb68e7c9258b1e,c3257facbf1b0a5da49d6a115f66df87</data>
    </node>
    <node id="&quot;BOARDMAN AND BULLOCK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Boardman and Bullock are researchers who have developed variants of the Item-and-Order working memory design."</data>
      <data key="d2">c3257facbf1b0a5da49d6a115f66df87</data>
    </node>
    <node id="&quot;HOUGHTON AND HARTLEY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Houghton and Hartley are researchers who have contributed to the Item-and-Order working memory design."</data>
      <data key="d2">c3257facbf1b0a5da49d6a115f66df87</data>
    </node>
    <node id="&quot;PAGE AND NORRIS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Page and Norris are researchers who have developed variants of the Item-and-Order working memory design."</data>
      <data key="d2">c3257facbf1b0a5da49d6a115f66df87</data>
    </node>
    <node id="&quot;RHODES ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Rhodes et al. are researchers who have developed variants of the Item-and-Order working memory design."</data>
      <data key="d2">c3257facbf1b0a5da49d6a115f66df87</data>
    </node>
    <node id="&quot;HOUGHTON&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Houghton is a researcher who has referred to Item-and-Order models as Competitive Queuing models."</data>
      <data key="d2">c3257facbf1b0a5da49d6a115f66df87</data>
    </node>
    <node id="&quot;FARRELL AND LEWANDOWSKY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Farrell and Lewandowsky are researchers who have provided experimental support for Item-and-Order working memory properties."</data>
      <data key="d2">c3257facbf1b0a5da49d6a115f66df87</data>
    </node>
    <node id="&quot;AVERBECK ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Averbeck et al. are researchers who have reported neurophysiological evidence supporting the primacy gradient in Item-and-Order working memory."
"Averbeck et al. is a research group that has provided neurophysiological evidence of primacy gradients and inhibition in sequential copying movements."</data>
      <data key="d2">1c0f47f0b77faab56cbeba0e1e3e7e70,c3257facbf1b0a5da49d6a115f66df87</data>
    </node>
    <node id="&quot;ITEM-AND-ORDER WM&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">c3257facbf1b0a5da49d6a115f66df87</data>
    </node>
    <node id="&quot;JONES ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Jones et al. is a research group that has reported performance characteristics of verbal working memory for a spatial serial recall task."</data>
      <data key="d2">1c0f47f0b77faab56cbeba0e1e3e7e70</data>
    </node>
    <node id="&quot;AGAM ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Agam et al. is a research group that has reported psychophysical evidence of Item-and-Order WM properties in humans performing sequential copying movements."
"Agam et al. is a research group that reported data consistent with the formation of list chunks, supporting Grossberg's predictions."</data>
      <data key="d2">1c0f47f0b77faab56cbeba0e1e3e7e70,97a9ce754fa34aaf01d6cce57560b247</data>
    </node>
    <node id="&quot;SILVER ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Silver et al. is a research group that has used Item-and-Order WMs to simulate neurophysiological data about spatial working memories."</data>
      <data key="d2">1c0f47f0b77faab56cbeba0e1e3e7e70</data>
    </node>
    <node id="&quot;MILLER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Miller is a psychologist whose work on immediate memory span is referenced in the text."
"Miller is a psychologist who proposed the concept of the immediate memory span."
"Miller is a researcher mentioned in the text who has made contributions to the understanding of immediate memory span."</data>
      <data key="d2">97a9ce754fa34aaf01d6cce57560b247,b69b23b14e0feccb488ba5412db0824c,c580fa74e3c36285cfae7df56340a990</data>
    </node>
    <node id="&quot;MURDOCK&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Murdock is a psychologist whose work on recall patterns is referenced in the text."</data>
      <data key="d2">97a9ce754fa34aaf01d6cce57560b247</data>
    </node>
    <node id="&quot;VON RESTORFF&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Von Restorff is a psychologist who studied the effects of similarity and distinctiveness in visual perception, leading to the concept of isolation effects."</data>
      <data key="d2">b69b23b14e0feccb488ba5412db0824c</data>
    </node>
    <node id="&quot;IMMEDIATE MEMORY SPAN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Immediate Memory Span is the limited number of items that can be held in short-term memory, as proposed by Miller (1956)."
"Immediate Memory Span refers to the maximum number of items that can be held in Working Memory for immediate use."</data>
      <data key="d2">b69b23b14e0feccb488ba5412db0824c,c580fa74e3c36285cfae7df56340a990</data>
    </node>
    <node id="&quot;TRANSIENT MEMORY SPAN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Transient Memory Span is a concept that differs from the Immediate Memory Span, suggesting a more dynamic and temporary holding capacity for items in memory."
"Transient Memory Span refers to the longest list length for which a Working Memory can store a primacy gradient without significant top-down Long-Term Memory component."</data>
      <data key="d2">b69b23b14e0feccb488ba5412db0824c,c580fa74e3c36285cfae7df56340a990</data>
    </node>
    <node id="&quot;SERIAL VERBAL LEARNING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Serial Verbal Learning is a process that involves learning and recalling a sequence of verbal items, which can be influenced by associative and competitive mechanisms."</data>
      <data key="d2">b69b23b14e0feccb488ba5412db0824c</data>
    </node>
    <node id="&quot;WM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"WM refers to Working Memory, a cognitive system that holds information temporarily for immediate use."</data>
      <data key="d2">c580fa74e3c36285cfae7df56340a990</data>
    </node>
    <node id="&quot;IMS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"IMS refers to a theoretical concept, possibly the Magical Number Seven, which is estimated to have a capacity limit of four plus or minus one."</data>
      <data key="d2">392028b79561bd7471cb68e7c9258b1e</data>
    </node>
    <node id="&quot;TMS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"TMS refers to a theoretical concept that is predicted to be smaller than the IMS."</data>
      <data key="d2">392028b79561bd7471cb68e7c9258b1e</data>
    </node>
    <node id="&quot;STORE 1 MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"STORE 1 model is a specific model defined by Bradski et al. in 1992 and 1994, which is a member of the STORE model family."
"The STORE 1 Model is a defined system consisting of neuronal populations and layers, used for processing input data."</data>
      <data key="d2">392028b79561bd7471cb68e7c9258b1e,9c685cea284029fa1f27ebaa280615a5</data>
    </node>
    <node id="&quot;1978A&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"1978a is the year when Grossberg proved that the TMS is smaller than the IMS."</data>
      <data key="d2">392028b79561bd7471cb68e7c9258b1e</data>
    </node>
    <node id="&quot;2001&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"2001 is the year when Cowan reviewed experimental data supporting the existence of a four plus or minus one WM capacity limit."</data>
      <data key="d2">392028b79561bd7471cb68e7c9258b1e</data>
    </node>
    <node id="&quot;1992&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"1992 is the year when Bradski et al. defined the STORE 1 model."
"1992 is the year when the first study by Ski et al. was published, introducing the STORE 1 Model."</data>
      <data key="d2">392028b79561bd7471cb68e7c9258b1e,9c685cea284029fa1f27ebaa280615a5</data>
    </node>
    <node id="&quot;1994&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"1994 is the year when Bradski et al. further developed the STORE model."
"1994 is the year when another study by Ski et al. was published, further developing the STORE 1 Model."</data>
      <data key="d2">392028b79561bd7471cb68e7c9258b1e,9c685cea284029fa1f27ebaa280615a5</data>
    </node>
    <node id="&quot;SKI ET AL.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Ski et al. are the authors of a study that introduced the STORE 1 Model."</data>
      <data key="d2">9c685cea284029fa1f27ebaa280615a5</data>
    </node>
    <node id="&quot;EQUATION (41)&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Equation (41) is a recurrent feedback loop that updates the STM pattern based on input when it is on."</data>
      <data key="d2">65a025a8610d85bf0d2b6c4979eb7439</data>
    </node>
    <node id="&quot;EQUATION (42)&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Equation (42) is mentioned in the context of Equation (41) and is likely a related recurrent feedback loop."</data>
      <data key="d2">65a025a8610d85bf0d2b6c4979eb7439</data>
    </node>
    <node id="&quot;SERIAL LEARNING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Serial Learning is mentioned as a process that enables fluently recalling stored sequences of items from a list."
"Serial Learning is a process that allows the Self-Organizing Avalanche to learn through time."
"Serial Learning is a method of learning where new units are continually synthesized as a result of practice, and the context of previous events can determine subsequent responses."</data>
      <data key="d2">2ea6b3379a87077d75e5c45024f4f3e2,65a025a8610d85bf0d2b6c4979eb7439,cdd2935776d71ef9fd3a33979af0b9b5</data>
    </node>
    <node id="&quot;DIXON AND HORTON&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Dixon and Horton are mentioned as authors of a study on serial learning in experimental psychology."</data>
      <data key="d2">2e76149ff772441e6627913bc1df5000</data>
    </node>
    <node id="&quot;HOVLAND&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Hovland is mentioned as an author of a study on serial learning in experimental psychology."</data>
      <data key="d2">2e76149ff772441e6627913bc1df5000</data>
    </node>
    <node id="&quot;HULL ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Hull et al. are mentioned as authors of a study on associative learning of temporal order information in experimental psychology."</data>
      <data key="d2">2e76149ff772441e6627913bc1df5000</data>
    </node>
    <node id="&quot;JUNG&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Jung is mentioned as an author of a study on serial learning in experimental psychology."</data>
      <data key="d2">2e76149ff772441e6627913bc1df5000</data>
    </node>
    <node id="&quot;MCGEOGH AND IRION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"McGeogh and Irion are mentioned as authors of a study on serial learning in experimental psychology."</data>
      <data key="d2">2e76149ff772441e6627913bc1df5000</data>
    </node>
    <node id="&quot;OSGOOD&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Osgood is mentioned as an author of a study on serial learning in experimental psychology."</data>
      <data key="d2">2e76149ff772441e6627913bc1df5000</data>
    </node>
    <node id="&quot;UNDERWOOD&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Underwood is mentioned as an author of a study on serial learning in experimental psychology."
"Underwood is a researcher who criticizes the applicability of serial learning methods in verbal learning research."</data>
      <data key="d2">2e76149ff772441e6627913bc1df5000,cdd2935776d71ef9fd3a33979af0b9b5</data>
    </node>
    <node id="&quot;AVALANCHE&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Avalanche is an example of a ritualistic encoding method that describes a performance insensitive to stimulus sampling and encoding LTM in spatial pattern units."
"Avalanche is a learning mechanism that encodes an arbitrary space-time pattern using a minimal number of cells, insensitive to environmental feedback."
"Avalanche is a type of circuit that sequentially activates a series of Outstars to sample a spatiotemporal pattern."
"Avalanche is a circuit that activates once and cannot be stopped, requiring command cells for sensitivity to environmental feedback."
"Avalanche is a system mentioned in the text, which is subject to modulation and sensitivity to environmental feedback."</data>
      <data key="d2">2e76149ff772441e6627913bc1df5000,44ddf121af4b66da2bfd6b2ac0637a23,7aeda101aa8aba76f319932f0bd568f7,88a3f14024e29666891496bb6cd7d0e4,bb19119aa74e1f624e402fcef651aac2</data>
    </node>
    <node id="&quot;SPATIOTEMPORAL PATTERN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Spatiotemporal Pattern is a sequence of spatial patterns that is sampled and learned by the Avalanche mechanism."</data>
      <data key="d2">bb19119aa74e1f624e402fcef651aac2</data>
    </node>
    <node id="&quot;OUTSTARS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Outstars are components of the Avalanche mechanism that sequentially sample a spatiotemporal pattern."
"Outstars are neurons within the Avalanche circuit that can fire only if activated by a command cell."
"Outstars are a type of neural structure mentioned in the text that sends signals to Avalanche Cells."</data>
      <data key="d2">03fedd128d2ad4ec7e1e1a60d26ba4f5,7aeda101aa8aba76f319932f0bd568f7,bb19119aa74e1f624e402fcef651aac2</data>
    </node>
    <node id="&quot;HVC-RA NETWORK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"HVC-RA Network is a neural network that controls songbird singing, and an Avalanche-type circuit occurs within it."</data>
      <data key="d2">44ddf121af4b66da2bfd6b2ac0637a23</data>
    </node>
    <node id="&quot;SONGBIRD SINGING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Songbird singing is a behavior controlled by the HVC-RA Network, which includes an Avalanche-type circuit."</data>
      <data key="d2">44ddf121af4b66da2bfd6b2ac0637a23</data>
    </node>
    <node id="&quot;COMMAND CELLS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Command cells are a type of neuron that activates the Avalanche circuit once a pulse is received."
"Command Cells are neurons that control stereotyped behaviors and are necessary for the Avalanche circuit to respond to environmental feedback."
"Command Cells are a type of neural structure found in invertebrates that control stereotyped behaviors, such as the rhythmic beating of crayfish swimmerets."</data>
      <data key="d2">03fedd128d2ad4ec7e1e1a60d26ba4f5,44ddf121af4b66da2bfd6b2ac0637a23,7aeda101aa8aba76f319932f0bd568f7</data>
    </node>
    <node id="&quot;NONSPECIFIC AROUSAL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Nonspecific arousal is a state of increased alertness or readiness, which may be triggered by the activation of the Avalanche circuit."</data>
      <data key="d2">44ddf121af4b66da2bfd6b2ac0637a23</data>
    </node>
    <node id="&quot;STEIN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Stein is a researcher who has studied the role of command cells in controlling the rhythmic beating of crayfish swimmerets."
"Stein is a person mentioned in the text who published a study on command cells in crayfish."</data>
      <data key="d2">03fedd128d2ad4ec7e1e1a60d26ba4f5,7aeda101aa8aba76f319932f0bd568f7</data>
    </node>
    <node id="&quot;FLEXIBLE PERFORMANCE&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">7aeda101aa8aba76f319932f0bd568f7</data>
    </node>
    <node id="&quot;AVALANCHE CELLS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Avalanche Cells are a type of neural structure mentioned in the text that can fire only if they receive signals from the previous Outstar source cell and from the command cell."</data>
      <data key="d2">03fedd128d2ad4ec7e1e1a60d26ba4f5</data>
    </node>
    <node id="&quot;CARLSON&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Carlson is a person mentioned in the text who published a study on command cells in invertebrates."
"Carlson is a researcher mentioned in the text, contributing to studies on behavioral acts in invertebrates."</data>
      <data key="d2">03fedd128d2ad4ec7e1e1a60d26ba4f5,88a3f14024e29666891496bb6cd7d0e4</data>
    </node>
    <node id="&quot;DETHIER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Dethier is a person mentioned in the text who published a study on command cells in invertebrates."
"Dethier is a researcher mentioned in the text, contributing to studies on behavioral acts in invertebrates."</data>
      <data key="d2">03fedd128d2ad4ec7e1e1a60d26ba4f5,88a3f14024e29666891496bb6cd7d0e4</data>
    </node>
    <node id="&quot;THEATER&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"Theater is mentioned as a location where a dance might be performed, highlighting the importance of certain events."</data>
      <data key="d2">88a3f14024e29666891496bb6cd7d0e4</data>
    </node>
    <node id="&quot;MOSQUITO&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Mosquito is mentioned as a potential distraction, emphasizing the need for a system to evaluate the importance of events."</data>
      <data key="d2">88a3f14024e29666891496bb6cd7d0e4</data>
    </node>
    <node id="&quot;COGEM THEORY&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"CogEM Theory is mentioned as a historical approach to reinforcement learning, focusing on incentive motivation and drive representations."</data>
      <data key="d2">88a3f14024e29666891496bb6cd7d0e4</data>
    </node>
    <node id="&quot;COMPUTATIONAL MODELS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Computational Models is mentioned as a potential area of study, suggesting a focus on theoretical and mathematical approaches."</data>
      <data key="d2">88a3f14024e29666891496bb6cd7d0e4</data>
    </node>
    <node id="&quot;COGNITIVE-EMOTIONAL-MOTOR (COGEM) THEORY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Cognitive-Emotional-Motor (CogEM) Theory is a model of reinforcement learning that emphasizes the role of incentive motivation and competition between drive representations."</data>
      <data key="d2">f373521b781482587f80fffc5623a1f9</data>
    </node>
    <node id="&quot;TELOS AND LISTELOS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"TELOS and lisTELOS are brain circuit models that focus on volitional control of behavioral choice."</data>
      <data key="d2">f373521b781482587f80fffc5623a1f9</data>
    </node>
    <node id="&quot;ADVANCED BRAINS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Advanced Brains are described as having mechanisms such as high-dimensional bRNNs, which are familiar in the context of the discussed models."</data>
      <data key="d2">f373521b781482587f80fffc5623a1f9</data>
    </node>
    <node id="&quot;CLAUS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Claus is a source mentioned in the text, likely an organization or a research group."</data>
      <data key="d2">eb6c9a7d24cc59ff93d554093a4360a4</data>
    </node>
    <node id="&quot;SCHULTZ ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Schultz et al. is a source mentioned in the text, likely a research group or a team of authors."</data>
      <data key="d2">eb6c9a7d24cc59ff93d554093a4360a4</data>
    </node>
    <node id="&quot;SELF-ORGANIZING MAPS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Self-Organizing Maps is a method mentioned in the text, used to allow source cells to self-organize as learned category cells."</data>
      <data key="d2">eb6c9a7d24cc59ff93d554093a4360a4</data>
    </node>
    <node id="&quot;INSTAR-OUTSTAR MAPS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Instar-Outstar maps are a type of learning circuit mentioned in the text, consisting of learned Instars and Outstars."</data>
      <data key="d2">eb6c9a7d24cc59ff93d554093a4360a4</data>
    </node>
    <node id="&quot;SELF-ORGANIZING AVALANCHE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Self-Organizing Avalanche is a system that learns its sampling cells, temporal order links, and output spatial patterns."
"Self-Organizing Avalanche is a concept that refers to a learning mechanism that can learn its sampling cells, temporal order links, and output spatial patterns."</data>
      <data key="d2">2ea6b3379a87077d75e5c45024f4f3e2,6e8f1f4e6c7865b14f3b5665aa62e12e</data>
    </node>
    <node id="&quot;DR. PAUL GROSSBERG&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Dr. Paul Grossberg is a researcher mentioned in the text, known for his contributions to the Self-Organizing Avalanche system."</data>
      <data key="d2">6e8f1f4e6c7865b14f3b5665aa62e12e</data>
    </node>
    <node id="&quot;MATHEMATICAL ANALYSIS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Mathematical Analysis is an event that Grossberg has provided to explain classical data properties such as the bowed serial position curve."
"Mathematical Analysis is a process used to understand the behavior of the ES2N reservoir model, involving the computation of its Jacobian matrix."</data>
      <data key="d2">2ea6b3379a87077d75e5c45024f4f3e2,90c1a399dd410f75f1f4bb03fe1f5f33</data>
    </node>
    <node id="&quot;CLASSICAL DATA PROPERTIES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Classical Data Properties are characteristics that Grossberg's mathematical analysis explains, such as the bowed serial position curve."</data>
      <data key="d2">2ea6b3379a87077d75e5c45024f4f3e2</data>
    </node>
    <node id="&quot;CONTEXT-SENSITIVE SELF-ORGANIZING AVALANCHE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Context-Sensitive Self-Organizing Avalanche is an extension of the Self-Organizing Avalanche that can learn sequences of previous events and make decisions based on whole sequences."</data>
      <data key="d2">2ea6b3379a87077d75e5c45024f4f3e2</data>
    </node>
    <node id="&quot;YOUNG&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Young is a researcher who expresses skepticism about the usefulness of serial learning methods for studying verbal learning processes."
"Young is a researcher mentioned in the text, contributing to the algebraic conditions for additive-sigmoid neuron reservoirs."</data>
      <data key="d2">3b592e5ac113a5c031925f91a182baa6,cdd2935776d71ef9fd3a33979af0b9b5</data>
    </node>
    <node id="&quot;VERBAL LEARNING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Verbal Learning refers to the acquisition and retention of new verbal units and sequences, which can be influenced by the context of previous events."</data>
      <data key="d2">cdd2935776d71ef9fd3a33979af0b9b5</data>
    </node>
    <node id="&quot;YOUNG (1968)&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Young (1968) is a serial learning expert who expresses concerns about the limitations of serial learning methods for studying verbal learning processes."</data>
      <data key="d2">54e2e54daeae6bcf60e5f0b98040259d</data>
    </node>
    <node id="&quot;UNDERWOOD (1966)&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Underwood (1966) is an author who highlights the success of a theory that works for many, comparing it to the Nobel Prize in psychology."</data>
      <data key="d2">54e2e54daeae6bcf60e5f0b98040259d</data>
    </node>
    <node id="&quot;CLASSICAL SERIAL LEARNING DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Classical Serial Learning Data refers to a set of data that has been used to support and challenge theories about serial learning."</data>
      <data key="d2">54e2e54daeae6bcf60e5f0b98040259d</data>
    </node>
    <node id="&quot;GROSSBERG (1969C)&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Grossberg (1969c) is an author who has provided explanations and simulations of classical serial learning data."</data>
      <data key="d2">54e2e54daeae6bcf60e5f0b98040259d</data>
    </node>
    <node id="&quot;GROSSBERG AND PEPE (1970, 1971)&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Grossberg and Pepe (1970, 1971) are authors who have contributed to the explanation and simulation of classical serial learning data."</data>
      <data key="d2">54e2e54daeae6bcf60e5f0b98040259d</data>
    </node>
    <node id="&quot;GROSSBERG (1978A, 1993)&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Grossberg (1978a, 1993) is an author who has reviewed the mechanisms summarized in the current review and provided explanations and simulations of classical serial learning data."</data>
      <data key="d2">54e2e54daeae6bcf60e5f0b98040259d</data>
    </node>
    <node id="&quot;ECHO STATE NETWORKS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Echo State Networks are a type of recurrent neural network architecture in reservoir computing that consist of a pool of randomly connected neurons, forming a genuine recurrent neural network."
"Echo State Networks are a type of recurrent neural network architecture in reservoir computing, consisting of a reservoir and a readout component."
"Echo State Networks is a type of recurrent neural network that uses a reservoir to capture intricate patterns and dynamics of the input data."
"Echo State Networks (ESNs) is a type of neural network used for time series prediction and other tasks, involving the use of a reservoir and a readout layer."
"Echo State Networks (ESNs) are a type of recurrent neural network that utilize a reservoir to capture and process data patterns."
"Echo State Networks are a type of recurrent neural network used for time series prediction and other tasks, which are being optimized using Hyperopt in the context of the paper."
"Echo State Networks are a type of reservoir computing architecture that ReservoirPy specializes in implementing."
"Echo State Networks are a type of recurrent neural network used for time series prediction and data assimilation."
"Echo State Networks are a type of recurrent neural network used for solving complex tasks, such as the one described in the text."
"Echo State Networks are a type of recurrent neural network known for their ability to learn and approximate complex functions."
"Echo State Networks (ESNs) are a type of RNN that uses a fast and easy-to-implement training method, introduced to outperform traditional RNN algorithms."
"Echo State Networks are a type of computational model that have become relevant and popular, particularly for applications in signal processing."
"Echo State Networks are a type of time-series processing model that work under the Echo State Property principle, which imposes an asymptotic fading of the memory of the input."
"Echo State Networks are a type of Recurrent Neural Network mentioned in the text, which are a focus of the reservoirpy library."
"Echo State Networks are a type of Recurrent Neural Network that uses a random recurrent layer of neurons, known as a reservoir, to perform computations."
"Echo State Networks (ESNs) are a type of recurrent neural network developed by Jaeger, known for their ability to project input data into a high-dimensional non-linear space."
"Echo State Networks are a type of neural network with a sparsely connected random hidden layer, where the weights of output neurons are the only part of the network that can change."
"Echo State Networks are a type of recurrent neural network architecture consisting of a reservoir and a readout."
"Echo State Networks are a type of recurrent neural network that use a reservoir to store and process information."
"Echo State Networks are a type of recurrent neural network used for time series prediction and data analysis."
"Echo State Networks are a type of neural network that operates a random, large, fixed, recurring network with the input signal, inducing a nonlinear response signal in each neuron, and connects a desired output signal by a trainable linear combination of all these response signals."
"Echo State Networks are a variant of reservoir computing that can be built in different ways, including with or without directly trainable input-to-output connections, and with different neurotypes and reservoir internal connectivity patterns."
"Echo State Networks are a type of recurrent neural network that uses a sparsely connected, randomly generated reservoir to process input signals."</data>
      <data key="d2">0e6f0f7cd882a638ecb571ef36068868,10112a11d47463e2aad7352c52922d61,136559fd2a1fbef4cc8a6b11abcb3eef,158f53cd85edbb4f2e4c77b78c5e7acc,257d4cf08ffc32b99856b6e31fa4221e,2a2a93486d6198ce228e77e120dc3c0c,41fa16855df7da666dc6fc38d2f8ee53,423cdb622c47fa8cec25f22eb9f9f01f,46913f0d73ba0b8cecfdf42bde9862f4,4d87a0d12ce76c7a493a24e1c4b06a83,578045eb341c5e05d5a912f634854499,5a9eaff8c67e594f49fae0318a502c6a,6de297d888d10db4c987b5eafc6398b2,711ec1b4879d910d0df0a477c9e240ba,73e81fd6509a2ba400a8435793ade3c5,82a734e7c7ada95b1c99783140dd7168,83fafb2423a01afae7e522917d79ace9,8965403859beb43a6ab7e5c8c916b857,8e16fc97c32c39d7961b52e21b99dc53,a3368f9cab1f65643dba089af5a1f95e,bc2d4d6bb706c3d06ffd2c9c2f362104,ed28ba3543e07641536ff1eb5e0749dd,f0b3b2a88425b0563005400ea246528b</data>
    </node>
    <node id="&quot;RECURRENT NEURAL NETWORK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Recurrent Neural Network is a type of artificial neural network that processes sequences of inputs or produces sequences of outputs, allowing past information to be used in processing future inputs."
"A Recurrent Neural Network (RNN) is a type of artificial neural network that processes sequences of inputs using internal state, allowing outputs from some nodes to affect future inputs to the same nodes. It is characterized by bi-directional flow of information and is suitable for tasks like handwriting and speech recognition."
"A Recurrent Neural Network (RNN) is a type of artificial neural network that allows information to flow bidirectionally between its layers, making it suitable for tasks such as handwriting recognition and speech recognition."
"The Recurrent Neural Network is described as a type of neural network that processes a time series and returns a collection of predictions, while updating a hidden state at each time step."
"Recurrent Neural Network is a type of neural network that has connections that form a directed cycle."</data>
      <data key="d2">5445391448d4ac43471e2bce5eb41a70,8e16fc97c32c39d7961b52e21b99dc53,dcd6355fc1ed8a61a1b70c50ce60fd36,e1bf3df1ff001613df1451d6d8bf3ee4,f5b970cf7201f4a918d8bd6a1267657c</data>
    </node>
    <node id="&quot;SUPPORT VECTOR MACHINES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Support Vector Machines are a set of supervised learning methods used for classification, regression, and outlier detection."</data>
      <data key="d2">8e16fc97c32c39d7961b52e21b99dc53</data>
    </node>
    <node id="&quot;TIME SERIES FORECASTING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Time Series Forecasting is the use of a model to predict future values based on previously observed values."
"Time Series Forecasting is the use of a model to predict future values based on previously observed values."
"Time Series Forecasting is the use of a model to predict future values based on previously observed values in time series analysis."
"Time series forecasting is an application that uses previous predictions to improve future forecasts, benefiting from the incorporation of past information through feedback connections."
"Time series forecasting is an application that uses previous predictions to improve future forecasts."
"Time Series Forecasting is a method used to predict future values based on past data."
"Time Series Forecasting is the process of predicting future values based on past observations."
"Time Series Forecasting is the use of a model to predict future values based on previously observed values in a time series."
"Time Series Forecasting is the process of predicting future values based on past observations, which is the primary task performed by the ESN Model."</data>
      <data key="d2">05ba4f2e1a9472bd286417154cb0c0d4,52d001cd1786e3d9f36e0c57538bc21e,57a27a1504a5ef7d330172c0ac1085c9,70c3a879c0f6e6b76a13d02d67bce1a8,8e16fc97c32c39d7961b52e21b99dc53,c4b54c2da2dda7e660de7bd6de6f13b4,dc76db79c20c315f30e0297619904b6f,e94f386a2ed7de2156b4864797cc199e,f70c7d3d89baaabbeaad57b58e379e08</data>
    </node>
    <node id="&quot;SEQUENCE GENERATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Sequence Generation is the process of creating a sequence of items, such as words in a sentence, using a model."</data>
      <data key="d2">8e16fc97c32c39d7961b52e21b99dc53</data>
    </node>
    <node id="&quot;INRIA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Inria is an organization that has published a document on Reservoir Computing and maintains a reservoirPy page and a github repository."
"INRIA is mentioned as the email domain of Xavier Hinaut, the contact person for the library."
"Inria is a French Research Institute that supports the development of ReservoirPy and is located at Bordeaux, France."
"Inria is a French Research Institute in Digital Sciences, supporting the development of ReservoirPy."</data>
      <data key="d2">20b16c2e1cb8813ade96fea5f9591631,296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba,8e16fc97c32c39d7961b52e21b99dc53</data>
    </node>
    <node id="&quot;RESERVOIRPY DOCUMENTATION&quot;">
      <data key="d0">"LOCATION"</data>
      <data key="d1">"ReservoirPy documentation is a resource where more information about the library and its components can be found."
"ReservoirPy documentation is a resource mentioned in the text for learning more about creating Echo State Networks."
"ReservoirPy documentation is a resource that provides detailed information about the library and its usage."</data>
      <data key="d2">475ca77684df5045266ddf079f2e37f1,6be085e79e86abc5b1a7eaff6bda1ec5,83fafb2423a01afae7e522917d79ace9</data>
    </node>
    <node id="&quot;WIKIPEDIA PAGE&quot;">
      <data key="d0">"LOCATION"</data>
      <data key="d1">"Wikipedia page is a resource where more information about Echo State Networks can be found."
"The Wikipedia page is a resource mentioned in the text for learning more about Echo State Networks."</data>
      <data key="d2">6be085e79e86abc5b1a7eaff6bda1ec5,83fafb2423a01afae7e522917d79ace9</data>
    </node>
    <node id="&quot;ESNS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ESNs are a type of neural network mentioned in the text, which are created using ReservoirPy."
"ESNs are a type of recurrent neural network known for their universal computation and approximation properties."
"ESNs are Echo State Networks, a type of recurrent neural network introduced as an alternative to RNNs due to their fast and simple training algorithms."
"ESNs are a type of neural network known for their fading memory property and ability to retain information."
"ESNs are mentioned as a popular flavor of RC, which consists of a RNN of recurrently connected neurons that enables to project input data into a high-dimensional non-linear space, which is then decoded by a single layer of neurons with trained connections."
"ESNs are a type of recurrent neural network that includes direct connections from input to readout, enabling advanced data processing."
"ESNs are a type of recurrent neural network known for their simplicity and efficiency in time series prediction and reservoir computation."
"ESNs are a type of machine learning model developed independently by Wolfgang Maass, used for reservoir computing."
"ESNs are a type of recurrent neural network known for their performance in time series prediction tasks."</data>
      <data key="d2">295606b4bc5d12929a913a3c79f93734,4b89d9404fd683ecd03d5846ee2d86ce,50d2bcd5726d191f4fee831a1c02fee1,6be085e79e86abc5b1a7eaff6bda1ec5,7f2d69f9a9baca70ffd25a6865189206,b32958d42199d47252887dc7be40ab5a,e805d3f438bd9c485639f1c69f917ae5,eafe89ad19a57846f953a1dfcf8571f8,f1fc6fbc8158d3da070d55544041a2ca</data>
    </node>
    <node id="&quot;FEATURE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A feature is an attribute associated with an input or sample, mentioned in the text."
"A feature is an attribute associated with an input or sample, such as a pixel in an image or the Euclidean distance to a goal state."</data>
      <data key="d2">6be085e79e86abc5b1a7eaff6bda1ec5,dd41fca2f283c4f8c8d1cba5b836da45</data>
    </node>
    <node id="&quot;LABEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Labels are the values that you're trying to figure out, mentioned in the text."</data>
      <data key="d2">6be085e79e86abc5b1a7eaff6bda1ec5</data>
    </node>
    <node id="&quot;HOUSE PRICE MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"House Price Model is a concept used to predict the price of a house based on various features such as the number of bedrooms, location, and size."</data>
      <data key="d2">8c15845717f6b8610fe30ac08cc78b4e</data>
    </node>
    <node id="&quot;LABELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Labels are the actual values that the House Price Model is trying to predict, in this case, the actual price of the house."</data>
      <data key="d2">8c15845717f6b8610fe30ac08cc78b4e</data>
    </node>
    <node id="&quot;RECURRENT NEURAL NETWORK (RNN)&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"RNN is a type of artificial neural network that allows the output from some nodes to affect subsequent input to the same nodes, creating a bi-directional flow of information. It is suitable for tasks like speech recognition and can use its internal state to process arbitrary sequences of inputs."
"Recurrent Neural Network (RNN) is a type of neural network that is dynamic and used for various applications, including time series prediction and pattern recognition."</data>
      <data key="d2">8c15845717f6b8610fe30ac08cc78b4e,a4b801e70cf2ba3a3101d34899450087</data>
    </node>
    <node id="&quot;CONVOLUTIONAL NEURAL NETWORKS (CNNS)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"CNNs are a type of neural network that belong to the class of finite impulse response networks, characterized by directed acyclic graphs that can be unrolled."</data>
      <data key="d2">04b89ad6396cb78ca75689473c47a247</data>
    </node>
    <node id="&quot;LONG SHORT-TERM MEMORY NETWORKS (LSTMS)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"LSTMs are a type of RNN that incorporates controlled storage mechanisms, known as gated states or gated memory, to handle long sequences of data."
"LSTMs are a type of RNN that addresses the vanishing gradient problem, allowing them to learn and remember information from long sequences."</data>
      <data key="d2">04b89ad6396cb78ca75689473c47a247,24a607f45ad989d81411fed4f2941884</data>
    </node>
    <node id="&quot;GATED RECURRENT UNITS (GRUS)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"GRUs are similar to LSTMs, but they use a simpler structure to control the flow of information, making them computationally more efficient."
"GRUs are a type of RNN that use gating mechanisms to control the flow of information, similar to LSTMs but with a simpler structure."</data>
      <data key="d2">04b89ad6396cb78ca75689473c47a247,24a607f45ad989d81411fed4f2941884</data>
    </node>
    <node id="&quot;FEEDBACK NEURAL NETWORKS (FNNS)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"FNNs are neural networks that incorporate time delays or feedback loops, replacing standard storage, to handle sequences and time-dependent data."
"FNNs are a type of neural network that incorporates time delays or feedback loops, replacing standard storage mechanisms."</data>
      <data key="d2">04b89ad6396cb78ca75689473c47a247,24a607f45ad989d81411fed4f2941884</data>
    </node>
    <node id="&quot;TURING COMPLETENESS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Turing Completeness is a theoretical capability of RNNs, which means that they can simulate any Turing machine, given enough time and resources."</data>
      <data key="d2">04b89ad6396cb78ca75689473c47a247</data>
    </node>
    <node id="&quot;HANDWRITING RECOGNITION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Handwriting Recognition is an application where RNNs are particularly useful for processing unsegmented, connected handwriting."
"Handwriting Recognition is a task that can be performed by a Recurrent Neural Network, as it allows the processing of arbitrary sequences of inputs."</data>
      <data key="d2">24a607f45ad989d81411fed4f2941884,f5b970cf7201f4a918d8bd6a1267657c</data>
    </node>
    <node id="&quot;SPEECH RECOGNITION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Speech Recognition is an application where RNNs are useful for recognizing and processing spoken language."
"Speech recognition is an application that uses previous phoneme activations to better predict subsequent phonemes, leveraging the memory capabilities of feedback connections."
"Speech recognition is an application that incorporates previous phoneme activations to better predict subsequent phonemes."
"Speech Recognition is another task that can be performed by a Recurrent Neural Network, as it allows the processing of temporal dynamic behavior."</data>
      <data key="d2">24a607f45ad989d81411fed4f2941884,57a27a1504a5ef7d330172c0ac1085c9,c4b54c2da2dda7e660de7bd6de6f13b4,f5b970cf7201f4a918d8bd6a1267657c</data>
    </node>
    <node id="&quot;SEQUENTIAL DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Sequential data is data arranged in sequences where the order matters, and each data point is dependent on other data points in this sequence."</data>
      <data key="d2">24a607f45ad989d81411fed4f2941884</data>
    </node>
    <node id="&quot;TIME SERIES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Time Series is a sequence of data points collected at successive equally spaced points in time, used for analysis and prediction."
"Time Series is a concept mentioned in the text, referring to a sequence of data points collected at regular time intervals."
"Time Series is a sequence of data points collected at successive equally spaced points in time, commonly used in various scientific and engineering disciplines."
"Time Series refers to the sequence of data points collected at regular time intervals, which is being forecasted using the ES2N model."
"A time series is mentioned as input to the recurrent neural network, which processes all entries of the time series through the layers of the neural network."
"Time Series refers to a sequence of data points collected at regular time intervals."
"Time series is a sequence of data points collected at regular time intervals."
"Time Series refers to the sequence of data points collected at regular time intervals."
"Time Series refers to a sequence of data points collected at regular time intervals, which is being analyzed and predicted using the Lorenz Model and NVAR Model."
"Time Series refers to a sequence of data points collected at regular time intervals, which is being analyzed and predicted in the provided text."
"Time Series refers to a sequence of data points collected at regular time intervals."</data>
      <data key="d2">0ae9f3cf96547c05eff54812cb72ac31,14bccd672d2f8dd2cd7300581c8844fb,2035514bf3ab5b7f12ae1321972551f1,29aad23ce67e778ac31d4fb287fd20c7,5445391448d4ac43471e2bce5eb41a70,70c3a879c0f6e6b76a13d02d67bce1a8,76963fa19a9caab847e50167f71c86a2,854761a5b5b5b90af10bc6b6c76cc355,8a015d76b241ce06eb72867bbb712edd,973d44d321c7ceee7add295c60b085d2,c838b1b4744bc0f400abf85f791950cf</data>
    </node>
    <node id="&quot;TIME SERIES ANALYSIS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Time Series Analysis is the process of analyzing Time Series data to extract meaningful statistics and characteristics."
"Time Series Analysis is a method for analyzing time series data, focusing on extracting meaningful statistics and characteristics."
"Time Series Analysis is the process of analyzing time series data to extract meaningful statistics and other characteristics."
"Time Series Analysis is a type of analysis that focuses on relationships between different points in time within a single series, distinct from cross-sectional studies and spatial data analysis."
"Time Series Analysis is a statistical method used to analyze data points collected at regular time intervals."
"Time Series Analysis is the primary focus of the text, used for various purposes such as forecasting, signal detection, data mining, and pattern recognition."
"Time Series Analysis is a statistical method used for analyzing data points collected at regular intervals, such as time."</data>
      <data key="d2">4b75a8a7637b05307e62f309c682d43b,70c3a879c0f6e6b76a13d02d67bce1a8,8a015d76b241ce06eb72867bbb712edd,8e69dad9d25c6b8f037f22592687e195,a2b394d556da06b8c14dd2f5e106343b,c7d17582a93a296eaaf9b9fca737ba51,dc76db79c20c315f30e0297619904b6f</data>
    </node>
    <node id="&quot;MATHEMATICS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Mathematics is the field of study that deals with numbers, shapes, and structures, and it is the foundation for Time Series Analysis."
"Mathematics is the field of study that deals with the logic of shape, quantity, and arrangement."</data>
      <data key="d2">70c3a879c0f6e6b76a13d02d67bce1a8,8a015d76b241ce06eb72867bbb712edd</data>
    </node>
    <node id="&quot;DATA POINTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Data Points are individual observations or measurements in a Time Series, which are collected at successive equally spaced points in time."</data>
      <data key="d2">8a015d76b241ce06eb72867bbb712edd</data>
    </node>
    <node id="&quot;SUCCESSIVE EQUALLY SPACED POINTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Successive Equally Spaced Points refer to the regular interval between data points in a Time Series."</data>
      <data key="d2">8a015d76b241ce06eb72867bbb712edd</data>
    </node>
    <node id="&quot;EXAMPLES OF TIME SERIES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Examples of Time Series include Heights of ocean tides, Counts of sunspots, and Daily closing value of the Dow Jones Industrial Average."</data>
      <data key="d2">8a015d76b241ce06eb72867bbb712edd</data>
    </node>
    <node id="&quot;REGRESSION ANALYSIS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Regression Analysis is a statistical method often used to test relationships between one or more different time series."
"Regression Analysis is a statistical method used to test relationships between one or more different time series."
"Regression Analysis is a statistical method used to test relationships between one or more different time series."
"Regression Analysis is a statistical technique used to infer relationships among two or more variables, focusing on uncertainty in the data."
"Regression Analysis is a technique used to approximate a function when only a set of points is provided, and the function is an operation on the real numbers."
"Regression Analysis is a statistical method used to understand the relationships between variables."</data>
      <data key="d2">4a7ca13b3f869961817e2aa723e67d24,630c86e110e2dabbe068f446b619cef3,70c3a879c0f6e6b76a13d02d67bce1a8,8e69dad9d25c6b8f037f22592687e195,9ec0dac4c72bcc2c78c7df43b9969fe7,dc76db79c20c315f30e0297619904b6f</data>
    </node>
    <node id="&quot;CROSS-SECTIONAL STUDIES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Cross-Sectional Studies involve data without a natural ordering of observations, such as explaining people's wages by their education levels."</data>
      <data key="d2">dc76db79c20c315f30e0297619904b6f</data>
    </node>
    <node id="&quot;SPATIAL DATA ANALYSIS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Spatial Data Analysis involves observations typically relating to geographical locations, such as house prices by location."</data>
      <data key="d2">dc76db79c20c315f30e0297619904b6f</data>
    </node>
    <node id="&quot;TEMPORAL ORDERING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Temporal Ordering refers to the natural ordering of time series data, distinguishing it from cross-sectional studies."</data>
      <data key="d2">dc76db79c20c315f30e0297619904b6f</data>
    </node>
    <node id="&quot;STOCHASTIC MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Stochastic Models reflect the fact that observations close together in time will be more closely related than observations further apart."
"Stochastic Models are used in time series analysis to account for the relationship between observations close together in time."</data>
      <data key="d2">dc76db79c20c315f30e0297619904b6f,e94f386a2ed7de2156b4864797cc199e</data>
    </node>
    <node id="&quot;TIME REVERSIBILITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Time Reversibility is a characteristic of time series models that express values for a given period as deriving from past values, rather than future values."</data>
      <data key="d2">dc76db79c20c315f30e0297619904b6f</data>
    </node>
    <node id="&quot;REAL-VALUED CONTINUOUS DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Real-Valued Continuous Data refers to data types such as temperature readings over time, used in time series analysis."</data>
      <data key="d2">e94f386a2ed7de2156b4864797cc199e</data>
    </node>
    <node id="&quot;DISCRETE NUMERIC DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Discrete Numeric Data refers to data types such as counts of events occurring in fixed intervals, used in time series analysis."</data>
      <data key="d2">e94f386a2ed7de2156b4864797cc199e</data>
    </node>
    <node id="&quot;DISCRETE SYMBOLIC DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Discrete Symbolic Data refers to sequences of characters such as letters and words in a language, used in time series analysis."</data>
      <data key="d2">e94f386a2ed7de2156b4864797cc199e</data>
    </node>
    <node id="&quot;NUMPY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"NumPy is an open-source organization that provides a library for working with arrays in Python, which is mentioned in the text."
"NumPy is a fundamental package for scientific computing in Python, providing a powerful N-dimensional array object and various derived objects."
"NumPy is an open-source Python library that provides a powerful N-dimensional array object and various routines for operations on arrays. It is used for scientific computing and is fundamental in ReservoirPy."
"NumPy is an open-source Python library used for numerical computing and array manipulation."
"Numpy is a library in Python used for numerical computations, providing functions to work with arrays and matrices."
"Numpy is a library for numerical computing in Python, used to create arrays and perform mathematical operations."
"NumPy is a library used for numerical computing in Python, and the ptp function mentioned in the text is a function provided by NumPy."
"numpy is a library used for numerical computing in Python."
"numpy is a library used for numerical computations in Python, which is mentioned in the code."
"Numpy is a library used for numerical computing in Python, providing functions for array manipulation and mathematical operations."
"numpy is a standard scientific library in Python used for numerical computations and array manipulation."
"Numpy is a library used for numerical computing in Python, as mentioned in the text."
"Numpy is a library used for numerical computing in Python, providing support for arrays and matrices."
"Numpy is a library used for numerical computing in Python, which is mentioned in the context of storing parameters."
"Numpy is a library used for numerical computing in Python, providing functions to generate matrices."
"Numpy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays."
"Numpy is a library used by ReservoirPy for data storage and computations."
"Numpy is a library used for numerical computing in Python, which is used to store and manipulate data in ReservoirPy."
"Numpy is a library used to perform numerical computations in Python, which is mentioned in the context of ESNs."
"Numpy is a library used for numerical computing in Python, used for handling the sine wave data."
"Numpy is a library used for numerical computations in Python, which is used in the provided code for handling arrays and mathematical operations."
"Numpy is a library used for numerical computations in the code."
"Numpy is a library used for numerical computing in Python."
"Numpy is a library for numerical computing in Python, used to generate and manipulate matrices."
"Numpy is a library used for numerical computing in Python, providing support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions."
"Numpy is a library used for numerical computations in Python."
"Numpy is a library used for numerical computing in Python."
"NumPy is a library used for numerical computing in Python."
"Numpy is a library used for numerical computations in the provided code."</data>
      <data key="d2">00d22666fe697ffb66c2392939f45b39,14bccd672d2f8dd2cd7300581c8844fb,1cbfde86d1258f2b267135412e50a590,2f4c992d69812866e6fce6dbb52d8612,325bd631a690a34736918180b01f6917,34b9ce80a22112b32e063179511af6e0,37549a8af907ce182bd36eec43002a7d,38b3e8ea0ec280360770513327b0d9d3,3a8ed31ef360d5587cc6411e9fce89d4,4073cafddb73621f26061385c5570659,4246748fef7001ea0bd03ac702565b0d,50d4e4aab1823b8df6573ccf227f24d0,5732296d26c7a572dc90d4af1172626a,593080a95ef7640b3925b07cad1bedd4,5f841065cf74ef7bbc28efb775d5585e,71f966d00b6d0eceb580d00b9cb86b1e,74c073137c970e32982756d008532cb8,7b9936d57ece8ba985947a7aca12e2c7,8294eed5fc10df1c118f9afa266910e4,8648b5740b93d805f139d9745e1171e8,a58317c7e13f27d513fc7671fd187ecb,bc2d4d6bb706c3d06ffd2c9c2f362104,cdc64af0dde941250d89b191d0666c9b,d5e39e29b61f6ea0ffe0c868ba7a4252,dddc79e4cd04d2d07e35930dd8458168,ef85a7b1ca82dc1446ea71964d607a73,f838f4cbb7060f4409ba2d174a396fb1,fe90abb0dde126fafbf44782aeb6738c,ff860bc63e3d697a6183c0b850689048</data>
    </node>
    <node id="&quot;API&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"API is a concept mentioned in the text, referring to an Application Programming Interface that allows software applications to communicate with each other."</data>
      <data key="d2">14bccd672d2f8dd2cd7300581c8844fb</data>
    </node>
    <node id="&quot;VERBOSITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Verbosity is a concept mentioned in the text, referring to the level of detail provided in the output of a program."
"Verbosity refers to the level of detail provided in the output of a program, with higher levels producing more detailed information."</data>
      <data key="d2">14bccd672d2f8dd2cd7300581c8844fb,f838f4cbb7060f4409ba2d174a396fb1</data>
    </node>
    <node id="&quot;SEED&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Seed is a concept mentioned in the text, referring to an initial value used to initialize a random number generator, making the results reproducible."
"A seed is an initial value used to initialize a pseudorandom number generator, ensuring deterministic and repeatable results."
"SEED is a hyperparameter used for reproducibility in experiments."
"SEED is a hyperparameter used for reproducibility in a system or process."
"seed is a parameter explored by hp_space, fixed to 1234, used for the ESN initialization."
"seed is a parameter that specifies the random state seed, ensuring reproducibility."
"seed is a parameter used to initialize the random number generator in the ESN."
"Seed is a parameter used to initialize the random number generator in the ESN model, ensuring reproducibility."
"SEED is a term used in the text, likely referring to an initial value or starting point used in the Reservoir system."
"Seed is a parameter in the Hyperopt configuration, which is mentioned in the text."</data>
      <data key="d2">09198e939639c229c2c97555f65b12a7,0982b8d1eb1e636b19fa2e9d9361e566,14bccd672d2f8dd2cd7300581c8844fb,3c4d88c41f6efbfccea6f8814bf8430e,72e6eee633bcb5b1458c4cee3975cee1,76f47f241e255f9f36646409d2ec30f1,80033e741d8e10abdcfe20dd17192152,adfc38e9dc5e6fd0fe67ce83dfa1f154,bba680a0a7dd439bd5b0fe1547ffe040,f838f4cbb7060f4409ba2d174a396fb1</data>
    </node>
    <node id="&quot;SOFTWARE APPLICATIONS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Software Applications are systems that use APIs to communicate with each other, enabling integration and interaction."</data>
      <data key="d2">f838f4cbb7060f4409ba2d174a396fb1</data>
    </node>
    <node id="&quot;APIS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"APIs define methods and data formats for applications to request and exchange information."</data>
      <data key="d2">f838f4cbb7060f4409ba2d174a396fb1</data>
    </node>
    <node id="&quot;NUMPY ARRAY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Numpy array is a powerful n-dimensional array object in Python, used for efficient storage and manipulation of large datasets."</data>
      <data key="d2">f838f4cbb7060f4409ba2d174a396fb1</data>
    </node>
    <node id="&quot;SCIPY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"SciPy is an open-source Python library used for scientific and technical computing. It builds on NumPy and provides a large collection of algorithms and functions for optimization and integration."
"SciPy is an open-source Python library used for scientific and technical computing, building on NumPy and providing a large collection of algorithms and functions."
"Scipy is a library in Python used for scientific computing, providing functions for optimization, integration, and sparse matrices."
"Scipy is a scientific computing library in Python, used for mathematical and scientific tasks, including generating the NARMA time series data."
"scipy is a standard scientific library in Python used for mathematical and scientific computations, including signal processing, optimization, and statistics."
"Scipy is a library used for scientific computing in Python, providing support for sparse matrices."
"Scipy is a library used for scientific computing in Python, which is mentioned in the context of storing parameters as sparse matrices."
"Scipy is a library used for scientific computing in Python, providing functions to handle sparse matrices."
"Scipy is a library for the Python programming language that provides mathematical functions and convenience functions for optimization, statistics, and other tasks."
"Scipy is a library used by ReservoirPy for computations."
"Scipy is a library used for scientific computing in Python, which is used in computations within ReservoirPy."
"Scipy is a library used for scientific computing in Python, which is mentioned in the context of ESNs for sparse matrices."
"Scipy is a library for scientific computing in Python, used to generate sparse matrices."
"Scipy is a library used for scientific computing in Python, providing support for mathematical functions, optimization, integration, and more."</data>
      <data key="d2">1cbfde86d1258f2b267135412e50a590,37549a8af907ce182bd36eec43002a7d,38b3e8ea0ec280360770513327b0d9d3,4246748fef7001ea0bd03ac702565b0d,5732296d26c7a572dc90d4af1172626a,74c073137c970e32982756d008532cb8,7b9936d57ece8ba985947a7aca12e2c7,8e2e87aa712be195790cf15483428d7f,bc2d4d6bb706c3d06ffd2c9c2f362104,cdc64af0dde941250d89b191d0666c9b,d5e39e29b61f6ea0ffe0c868ba7a4252,ef85a7b1ca82dc1446ea71964d607a73,fe90abb0dde126fafbf44782aeb6738c,ff860bc63e3d697a6183c0b850689048</data>
    </node>
    <node id="&quot;RNNS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"RNNs are a type of artificial neural network designed to process sequential or time-series data, with key features including sequential data handling and shared parameters."
"RNNs are Recurrent Neural Networks, a type of artificial neural network used for sequential data processing."
"RNNs are a type of technology used for training and processing time series data."
"RNNs are a type of neural network mentioned in the text, used for information computation."
"RNNs are a type of neural network that operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step."

"RNNs are a type of neural network that have proven successful in areas such as language processing, but were once known for their slow computation and error-prone nature."</data>
      <data key="d2">001240f9b2caf047ee61a89e03f7b309,418f92b0dd08e03a20637ffec8193bfc,4b89d9404fd683ecd03d5846ee2d86ce,797480b3d8c00dbb7f02fccb2ab8256a,7ede01f521333d9e39fc34a245103242,b32958d42199d47252887dc7be40ab5a,eafe89ad19a57846f953a1dfcf8571f8</data>
    </node>
    <node id="&quot;BACKPROPAGATION OF ERROR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Backpropagation of Error is a method used to train neural network models by estimating gradients."
"Backpropagation of Error is a method used to train neural network models by estimating gradients, which helps the model learn by adjusting its parameters."</data>
      <data key="d2">001240f9b2caf047ee61a89e03f7b309,8fdd0220497c9b9d8c2ece14be6a8f25</data>
    </node>
    <node id="&quot;GRADIENT DESCENT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Gradient Descent is a method of finding the minimum value of a function, often used in conjunction with backpropagation to update the network parameters."
"Gradient Descent is a method of finding the minimum value of a function, often used in conjunction with backpropagation to update the network parameters."
"Gradient Descent is an optimization algorithm used to minimize the error in machine learning models."
"Gradient Descent is an optimization algorithm used to minimize the error term in neural networks."
"Gradient Descent is a method used for optimizing parameters in machine learning models, facing challenges in standard RNN architectures due to vanishing gradients."</data>
      <data key="d2">0bb54b1de8d2297293defe94addb8076,1aec5b03f663d1614b2ecbf97981a5c2,8fdd0220497c9b9d8c2ece14be6a8f25,eafe89ad19a57846f953a1dfcf8571f8,f60e4bd6b9e356b88d3a008130e8ac4b</data>
    </node>
    <node id="&quot;STOCHASTIC GRADIENT DESCENT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Stochastic Gradient Descent is a variant of gradient descent that updates the parameters using random subsets of data."
"Stochastic Gradient Descent is a variant of gradient descent that updates the parameters using random subsets of data."</data>
      <data key="d2">8fdd0220497c9b9d8c2ece14be6a8f25,f60e4bd6b9e356b88d3a008130e8ac4b</data>
    </node>
    <node id="&quot;NEURAL NETWORK MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Neural Network Models are computational models inspired by the structure and function of the human brain, used for tasks such as pattern recognition and decision-making."</data>
      <data key="d2">8fdd0220497c9b9d8c2ece14be6a8f25</data>
    </node>
    <node id="&quot;LINEAR REGRESSION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Linear Regression is a method used to predict the value of one variable based on the value of another variable, estimating the coefficients of the linear equation that best fits the data points."
"Linear Regression is a statistical method used to predict the relationship between two variables, finding the best-fit line that minimizes the differences between predicted and actual values."
"Linear Regression is a statistical technique used to transform data into actionable information, uncovering patterns and relationships."
"Linear Regression is a statistical method used for modeling relationships between variables, with shortcomings such as instability of the estimate and unreliability of the forecast in a high-dimensional context."
"Linear Regression is a statistical method used for training connections in the Echo State Network."
"Linear Regression is a statistical method used to find the relationship between two variables by fitting a linear equation to the observed data."
"Linear Regression is a method used to solve tasks using a simple mathematical model."
"Linear Regression is a statistical method mentioned in the text, used by offline readouts and the Ridge Readout."</data>
      <data key="d2">4b0fe17444b892af954d2561fec36eb6,573ef2ebe6a637a429cdc073a8508ca4,5d3baa9818a4e01fe1196c43378a2cea,6a4432cd530b28770e2b903fe242a0d1,861c28cb739722ddeb0babb7e1427409,d25cd385546ec6a033287e75d65a551a,f60e4bd6b9e356b88d3a008130e8ac4b,f730c6800099724052a2d061f3cd8c2e</data>
    </node>
    <node id="&quot;BACKPROPAGATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Backpropagation is a mathematical algorithm used in the learning of artificial neural networks, commonly used with optimization algorithms like gradient descent or stochastic gradient descent."</data>
      <data key="d2">f60e4bd6b9e356b88d3a008130e8ac4b</data>
    </node>
    <node id="&quot;LEAST SQUARES METHOD&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Least Squares Method is a statistical method used to find the best-fit line for a set of paired data, minimizing the differences between the predicted values and the actual values."
"The 'Least Squares' Method is a technique used in Linear Regression to find the best-fit line by minimizing the sum of the squares of the differences between the observed values and the values predicted by the line."</data>
      <data key="d2">573ef2ebe6a637a429cdc073a8508ca4,f60e4bd6b9e356b88d3a008130e8ac4b</data>
    </node>
    <node id="&quot;ORGANIZATIONS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Organizations are mentioned as benefiting from Linear Regression, using it to transform raw data into actionable information and make better decisions."</data>
      <data key="d2">573ef2ebe6a637a429cdc073a8508ca4</data>
    </node>
    <node id="&quot;IBM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"IBM is referenced as a source of detailed information about Linear Regression."
"IBM is a source of information about Linear Regression."
"IBM is a source of information about Ridge Regression, providing detailed explanations and resources."
"IBM is a technology company that provides information about overfitting on their website."</data>
      <data key="d2">173a2da2c7ea80f95b04db8422ced004,4b0fe17444b892af954d2561fec36eb6,573ef2ebe6a637a429cdc073a8508ca4,b5b73413fbe4ab8b61c4a939fe6c6a2b</data>
    </node>
    <node id="&quot;INDEPENDENT VARIABLE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Independent Variable is a variable that is used to predict or explain the changes in the dependent variable."</data>
      <data key="d2">573ef2ebe6a637a429cdc073a8508ca4</data>
    </node>
    <node id="&quot;DEPENDENT VARIABLE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Dependent Variable is a variable that is being predicted or explained by the independent variable."</data>
      <data key="d2">573ef2ebe6a637a429cdc073a8508ca4</data>
    </node>
    <node id="&quot;LINEAR REGRESSION CALCULATORS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Linear Regression Calculators are tools that use the 'least squares' method to find the best-fit line for a set of paired data."</data>
      <data key="d2">573ef2ebe6a637a429cdc073a8508ca4</data>
    </node>
    <node id="&quot;OVERFITTING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Overfitting occurs when a model performs well on training data but poorly on new, unseen data."

"Overfitting occurs when a model learns the training data too well, including its noise and irrelevant details, resulting in poor generalization on new data."
"Overfitting is a problem in machine learning where a model learns the training data too well, including noise and irrelevant details, leading to poor generalization on new data."
"Overfitting occurs when a model learns the training data too well, including noise and outliers, leading to poor generalization on new data."</data>
      <data key="d2">173a2da2c7ea80f95b04db8422ced004,4b0fe17444b892af954d2561fec36eb6,77c3759b4ed32509aaf1403c6fa8030f,87757855658e1d198ec49a3290760dd5,b09c66bc81fd63720bef0cfa941ee65b</data>
    </node>
    <node id="&quot;MULTICOLLINEARITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Multicollinearity is a situation where two or more independent variables are highly correlated."
</data>
      <data key="d2">4b0fe17444b892af954d2561fec36eb6,b5b73413fbe4ab8b61c4a939fe6c6a2b</data>
    </node>
    <node id="&quot;FEEDBACK CONNECTIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Feedback Connections in Reservoir Computing architectures help stabilize and control the activity of neurons in the reservoir, maintaining a balance between sensitivity to input signals and robustness against noise."
"Feedback Connections are structural links established between nodes in an Echo State Network (ESN) that allow recurrent information flow within the network, utilizing past activations to influence current processing."
"Feedback connections are connections from the readout to the input, which can be used for generating signals or long term forecasting."
"Feedback connections are a feature of ESNs that allow nodes to access the state of other nodes with a time delay of one timestep."
"Feedback Connections are a technique used in reservoir computing models to improve data processing and prediction."</data>
      <data key="d2">3ecffab3c205dece73b47f9a7004fc89,7f2d69f9a9baca70ffd25a6865189206,b5b73413fbe4ab8b61c4a939fe6c6a2b,c82c9d05b211ff65131f70eb8cb13513,ead6383a44acd8ebd17907b85a910455</data>
    </node>
    <node id="&quot;RESERVOIR NEURONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Reservoir Neurons in Reservoir Computing architectures process input signals, and their connections do not need to be trained as they are predefined."
"Reservoir neurons are the individual units within a reservoir that process and store information from the input data, which are activated and visualized in the provided code."</data>
      <data key="d2">a58317c7e13f27d513fc7671fd187ecb,b5b73413fbe4ab8b61c4a939fe6c6a2b</data>
    </node>
    <node id="&quot;RANDOM HIGH-DIMENSIONAL VECTOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Random High-Dimensional Vector refers to the activations of the reservoir in Echo State Networks, which capture intricate patterns and dynamics of the input data."
"A Random High-Dimensional Vector refers to the activations of the reservoir in Echo State Networks, which capture intricate patterns and dynamics of the input data."</data>
      <data key="d2">0e6f0f7cd882a638ecb571ef36068868,82a734e7c7ada95b1c99783140dd7168</data>
    </node>
    <node id="&quot;READOUT LAYER&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Readout Layer is a component of Echo State Networks that is trained to decode the high-dimensional activation vectors from the reservoir and produce accurate predictions."
"Readout Layer is a component of an Echo State Network (ESN) that processes the output data."
"Readout Layer is a part of a neural network model that processes the combined input vector to produce an output."</data>
      <data key="d2">0e6f0f7cd882a638ecb571ef36068868,4da651284dbab3f68dc3cae41e6e0311,a35f6cae32a3d24b18ee17ec0471a9d4</data>
    </node>
    <node id="&quot;RIDGE NODE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Ridge Node is a form of regularized linear regression used to create the readout in Echo State Networks, which helps prevent overfitting and ensures the model generalizes well to new data."
"Ridge node is a type of node mentioned in the text, which is able to perform regularized linear regression on the reservoir&#8217;s activations."</data>
      <data key="d2">0e6f0f7cd882a638ecb571ef36068868,9b360c6a33aafa6827417de5bd4faa82</data>
    </node>
    <node id="&quot;GITHUB&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"GitHub is a platform used to host and share the code and resources mentioned in the text, such as the first tutorial on Echo State Networks."
"GitHub is a web-based hosting service for version control using Git, where the first tutorial on ReservoirPy is located."
"GitHub is mentioned as the repository where new implementations are regularly added to ReservoirPy."</data>
      <data key="d2">0e6f0f7cd882a638ecb571ef36068868,77c3759b4ed32509aaf1403c6fa8030f,fcac967511cf2b019fd856e23d2e91d9</data>
    </node>
    <node id="&quot;ECHO STATE NETWORK (ESN)&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Echo State Network (ESN) is a type of recurrent neural network used for time series prediction and reservoir computing."
"Echo State Network (ESN) is a type of recurrent neural network that utilizes a reservoir to process input data and a readout layer to process output data."
"Echo State Network (ESN) is a type of recurrent neural network that uses a reservoir of nodes to process input data."
"Echo State Network (ESN) is a type of neural network used for time series prediction and analysis."
"Echo State Network (ESN) is a type of recurrent neural network used for time series prediction and other tasks, which utilizes feedback connections to incorporate past activations into current processing."
"Echo State Network (ESN) is a type of recurrent neural network that uses a sparsely connected reservoir to store past information and a readout layer to produce outputs."
"ESN is a type of network with a sparsely connected random hidden layer, used for reproducing certain time series."
"Echo State Network (ESN) is a type of recurrent neural network used for time series prediction and data analysis."
"Echo State Network (ESN) is a type of Recurrent Neural Network (RNN) that operates a random, large, fixed neural network with the input signal."</data>
      <data key="d2">1365a36c76afc697ac626fd0f784804a,32f8cfb6373e6cb4d6daa32e52aa74fc,3ecffab3c205dece73b47f9a7004fc89,4da651284dbab3f68dc3cae41e6e0311,57a27a1504a5ef7d330172c0ac1085c9,77c3759b4ed32509aaf1403c6fa8030f,a4b801e70cf2ba3a3101d34899450087,a8c0edd2cdddb7d6d899284063b541f5,b641b2be224e677674f7d3523e87ccde</data>
    </node>
    <node id="&quot;TIME CONSTANT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Time Constant is a parameter in an Echo State Network (ESN) that affects how quickly the neurons in the reservoir update their states in response to inputs."</data>
      <data key="d2">77c3759b4ed32509aaf1403c6fa8030f</data>
    </node>
    <node id="&quot;LEAKING RATE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Leaking Rate is a parameter in an Echo State Network (ESN) that controls the rate at which neurons forget previous states."
"Leaking Rate is a parameter used in Reservoir Computing to control the rate at which the reservoir state decays over time."
"Leaking Rate is a parameter in Reservoir Computing that controls the rate at which the state of the reservoir decays over time."
"Leaking Rate is a parameter that controls the time constant of the ESN, influencing the inertia and recall of previous states."
"Leaking Rate is a parameter mentioned in the text, which is log-uniformly distributed between 1e-3 and 1."
"Leaking Rate is a parameter in Reservoir Computing that controls the rate at which information is lost from the reservoir."
"The Leaking Rate is a parameter being explored, which is log-uniformly distributed between 1e-3 and 1."</data>
      <data key="d2">0a9b132ecb1c4b63fdbb0e144295362e,26d78bc91458f47d4053954505c45f92,2d8ea1123f365fb047b024022ba4fdc4,65ba78d1f678e080bd930319c54234ef,77c3759b4ed32509aaf1403c6fa8030f,8553a88d9aaf4f71d359c721a1f6fa70,8ecf03267c90a64376f5040307d98195</data>
    </node>
    <node id="&quot;SPECTRAL RADIUS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Spectral Radius is a property of the reservoir matrix in an Echo State Network (ESN) that determines the stability and memory capacity of the reservoir."
"Spectral Radius is a mathematical concept that determines the stability and memory capacity of the reservoir in an Echo State Network (ESN)."
"Spectral Radius is a hyper-parameter in Reservoir Computing that affects the stability and dynamics of the reservoir layer."
"Spectral radius is a term used in the context of reservoir computing models, referring to the maximum absolute eigenvalue of the reservoir matrix W."
"Spectral Radius is a mathematical concept used in the context of reservoirs, with values close to 1 associated with stable dynamics and values further from 1 associated with chaotic dynamics."
"Spectral Radius is a mathematical concept used in the context of a reservoir, with a value close to 1 having theoretical implications for the reservoir states."
"Spectral Radius is a parameter in Reservoir Computing that determines the stability and speed of the reservoir's dynamics."
"Spectral Radius is a parameter mentioned in the text, which is log-uniformly distributed between 1e-2 and 10."
"Spectral Radius is a mathematical concept used in the analysis of matrices, representing the maximum magnitude of the eigenvalues."
"The Spectral Radius is a concept mentioned in the context of the Echo State Property and its relationship with the reservoir weight matrix."
"The Spectral Radius is a mathematical concept used to analyze the behavior of a Reservoir Weight Matrix."
"Spectral Radius is a property of a matrix mentioned in the context of creating random sparse matrices."
"Spectral Radius is a parameter used to control the dynamics of the reservoir in the Echo State Network (ESN) model."
"Spectral Radius is a parameter of the Reservoir component that determines the stability and speed of the system."
"Spectral Radius is a concept used in the provided code to describe the maximum eigenvalue of a matrix."
"Spectral Radius is a mathematical concept used in the analysis of matrices, which is mentioned in the context of a neural network component."
"Spectral Radius is a parameter in Reservoir Computing that determines the stability and dynamics of the reservoir."
"The Spectral Radius is a parameter being explored, which is log-uniformly distributed between 1e-2 and 10."</data>
      <data key="d2">01f8dd8235ba0d4cf0837b5ea958ec95,0a9b132ecb1c4b63fdbb0e144295362e,1365a36c76afc697ac626fd0f784804a,1f30b86a46d4819603edc730df816c49,26d78bc91458f47d4053954505c45f92,3b592e5ac113a5c031925f91a182baa6,4073cafddb73621f26061385c5570659,65ba78d1f678e080bd930319c54234ef,716940af834825642e01a3cb59a7e006,72e6eee633bcb5b1458c4cee3975cee1,77c3759b4ed32509aaf1403c6fa8030f,82f7e4647b9da5d5063fe92613f4fbcb,8e1f4f13f617b982d272175296ec99d3,8ecf03267c90a64376f5040307d98195,94fd1ebf256db17e4ac2255b89caa473,a9f53979e9dbe6b936ff3374c73006dd,b957e1bf5bf175c7630222ca742c7933,ff860bc63e3d697a6183c0b850689048</data>
    </node>
    <node id="&quot;ECHO STATE NETWORKS (ESNS)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Echo State Networks (ESNs) are a type of recurrent neural network that use a reservoir of interconnected neurons to process input data."
"ESNs are a type of recurrent neural network used for time series prediction and analysis."
"Echo State Networks (ESNs) are a type of recurrent neural network used for training and prediction tasks."
"Echo State Networks (ESNs) are a type of recurrent neural network that uses a sparsely connected reservoir to process sequential data."
"Echo State Networks (ESNs) are a type of recurrent neural network used for time series prediction and generation."
"ESNs are a type of recurrent neural network used for time series prediction and generation."
"Echo State Networks (ESNs) are a type of reservoir computing model used to encode inputs in a high-dimensional space and predict outputs."
"Echo State Networks (ESNs) are a type of recurrent neural network used for time series prediction, mentioned in the text."
"Echo State Networks (ESNs) are a type of Reservoir Computing architecture supported by ReservoirPy."
"Echo State Networks (ESNs) are a type of recurrent neural network architecture built using a reservoir and a readout, and they are created using ReservoirPy."</data>
      <data key="d2">01f8dd8235ba0d4cf0837b5ea958ec95,0c3779349544e78c4d650ccf76623127,0d922ae20673124fc4588949e3863ed0,37549a8af907ce182bd36eec43002a7d,4b78fdc153f982e64291112395c316c7,74c073137c970e32982756d008532cb8,a2b183778107462d474c53e4ec0a9221,af2db1cc5ab6b16acae2c93d3facb668,d0a69d653d08e58959dd8d0f2033e697,fa7c410cf411eb68eb517e23427ec1c8</data>
    </node>
    <node id="&quot;RESERVOIR MATRIX&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Reservoir Matrix is a component of an Echo State Network (ESN) that represents the connections between the reservoir neurons."</data>
      <data key="d2">01f8dd8235ba0d4cf0837b5ea958ec95</data>
    </node>
    <node id="&quot;RESERVOIR DYNAMICS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Reservoir Dynamics refers to the behavior of the reservoir neurons in an Echo State Network (ESN), which can be influenced by the spectral radius."</data>
      <data key="d2">01f8dd8235ba0d4cf0837b5ea958ec95</data>
    </node>
    <node id="&quot;CHAOTICITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Chaoticity is a measure of the complexity and unpredictability in the behavior of the reservoir neurons, which can be altered by changing the spectral radius."</data>
      <data key="d2">01f8dd8235ba0d4cf0837b5ea958ec95</data>
    </node>
    <node id="&quot;NP.PI&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"np.pi is a mathematical constant representing the ratio of a circle's circumference to its diameter, approximately equal to 3.14159."
"np.pi represents the mathematical constant &#960; (pi), approximated in python by 3.141592653589793."</data>
      <data key="d2">01f8dd8235ba0d4cf0837b5ea958ec95,475ca77684df5045266ddf079f2e37f1</data>
    </node>
    <node id="&quot;NP.SIN&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"np.sin is a function in numpy that computes the sine of all elements in the input array."</data>
      <data key="d2">475ca77684df5045266ddf079f2e37f1</data>
    </node>
    <node id="&quot;NP.LINSPACE&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"np.linspace is a function in numpy that returns evenly spaced numbers over a specified interval."</data>
      <data key="d2">475ca77684df5045266ddf079f2e37f1</data>
    </node>
    <node id="&quot;NP.RESHAPE&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"np.reshape is a function in numpy that gives a new shape to an array without changing its data."</data>
      <data key="d2">475ca77684df5045266ddf079f2e37f1</data>
    </node>
    <node id="&quot;SINGLE TIMESTEP OF DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A single timestep of data refers to one discrete time point in a sequence of data, representing the value of the variable being measured at that particular moment in time."</data>
      <data key="d2">475ca77684df5045266ddf079f2e37f1</data>
    </node>
    <node id="&quot;TIMESTEP&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Timestep is a discrete time point in a sequence of data used for training or evaluating a model in time series analysis."
"A Timestep is a single point in time within a Timeseries, representing the data value at that specific moment."
"A Timestep is a single data point collected or recorded at a specific time interval, representing a moment in time."
"Timestep is a term used to refer to a single point in time, mentioned in the text."
"Timestep is a single point in a timeseries, used for triggering nodes such as Reservoir in ReservoirPy."</data>
      <data key="d2">0e0afab060f214d46062c9886e762002,56cde5dc9d350498c1544cd57733ca8f,af2db1cc5ab6b16acae2c93d3facb668,c41b9b19460dc63e06639ea4bbbd1515,fa082948fa919150e9c06c6f5c1b53b0</data>
    </node>
    <node id="&quot;INPUT DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input Data is the sequence of data used for training or evaluating a model, such as hourly temperature data."
"Input Data refers to the data fed into the model, which is bypassed by the reservoir and directly fed to the readout layer."
"Input Data refers to the timeseries data used to train and test the ESN model."
"Input Data is the data used to train or make predictions using a model, such as an Echo State Network (ESN), in this context."
"Input Data refers to the data used to train the ESN model, in this case, a sine wave."
</data>
      <data key="d2">1365a36c76afc697ac626fd0f784804a,4da651284dbab3f68dc3cae41e6e0311,56cde5dc9d350498c1544cd57733ca8f,693e4d1e43289f46866236c10207a17e,7b294b788fe5ee385d08c4aabe2ca71d,b338d2dcc1fe6ccf42407444c02cad7c</data>
    </node>
    <node id="&quot;STATE VECTOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"State Vector is the internal representation of the reservoir's current state, updated after processing each timestep of data."</data>
      <data key="d2">56cde5dc9d350498c1544cd57733ca8f</data>
    </node>
    <node id="&quot;NULL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Null is a special value used to represent the absence of a value or object in programming."</data>
      <data key="d2">56cde5dc9d350498c1544cd57733ca8f</data>
    </node>
    <node id="&quot;PROGRAMMING&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">56cde5dc9d350498c1544cd57733ca8f</data>
    </node>
    <node id="&quot;NULL VECTOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Null Vector is a vector with all elements equal to zero, often used as an initial state for reservoir computing."</data>
      <data key="d2">baeb61b8c35e75d37a338fafd6a417fa</data>
    </node>
    <node id="&quot;SHAPE ATTRIBUTE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Shape Attribute is used to determine the size and structure of arrays, such as the state vector in reservoir computing."</data>
      <data key="d2">baeb61b8c35e75d37a338fafd6a417fa</data>
    </node>
    <node id="&quot;EMPTY FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Empty Function is used to create a new array without initializing the entries, allowing for later data filling."</data>
      <data key="d2">baeb61b8c35e75d37a338fafd6a417fa</data>
    </node>
    <node id="&quot;OUTPUT DIMENSION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Output Dimension refers to the size of the output from a reservoir, which is used to specify the size of the state vector."</data>
      <data key="d2">baeb61b8c35e75d37a338fafd6a417fa</data>
    </node>
    <node id="&quot;NP.EMPTY&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"np.empty is a function that creates a new array of the specified shape and size, but without initializing the entries, resulting in an array with random values."</data>
      <data key="d2">53e61c078c8f43b7a9b0efb347f394a6</data>
    </node>
    <node id="&quot;RESERVOIR.OUTPUT_DIM&quot;">
      <data key="d0">"ATTRIBUTE"</data>
      <data key="d1">"reservoir.output_dim is an attribute that specifies the number of output dimensions of the reservoir, defining the second dimension of the 'states' array."</data>
      <data key="d2">53e61c078c8f43b7a9b0efb347f394a6</data>
    </node>
    <node id="&quot;STATES&quot;">
      <data key="d0">"VARIABLE"</data>
      <data key="d1">"states is a variable that represents an array used to store the activations of neurons in a reservoir computing system.""States are variables that store the internal state of a system or model, representing the current condition or configuration of the system, and are used to calculate the next state based on the current state and input."
"States are specific conditions or configurations within a system or environment, often used in contexts like reinforcement learning."
"States refer to the internal representations or memory of a reservoir node, which can be reset, modified, or fed to a node at any time."</data>
      <data key="d2">53e61c078c8f43b7a9b0efb347f394a6,54b1174770e13d4a2bc0916db477cc56,9e84667b4aeb0789808517f0912043ce</data>
      <data key="d3">"VARIABLES"</data>
    </node>
    <node id="&quot;STATES[:, :20]&quot;">
      <data key="d0">"SLICE"</data>
      <data key="d1">"states[:, :20] is a slice notation that selects all rows in the 'states' array and the first 20 columns, used to access and visualize the activations of the first 20 neurons across all timesteps in the timeseries."</data>
      <data key="d2">53e61c078c8f43b7a9b0efb347f394a6</data>
    </node>
    <node id="&quot;FOR-LOOP&quot;">
      <data key="d0">"CONTROL FLOW STATEMENT"</data>
      <data key="d1">"A for-loop is a control flow statement used to repeatedly execute a block of code a certain number of times or over a sequence of elements, useful for tasks like processing each element in a dataset or performing a series of computations multiple times."
"A for-loop is a control flow statement in programming used to repeatedly execute a block of code a certain number of times or over a sequence of elements."</data>
      <data key="d2">53e61c078c8f43b7a9b0efb347f394a6,54b1174770e13d4a2bc0916db477cc56</data>
    </node>
    <node id="&quot;FEATURES&quot;">
      <data key="d0">"ATTRIBUTES"</data>
      <data key="d1">"Features are attributes or properties that describe and represent data points, used as input for machine learning models to learn patterns and make predictions."
"Features are attributes or properties associated with an input or sample, such as pixels in an image or variables in a dataset."
"Features are attributes associated with inputs or samples, such as pixels in images or Euclidean distance in states, used to train machine learning models."
"Features are the variables or attributes used in the machine learning model."
"Features refer to the individual variables or measurements in the input data."</data>
      <data key="d2">2b1ebc74c60c857b93e8f426a9cd7605,53e61c078c8f43b7a9b0efb347f394a6,54b1174770e13d4a2bc0916db477cc56,9e84667b4aeb0789808517f0912043ce,e72d27e122bf954a854a23a367c9c609</data>
    </node>
    <node id="&quot;INPUTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Inputs are examples from a dataset that are passed to a model for processing, such as data points in supervised learning."</data>
      <data key="d2">54b1174770e13d4a2bc0916db477cc56</data>
    </node>
    <node id="&quot;MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A model is a function that takes inputs and produces outputs, such as a neural network or a linear regression model."
"The Model is a machine learning system that learns by adjusting its parameters to minimize errors and make accurate predictions."
"Model is a general term for a system or framework that can integrate different components, such as nodes, for a specific task."
"Model is a mathematical representation of a system or process, used to make predictions or decisions based on data."
"Model refers to a mathematical representation of a system or process, used to make predictions or decisions."
"Model is a machine learning model used for training and testing."
"Model refers to a mathematical representation used to understand or predict a system, in this case, a timeseries model with a delay of k."
"A Model is a graph of nodes that can be trained as a whole, allowing for complex operations to be represented."
"Model refers to the overall reservoir computing model being created and trained."
"Model is a concept in ReservoirPy, which is a structure that can be created using nodes such as ESN and Ridge. However, an ESN node cannot be integrated into a Model."
"Model is a machine learning model used for one-step-ahead prediction, trained on data from the NVAR."
"The Model is a machine learning model that learns linear coefficients, which are displayed in a plot."
"Model is a function or a process used in the provided code, which could be a part of a machine learning model or a mathematical model."
"The Model is a mathematical representation of a system, in this case, a model for generating attractor behavior."
"Model is a system used to infer values based on input data."
"Model is a component in a reservoir model, used for data processing and analysis."
"The Model is a combination of the Reservoir and Ridge components, used for time series prediction."
"Model is a sequence-to-vector model used for classification tasks."</data>
      <data key="d2">00648b24263129fdae8652f1a3339041,29ce72a8f609c311ebb852cc96aee54d,2b1ebc74c60c857b93e8f426a9cd7605,3bee7b78d0ab9582cc9bffe9e305df2e,3edbc4fd903a173282dd592f5e8437d1,3ff318aebcb07ca141d0a40730d96c7c,46dcc47b4358d3895c1eeb1182c6f997,4f0156c4eb24a5c168fff8417c6f046f,60639eb7c0f26a58e503c93e29c050b3,75e530c1a04e30b373dc7cc68e3ad819,7b8e1f350eefb392053be12f35fe7daf,8648b5740b93d805f139d9745e1171e8,a0feae89e52a4291db0a512a3a102d8e,c41b9b19460dc63e06639ea4bbbd1515,c9e71660f79df626c288b3a58eab0f2f,dc46bcef51e88747b544f7efb111203a,dd41fca2f283c4f8c8d1cba5b836da45,df811c27ddc46d5b90c5863a52666a4b</data>
    </node>
    <node id="&quot;IMAGE CLASSIFICATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Image classification is a task where a model takes an image as input and produces a label or category as output."
"Image Classification is a machine learning task where models are trained to categorize images into different classes based on visual features."</data>
      <data key="d2">dd41fca2f283c4f8c8d1cba5b836da45,e72d27e122bf954a854a23a367c9c609</data>
    </node>
    <node id="&quot;MACHINE TRANSLATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Machine translation is a task where a model takes a sentence or word as input and produces a translated sentence or word as output."
"Machine Translation is a natural language processing task where models are trained to translate text from one language to another."</data>
      <data key="d2">dd41fca2f283c4f8c8d1cba5b836da45,e72d27e122bf954a854a23a367c9c609</data>
    </node>
    <node id="&quot;REINFORCEMENT LEARNING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Reinforcement learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize a reward."
"Reinforcement Learning is a type of machine learning where models learn to make decisions by taking actions in an environment and receiving rewards or penalties."</data>
      <data key="d2">dd41fca2f283c4f8c8d1cba5b836da45,e72d27e122bf954a854a23a367c9c609</data>
    </node>
    <node id="&quot;SUPERVISED LEARNING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Supervised Learning is a type of machine learning where models learn from data that has already been labeled, using techniques such as regression and classification."
"Supervised Learning is a type of machine learning where models learn from data that has already been labeled, using techniques such as regression and classification."
"Supervised Learning is a concept mentioned in the text."</data>
      <data key="d2">58330f62da357197950f63388e4ceaff,dbca0570761b1698d32f0c0bfb593b1a,e72d27e122bf954a854a23a367c9c609</data>
    </node>
    <node id="&quot;UNSUPERVISED LEARNING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Unsupervised Learning is a type of machine learning where models try to find patterns in data that doesn't have labels, using techniques such as grouping and clustering."
"Unsupervised Learning is a type of machine learning where models try to find patterns in data that doesn't have labels, using techniques such as clustering."
"Unsupervised Learning is a type of machine learning where the model learns patterns from unlabeled data."</data>
      <data key="d2">58330f62da357197950f63388e4ceaff,7b6ff30ef255db2d2c68326d78cf0115,e72d27e122bf954a854a23a367c9c609</data>
    </node>
    <node id="&quot;CONTEXT MANAGER&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Context Manager in Python is a construct that allows for setup and cleanup actions around a block of code, using the `with` statement. In the case of the reservoir, it temporarily changes the state of the reservoir for operations inside its block."
"Context Manager is a Python feature that allows for the temporary modification of a context, such as the feedback received by a node in a model, using a with statement."</data>
      <data key="d2">58330f62da357197950f63388e4ceaff,b338d2dcc1fe6ccf42407444c02cad7c</data>
    </node>
    <node id="&quot;FROM_STATE&quot;">
      <data key="d0" />
      <data key="d1">
"from_state is a parameter used to initialize the reservoir with a specific state at the start of a simulation or training process."</data>
      <data key="d2">58330f62da357197950f63388e4ceaff,cb71a9bc3b00e7abcd1a53004abdea69</data>
    </node>
    <node id="&quot;WITH_STATE&quot;">
      <data key="d0" />
      <data key="d1">
"with_state is a parameter used to continue the simulation or training process from a specific state, updating the reservoir's state during its operation."</data>
      <data key="d2">58330f62da357197950f63388e4ceaff,cb71a9bc3b00e7abcd1a53004abdea69</data>
    </node>
    <node id="&quot;RUN(X)&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"run(X) is a method that processes the entire timeseries X through the reservoir node, updating the reservoir's state at each timestep based on the input data."</data>
      <data key="d2">cb71a9bc3b00e7abcd1a53004abdea69</data>
    </node>
    <node id="&quot;RECURRENT NETWORK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Recurrent network is a type of artificial neural network characterized by bi-directional flow of information, allowing outputs from some nodes to affect future inputs."</data>
      <data key="d2">cb71a9bc3b00e7abcd1a53004abdea69</data>
    </node>
    <node id="&quot;ARTIFICIAL NEURAL NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"An Artificial Neural Network (ANN) is a machine learning model inspired by biological neural networks in animal brains. It consists of connected units called neurons, which process and transmit signals through connections with adjustable weights. ANNs are used for tasks like predictive modeling and adaptive control, learning from data through training methods."
"An artificial neural network (ANN) is a machine learning model inspired by biological neural networks in animal brains, used for tasks like predictive modeling and adaptive control."
"Artificial Neural Network is a broader term that refers to a network of interconnected nodes or neurons, which can be either feedforward or recurrent."</data>
      <data key="d2">7e6b5dcab1703bfec57161a4d5543848,e1bf3df1ff001613df1451d6d8bf3ee4,f5b970cf7201f4a918d8bd6a1267657c</data>
    </node>
    <node id="&quot;RESERVOIR NODE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A reservoir node is a component of a Recurrent Neural Network that updates the reservoir's state at each timestep based on the input data. It returns the activations of the reservoir neurons for the whole timeseries, transforming the input data into high-dimensional representations."
"The Reservoir Node is a component of the ESN Model that processes the data input, storing and manipulating the information for further processing."
"Reservoir Node is a component of the Echo State Network (ESN) model that processes the input data."</data>
      <data key="d2">32f8cfb6373e6cb4d6daa32e52aa74fc,55ea2a468a24d087a0563cf9fb654dc0,e1bf3df1ff001613df1451d6d8bf3ee4</data>
    </node>
    <node id="&quot;RIDGE READOUT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Ridge Readout is an offline readout that uses linear regression to learn the connections from the reservoir to the readout neurons."
"A Ridge Readout is a type of readout node used in reservoir computing that utilizes ridge regression to learn the connections from the reservoir to the readout neurons."
"Ridge Readout is a machine learning technique that uses a regularization term to prevent overfitting and improve the model's generalization."
"Ridge Readout is a specific type of readout that uses linear regression with a ridge parameter to improve performance."
"The Ridge Readout is a type of readout mentioned in the text, which uses linear regression and ridge regularization to solve a task."
"Ridge Readout is a specific type of readout node used in the Reservoir Computing method."</data>
      <data key="d2">5d3baa9818a4e01fe1196c43378a2cea,7e6b5dcab1703bfec57161a4d5543848,87757855658e1d198ec49a3290760dd5,b09c66bc81fd63720bef0cfa941ee65b,d25cd385546ec6a033287e75d65a551a,f2d5625f36aa4cb036089ce89ec607eb</data>
    </node>
    <node id="&quot;ONLINE READOUTS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Online Readouts are readouts that can update their weights continuously as new data arrives, making them suitable for real-time applications."</data>
      <data key="d2">7e6b5dcab1703bfec57161a4d5543848</data>
    </node>
    <node id="&quot;WIKIPEDIA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Wikipedia is an online encyclopedia that provides information on various topics, including Neural Networks and their components."</data>
      <data key="d2">7e6b5dcab1703bfec57161a4d5543848</data>
    </node>
    <node id="&quot;FITTING PROCESS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Fitting Process is the training phase where a model learns the connections from the reservoir to the readout neurons based on the provided data."</data>
      <data key="d2">87757855658e1d198ec49a3290760dd5</data>
    </node>
    <node id="&quot;RIDGE PARAMETER&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Ridge Parameter is a hyperparameter in Ridge Readout that controls the strength of the regularization term, with a value of 1e-7 used to prevent overfitting."
"The Ridge Parameter is a regularization term used in machine learning models to prevent overfitting."</data>
      <data key="d2">173a2da2c7ea80f95b04db8422ced004,b09c66bc81fd63720bef0cfa941ee65b</data>
    </node>
    <node id="&quot;TEACHER VECTORS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Teacher Vectors are the target outputs used during the training of a machine learning model."
"Teacher Vectors are the actual target values used as feedback in Forced Feedback technique."
"Teacher Vectors are the actual target values used as feedback during the training phase of an Echo State Network (ESN)."</data>
      <data key="d2">0d922ae20673124fc4588949e3863ed0,173a2da2c7ea80f95b04db8422ced004,3ecffab3c205dece73b47f9a7004fc89</data>
    </node>
    <node id="&quot;TRAINING TASK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Training Task is a specific objective that a machine learning model needs to achieve during training, such as prediction or classification."
"The Training Task is the process of specifying the objective for the Model, such as predicting future values or classifying data."
"The Training Task is mentioned in the text, where the Ridge Readout is trained to create a mapping from input to target timeseries to solve a specific task."</data>
      <data key="d2">173a2da2c7ea80f95b04db8422ced004,5d3baa9818a4e01fe1196c43378a2cea,c41b9b19460dc63e06639ea4bbbd1515</data>
    </node>
    <node id="&quot;TIMESERIES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Timeseries is a sequence of data points collected or recorded at successive time intervals, capturing the evolution of data over time."
"A Timeseries is a sequence of Timesteps, capturing the evolution of data over time."
"Timeseries is a data set being used in the context of training the ESN to make one-step-ahead forecasts."
"Timeseries is a sequence of data points indexed in time order, used in the context of the Echo State Network."
"Timeseries is a sequence of data points indexed in time order, used in this context to describe the data generated by the NARMA system."
"Timeseries refers to a sequence of data points collected at regular time intervals."
"Timeseries refers to a sequence of data points indexed in time order, used to visualize and analyze the behavior of the Mackey-Glass Equation."
"Timeseries is a sequence of data points collected at regular time intervals, used for prediction and generation tasks."
"Timeseries are sequential data used in the tutorial to train the Echo State Network (ESN) created using ReservoirPy."
"Timeseries is a sequence of data points, which can be used as input for nodes such as Reservoir in ReservoirPy."
"Timeseries is a sequence of data points collected at regular time intervals, used as input for the Reservoir."
"Timeseries is a sequence of data points indexed in time order, which is being generated and compared to real timeseries data."</data>
      <data key="d2">0e0afab060f214d46062c9886e762002,3edbc4fd903a173282dd592f5e8437d1,518f1e492b92054cf2f5c5289444da02,70db98fabc82fc96ecf8cc2c023b586b,74c073137c970e32982756d008532cb8,7f70879016c133fe58e4838172a69613,8e2e87aa712be195790cf15483428d7f,c41b9b19460dc63e06639ea4bbbd1515,c4f5a27caf9dd9c1d972492c1147efa0,f730c6800099724052a2d061f3cd8c2e,f7f7dbc1e69b3b0e801bc5ba9c0cabca,fa082948fa919150e9c06c6f5c1b53b0</data>
    </node>
    <node id="&quot;READOUT NODE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Readout Node is a component of the Model that is trained as a standalone node to perform a specific task, such as predicting future values or classifying data."
"The Readout Node is a component in a machine learning model that takes the internal states of the Reservoir and makes predictions based on them."
"The Readout Node is a component of the ESN Model that outputs the final processed data."
"Readout Node is a component of the Echo State Network (ESN) model that produces the output based on the processed input data."</data>
      <data key="d2">32f8cfb6373e6cb4d6daa32e52aa74fc,55ea2a468a24d087a0563cf9fb654dc0,c41b9b19460dc63e06639ea4bbbd1515,fa082948fa919150e9c06c6f5c1b53b0</data>
    </node>
    <node id="&quot;WARMUP PARAMETER&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Warmup Parameter is a setting used to discard a specified number of initial Timesteps when training the Readout Node."
"The Warmup Parameter is used to discard a specified number of initial timesteps when training the readout in Echo State Networks, ensuring the model trains on stable and relevant activations."</data>
      <data key="d2">fa082948fa919150e9c06c6f5c1b53b0,fa7c410cf411eb68eb517e23427ec1c8</data>
    </node>
    <node id="&quot;CANONICAL METHOD&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Canonical Method is a common approach for creating and training Echo State Networks, involving training the reservoir and readout as a connected model."</data>
      <data key="d2">fa7c410cf411eb68eb517e23427ec1c8</data>
    </node>
    <node id="&quot;NON-CANONICAL METHOD&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Non-Canonical Method is an alternative approach for creating and training Echo State Networks, involving training the reservoir and readout separately."</data>
      <data key="d2">fa7c410cf411eb68eb517e23427ec1c8</data>
    </node>
    <node id="&quot;FORECASTING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Forecasting is the process of making predictions about future data points based on past data, analyzing patterns and trends in existing data to estimate future values."
"Forecasting is the process of making predictions about future data points based on past data, involving the analysis of patterns and trends in existing data."
"Forecasting is the process of predicting future values based on past data."
"Forecasting is the process of predicting future values based on past data, in this case, 10 steps ahead for the Double-Scroll Attractor."
"Forecasting is a specific type of prediction that involves making predictions about specific points in time."

"Forecasting is the process of predicting future values based on historical data."
"Forecasting is the process of predicting future values based on historical data, which is demonstrated in the provided code using the ReservoirPy library."</data>
      <data key="d2">2d8ea1123f365fb047b024022ba4fdc4,423d3b5ec1acc9a4cb448a15d3b6b595,4a7ca13b3f869961817e2aa723e67d24,4ac00cf37a752d89d55a749c01c6f6fd,9261efcc24379d9c0b2d35a2fde8275d,b2beacacc8c190393e4583a69518378c,cc12e22afbf2ef530100516df59d24f2,fa7c410cf411eb68eb517e23427ec1c8</data>
    </node>
    <node id="&quot;MATRIX&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Matrix refers to a mathematical structure that represents a collection of numbers arranged in rows and columns, often used in various mathematical and computational operations."</data>
      <data key="d2">cc12e22afbf2ef530100516df59d24f2</data>
    </node>
    <node id="&quot;WEIGHT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Weight in a matrix refers to the individual numerical values that define the strength of connections between nodes in a neural network."</data>
      <data key="d2">cc12e22afbf2ef530100516df59d24f2</data>
    </node>
    <node id="&quot;PARALLELIZATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Parallelization is the process of dividing a computational task into smaller sub-tasks that can be executed simultaneously on multiple processors or cores to improve performance and efficiency."
"Parallelization is the process of dividing a computational task into smaller sub-tasks that can be executed simultaneously on multiple processors or cores."
"Parallelization is the process of distributing a task across multiple processors or cores to speed up computation and reduce overall processing time."
"Parallelization refers to the distribution of computational tasks across multiple processors or cores to speed up computation and reduce overall processing time."
"Parallelization refers to the process of running multiple tasks simultaneously to speed up computation."
"Parallelization is a technique used in the ESN node to enable processing of multiple sequences or timesteps simultaneously, improving efficiency."</data>
      <data key="d2">1cbfde86d1258f2b267135412e50a590,3bee7b78d0ab9582cc9bffe9e305df2e,593080a95ef7640b3925b07cad1bedd4,a16039f06e545c915f8e7668c39c3e5c,cc12e22afbf2ef530100516df59d24f2,ead6383a44acd8ebd17907b85a910455</data>
    </node>
    <node id="&quot;NEURAL NETWORK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Neural Network is a computational model inspired by the structure and function of the human brain, used for various tasks such as pattern recognition, decision-making, and prediction."
"Neural Network is a type of machine learning model inspired by the structure and function of the human brain. It consists of interconnected nodes or neurons that process information and learn patterns from data."
"Neural Network is a type of machine learning model inspired by the structure and function of the human brain, used for various tasks such as pattern recognition and decision-making."
"Neural Network is a type of machine learning model used for training, which includes the ESN network."</data>
      <data key="d2">894d59d781535ca85389c4226715c007,8e2e87aa712be195790cf15483428d7f,a16039f06e545c915f8e7668c39c3e5c,cc12e22afbf2ef530100516df59d24f2</data>
    </node>
    <node id="&quot;TIME SERIES DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Time Series Data refers to a sequence of data points collected at regular time intervals, such as predicting the next values of a sine wave."
"Time Series Data are observations that have a natural temporal ordering, making Time Series Analysis distinct from cross-sectional studies and spatial data analysis."
"Time Series Data is a one-dimensional panel data set, which is also a type of Panel Data."
"Time Series Data refers to a sequence of data points collected at regular time intervals."
"Time Series Data refers to a sequence of data points collected at regular time intervals."</data>
      <data key="d2">2bcc39da2ecef3011cc3da428fca5dd5,8e69dad9d25c6b8f037f22592687e195,a000a3fbf1f8fad62e4c25b495858c79,c087c124713c7ade4223617d95928cbf,cc12e22afbf2ef530100516df59d24f2</data>
    </node>
    <node id="&quot;MANUAL MANAGEMENT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Manual Management refers to the process of manually managing state transitions and fitting processes outside the integrated model workflow, potentially using different configurations or additional preprocessing steps."</data>
      <data key="d2">cc12e22afbf2ef530100516df59d24f2</data>
    </node>
    <node id="&quot;DEEP ARCHITECTURE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Deep Architecture refers to a neural network with multiple hidden layers between the input and output layers, allowing it to learn and represent more complex patterns and features in the data."</data>
      <data key="d2">a16039f06e545c915f8e7668c39c3e5c</data>
    </node>
    <node id="&quot;MACHINE LEARNING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Machine Learning is a field of artificial intelligence that focuses on developing algorithms and statistical models that enable computers to learn patterns and make predictions or decisions based on data."
"Machine Learning is a field of artificial intelligence that focuses on developing algorithms and statistical models to enable machines to learn and make decisions based on data."
"Machine Learning is the field of study that the IPReservoir model falls under, involving the use of algorithms to analyze and learn patterns from data."
"Machine Learning is mentioned in the text as a field related to Reservoir Computing."
"Machine Learning is a field of artificial intelligence that enables systems to learn patterns and make predictions from data."</data>
      <data key="d2">136559fd2a1fbef4cc8a6b11abcb3eef,6de297d888d10db4c987b5eafc6398b2,8e2e87aa712be195790cf15483428d7f,a16039f06e545c915f8e7668c39c3e5c,eadceb9674dd1ce90473d99e0b58e141</data>
    </node>
    <node id="&quot;DEEP ARCHITECTURES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Deep Architectures refer to models containing multiple layers of reservoirs, enabling the capture of complex patterns in data."
"Deep Architectures refer to the practice of combining nodes in various ways to create more complex structures than a simple reservoir and readout."</data>
      <data key="d2">8965403859beb43a6ab7e5c8c916b857,f0c8d4d322d73f46464e3e9f6914f2ee</data>
    </node>
    <node id="&quot;INPUT-TO-READOUT CONNECTIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input-to-readout connections in Echo State Networks allow the input data to be directly fed to the readout layer, bypassing the reservoir."
"Input-to-readout connections are direct connections from input to readout in more advanced ESNs, allowing for more complex modeling."
"Input-to-readout connections refer to the direct connections from input to readout in advanced ESNs, allowing for more complex data processing."
"Input-to-readout Connections refer to the connections between the input layer and the readout layer in a reservoir computing model."</data>
      <data key="d2">71f966d00b6d0eceb580d00b9cb86b1e,8965403859beb43a6ab7e5c8c916b857,ead6383a44acd8ebd17907b85a910455,f1fc6fbc8158d3da070d55544041a2ca</data>
    </node>
    <node id="&quot;INPUT NODES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input Nodes are nodes in an Echo State Network (ESN) that receive and process the input data."</data>
      <data key="d2">b641b2be224e677674f7d3523e87ccde</data>
    </node>
    <node id="&quot;RESERVOIR NODES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Reservoir Nodes are nodes in an Echo State Network (ESN) that store and process the input data, allowing the network to learn and capture patterns."</data>
      <data key="d2">b641b2be224e677674f7d3523e87ccde</data>
    </node>
    <node id="&quot;READOUT NODES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Readout Nodes are nodes in an Echo State Network (ESN) that produce the final output based on the processed input data."</data>
      <data key="d2">b641b2be224e677674f7d3523e87ccde</data>
    </node>
    <node id="&quot;CHAINING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Chaining refers to the sequential connection of nodes, such as input, reservoir, and readout nodes, to form a complete model in an Echo State Network (ESN)."</data>
      <data key="d2">b641b2be224e677674f7d3523e87ccde</data>
    </node>
    <node id="&quot;MANY-TO-ONE CONNECTIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Many-to-one Connections refer to a setup where multiple nodes or data sources are connected to a single node, allowing the aggregation of multiple inputs before feeding them into a single readout node."</data>
      <data key="d2">b641b2be224e677674f7d3523e87ccde</data>
    </node>
    <node id="&quot;ONE-TO-MANY CONNECTIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"One-to-many Connections refer to a setup where the output from a single node is distributed to multiple subsequent nodes, allowing the same data to be processed in different ways by different nodes."
"One-to-Many Connections refer to a setup where the output from a single node is distributed to multiple subsequent nodes, allowing the same data to be processed in different ways by different nodes."</data>
      <data key="d2">a35f6cae32a3d24b18ee17ec0471a9d4,b641b2be224e677674f7d3523e87ccde</data>
    </node>
    <node id="&quot;SPECIAL CONCAT NODE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Special Concat Node is a component that combines inputs from different sources into a single vector."</data>
      <data key="d2">a35f6cae32a3d24b18ee17ec0471a9d4</data>
    </node>
    <node id="&quot;ITERABLES OF NODES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Iterables of Nodes refer to using a collection of nodes in a neural network, allowing for flexible and complex connections, such as many-to-one or one-to-many connections."</data>
      <data key="d2">a35f6cae32a3d24b18ee17ec0471a9d4</data>
    </node>
    <node id="&quot;NODES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Nodes are processing units in a network, capable of receiving and transmitting data."</data>
      <data key="d2">e9f7bc2274e59b0767e1172a848ddca9</data>
    </node>
    <node id="&quot;CONCAT NODE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Concat Node is a special type of node that aggregates multiple input vectors into a single concatenated vector, allowing subsequent nodes to process the combined input."
"The Concat Node is a component that aggregates multiple input vectors into a single concatenated vector, allowing subsequent nodes to process this combined input."</data>
      <data key="d2">55ea2a468a24d087a0563cf9fb654dc0,e9f7bc2274e59b0767e1172a848ddca9</data>
    </node>
    <node id="&quot;ESN MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The ESN Model is a data processing model that includes nodes such as input node, reservoir node, concat node, and readout node."
"ESN Model is a type of model that combines the input, reservoir, and readout components."
"ESN model is a combination of a Reservoir and Ridge, used for predicting timeseries."
"ESN Model is a type of model containing a reservoir and a readout, used for training and running on timeseries data."
"ESN Model is a machine learning model used for time series prediction, as demonstrated in the provided text."
"ESN Model is a type of neural network model created using the ReservoirPy library."
"ESN Model is a machine learning model used for time series forecasting, developed by Jaeger and Haas."</data>
      <data key="d2">52d001cd1786e3d9f36e0c57538bc21e,55ea2a468a24d087a0563cf9fb654dc0,58115d5a63315a84d9c8d4e6ddc98ffd,8294eed5fc10df1c118f9afa266910e4,8ade7819a5f8d1ec26e9bdbd059142e6,8c520b4037fe01ffce62d46b67175e67,973d44d321c7ceee7add295c60b085d2</data>
    </node>
    <node id="&quot;INPUT NODE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Input Node is a component of the ESN Model that receives and processes the initial data input."</data>
      <data key="d2">55ea2a468a24d087a0563cf9fb654dc0</data>
    </node>
    <node id="&quot;DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Data refers to the input data used in the Echo State Network (ESN) model."
"Data is a concept used in the context of a model, serving as input for the reservoir and readout nodes."
"Data is a component in a reservoir model, used for data processing and analysis."
"Data refers to the input and output data used in the reservoir computing model."</data>
      <data key="d2">069ae9388dfd52fec9c184c7168f64dd,32f8cfb6373e6cb4d6daa32e52aa74fc,8648b5740b93d805f139d9745e1171e8,cf15a09e77b695a117e1cca05461aea2</data>
    </node>
    <node id="&quot;CONCATENATE NODE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Concatenate Node is a component of the Echo State Network (ESN) model that combines multiple inputs into a single input stream."</data>
      <data key="d2">32f8cfb6373e6cb4d6daa32e52aa74fc</data>
    </node>
    <node id="&quot;FEEDBACK CONNECTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Feedback Connection is a mechanism in the Echo State Network (ESN) model that allows the state of one node to influence another node with a one-timestep delay."
"Feedback Connection is a mechanism in an Echo State Network (ESN) that allows the state of one node to influence another node with a one-timestep delay, enabling the network to remember and utilize past information for current processing."
"Feedback Connection is a mechanism that allows the receiver node to access the state of the sender node with a one-timestep delay, creating a copy of the receiver holding a reference to the sender."
"A Feedback Connection is a type of connection between nodes that delays the signal, allowing for more complex operations to be represented."</data>
      <data key="d2">32f8cfb6373e6cb4d6daa32e52aa74fc,333ecf478bfbd4291de9f193bbf0443a,57a27a1504a5ef7d330172c0ac1085c9,dc46bcef51e88747b544f7efb111203a</data>
    </node>
    <node id="&quot;CONTROL SYSTEMS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Control systems is an application that uses past control signals to adjust future control actions, taking advantage of the dynamic capabilities of feedback connections."
"Control systems is an application that uses past control signals to adjust future control actions for better system stability."</data>
      <data key="d2">57a27a1504a5ef7d330172c0ac1085c9,c4b54c2da2dda7e660de7bd6de6f13b4</data>
    </node>
    <node id="&quot;'&gt;&gt;' OPERATOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The '&gt;&gt;' operator is used to chain connections between nodes in sequence."</data>
      <data key="d2">c4b54c2da2dda7e660de7bd6de6f13b4</data>
    </node>
    <node id="&quot;'&lt;&lt;' OPERATOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The '&lt;&lt;' operator creates a feedback connection, where the receiver node can access the state of the sender node with a one-timestep delay."</data>
      <data key="d2">c4b54c2da2dda7e660de7bd6de6f13b4</data>
    </node>
    <node id="&quot;'&lt;&lt;=' OPERATOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The '&lt;&lt;=' operator also establishes a feedback connection but without creating a copy of the receiver node."</data>
      <data key="d2">c4b54c2da2dda7e660de7bd6de6f13b4</data>
    </node>
    <node id="&quot;IN-PLACE FEEDBACK CONNECTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"In-place Feedback Connection is a variant of the Feedback Connection that does not create a copy of the receiver node, allowing it to directly hold the reference to the sender node's state."</data>
      <data key="d2">333ecf478bfbd4291de9f193bbf0443a</data>
    </node>
    <node id="&quot;FORCED FEEDBACK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Forced Feedback is a technique used in training Echo State Networks (ESNs) to provide teacher vectors as feedback to the reservoir during training."
"Forced feedback is an external feedback timeseries provided to a receiver node during runtime to replace missing feedback signals and maintain network functionality."</data>
      <data key="d2">0d922ae20673124fc4588949e3863ed0,9f1e5883a5f969a6d913beed5a5abd4f</data>
    </node>
    <node id="&quot;MODEL.FIT()&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Model.fit() is a method used to train an Echo State Network (ESN) by optimizing its parameters to minimize the error between predicted and actual target values."
"Model.fit() is a method used to train a model, such as an Echo State Network (ESN), on input data."</data>
      <data key="d2">3ecffab3c205dece73b47f9a7004fc89,b338d2dcc1fe6ccf42407444c02cad7c</data>
    </node>
    <node id="&quot;FORCE FEEDBACK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Force Feedback is a training technique used in an Echo State Network (ESN) that involves using teacher vectors (actual target values) as feedback to stabilize the training process and improve learning."</data>
      <data key="d2">3ecffab3c205dece73b47f9a7004fc89</data>
    </node>
    <node id="&quot;TEACHER FORCING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Teacher Forcing is a training technique used in sequence modeling where the model is provided with the correct input at each time step during training, allowing it to learn the sequence more effectively."
"Teacher Forcing is a training technique used in sequence modeling where true output values are fed as inputs to the network during training."
"Teacher Forcing is a technique used during the generation of system states, where correct outputs are written into the output units."</data>
      <data key="d2">3ecffab3c205dece73b47f9a7004fc89,6a4432cd530b28770e2b903fe242a0d1,d0a69d653d08e58959dd8d0f2033e697</data>
    </node>
    <node id="&quot;SEQUENCE MODELING&quot;">
      <data key="d0" />
      <data key="d1">
"Sequence Modeling is a field of machine learning that deals with processing sequential data, such as time series or text."</data>
      <data key="d2">3ecffab3c205dece73b47f9a7004fc89,d0a69d653d08e58959dd8d0f2033e697</data>
    </node>
    <node id="&quot;CONVERGENCE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Convergence in machine learning refers to the process where the training algorithm iteratively adjusts the model's parameters until the model reaches a stable state with minimal error."
"Convergence is the process where a machine learning model reaches a stable state with minimal error, indicating it has learned patterns in the training data."</data>
      <data key="d2">9f1e5883a5f969a6d913beed5a5abd4f,d0a69d653d08e58959dd8d0f2033e697</data>
    </node>
    <node id="&quot;MACHINE LEARNING MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"A machine learning model that relies on its own predictions and may face challenges in generating reasonable outputs."
"Machine Learning Model is a concept that refers to a mathematical model that is trained on data to make predictions or decisions without being explicitly programmed to perform the task."
"The machine learning model is the subject of the text, with various components and parameters being described, indicating a significant event or process."</data>
      <data key="d2">96366d7c23d50de6294c54c3444eac86,9f1e5883a5f969a6d913beed5a5abd4f,bd4cf5e35045463b7f0d8da82debc122</data>
    </node>
    <node id="&quot;SHIFT FEEDBACK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Shift feedback ensures that the forced feedback timeseries is shifted by one timestep relative to the input timeseries, aligning past outputs with future inputs."</data>
      <data key="d2">9f1e5883a5f969a6d913beed5a5abd4f</data>
    </node>
    <node id="&quot;GENERATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Generation refers to the process of predicting the next data point in a timeseries and using this prediction as input to generate subsequent data points."
"Generation refers to the process of predicting future values of a timeseries based on past data, using a trained ESN model."
"Generation refers to the process of creating new data using feedback connections."</data>
      <data key="d2">0c3779349544e78c4d650ccf76623127,4b78fdc153f982e64291112395c316c7,ead6383a44acd8ebd17907b85a910455</data>
    </node>
    <node id="&quot;LONG-TERM FORECASTING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Long-term forecasting involves using a trained model to predict future values of a timeseries over an extended period, beyond the immediate next data point."
"Long-term Forecasting involves predicting many steps ahead in a timeseries, providing a sequence of future values."</data>
      <data key="d2">0c3779349544e78c4d650ccf76623127,4b78fdc153f982e64291112395c316c7</data>
    </node>
    <node id="&quot;SHIFT_FB=TRUE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"shift_fb=True is a parameter used to ensure the correct temporal alignment between input and feedback timeseries in Echo State Networks (ESNs)."</data>
      <data key="d2">0c3779349544e78c4d650ccf76623127</data>
    </node>
    <node id="&quot;WITH_FEEDBACK() CONTEXT MANAGER&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The with_feedback() context manager is used to temporarily change the feedback received by the reservoir in Echo State Networks (ESNs) for experimentation or manipulation."</data>
      <data key="d2">0c3779349544e78c4d650ccf76623127</data>
    </node>
    <node id="&quot;ESN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ESN is a type of recurrent neural network that uses a sparse, randomly generated reservoir to capture and maintain the temporal dynamics of the input data."
"ESN is a type of neural network used for processing time series data, which uses the last state of the network as the starting point for prediction."
"ESN stands for Echo State Network, a type of recurrent neural network used for time series prediction and data analysis."
"ESN stands for Echo State Network, a type of recurrent neural network used for time series prediction and data analysis."
"ESN is an abbreviation for Echo State Networks, a type of recurrent neural network used for time series prediction and modeling."
"ESN is a type of recurrent neural network used for standalone training and running, optimized for parallel computation."
"ESN is a type of reservoir computing model used for training and running."
"ESN is an abbreviation for Echo State Network, a type of recurrent neural network used for time series prediction and other tasks."
"ESN is a model function used in the optimization process, often combined with the loss function to find local minima."
"ESN is a type of computing machine that encodes inputs in a high-dimensional space using a reservoir and reads out the desired output from the activations of the reservoir."
"ESN is an Echo State Network, which is a type of reservoir used in the context of Reservoir Computing."
"ESN stands for Echo State Network, a type of reservoir computing used for timeseries prediction."
"ESN stands for Echo State Network, which is a type of recurrent neural network used for time series prediction and data modeling."
"ESN stands for Echo State Network, a type of recurrent neural network mentioned in the text."
"ESN is an abbreviation for Echo State Network, a variant of RNN used in the reservoir computing idea."
"ESN is a system described in the text, which includes a reservoir, input states, and output weights."
"ESN is a system or model that uses a reservoir to process information, with the echo state property being a key feature."
"ESN stands for Echo State Network, a type of neural network mentioned in the text."
"ESN is a type of reservoir computing model that uses a dense reservoir and a ridge regression output layer."
"ESN is a type of reservoir computing model used for time series prediction and analysis."
"ESN is a type of reservoir computing model used for data processing and analysis."
"ESN is a type of neural network used in the analysis, focusing on memory capacity."
"ESN is a type of reservoir computing system used for time series prediction and data analysis."
"ESN is a type of reservoir computing model used for time series prediction, with specific hyperparameters such as units, learning rate, and spectral radius."
"ESN is a type of neural network used for time series prediction and analysis."
"ESN is a model used for time series prediction, as seen in the provided text."
"ESN is a machine learning model used for time series prediction, with specific hyperparameters and a noisy_tanh activation function."
"ESN is a type of recurrent neural network used for time series prediction, as demonstrated in the provided code."
"ESN is a model or algorithm used for prediction, as indicated by its use in the provided code."
"ESN stands for Echo State Network, a type of reservoir computing network mentioned in the text."
"ESN is a type of neural network used for training, with parameters such as spectral radius, input scaling, and regularization."
"ESN is a type of reservoir-based recurrent neural network used for time series prediction and training."
"ESN is a machine learning model used for time series prediction and forecasting."
"ESN is a machine learning model used for time series forecasting and generation tasks."
"ESN is a machine learning model used for time series prediction and generation."
"ESN is a type of reservoir computing model used for time series prediction and generation."
"ESN is a machine learning model used for time series prediction, consisting of a reservoir and a readout component."
"ESN stands for Echo State Network, a type of recurrent neural network used for time series prediction and data analysis."
"ESN is a type of neural network used for training, which includes a special node E."
"ESN is a type of recurrent neural network that uses a special node for training, allowing parallelization of states computations and speeding up the training process."
"ESN is an acronym for Echo State Network, a type of recurrent neural network used for processing and predicting data."
"ESN stands for Echo State Network, a type of Reservoir Computing architecture."
"ESN is an acronym for Echo State Network, a type of recurrent neural network used in the model."
"ESN stands for Echo State Network, a type of recurrent neural network used for time series prediction and analysis."
"ESN stands for Echo State Network, a type of reservoir computing model used for prediction and generation tasks."
"ESN is a model used for long-term forecasting and generative tasks, with customizable weight matrices and initializer functions."
"ESN stands for Echo State Network, which is a type of recurrent neural network that uses a sparsely connected reservoir to process input data."
"ESN is a type of node in ReservoirPy, which can be created from standard Reservoir and Ridge nodes. It is used for data processing and can not be integrated into a Model."
"ESN is a type of reservoir computing node created from the Reservoir and Ridge nodes in ReservoirPy, which supports parallelization and can be used for training and running."
"ESN is a type of model used for time series prediction, which includes a reservoir and a readout."
"ESN is a type of neural network that uses a sparsely connected reservoir to process input data."
"ESN stands for Echo State Network, which is a type of recurrent neural network that uses a reservoir to process input data."
"ESN is a model that combines a reservoir and a readout to solve complex tasks, such as speech recognition or chaotic timeseries forecasting."
"ESN stands for Echo State Network, a type of recurrent neural network used for time series prediction and other tasks."
"ESN stands for Echo State Network, which is a type of recurrent neural network used for time series prediction and data analysis."
"ESN stands for echo state network, a type of recurrent neural network used for time series prediction and other tasks."
"ESN is a type of reservoir-based neural network used for training and prediction."
"ESN is a model used for time series prediction, as indicated by the code and the context."
"ESN is an abbreviation for Echo State Network, a type of recurrent neural network used for time series prediction and data analysis."
"ESN is a machine learning model used for time series prediction and generative tasks."
"esn is a model used for training and generating data."
"ESN is a type of machine learning model used for time series prediction, developed by the ReservoirPy library."
"ESN is a type of reservoir computing model used in the code."
"ESN is a type of neural network used for training, with parameters such as units, leak rate, spectral radius, and inputs scaling."
"ESN is a machine learning model used for time series prediction, consisting of a reservoir and a readout component."
"ESN is a type of neural network used for time series prediction and analysis, with hyperparameters such as UNITS, LEAK_RATE, and SPECTRAL_RADIUS."</data>
      <data key="d2">00d22666fe697ffb66c2392939f45b39,069ae9388dfd52fec9c184c7168f64dd,088d2280349d652200861994c09d7dd5,09198e939639c229c2c97555f65b12a7,0ae9f3cf96547c05eff54812cb72ac31,0b6c69085074b2cf23267eb149068b9f,0c5a253fb2bcebe8674581a5dc12fd96,1298c65a923053e1de35aacddc13832c,12d680622df43439e6de83058b734953,18e4624a6da9e8e6d9b9b2ed260bf9b2,1cbfde86d1258f2b267135412e50a590,1db5e6cd356c6066227de5e273de1abe,24b347e60cb01aea26f46f3067f5a0f0,251a50c2ae8ceea4fd7da1127cc5f461,29aad23ce67e778ac31d4fb287fd20c7,31ee481e47ac3a0b970199e72a0e0d31,3695f5d218cdda0a91ae6a2f9b296837,36e4df75a46fb977f9516f2d2f1f9bc2,38b3e8ea0ec280360770513327b0d9d3,3ae4ccee74392bfe317d8132e99a3aa9,3bee7b78d0ab9582cc9bffe9e305df2e,424bf7c7b82dc966139c25f7c9ccffb7,4f7b43545046f0e6f9b6fb3816da1d79,4fb25ea25c60216b307931b5edacc5cb,593080a95ef7640b3925b07cad1bedd4,593306edfb8d4c7ef4b99d24fa009970,5f841065cf74ef7bbc28efb775d5585e,6425e3620184116a3ee92d5690e4f891,693e4d1e43289f46866236c10207a17e,6a4432cd530b28770e2b903fe242a0d1,6a7bea5f60347ea864c06adc327829dc,71366a4c7e791080872ba783d3787bd7,72e6eee633bcb5b1458c4cee3975cee1,76963fa19a9caab847e50167f71c86a2,7b294b788fe5ee385d08c4aabe2ca71d,7b9936d57ece8ba985947a7aca12e2c7,7f70879016c133fe58e4838172a69613,80c9f51870e239404ed671ef0374f191,857acb0c734bd5d7f1fec5f8f7aeb2cc,894d59d781535ca85389c4226715c007,9078b0f36522f21a9e8e1aadac48ed9c,993a69efae014a8f8d6ec0c235104d46,9b360c6a33aafa6827417de5bd4faa82,a621b44739e0cb4379645a4a58f16697,b338d2dcc1fe6ccf42407444c02cad7c,bf4eaad93f89884d02cdad6a50f145a6,c53825a1ab5e01a794a428988435a7a7,cc1fb6ca5695434ad0279c2606e928af,cdc64af0dde941250d89b191d0666c9b,cf15a09e77b695a117e1cca05461aea2,d0b9bbbd7257712eafd2eda5db1d0a8d,d2cc4243ed9b1bb887527f7cc1153033,d4563a00dc04ebf7bcf01e5062fde46f,d7ac2f6fb13af389417785f2f3152c52,d8e227aa2ab10a1fa9952614e3dea820,df811c27ddc46d5b90c5863a52666a4b,e396354e3a9be76616392af11f56e671,ead6383a44acd8ebd17907b85a910455,eb7a223eeb120e3fcc45a96a6018707d,f0c8d4d322d73f46464e3e9f6914f2ee,f16792dcee6dab8ea8f8c8c6793bbc3d,f18a060e6d2bb1da70432cbc71378770,f730c6800099724052a2d061f3cd8c2e,f7f7dbc1e69b3b0e801bc5ba9c0cabca,fd81bdceb3e2b91ac2605a3d201d1eb4,fe90abb0dde126fafbf44782aeb6738c</data>
    </node>
    <node id="&quot;CUSTOM WEIGHT MATRIX&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Custom Weight Matrix is a matrix used to connect the input layer to the reservoir in an ESN, allowing for the adjustment of the input-reservoir connections to improve the model's performance."
"A custom weight matrix is a manually specified array of weights used in neural networks to control their behavior and performance."</data>
      <data key="d2">693e4d1e43289f46866236c10207a17e,d8e227aa2ab10a1fa9952614e3dea820</data>
    </node>
    <node id="&quot;CUSTOM INITIALIZER FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A custom initializer function is a user-defined function used to generate initial weights for the parameters of a neural network node, allowing for tailored initialization."</data>
      <data key="d2">d8e227aa2ab10a1fa9952614e3dea820</data>
    </node>
    <node id="&quot;**KWARGS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"**kwargs allows a function to accept additional keyword arguments, providing flexibility in function usage."</data>
      <data key="d2">d8e227aa2ab10a1fa9952614e3dea820</data>
    </node>
    <node id="&quot;NP.RANDOM.NORMAL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"np.random.normal is a function from the NumPy library used to generate random numbers from a normal distribution."
"np.random.normal generates random numbers from a normal (Gaussian) distribution."
"np.random.normal is a function used to generate random numbers from a normal distribution."</data>
      <data key="d2">3a3b7a67b23341dcd1b04ec5b61683f6,4a8a4a7eeebd68a535cf84cfaecebaba,d8e227aa2ab10a1fa9952614e3dea820</data>
    </node>
    <node id="&quot;RESERVOIRPY.MAT_GEN&quot;">
      <data key="d0">"MODULE"</data>
      <data key="d1">"reservoirpy.mat_gen provides initializers for creating custom weight matrices from various statistical distributions."
"reservoirpy.mat_gen is a submodule of ReservoirPy that provides initializers for creating custom weight matrices."</data>
      <data key="d2">4a8a4a7eeebd68a535cf84cfaecebaba,8f2f2cfd667a304a288723de779c9bee</data>
    </node>
    <node id="&quot;RESERVOIRPY.MAT_GEN.RANDOM_SPARSE&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"reservoirpy.mat_gen.random_sparse is a function that generates a sparse weight matrix for a reservoir."</data>
      <data key="d2">4a8a4a7eeebd68a535cf84cfaecebaba</data>
    </node>
    <node id="&quot;PLT.HIST&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"plt.hist is a function used to create a histogram of data."
"plt.hist is a function used for plotting a histogram of the weights distribution in the Reservoir matrix."</data>
      <data key="d2">3a3b7a67b23341dcd1b04ec5b61683f6,4a8a4a7eeebd68a535cf84cfaecebaba</data>
    </node>
    <node id="&quot;NP.RAVEL&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"np.ravel is a function used to flatten a multi-dimensional array into a one-dimensional array."
"np.ravel is a function that flattens a multi-dimensional array into a one-dimensional array."</data>
      <data key="d2">4a8a4a7eeebd68a535cf84cfaecebaba,c9e71660f79df626c288b3a58eab0f2f</data>
    </node>
    <node id="&quot;RANDOM_SPARSE&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"random_sparse is a function that creates a sparse matrix with randomly distributed non-zero elements based on a specified statistical distribution."
"random_sparse is a function from the reservoirpy.mat_gen module that creates a random sparse matrix initializer."
"random_sparse is a function used to generate random sparse matrices with specified properties."</data>
      <data key="d2">3a3b7a67b23341dcd1b04ec5b61683f6,8f2f2cfd667a304a288723de779c9bee,96c47d9b671ce319abe9c6ba2b8ae122</data>
    </node>
    <node id="&quot;UNIFORM DISTRIBUTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A uniform distribution generates values that are evenly spread within a specified range."
"A uniform distribution generates non-zero elements of a matrix from a uniform distribution, ensuring values are evenly spread within a specified range."
"Uniform Distribution is a type of probability distribution used to generate matrices."</data>
      <data key="d2">8559ec6650745de27a3f41815fbfde09,8f2f2cfd667a304a288723de779c9bee,d5e39e29b61f6ea0ffe0c868ba7a4252</data>
    </node>
    <node id="&quot;DELAYED MATRIX CREATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Delayed matrix creation initializes the parameters only when needed, such as at the first run."</data>
      <data key="d2">8559ec6650745de27a3f41815fbfde09</data>
    </node>
    <node id="&quot;MATRIX CREATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Matrix creation generates the parameters immediately upon calling the initializer function."</data>
      <data key="d2">8559ec6650745de27a3f41815fbfde09</data>
    </node>
    <node id="&quot;GAUSSIAN DISTRIBUTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Gaussian distribution, also known as a normal distribution, is a probability distribution characterized by its bell-shaped curve, symmetric around the mean, defined by its mean and standard deviation."
"The Gaussian Distribution is a probability distribution used to model the distribution of reservoir activations."
"Gaussian Distribution is a probability distribution characterized by a mean (&#956;) and standard deviation (&#963;)."</data>
      <data key="d2">388cc054a99cc5cadff33147f95d6156,8559ec6650745de27a3f41815fbfde09,cb7823dcc9852e6a6f9e3607cb55134f</data>
    </node>
    <node id="&quot;BERNOULLI RANDOM VARIABLE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Bernoulli random variable is a binary variable that takes the value 1 with probability \( p \) and 0 with probability \( 1-p \), representing the outcome of a Bernoulli trial."
"Bernoulli Random Variable is a binary variable that takes the value 1 with probability p and 0 with probability 1-p, representing the outcome of a Bernoulli trial."</data>
      <data key="d2">1cbfde86d1258f2b267135412e50a590,8559ec6650745de27a3f41815fbfde09</data>
    </node>
    <node id="&quot;NORMAL DISTRIBUTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Normal Distribution is a probability distribution characterized by its bell-shaped curve, defined by its mean and standard deviation."
"Normal Distribution is a type of probability distribution used to generate matrices."</data>
      <data key="d2">1cbfde86d1258f2b267135412e50a590,d5e39e29b61f6ea0ffe0c868ba7a4252</data>
    </node>
    <node id="&quot;MULTIPROCESSING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Multiprocessing is a method of parallel computation that involves using multiple processors or cores to execute tasks simultaneously, improving performance and efficiency."
"Multiprocessing is a method of parallelizing tasks by using multiple processors or cores to execute tasks simultaneously."</data>
      <data key="d2">18e4624a6da9e8e6d9b9b2ed260bf9b2,593080a95ef7640b3925b07cad1bedd4</data>
    </node>
    <node id="&quot;ESN OFFLINE TRAINING WITH RIDGE REGRESSION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"ESN offline training with ridge regression refers to the process of training an Echo State Network using ridge regression for offline tasks."</data>
      <data key="d2">18e4624a6da9e8e6d9b9b2ed260bf9b2</data>
    </node>
    <node id="&quot;JOBLIB&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Joblib is a Python library for lightweight pipelining, providing tools for transparent disk-caching of functions and parallel computing."
"Joblib is a Python library that provides utilities for caching, parallelization, and persistence of Python objects, which is mentioned in the text."
"Joblib is a Python library used for parallelizing CPU-bound tasks, such as data loading and processing."
"Joblib is a library in Python for providing lightweight pipelining in Python jobs, which can be used to parallelize the computation of node states over independent sequences of inputs in ESN training/running."
"joblib is a library that provides utilities for parallel and efficient computation, used in the provided code."</data>
      <data key="d2">085c9d7a2af51b93826fc393600682d8,18e4624a6da9e8e6d9b9b2ed260bf9b2,9fdaabd6c7e893a275a3848c10007477,f3b5b178557c4991ab5b81d869a4752e,fe90abb0dde126fafbf44782aeb6738c</data>
    </node>
    <node id="&quot;COMPUTING NODE STATES OVER INDEPENDENT SEQUENCES OF INPUTS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Computing node states over independent sequences of inputs involves processing each sequence separately and in parallel, calculating the activations or outputs of nodes for each input sequence."</data>
      <data key="d2">18e4624a6da9e8e6d9b9b2ed260bf9b2</data>
    </node>
    <node id="&quot;COMPUTING NODE STATES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Computing Node States refers to the process of calculating the activations or states of the nodes for each input sequence, which is mentioned in the text."</data>
      <data key="d2">085c9d7a2af51b93826fc393600682d8</data>
    </node>
    <node id="&quot;SEQUENCES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Sequences are mentioned in the text as data that can have varying lengths, such as timeseries or spoken sentences."
"Sequences are arrays of data points used as input or target values in the example."</data>
      <data key="d2">085c9d7a2af51b93826fc393600682d8,6f3d0ba82cf3b10b19b64f73ace8c695</data>
    </node>
    <node id="&quot;DIFFERENT LENGTHS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Different Lengths between sequences is mentioned as a challenge, with techniques like padding, truncation, dynamic batching, and sequence masking suggested as solutions."</data>
      <data key="d2">085c9d7a2af51b93826fc393600682d8</data>
    </node>
    <node id="&quot;PADDING&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"Padding is a method used to extend shorter sequences with a specific value to match the length of the longest sequence."</data>
      <data key="d2">33a235bffff79a56e3ff5e5a9e86a3de</data>
    </node>
    <node id="&quot;TRUNCATION&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"Truncation is a method used to cut longer sequences to match the length of the shortest or a predefined length."</data>
      <data key="d2">33a235bffff79a56e3ff5e5a9e86a3de</data>
    </node>
    <node id="&quot;DYNAMIC BATCHING&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"Dynamic Batching is a method that groups sequences of similar lengths in batches to minimize padding."</data>
      <data key="d2">33a235bffff79a56e3ff5e5a9e86a3de</data>
    </node>
    <node id="&quot;SEQUENCE MASKING&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"Sequence Masking is a method used to ignore padded values during processing and computation."</data>
      <data key="d2">33a235bffff79a56e3ff5e5a9e86a3de</data>
    </node>
    <node id="&quot;DIMENSIONALITY REDUCTION&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"Dimensionality Reduction is a method used to apply techniques such as PCA (Principal Component Analysis) to reduce the number of dimensions in sequences to a common size."</data>
      <data key="d2">33a235bffff79a56e3ff5e5a9e86a3de</data>
    </node>
    <node id="&quot;FEATURE ENGINEERING&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"Feature Engineering is a method used to add or remove features to match the required dimensionality."
"Feature Engineering involves adding or removing features to match the required dimensionality."</data>
      <data key="d2">33a235bffff79a56e3ff5e5a9e86a3de,6f3d0ba82cf3b10b19b64f73ace8c695</data>
    </node>
    <node id="&quot;INTERPOLATION&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"Interpolation is a method used to adjust the length of sequences by interpolating additional points."
"Interpolation is a method used in curve fitting where an exact fit to the data is required."
"Interpolation is a method used to estimate unknown quantities between known quantities, often used in the construction of economic time series."
"Interpolation is a method used to estimate values between known data points."
"Interpolation is a technique used to approximate a function when only a set of points is provided, and the function is an operation on the real numbers."</data>
      <data key="d2">33a235bffff79a56e3ff5e5a9e86a3de,472b44b36407c9a89cf5c51459188263,630c86e110e2dabbe068f446b619cef3,9ec0dac4c72bcc2c78c7df43b9969fe7,bde7c826c746ece93a512a0cf167fa3e</data>
    </node>
    <node id="&quot;EXTRAPOLATION&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"Extrapolation is a method used to adjust the length of sequences by reducing points to match a common length."
"Extrapolation is the use of a fitted curve beyond the range of the observed data, subject to a degree of uncertainty."
"Extrapolation is a method used to predict values of a function beyond the range of the observed data, subject to a degree of uncertainty."
"Extrapolation is a method used to estimate values beyond the range of known data points, subject to greater uncertainty."
"Extrapolation is a technique used to approximate a function when only a set of points is provided, and the function is an operation on the real numbers."</data>
      <data key="d2">33a235bffff79a56e3ff5e5a9e86a3de,472b44b36407c9a89cf5c51459188263,630c86e110e2dabbe068f446b619cef3,9ec0dac4c72bcc2c78c7df43b9969fe7,bde7c826c746ece93a512a0cf167fa3e</data>
    </node>
    <node id="&quot;DIMENSIONALITY REDUCTION TECHNIQUES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Dimensionality Reduction Techniques include methods such as PCA (Principal Component Analysis) to reduce the number of dimensions in sequences to a common size."</data>
      <data key="d2">6f3d0ba82cf3b10b19b64f73ace8c695</data>
    </node>
    <node id="&quot;INTERPOLATION OR EXTRAPOLATION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Interpolation or Extrapolation adjusts the length of sequences by interpolating additional points or reducing points to match the desired length."</data>
      <data key="d2">6f3d0ba82cf3b10b19b64f73ace8c695</data>
    </node>
    <node id="&quot;TRANSFORMATION TECHNIQUES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Transformation Techniques include using embedding layers to transform input sequences to a uniform dimensionality before feeding them into the model."</data>
      <data key="d2">6f3d0ba82cf3b10b19b64f73ace8c695</data>
    </node>
    <node id="&quot;NP.LINSPACE()&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"np.linspace() is a function in NumPy that generates an array of evenly spaced values between specified endpoints."</data>
      <data key="d2">6f3d0ba82cf3b10b19b64f73ace8c695</data>
    </node>
    <node id="&quot;NP.SIN()&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"np.sin() is a function in NumPy that computes the sine of input values."</data>
      <data key="d2">6f3d0ba82cf3b10b19b64f73ace8c695</data>
    </node>
    <node id="&quot;LINSPACE()&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"linspace() is a function used to generate an array of evenly spaced values, which is used to create a sine wave for input and target sequences."</data>
      <data key="d2">df811c27ddc46d5b90c5863a52666a4b</data>
    </node>
    <node id="&quot;RESERVOIR.NODES.ESN&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"reservoir.nodes.ESN is a specific implementation of ESN, designed for standalone use and parallelized training and running."</data>
      <data key="d2">df811c27ddc46d5b90c5863a52666a4b</data>
    </node>
    <node id="&quot;CPU CORES&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"CPU cores are processing units in a computer's central processing unit, used for executing instructions and performing computations."</data>
      <data key="d2">df811c27ddc46d5b90c5863a52666a4b</data>
    </node>
    <node id="&quot;WORKERS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"workers refer to the number of parallel processes used for training and running the ESN, which can be managed to optimize system resources."
"workers is a parameter that specifies the number of parallel processes to use for training and running the ESN."</data>
      <data key="d2">3695f5d218cdda0a91ae6a2f9b296837,df811c27ddc46d5b90c5863a52666a4b</data>
    </node>
    <node id="&quot;BACKEND&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"backend is a parameter that specifies the method of execution for the ESN."</data>
      <data key="d2">3695f5d218cdda0a91ae6a2f9b296837</data>
    </node>
    <node id="&quot;COMPLEX MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Complex Models are advanced reservoir computing models that can handle more complex tasks, such as Hierarchical ESNs, Deep ESNs, and Multi-inputs ESNs."
"Complex Models in reservoir computing refer to models that are more sophisticated and capable of handling intricate tasks, such as Hierarchical ESNs, Deep ESNs, and Multi-inputs ESNs."</data>
      <data key="d2">22499cd4a0b7216dad5b05eb109fcb73,3695f5d218cdda0a91ae6a2f9b296837</data>
    </node>
    <node id="&quot;HIERARCHICAL ESNS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Hierarchical ESNs are a type of complex model that utilize multiple layers of ESNs to handle hierarchical structures and tasks."
"Hierarchical ESNs are complex models where nodes are connected in a sequential manner, forming a hierarchy. They allow data to flow through multiple layers of reservoirs and readouts."</data>
      <data key="d2">22499cd4a0b7216dad5b05eb109fcb73,3695f5d218cdda0a91ae6a2f9b296837</data>
    </node>
    <node id="&quot;DEEP ESNS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Deep ESNs are a type of complex model that utilize multiple layers of ESNs to learn deep representations of data."
"Deep ESNs involve multiple layers of reservoirs connected in series and parallel pathways. They create a more intricate structure that can handle complex tasks."</data>
      <data key="d2">22499cd4a0b7216dad5b05eb109fcb73,3695f5d218cdda0a91ae6a2f9b296837</data>
    </node>
    <node id="&quot;MULTI-INPUTS ESNS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Multi-inputs ESNs are a type of complex model that can handle multiple input streams simultaneously, allowing for more complex data processing."
"Multi-inputs ESNs are complex models that can handle multiple input sources, allowing for the integration of diverse data streams."</data>
      <data key="d2">22499cd4a0b7216dad5b05eb109fcb73,3695f5d218cdda0a91ae6a2f9b296837</data>
    </node>
    <node id="&quot;DICTIONARY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A dictionary in Python is a data structure that stores key-value pairs. In the context of training models, a dictionary can be used to specify the target outputs for each readout node during training."
"A dictionary in Python is a collection data type that stores data in pairs, where each key is associated with a value, and each key must be unique."
"A dictionary in Python is a collection data type that stores data in pairs, where each key is associated with a value. It's used to specify the parameters of a model when using a ScikitLearnNode."</data>
      <data key="d2">22499cd4a0b7216dad5b05eb109fcb73,84cacfea14ea9ff46a34150e77a0767a,861c28cb739722ddeb0babb7e1427409</data>
    </node>
    <node id="&quot;DEEP ESN&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Deep ESNs are a type of model that involve multiple layers of reservoirs connected in series and parallel pathways, enhancing depth and complexity."
"Deep ESN is a type of model used for time series prediction, involving multiple reservoirs and readouts."
"Deep ESN is a type of reservoir computing model mentioned in the text, consisting of multiple interconnected reservoirs."</data>
      <data key="d2">2336a57d055095c6ffa9d156ddee0096,c7c2383410ac00bad82831596a2d27a6,e39809b687cd044a7918eca37727a188</data>
    </node>
    <node id="&quot;MULTI-INPUTS ESN&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Multi-inputs ESNs are a type of model that handle multiple inputs and process them through different pathways before merging the results, allowing integration and processing of diverse input data streams."</data>
      <data key="d2">c7c2383410ac00bad82831596a2d27a6</data>
    </node>
    <node id="&quot;MACKEY-GLASS EQUATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Mackey-Glass equation is a set of delayed differential equations describing the temporal behavior of different physiological signals, often used to study chaotic systems."
"Mackey-Glass Equation is a set of delayed differential equations used to study chaotic systems and describe the behavior of physiological signals."
"The Mackey-Glass Equation is a nonlinear differential equation used to model the dynamics of a chemical reaction, known for its chaotic behavior."
"The Mackey-Glass Equation is a chaotic system used for timeseries forecasting in the text."
"The Mackey-Glass Equation is a mathematical model used to describe the dynamics of a chemical reaction."
"The Mackey-Glass Equation is a time series data generation equation used in the context of the ESN analysis."</data>
      <data key="d2">1db5e6cd356c6066227de5e273de1abe,50d4e4aab1823b8df6573ccf227f24d0,518f1e492b92054cf2f5c5289444da02,9f13e40ee24c7913a62781d708e3b47e,c5c29ba06a5cc70a086c2c2c8858e5aa,c7c2383410ac00bad82831596a2d27a6</data>
    </node>
    <node id="&quot;CHAOTIC SYSTEMS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Chaotic Systems are complex and unpredictable systems that exhibit sensitive dependence on initial conditions."
</data>
      <data key="d2">9f13e40ee24c7913a62781d708e3b47e,af2db1cc5ab6b16acae2c93d3facb668</data>
    </node>
    <node id="&quot;TIME DELAY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Time Delay is a parameter in the Mackey-Glass Equation that controls the chaotic behavior of the system."</data>
      <data key="d2">9f13e40ee24c7913a62781d708e3b47e</data>
    </node>
    <node id="&quot;DATA RESCALING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Data Rescaling is a preprocessing step that standardizes data, improving performance and stability."</data>
      <data key="d2">9f13e40ee24c7913a62781d708e3b47e</data>
    </node>
    <node id="&quot;FUNCTION .CM.MAGMA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Function .cm.magma is not mentioned in the provided text, so it's unclear what this entity represents without additional context."</data>
      <data key="d2">9f13e40ee24c7913a62781d708e3b47e</data>
    </node>
    <node id="&quot;MACKEY-GLASS TIMESERIES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Mackey-Glass timeseries is a mathematical model used for time series analysis, requiring rescaling between -1 and 1 for preprocessing."
"Mackey-Glass Timeseries is a synthetic dataset used for chaotic timeseries prediction tasks."
"Mackey-Glass timeseries is a chaotic univariate timeseries used as inputs for the reservoir computing model."

"Mackey-Glass timeseries is a dataset mentioned in the text, used for training and forecasting."
"Mackey-Glass timeseries is a mathematical function used as a benchmark for time series prediction and data analysis."
"Mackey-Glass Timeseries is a type of data series generated by the Mackey-Glass Equations."
"Mackey-Glass Timeseries is a type of time series data generated by the Mackey-Glass Equations, which is used for analysis and modeling."
"Mackey-Glass Timeseries is a dataset used in the provided code for demonstrating reservoir computing."</data>
      <data key="d2">238049de5f28dca3e857a46a8b1bed03,2f4c992d69812866e6fce6dbb52d8612,4073cafddb73621f26061385c5570659,4ac00cf37a752d89d55a749c01c6f6fd,6daefaa8fbd5c1492f2d832d79841463,94fd1ebf256db17e4ac2255b89caa473,a1adb5de4156f0a4a448caf79056e886,a2b183778107462d474c53e4ec0a9221,d7ac2f6fb13af389417785f2f3152c52</data>
    </node>
    <node id="&quot;STANDARDIZATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Standardization is a preprocessing step that scales data to have a mean of 0 and a standard deviation of 1, improving performance and stability."</data>
      <data key="d2">4ac00cf37a752d89d55a749c01c6f6fd</data>
    </node>
    <node id="&quot;MAGMA COLORMAP&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Magma colormap is a color scheme used to visualize data, with dark purple representing the first step and bright yellow representing the last step."</data>
      <data key="d2">4ac00cf37a752d89d55a749c01c6f6fd</data>
    </node>
    <node id="&quot;NP.ARANGE() FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"np.arange() function generates an array of values within a specified range."</data>
      <data key="d2">4ac00cf37a752d89d55a749c01c6f6fd</data>
    </node>
    <node id="&quot;NP.ARANGE(0, 500)&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"np.arange(0, 500) is a function call that generates an array of numbers from 0 to 499."</data>
      <data key="d2">7d747b0c740e8b5b60726dcf7dcadef5</data>
    </node>
    <node id="&quot;X_TRAIN&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"X_train is a variable that likely contains training data."
"X_train is a component of a machine learning model used for processing data."
"x_train is a dataset used for training the machine learning model."
"X_train is a subset of the X variable used for training the machine learning model."
"x_train is a subset of the input data used for training the model."
"x_train is the input data used for training machine learning models."
"X_train is a variable representing the training input data used to train the ESN system."
"X_train is a variable that likely contains the training input data for the ESN model."
"X_train is a variable representing the input data used for training the reservoir computing model."
"X_train is a concept or variable used in the provided code, likely representing the training data input."
"X_train is a subset of the input data used for training the ESN."
"X_train is a subset of the input data used for training the ESN model."
"X_train is a variable used to store the training input data, which is mentioned in the text."
"X_train is a set of input data used for training the model."
"X_train is a variable in the code that represents the training input data."
"X_train is a dataset used for training a machine learning model, containing input features."
"X_train is a dataset used for training the Reservoir Model."</data>
      <data key="d2">0036fb6f489e13c0db0f1c02bf3323be,0970cd32ce54f6ee1180ab237fdcefe1,0982b8d1eb1e636b19fa2e9d9361e566,1298c65a923053e1de35aacddc13832c,3ff318aebcb07ca141d0a40730d96c7c,71366a4c7e791080872ba783d3787bd7,72e6eee633bcb5b1458c4cee3975cee1,751b176a8d6149a853e597c65a6fe0cf,75e530c1a04e30b373dc7cc68e3ad819,7d747b0c740e8b5b60726dcf7dcadef5,7f2d69f9a9baca70ffd25a6865189206,80c9f51870e239404ed671ef0374f191,a0feae89e52a4291db0a512a3a102d8e,b3361508c3e49b5bb3089f10e31d2c81,dc3bd3697a140b64d70e0e3ac6db6c7e,f3e58b69b1a93175e3094a2ba65c0429,f70c7d3d89baaabbeaad57b58e379e08</data>
    </node>
    <node id="&quot;PLT.PLOT&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"plt.plot is a function call from the matplotlib library used to create a plot."</data>
      <data key="d2">7d747b0c740e8b5b60726dcf7dcadef5</data>
    </node>
    <node id="&quot;TO_FORECASTING&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"to_forecasting is a function that likely prepares data for forecasting tasks."</data>
      <data key="d2">7d747b0c740e8b5b60726dcf7dcadef5</data>
    </node>
    <node id="&quot;X&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"X is a variable that likely contains data to be forecasted."
"X is a variable or feature used in the time series forecasting process."
"X is a variable representing the input data in the time series."
"X is a variable that likely contains the input data for the ESN model."
"X is a variable representing the input data used for running the trained reservoir computing model."
"X is a variable used in the provided code, which could represent a data structure or a mathematical object."
"X is a variable or set of variables used in the mathematical model."
"X is a variable representing a sine wave, which is used as input data in the ESN model."
"X is the input data used for training and prediction in the ESN."
"X is a variable used to represent input data in the context of data analysis and machine learning."</data>
      <data key="d2">09198e939639c229c2c97555f65b12a7,09ea760dd2f000c961d1cfd4ea795da5,29ce72a8f609c311ebb852cc96aee54d,3ff318aebcb07ca141d0a40730d96c7c,593306edfb8d4c7ef4b99d24fa009970,7b8e1f350eefb392053be12f35fe7daf,7d747b0c740e8b5b60726dcf7dcadef5,7f2d69f9a9baca70ffd25a6865189206,e396354e3a9be76616392af11f56e671,f70c7d3d89baaabbeaad57b58e379e08</data>
    </node>
    <node id="&quot;READOUT.WOUT&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"readout.Wout is a variable that likely contains weights in a neural network model."</data>
      <data key="d2">7d747b0c740e8b5b60726dcf7dcadef5</data>
    </node>
    <node id="&quot;OFFLINE TRAINING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Offline Training is a method where a model is trained on a complete dataset in one go."
"Offline Training is a method where the model is trained on a complete dataset in one go, often requiring significant memory and processing power."</data>
      <data key="d2">7d747b0c740e8b5b60726dcf7dcadef5,c9e71660f79df626c288b3a58eab0f2f</data>
    </node>
    <node id="&quot;ONLINE TRAINING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Online Training is a method where a model is updated incrementally as new data arrives."
"Online Training is a method that updates the model incrementally as new data arrives, allowing the model to learn and adapt in real-time."</data>
      <data key="d2">7d747b0c740e8b5b60726dcf7dcadef5,c9e71660f79df626c288b3a58eab0f2f</data>
    </node>
    <node id="&quot;MODEL TRAINING&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">7d747b0c740e8b5b60726dcf7dcadef5</data>
    </node>
    <node id="&quot;DATASET&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Dataset is a collection of data used for training or testing the model."
"Dataset is the input data used for training and testing a machine learning model, which is split into training and testing sets in the objective function."

"dataset is an object that contains the data used for training and testing an Echo State Network (ESN)."
"dataset is a collection of data used for training and testing the machine learning model."
"Dataset refers to a collection of data used for analysis or modeling."
"The Dataset is a component of the analysis, not explicitly named in the text."
"Dataset is a variable used to store the input and target data for training and testing, which is mentioned in the text."</data>
      <data key="d2">0982b8d1eb1e636b19fa2e9d9361e566,1db191f05801d40d5a346febd10d3352,3edbc4fd903a173282dd592f5e8437d1,80033e741d8e10abdcfe20dd17192152,c9e71660f79df626c288b3a58eab0f2f,d4684af3c445d312afe4d838abc45502,f16792dcee6dab8ea8f8c8c6793bbc3d,f70c7d3d89baaabbeaad57b58e379e08</data>
    </node>
    <node id="&quot;NP.R_&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"np.r_ is a shorthand for 'np.concatenate', used to concatenate arrays along the first axis."
"np.r_ is a function in NumPy used for concatenating arrays along a specified axis."</data>
      <data key="d2">9078b0f36522f21a9e8e1aadac48ed9c,c9e71660f79df626c288b3a58eab0f2f</data>
    </node>
    <node id="&quot;NP.ARANGE&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"np.arange is a function that returns evenly spaced values within a given interval."</data>
      <data key="d2">c9e71660f79df626c288b3a58eab0f2f</data>
    </node>
    <node id="&quot;BIAS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Bias is a constant value added to the input of a neuron before the activation function to help the model make more accurate predictions."
"Bias is a constant value added to the input of a neuron to help the model make more accurate predictions."
"Bias refers to the inherent assumptions or preferences that can influence the results of a model, and it is a term used in machine learning to describe this effect."</data>
      <data key="d2">9078b0f36522f21a9e8e1aadac48ed9c,c9e71660f79df626c288b3a58eab0f2f,e805d3f438bd9c485639f1c69f917ae5</data>
    </node>
    <node id="&quot;WOUT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Wout is a weight matrix used in the output layer of a neural network."
"Wout refers to the weights or coefficients learned by the model."</data>
      <data key="d2">2b1ebc74c60c857b93e8f426a9cd7605,9078b0f36522f21a9e8e1aadac48ed9c</data>
    </node>
    <node id="&quot;NP.ABS&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"np.abs is a function in NumPy used to compute the absolute values of elements in an array."</data>
      <data key="d2">9078b0f36522f21a9e8e1aadac48ed9c</data>
    </node>
    <node id="&quot;SAMPLE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"sample is a subset of data used for analysis or testing in a machine learning context."</data>
      <data key="d2">9078b0f36522f21a9e8e1aadac48ed9c</data>
    </node>
    <node id="&quot;ABSOLUTE DEVIATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Absolute deviation is the absolute difference between the true values and the predicted values in a machine learning context."
"Absolute deviation is a measure of the difference between the True value and the ESN prediction in data analysis or modeling."</data>
      <data key="d2">9078b0f36522f21a9e8e1aadac48ed9c,ee83abbbbc707d8131952b2b01ebc268</data>
    </node>
    <node id="&quot;MODEL-0&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Model-0 is a specific model or iteration being processed in the context of the provided line of code."
"Model-0 is a machine learning model that is processing data points and is mentioned in the context of running iterations and calculating metrics."</data>
      <data key="d2">9078b0f36522f21a9e8e1aadac48ed9c,c122738fd421d3d662f759af5a0a23f3</data>
    </node>
    <node id="&quot;X_TEST1&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"X_test1 is a variable representing the input data for testing in a machine learning context."
"X_test1 is an input data set used to test the Model-0."</data>
      <data key="d2">9078b0f36522f21a9e8e1aadac48ed9c,c122738fd421d3d662f759af5a0a23f3</data>
    </node>
    <node id="&quot;Y_TEST1&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"y_test1 is the actual output data set used to compare with the predicted output from Model-0."</data>
      <data key="d2">c122738fd421d3d662f759af5a0a23f3</data>
    </node>
    <node id="&quot;Y_PRED1&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"y_pred1 is the predicted output data set from Model-0."</data>
      <data key="d2">c122738fd421d3d662f759af5a0a23f3</data>
    </node>
    <node id="&quot;ACCURACY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Accuracy is a metric that measures how often the model's predictions are correct."</data>
      <data key="d2">c122738fd421d3d662f759af5a0a23f3</data>
    </node>
    <node id="&quot;PRECISION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Precision is a metric that measures how good the model is at identifying true positives."</data>
      <data key="d2">c122738fd421d3d662f759af5a0a23f3</data>
    </node>
    <node id="&quot;RECALL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Recall is a metric that measures how good the model is at identifying true positives and true negatives."</data>
      <data key="d2">c122738fd421d3d662f759af5a0a23f3</data>
    </node>
    <node id="&quot;MEAN SQUARED ERROR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Mean Squared Error is a metric that measures how far off the model's predictions are from the actual values."</data>
      <data key="d2">c122738fd421d3d662f759af5a0a23f3</data>
    </node>
    <node id="&quot;R^2 OR RSQUARE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"R^2 or rsquare is a statistical measure that indicates how well the predicted values match the actual values."</data>
      <data key="d2">c122738fd421d3d662f759af5a0a23f3</data>
    </node>
    <node id="&quot;NRMSE OR NRMSE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"NRMSE or nrmse is a metric that measures the average difference between the predicted and actual values, normalized by the range of the actual values."</data>
      <data key="d2">c122738fd421d3d662f759af5a0a23f3</data>
    </node>
    <node id="&quot;RSQUARE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"rsquare is a statistical measure that indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s)."
"rsquare is a metric used to evaluate the performance of a machine learning model, mentioned in the provided code."</data>
      <data key="d2">0753d4e507badadd900c522ee03ad28d,d8022c6a3caf781300e2abc1dfd2ed44</data>
    </node>
    <node id="&quot;NRMSE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"NRMSE is a measure of the differences between predicted values and actual values, normalized by the range or mean of the actual values. It provides a dimensionless measure of prediction accuracy."
"nrmse is a metric used to evaluate the performance of a machine learning model, mentioned in the provided code."
"NRMSE stands for Normalized Root Mean Square Error, a metric used to evaluate the performance of a model."
"NRMSE is a metric used for optimization in the context of a graph."
"NRMSE is a metric used to evaluate the performance of a model, representing the normalized root mean square error."</data>
      <data key="d2">0753d4e507badadd900c522ee03ad28d,6425e3620184116a3ee92d5690e4f891,7cb18067f7d75fd3cd20998c669a1741,82ff270b1bbdfe0ee11e603de1e326c7,d8022c6a3caf781300e2abc1dfd2ed44</data>
    </node>
    <node id="&quot;ONE-TIMESTEP-AHEAD FORECASTING TASK&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"One-timestep-ahead forecasting task involves predicting the next value in a time series based on the current and previous values."
"This task involves predicting the next time step in a time series based on previous data."</data>
      <data key="d2">0ae9f3cf96547c05eff54812cb72ac31,d8022c6a3caf781300e2abc1dfd2ed44</data>
    </node>
    <node id="&quot;CLOSED LOOP GENERATIVE MODE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Closed loop generative mode is a system where the output of the system is fed back into the input, creating a continuous loop of generation and feedback."
"Closed Loop Generative Mode is a system where the output of the model is fed back as input for subsequent prediction."
"Closed loop generative mode involves running the ESN on its own predictions to generate new data."</data>
      <data key="d2">05ba4f2e1a9472bd286417154cb0c0d4,0ae9f3cf96547c05eff54812cb72ac31,d8022c6a3caf781300e2abc1dfd2ed44</data>
    </node>
    <node id="&quot;NUMPY.VSTACK()&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"numpy.vstack() is a function that stacks arrays vertically (row-wise), concatenating them along the first axis (rows)."</data>
      <data key="d2">05ba4f2e1a9472bd286417154cb0c0d4</data>
    </node>
    <node id="&quot;NUMPY.ZEROS()&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"numpy.zeros() is a function that creates an array of the specified shape filled with zeros."</data>
      <data key="d2">05ba4f2e1a9472bd286417154cb0c0d4</data>
    </node>
    <node id="&quot;NP.ZEROS()&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"np.zeros() is a function that creates an array of a specified shape filled with zeros."</data>
      <data key="d2">d55aca098e1ba2aea26a4bf33cc2d4a2</data>
    </node>
    <node id="&quot;ONLINE LEARNING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Online Learning is a method that updates the parameters of a model incrementally with each new sample of data, allowing the model to adapt and continuously improve as new data becomes available."
"Online Learning is a method in Machine Learning where the model is updated incrementally as new data becomes available, allowing for real-time adaptation."
"Online Learning is a process that allows for the continuous updating of a model using new data as it becomes available."</data>
      <data key="d2">1b9bc5f1bd54d2b0c90359b6ed022bb6,6de297d888d10db4c987b5eafc6398b2,d55aca098e1ba2aea26a4bf33cc2d4a2</data>
    </node>
    <node id="&quot;FORCE ALGORITHM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The First Order Reduced and Controlled Error (FORCE) algorithm is a learning algorithm that focuses on reducing the output error and maintaining it small, rather than minimizing it as quickly as possible. It is characterized by its emphasis on decreasing the number of modifications needed to keep the error small."
"FORCE Algorithm is a learning rule used to update the readout parameters at every timestep of input series in the ESN model."
"FORCE Algorithm is a learning rule used to update the readout parameters at every timestep of input series, developed by Sussillo and Abott in 2009."</data>
      <data key="d2">1b9bc5f1bd54d2b0c90359b6ed022bb6,424bf7c7b82dc966139c25f7c9ccffb7,d55aca098e1ba2aea26a4bf33cc2d4a2</data>
    </node>
    <node id="&quot;ZIP()&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"zip() is a function that takes iterables as arguments and returns an iterator. This iterator generates a series of tuples containing elements from each of the input iterables, paired together."</data>
      <data key="d2">d55aca098e1ba2aea26a4bf33cc2d4a2</data>
    </node>
    <node id="&quot;FIRST ORDER REDUCED AND CONTROLLED ERROR (FORCE) ALGORITHM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The FORCE algorithm is a learning algorithm that reduces output error and maintains it small, focusing on decreasing the number of modifications needed to keep the error small."</data>
      <data key="d2">4d8b9e762d08c8cdf5189130be11021e</data>
    </node>
    <node id="&quot;ENUMERATE() FUNCTION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The enumerate() function is used to iterate through a sequence and returns both the index and the value in each iteration."</data>
      <data key="d2">4d8b9e762d08c8cdf5189130be11021e</data>
    </node>
    <node id="&quot;ZIP() FUNCTION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The zip() function returns a zip object, which is an iterator of tuples where the first item in each passed iterator is paired together, and then the second item in each passed iterator are paired together etc."</data>
      <data key="d2">4d8b9e762d08c8cdf5189130be11021e</data>
    </node>
    <node id="&quot;LORENZ CHAOTIC ATTRACTOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Lorenz chaotic attractors are solutions to the Lorenz system of differential equations that exhibit chaotic behavior, demonstrating how small differences in initial conditions can lead to vastly different outcomes."
"Lorenz Chaotic Attractor is a mathematical model used to describe chaotic systems, mentioned in the text."
"Lorenz chaotic attractor is a mathematical model used to describe the behavior of a system of differential equations, often used as a test case for dynamical systems."</data>
      <data key="d2">4d114857b77ff15b495bb6456c9ad30c,af2db1cc5ab6b16acae2c93d3facb668,b483c6bbce54156c724905b340aa2e85</data>
    </node>
    <node id="&quot;LORENZ SYSTEM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Lorenz system is a set of differential equations used to model atmospheric convection, which exhibits chaotic behavior."
"The Lorenz system is a set of ordinary differential equations that describe a flow of fluid in a 3D space. It is known for its chaotic behavior, which is represented by the Lorenz attractor."
"The Lorenz System is a set of ordinary differential equations that describe a flow of fluid in a layer of depth D, subject to a temperature gradient along the y-axis."
"The Lorenz System is a set of ordinary differential equations that describe a flow which has chaotic solutions for certain parameter values and initial conditions."</data>
      <data key="d2">4d114857b77ff15b495bb6456c9ad30c,715720b663e85c5e16cbf8b1ef4ec208,ac3456a3574f6939fcb6d5242c202810,d4080e34001a0ebe22f20efdb204240b</data>
    </node>
    <node id="&quot;EDWARD LORENZ&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Edward Lorenz is a meteorologist who discovered Lorenz chaotic attractors while studying atmospheric convection."</data>
      <data key="d2">4d114857b77ff15b495bb6456c9ad30c</data>
    </node>
    <node id="&quot;H&#201;NON MAP&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The H&#233;non map is a mathematical function that generates complex patterns and exhibits chaotic behavior, similar to the Lorenz chaotic attractor."
"The H&#233;non map is a discrete-time dynamical system that maps a point to a new point using specific equations, known for its chaotic behavior."
"The H&#233;non map is a mathematical model that exhibits chaotic behavior, often used to illustrate complex dynamics."
"H&#233;non Map is a mathematical model used to describe chaotic systems, mentioned in the text."
"H&#233;non map is a discrete-time dynamical system that exhibits chaotic behavior, often used for testing and comparing the performance of numerical methods."</data>
      <data key="d2">4d114857b77ff15b495bb6456c9ad30c,9ada201f787cd4e88cd18dae60de346d,af2db1cc5ab6b16acae2c93d3facb668,b483c6bbce54156c724905b340aa2e85,d4080e34001a0ebe22f20efdb204240b</data>
    </node>
    <node id="&quot;H&#201;NON&#8211;POMEAU ATTRACTOR/MAP&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The H&#233;non&#8211;Pomeau attractor/map is an alternative name for the H&#233;non map, which generates complex patterns and exhibits chaotic behavior."</data>
      <data key="d2">4d114857b77ff15b495bb6456c9ad30c</data>
    </node>
    <node id="&quot;LORENZ ATTRACTOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Lorenz attractor is a visual representation of the chaotic behavior in the Lorenz system, resembling the shape of a butterfly."
"Lorenz Attractor is a mathematical concept defined by three coupled differential equations, used for forecasting."
"Lorenz Attractor is a mathematical concept defined by three coupled differential equations, used for forecasting."
"Lorenz Attractor is a mathematical model used to describe the behavior of a system of differential equations, representing a chaotic system."
"The Lorenz Attractor is a complex, chaotic structure that emerges from the Lorenz System's equations, representing the long-term behavior of the system."
"The Lorenz Attractor is a strange attractor that can arise from the Lorenz System, a set of equations describing a flow with chaotic solutions."
"Lorenz Attractor is a mathematical concept used in the analysis of the data."</data>
      <data key="d2">41ea479401d7ff7c83c9d38c91d76cd9,60639eb7c0f26a58e503c93e29c050b3,715720b663e85c5e16cbf8b1ef4ec208,74dd26ac71a37c92f3eda8552701ca33,ac2eb4232eaa7c1adbf00d4a0be3d799,ac3456a3574f6939fcb6d5242c202810,d4080e34001a0ebe22f20efdb204240b</data>
    </node>
    <node id="&quot;H&#201;NON STRANGE ATTRACTOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The H&#233;non strange attractor is a fractal structure that results from the chaotic behavior of the H&#233;non map."
"The H&#233;non strange attractor is a fractal structure that is the attractor of the H&#233;non map, a mathematical model that models the Lorenz model."</data>
      <data key="d2">9ada201f787cd4e88cd18dae60de346d,d4080e34001a0ebe22f20efdb204240b</data>
    </node>
    <node id="&quot;LORENZ SYSTEM WIKIPEDIA PAGE&quot;">
      <data key="d0">"LOCATION"</data>
      <data key="d1">"The Lorenz system Wikipedia page is a webpage that provides information about the Lorenz system and its properties."</data>
      <data key="d2">d4080e34001a0ebe22f20efdb204240b</data>
    </node>
    <node id="&quot;H&#201;NON MAP WIKIPEDIA PAGE&quot;">
      <data key="d0">"LOCATION"</data>
      <data key="d1">"The H&#233;non map Wikipedia page is a webpage that provides information about the H&#233;non map and its properties."</data>
      <data key="d2">d4080e34001a0ebe22f20efdb204240b</data>
    </node>
    <node id="&quot;LOGISTIC MAP&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Logistic map is a polynomial mapping of degree 1 that generates a sequence of numbers, known for its chaotic behavior under certain conditions."
"The logistic map is a polynomial mapping of degree 2 that models population dynamics with reproduction and density-dependent mortality, showing chaotic behavior for certain values of a parameter."
"Logistic Map is a mathematical model used to describe chaotic systems, mentioned in the text."
"Logistic map is a discrete-time dynamical system that models population growth, often used to study the onset of chaotic behavior."</data>
      <data key="d2">9ada201f787cd4e88cd18dae60de346d,af2db1cc5ab6b16acae2c93d3facb668,b483c6bbce54156c724905b340aa2e85,d4080e34001a0ebe22f20efdb204240b</data>
    </node>
    <node id="&quot;LORENZ MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Lorenz model is a system of ordinary differential equations that describes the flow of fluid in a three-dimensional space, known for its chaotic behavior."
"The Lorenz Model is a mathematical model used to study the behavior of a system of differential equations, representing a complex system with chaotic properties."</data>
      <data key="d2">9ada201f787cd4e88cd18dae60de346d,c838b1b4744bc0f400abf85f791950cf</data>
    </node>
    <node id="&quot;DOUBLE SCROLL ATTRACTOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Double scroll attractors, also known as Chua's attractors, are strange attractors observed in a chaotic electronic circuit called Chua's circuit, characterized by a system of three nonlinear ordinary differential equations and a piecewise-linear equation."
"Double scroll attractors are strange attractors observed in a chaotic electronic circuit called Chua's circuit, characterized by a system of three nonlinear ordinary differential equations and a piecewise-linear equation. They resemble two interconnected rings in three-dimensional space and exhibit fractal-like structures."
"The Double Scroll Attractor is a mathematical concept used in the example, representing a complex system of equations."
"Double Scroll Attractor is a mathematical model used to describe chaotic systems, mentioned in the text."
"Double scroll attractor is a mathematical model used to describe the behavior of a system of differential equations, often used to study the dynamics of coupled oscillators."
"The Double Scroll Attractor is a mathematical model used to forecast strange attractors, defined by a set of differential equations."
"The Double Scroll Attractor is a mathematical concept described in the text, representing a complex system of equations."</data>
      <data key="d2">538ff8c18495002c85cbc9020b0146f9,684b1edf65b327cc06ceb69ca1279d74,770691846086629ac7d541f51760552c,91704ce63f9ba41247fdc452a7a62ba6,9ada201f787cd4e88cd18dae60de346d,af2db1cc5ab6b16acae2c93d3facb668,b483c6bbce54156c724905b340aa2e85</data>
    </node>
    <node id="&quot;CHUA'S CIRCUIT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Chua's circuit is an electronic circuit that exhibits chaotic behavior, in which double scroll attractors, also known as Chua's attractors, are observed."
"Chua's circuit is an electronic circuit that exhibits chaotic behavior, in which the Double scroll attractor is observed."</data>
      <data key="d2">538ff8c18495002c85cbc9020b0146f9,9ada201f787cd4e88cd18dae60de346d</data>
    </node>
    <node id="&quot;CHAOTIC BEHAVIOR&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">9ada201f787cd4e88cd18dae60de346d</data>
    </node>
    <node id="&quot;SCIKIT-LEARN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"scikit-learn is a machine learning library for Python that provides simple and efficient tools for data analysis and modeling."
"Scikit-Learn is a machine learning library in Python that provides simple and efficient tools for data mining and data analysis."
"Scikit-learn is a popular machine learning library that provides various algorithms and tools for data analysis and modeling."
"scikit-learn is a machine learning library that implements various classifiers such as RidgeClassifier, LogisticRegression, and Perceptron."
"Scikit-learn is a library used for machine learning in Python, providing implementations for RidgeClassifier, LogisticRegression, and Perceptron."
"scikit-learn is mentioned as a library that offers a simple API to apply ML techniques, but it lacks flexibility and is not meant to create complex neural networks operating on timeseries, with feedback loops and online learning rules."
"scikit-learn is a machine learning library in Python that provides simple and efficient tools for data mining and data analysis."
"scikit-learn is an open-source machine learning library for Python, which offers a high-level object-oriented API for end-to-end training, similar to the functionality provided by reservoirpy for Reservoir Computing."
"scikit-learn is a machine learning library that provides tools for data analysis and modeling."
"scikit-learn is a machine learning library that experiences have been shared from in a conference."
"Scikit-Learn is a library used for machine learning in Python, providing tools for data analysis and modeling."
"Scikit-learn is a library used for machine learning tasks, providing algorithms such as RidgeClassifier, LogisticRegression, and Perceptron."</data>
      <data key="d2">0036fb6f489e13c0db0f1c02bf3323be,295606b4bc5d12929a913a3c79f93734,538ff8c18495002c85cbc9020b0146f9,82de30f43839f4985de20a981b524af1,b130d3d59f0d3a2bb4feac9fdb85ed5b,bc2d4d6bb706c3d06ffd2c9c2f362104,c05906c1f12c4edfc32a04aa9935067e,ce7b58ffc7f43f36bc78154597d01903,d58662ee42c14a0787d839ebfd0a6e9b,d622f95153798af8bb6f485db54aaea3,eebc9d7d2b66e3898b7d068c38fd200f,f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </node>
    <node id="&quot;MODULENOTFOUNDERROR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"ModuleNotFoundError is a Python exception that occurs when an imported module cannot be found."</data>
      <data key="d2">538ff8c18495002c85cbc9020b0146f9</data>
    </node>
    <node id="&quot;GLOB.GLOB()&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"glob.glob() is a Python function that retrieves all file paths matching a specified pattern and returns a list of those paths."
"glob.glob() is a function that retrieves all file paths matching a specified pattern and returns them as a list."</data>
      <data key="d2">538ff8c18495002c85cbc9020b0146f9,fb40afaf160923869aba1456b3a1ddca</data>
    </node>
    <node id="&quot;FILE PATHS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">538ff8c18495002c85cbc9020b0146f9</data>
    </node>
    <node id="&quot;IKIT-LEARN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ikit-learn is a machine learning library that provides tools for data analysis and modeling."</data>
      <data key="d2">fb40afaf160923869aba1456b3a1ddca</data>
    </node>
    <node id="&quot;PARALLEL()&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"Parallel() is a function from the Joblib library that enables parallel processing, allowing for the concurrent execution of tasks."</data>
      <data key="d2">fb40afaf160923869aba1456b3a1ddca</data>
    </node>
    <node id="&quot;DELAYED()&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"delayed() is a function from the Joblib library that creates a lazy evaluation of the function it wraps, enabling parallel processing."
"delayed() is a function in Joblib used to create a lazy evaluation of the function it wraps, allowing for concurrent execution of tasks."</data>
      <data key="d2">f3b5b178557c4991ab5b81d869a4752e,fb40afaf160923869aba1456b3a1ddca</data>
    </node>
    <node id="&quot;PD.READ_CSV()&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"pd.read_csv() is a function from the pandas library that reads a CSV file and returns a DataFrame."</data>
      <data key="d2">fb40afaf160923869aba1456b3a1ddca</data>
    </node>
    <node id="&quot;R4-DATA/EXPERIMENTS/&quot;">
      <data key="d0">"DIRECTORY"</data>
      <data key="d1">"r4-data/experiments/ is a directory where files are located for data analysis and modeling."</data>
      <data key="d2">fb40afaf160923869aba1456b3a1ddca</data>
    </node>
    <node id="&quot;PD.READ_CSV&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"pd.read_csv is a function in the pandas library used to read a CSV file and return a DataFrame."</data>
      <data key="d2">f3b5b178557c4991ab5b81d869a4752e</data>
    </node>
    <node id="&quot;PARALLEL&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"parallel is a function in Joblib used to run tasks concurrently, allowing for faster data loading and processing."</data>
      <data key="d2">f3b5b178557c4991ab5b81d869a4752e</data>
    </node>
    <node id="&quot;NP.ROLL&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"np.roll is a function in the NumPy library used to shift the elements of an array by a specified number of positions."
"np.roll is a function that circularly shifts the elements of an array by a specified number of positions."</data>
      <data key="d2">584889d2db258e32e7f673d3c0a0e603,f3b5b178557c4991ab5b81d869a4752e</data>
    </node>
    <node id="&quot;RMSE&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"rmse is a function used to calculate the Root Mean Square Error, a common metric for evaluating the performance of a model."

"RMSE (Root Mean Squared Error) is a loss function used to evaluate the quality of the parameters chosen in the optimization process."
"RMSE is the Root Mean Square Error, a metric used to evaluate the performance of the Echo State Network."

"RMSE stands for Root Mean Squared Error, a metric used to evaluate the performance of a prediction model."
"RMSE stands for Root Mean Square Error, which is a measure of the differences between values predicted by a model and the values observed."</data>
      <data key="d2">251a50c2ae8ceea4fd7da1127cc5f461,584889d2db258e32e7f673d3c0a0e603,7c7818502732457fb71aacdd9a90ee36,bf4eaad93f89884d02cdad6a50f145a6,d15f6d075c072f0335b5332f11c00299,f3b5b178557c4991ab5b81d869a4752e,f730c6800099724052a2d061f3cd8c2e</data>
    </node>
    <node id="&quot;ARRAY ELEMENTS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">f3b5b178557c4991ab5b81d869a4752e</data>
    </node>
    <node id="&quot;MODEL PERFORMANCE&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">f3b5b178557c4991ab5b81d869a4752e</data>
    </node>
    <node id="&quot;ROOT MEAN SQUARED ERROR (RMSE)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"RMSE is a statistical measure that quantifies the average magnitude of the errors between predicted and actual values."
"Root Mean Squared Error (RMSE) is a loss function used to evaluate the quality of parameters in the optimization process."
"RMSE is a loss function used to evaluate the quality of parameters in optimization algorithms, such as hyperopt."</data>
      <data key="d2">251a50c2ae8ceea4fd7da1127cc5f461,4f7b43545046f0e6f9b6fb3816da1d79,584889d2db258e32e7f673d3c0a0e603</data>
    </node>
    <node id="&quot;AVERAGED RMSE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Averaged RMSE is the average of the Root Mean Squared Errors, providing an overall measure of prediction accuracy."
"Averaged RMSE is a metric used to evaluate the performance of a model, with a lower value indicating better accuracy."</data>
      <data key="d2">1e8ee805d22cd143d2372d300997d253,584889d2db258e32e7f673d3c0a0e603</data>
    </node>
    <node id="&quot;AVERAGED RMSE (WITH THRESHOLD)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Averaged RMSE (with threshold) is a variant of Averaged RMSE that considers only predictions within a specific range, providing a more focused measure of prediction accuracy."</data>
      <data key="d2">584889d2db258e32e7f673d3c0a0e603</data>
    </node>
    <node id="&quot;GLOB.GLOB&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"glob.glob is a function that finds all pathnames matching a specified pattern, allowing for the retrieval of files with a specific extension."</data>
      <data key="d2">584889d2db258e32e7f673d3c0a0e603</data>
    </node>
    <node id="&quot;SORTED&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"sorted is a function that sorts the elements of a list in ascending order."</data>
      <data key="d2">584889d2db258e32e7f673d3c0a0e603</data>
    </node>
    <node id="&quot;LBR.FEATURE.MFCC&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"lbr.feature.mfcc is a function that computes the Mel-frequency cepstral coefficients (MFCCs) of a given audio signal, which are commonly used for speech and audio processing tasks."</data>
      <data key="d2">584889d2db258e32e7f673d3c0a0e603</data>
    </node>
    <node id="&quot;CIRCULAR SHIFT&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">584889d2db258e32e7f673d3c0a0e603</data>
    </node>
    <node id="&quot;FILE RETRIEVAL&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">584889d2db258e32e7f673d3c0a0e603</data>
    </node>
    <node id="&quot;ASCENDING ORDER&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">584889d2db258e32e7f673d3c0a0e603</data>
    </node>
    <node id="&quot;SPEECH AND AUDIO PROCESSING&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">584889d2db258e32e7f673d3c0a0e603</data>
    </node>
    <node id="&quot;SORTED() FUNCTION&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"The sorted() function is a built-in Python function that sorts elements in a list in ascending order."</data>
      <data key="d2">7bea9e814256104a2d7ede466ddc3364</data>
    </node>
    <node id="&quot;GLOB.GLOB() FUNCTION&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"The glob.glob() function is a Python function that finds all pathnames matching a specified pattern according to the rules used by the Unix shell, making it useful for file operations."</data>
      <data key="d2">7bea9e814256104a2d7ede466ddc3364</data>
    </node>
    <node id="&quot;MFCCS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"MFCCs (Mel-Frequency Cepstral Coefficients) are features commonly used in speech and audio processing to capture the spectral properties of an audio signal."
</data>
      <data key="d2">7bea9e814256104a2d7ede466ddc3364,e3828ad4e78d575fabb543e0eab86160</data>
    </node>
    <node id="&quot;LIBROSA LIBRARY&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"The librosa library is a Python library for analyzing audio and music. It has functions for extracting MFCCs and other features from audio signals."</data>
      <data key="d2">7bea9e814256104a2d7ede466ddc3364</data>
    </node>
    <node id="&quot;DELTA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Delta refers to a change or difference, but in the provided text, the term 'delta' is not explicitly defined or used in the context of audio processing or MFCCs."
"Delta is a feature extraction technique used to capture the rate of change of MFCC coefficients over time."</data>
      <data key="d2">7bea9e814256104a2d7ede466ddc3364,adfade0d7bc85c6420e61ecd1ce7095c</data>
    </node>
    <node id="&quot;LIBROSA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"librosa is a Python library used for analyzing audio and music, providing functions for feature extraction and processing."
"librosa is a Python library for audio and music analysis, which provides functions for calculating Mel-Frequency Cepstral Coefficients (MFCCs) and their delta coefficients."
"librosa is a Python library for audio and music analysis, used for processing audio data."
"Librosa is a Python library for audio and music analysis, used for processing audio data in the text."</data>
      <data key="d2">1e8ee805d22cd143d2372d300997d253,7c8a0a6b9506a584f1c98495097d48ee,aea362ee35c2a3a01b76020d0b892cbd,e3828ad4e78d575fabb543e0eab86160</data>
    </node>
    <node id="&quot;MEL-FREQUENCY CEPSTRAL COEFFICIENTS (MFCCS)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"MFCCs are a type of feature used in audio signal processing to capture the spectral properties of the audio signal."
"MFCCs are a feature extraction technique used in audio and speech processing, representing the short-term power spectrum of a signal."</data>
      <data key="d2">7c8a0a6b9506a584f1c98495097d48ee,e3828ad4e78d575fabb543e0eab86160</data>
    </node>
    <node id="&quot;DELTA COEFFICIENTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Delta Coefficients are derived from the MFCCs and capture the temporal dynamics of the MFCCs, enhancing the feature set for tasks like audio and speech processing."
"Delta coefficients are the first-order differences of the MFCCs, capturing the temporal dynamics of the MFCCs and enhancing the feature set for tasks like audio and speech processing."
"Delta coefficients are a measure of the rate of change of a signal, used in audio and music analysis."</data>
      <data key="d2">7c8a0a6b9506a584f1c98495097d48ee,aea362ee35c2a3a01b76020d0b892cbd,e3828ad4e78d575fabb543e0eab86160</data>
    </node>
    <node id="&quot;SECOND-ORDER DIFFERENCES (OR DELTA-DELTAS)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Second-order differences (or delta-deltas) of the MFCCs are the second-order derivatives of the input feature matrix, capturing the acceleration or the rate of change of the delta coefficients."</data>
      <data key="d2">7c8a0a6b9506a584f1c98495097d48ee</data>
    </node>
    <node id="&quot;EDGE EFFECTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Edge effects refer to artifacts or distortions that occur at the boundaries of a signal, which can affect the accuracy of analysis."</data>
      <data key="d2">aea362ee35c2a3a01b76020d0b892cbd</data>
    </node>
    <node id="&quot;DATAFRAME&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"DataFrame is a two-dimensional labeled data structure with columns of potentially different types, used for data manipulation and analysis."</data>
      <data key="d2">aea362ee35c2a3a01b76020d0b892cbd</data>
    </node>
    <node id="&quot;NAMED TUPLES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Named tuples are a subclass of the built-in tuple type that have fields accessible by attribute lookup as well as being indexable and iterable."</data>
      <data key="d2">aea362ee35c2a3a01b76020d0b892cbd</data>
    </node>
    <node id="&quot;ONE-HOT ENCODING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"One-hot encoding is a process of converting categorical data variables so they can be provided to machine learning algorithms, which require numerical input."</data>
      <data key="d2">aea362ee35c2a3a01b76020d0b892cbd</data>
    </node>
    <node id="&quot;CATEGORICAL DATA&quot;">
      <data key="d0" />
      <data key="d1">
"Categorical data is data that consists of labels or categories, rather than numerical values. In the given example, the phrase labels are categorical data that is transformed into a binary matrix representation using the OneHotEncoder function."</data>
      <data key="d2">aea362ee35c2a3a01b76020d0b892cbd,d5477dbd84525291f2e017ae618de222</data>
    </node>
    <node id="&quot;ONEHOTENCODER&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"OneHotEncoder is a function used for transforming categorical data into a binary matrix representation, which is useful for feeding categorical data into machine learning models."
"OneHotEncoder is a function used for converting categorical data into a format that can be used by machine learning algorithms."
"OneHotEncoder is a function from the scikit-learn library used for one-hot encoding of categorical variables."</data>
      <data key="d2">71366a4c7e791080872ba783d3787bd7,84a64dd2c683e779d55aaccea16b1032,d5477dbd84525291f2e017ae618de222</data>
    </node>
    <node id="&quot;SPARSE_OUTPUT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"sparse_output is a parameter of the OneHotEncoder function that determines whether the output should be a sparse matrix or a dense array. A sparse matrix is a data structure that only stores non-zero elements, which can be more memory-efficient for large datasets with many zeros."
"sparse_output is a parameter in the OneHotEncoder function that specifies the format of the resulting one-hot encoded matrix."</data>
      <data key="d2">84a64dd2c683e779d55aaccea16b1032,d5477dbd84525291f2e017ae618de222</data>
    </node>
    <node id="&quot;MACHINE LEARNING MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Machine learning models are algorithms that can learn patterns from data and make predictions or decisions based on that learning. One-hot encoding is a preprocessing step that can improve the performance of machine learning models by converting categorical data into a format that is more suitable for the models."</data>
      <data key="d2">d5477dbd84525291f2e017ae618de222</data>
    </node>
    <node id="&quot;FLATTEN()&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"flatten() is a function that converts a multi-dimensional array into a one-dimensional array."</data>
      <data key="d2">84a64dd2c683e779d55aaccea16b1032</data>
    </node>
    <node id="&quot;ARGMAX()&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"argmax() is a function that returns the indices of the maximum values along the specified axis."</data>
      <data key="d2">84a64dd2c683e779d55aaccea16b1032</data>
    </node>
    <node id="&quot;HYPEROPT&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Hyperopt is a Python library used for optimizing hyperparameters of machine learning models."
"Hyperopt is a Python library used for optimizing hyperparameters of machine learning models, leveraging algorithms like random search, grid search, and Bayesian optimization."
"Hyperopt is a library used for optimization, which expects the objective function to have local minima that can be reached by shifting the parameters."
"Hyperopt is a library used for hyperparameter optimization, mentioned in the text."
"Hyperopt is a Python library used for hyperparameter optimization, which is being used in the context of Echo State Networks."
"Hyperopt is a Python library used for hyperparameter optimization, which is utilized by ReservoirPy."
"Hyperopt is a Python library for hyperparameter optimization, which may be used in the Python Notebooks for reservoir computing."
"Hyperopt is a Python library for hyperparameter optimization, used in conjunction with ReservoirPy."
"Hyperopt is a software library used for hyperparameter optimization."
"Hyperopt is a Python library used for hyperparameter optimization, which involves finding the best combination of parameters for a given model or function."
"Hyperopt is a tool used to approximate a function and find a minimum, mentioned in the text."
"Hyperopt is a Python library used for hyperparameter optimization, mentioned in the provided text."
"Hyperopt is a Python library used for hyperparameter optimization."
"Hyperopt is a tool used for hyperparameter optimization, which is mentioned in the text."
"Hyperopt is a tool mentioned for optimizing parameters, which uses a random distribution of parameters and enough trials to increase the chance of finding a relevant minimum."
"Hyperopt is a Python library for optimizing the hyperparameters of machine learning algorithms, mentioned in the text."
"Hyperopt is a python library for optimizing the hyperparameters of machine learning algorithms, authored by James Bergstra, Dan Yamins, and David D Cox."
"Hyperopt is a Python library for hyperparameter optimization, which is used in the provided code to explore different sets of parameters."
"Hyperopt is a tool used for hyperparameter optimization, which is being used to explore different sets of parameters."
"Hyperopt is a Python library for hyperparameter optimization, which is mentioned in the text."
"Hyperopt is a Python library for hyperparameter optimization, mentioned in the text."</data>
      <data key="d2">0753d4e507badadd900c522ee03ad28d,0982b8d1eb1e636b19fa2e9d9361e566,0a9b132ecb1c4b63fdbb0e144295362e,0b6c69085074b2cf23267eb149068b9f,11749b7d0fdadf05ea29da6025618407,251a50c2ae8ceea4fd7da1127cc5f461,25743a99f36f3e56551ffafbba8d15c4,280cbdf53022bbaed48ccb34ebe142bc,3b4d50c051c177770830f7c0a6b3dd69,46913f0d73ba0b8cecfdf42bde9862f4,65ba78d1f678e080bd930319c54234ef,6a6d88a8f9731e1ed05b786e0a9ba6dc,75e530c1a04e30b373dc7cc68e3ad819,7c7818502732457fb71aacdd9a90ee36,82de30f43839f4985de20a981b524af1,82ff270b1bbdfe0ee11e603de1e326c7,84a64dd2c683e779d55aaccea16b1032,870f29520f7a1c42eecb0c4ff855f09e,9abdbd696e340cb5dd8c66ac5cd30c67,a3a74dc4754a8c8b0730f808285893e2,c5413fef3b2d7e4d688c66e6046b56c7</data>
    </node>
    <node id="&quot;NP.ARGMAX()&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"np.argmax() is a function that returns the indices of the maximum values along the specified axis."</data>
      <data key="d2">3b4d50c051c177770830f7c0a6b3dd69</data>
    </node>
    <node id="&quot;RESERVOIRPY.HYPER&quot;">
      <data key="d0">"MODULE"</data>
      <data key="d1">"reservoirpy.hyper is a module in the ReservoirPy library designed for optimizing hyperparameters of Echo State Networks (ESNs)."</data>
      <data key="d2">3b4d50c051c177770830f7c0a6b3dd69</data>
    </node>
    <node id="&quot;UNITS&quot;">
      <data key="d0">"HYPERPARAMETER"</data>
      <data key="d1">"UNITS is a hyperparameter that refers to the number of neurons inside the reservoir of an Echo State Network (ESN)."
"UNITS is a hyperparameter that refers to the number of neurons inside the reservoir."
"Units refer to the number of processing elements in the reservoir of Reservoir Computing."
"Units refer to the individual nodes or processing elements in a neural network, such as the reservoir in the Reservoir Computing model."
"Units refer to the number of neurons or units in a neural network node."
"Units are a measure or quantity mentioned in the text, likely used in the context of the ES2N system."
"UNITS is a parameter used in the machine learning model, likely representing a concept or element within its architecture."
"units refers to the number of neurons in the reservoir of the ESN."
"Units refer to the number of nodes in the Reservoir component of the ESN model."
"UNITS is a term used to refer to a specific number of units or components in the Reservoir system."</data>
      <data key="d2">09198e939639c229c2c97555f65b12a7,3b4d50c051c177770830f7c0a6b3dd69,72e6eee633bcb5b1458c4cee3975cee1,8e1f4f13f617b982d272175296ec99d3,8ecf03267c90a64376f5040307d98195,96366d7c23d50de6294c54c3444eac86,9dd8e12c7acbe10cf34817ff14780d24,a8df60a94e25d863b436f47f4f8e6a6d,bba680a0a7dd439bd5b0fe1547ffe040,cedd895dd1ddc09e70c8799effdf7427</data>
    </node>
    <node id="&quot;SPECTRAL_RADIUS&quot;">
      <data key="d0">"HYPERPARAMETER"</data>
      <data key="d1">"SPECTRAL_RADIUS is a hyperparameter that refers to the maximum absolute eigenvalue of the reservoir matrix in an Echo State Network (ESN). It affects the stability and chaos of the dynamics."
"SPECTRAL_RADIUS is a hyperparameter that refers to the maximum absolute eigenvalue of the reservoir matrix, influencing the dynamics' stability and chaos."
"spectral_radius is a parameter used in the machine learning model, likely representing a mathematical concept related to its architecture."
"Spectral radius is a mathematical concept mentioned in the text, potentially related to the analysis of data or models."
"spectral_radius is a parameter used to set the spectral radius of the reservoir's weight matrix in the ESN."
"SPECTRAL_RADIUS is a mathematical concept used in the text, likely in the context of the Reservoir system."</data>
      <data key="d2">09198e939639c229c2c97555f65b12a7,3b4d50c051c177770830f7c0a6b3dd69,96366d7c23d50de6294c54c3444eac86,97f5d2e9d34b3b50f8e922fc4bb7f824,a8df60a94e25d863b436f47f4f8e6a6d,bba680a0a7dd439bd5b0fe1547ffe040</data>
    </node>
    <node id="&quot;CORRELATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Correlation is a statistical relationship between two variables, ranging from -1 to 1, indicating the degree and direction of their relationship."
"Correlation is a measure used to determine the relationship between two variables, in this context, the correlation between reservoir states and inputs."
"Correlation is a statistical concept used to measure the relationship between two variables, which is mentioned in the text."</data>
      <data key="d2">8553a88d9aaf4f71d359c721a1f6fa70,a8df60a94e25d863b436f47f4f8e6a6d,b957e1bf5bf175c7630222ca742c7933</data>
    </node>
    <node id="&quot;NP.CORRCOEF()&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"np.corrcoef() is a function used to calculate the Pearson correlation coefficient matrix between two or more variables."</data>
      <data key="d2">a8df60a94e25d863b436f47f4f8e6a6d</data>
    </node>
    <node id="&quot;INPUT_SCALING&quot;">
      <data key="d0">"HYPERPARAMETER"</data>
      <data key="d1">"INPUT_SCALING is a hyperparameter that refers to a coefficient applied to the inputs of the reservoir, influencing the correlation between states and inputs."
"INPUT_SCALING is a hyperparameter that adjusts the gain of the inputs to the reservoir, influencing the correlation between states and inputs."
"input_scaling is a parameter explored by hp_space that represents the input scaling, fixed to 1.0 in the provided configuration."
"input_scaling is a parameter explored by hp_space, fixed to 1.0."
"input_scaling is a parameter that specifies the input scaling, which is fixed."
"input_scaling is a parameter used in the machine learning model, likely representing a mathematical concept related to its architecture."
"Input scaling is a mathematical concept mentioned in the text, potentially related to the analysis of data or models."
"input_scaling is a parameter used to scale the input data in the ESN."</data>
      <data key="d2">09198e939639c229c2c97555f65b12a7,5cea9edfd65fcfa25a081554300b28cc,76f47f241e255f9f36646409d2ec30f1,80033e741d8e10abdcfe20dd17192152,96366d7c23d50de6294c54c3444eac86,97f5d2e9d34b3b50f8e922fc4bb7f824,a8df60a94e25d863b436f47f4f8e6a6d,adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </node>
    <node id="&quot;PERSON CORRELATION COEFFICIENT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Person Correlation Coefficient is a statistical measure used to determine the linear relationship between two continuous variables."</data>
      <data key="d2">76f47f241e255f9f36646409d2ec30f1</data>
    </node>
    <node id="&quot;RC_CONNECTIVITY&quot;">
      <data key="d0">"HYPERPARAMETER"</data>
      <data key="d1">"RC_CONNECTIVITY is a hyperparameter that determines the density of the reservoir's internal matrix."
"RC_CONNECTIVITY is a term used in the text, likely referring to the connectivity or interconnection of units within the Reservoir system."</data>
      <data key="d2">76f47f241e255f9f36646409d2ec30f1,bba680a0a7dd439bd5b0fe1547ffe040</data>
    </node>
    <node id="&quot;INPUT_CONNECTIVITY&quot;">
      <data key="d0">"HYPERPARAMETER"</data>
      <data key="d1">"INPUT_CONNECTIVITY is a hyperparameter that determines the density of the reservoir's input matrix."
"input_connectivity is a parameter used to set the sparsity of the input-to-reservoir weight matrix in the ESN."
"INPUT_CONNECTIVITY is a term used in the text, likely referring to the connection or interaction of external inputs with the Reservoir system."</data>
      <data key="d2">09198e939639c229c2c97555f65b12a7,76f47f241e255f9f36646409d2ec30f1,bba680a0a7dd439bd5b0fe1547ffe040</data>
    </node>
    <node id="&quot;REGULARIZATION&quot;">
      <data key="d0">"HYPERPARAMETER"</data>
      <data key="d1">"REGULARIZATION is a hyperparameter that represents the regularization coefficient for ridge regression."
"Regularization is a technique used to smooth the output of the ESN Models and reduce overfitting."
"regularization is a parameter used to control the amount of regularization in the Ridge readout of the ESN."
"Regularization is a technique used to prevent overfitting in the ESN model by adding a penalty term to the loss function."</data>
      <data key="d2">09198e939639c229c2c97555f65b12a7,5972cf7d440b1c3fdc0f05fca305f18d,72e6eee633bcb5b1458c4cee3975cee1,76f47f241e255f9f36646409d2ec30f1</data>
    </node>
    <node id="&quot;LEAK_RATE&quot;">
      <data key="d0">"HYPERPARAMETER"</data>
      <data key="d1">"LEAK_RATE is a hyperparameter that controls the time constant of the ESN, influencing the inertia and recall of previous states."
"LEAK_RATE is a hyperparameter that controls the time constant of a system, influencing its inertia and recall of previous states."
"leak_rate is a parameter used to control the leakage of the reservoir's neurons in the ESN."
"LEAK_RATE is a term used in the text, likely referring to a rate of information loss or decay in the Reservoir system."</data>
      <data key="d2">09198e939639c229c2c97555f65b12a7,3c4d88c41f6efbfccea6f8814bf8430e,76f47f241e255f9f36646409d2ec30f1,bba680a0a7dd439bd5b0fe1547ffe040</data>
    </node>
    <node id="&quot;DOUBLE-SCROLL ATTRACTOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Double-Scroll Attractor is a type of chaotic attractor observed in certain nonlinear dynamical systems."
"A Double-Scroll Attractor is a type of chaotic attractor observed in certain nonlinear dynamical systems, characterized by its distinctive shape of intertwined scrolls or spirals."
"Double-scroll Attractor is a dynamic system that generates a complex pattern, used as a subject in the optimization and experimentation process."
"Double-Scroll Attractor is a mathematical concept used in the example, representing a complex system of differential equations."</data>
      <data key="d2">2d8ea1123f365fb047b024022ba4fdc4,3c4d88c41f6efbfccea6f8814bf8430e,76f47f241e255f9f36646409d2ec30f1,7c7818502732457fb71aacdd9a90ee36</data>
    </node>
    <node id="&quot;DOUBLESCROLL() FUNCTION&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"The doublescroll() function is used to simulate the dynamics of a system that generates a Double-Scroll Attractor."</data>
      <data key="d2">3c4d88c41f6efbfccea6f8814bf8430e</data>
    </node>
    <node id="&quot;.CIVIDIS() FUNCTION&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"The .cividis() function is used to obtain a color in a system or process."</data>
      <data key="d2">3c4d88c41f6efbfccea6f8814bf8430e</data>
    </node>
    <node id="&quot;RK23&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"RK23 is a system that generates a double-scroll attractor, used in the context of optimization and experimentation."</data>
      <data key="d2">7c7818502732457fb71aacdd9a90ee36</data>
    </node>
    <node id="&quot;OPTIMIZATION ALGORITHMS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Optimization Algorithms are methods used to find the minimum of a function, often relying on the hypothesis of convexity."</data>
      <data key="d2">4f7b43545046f0e6f9b6fb3816da1d79</data>
    </node>
    <node id="&quot;R-SQUARED (R^2)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"R-squared (R^2) is an additional metric used to assess the goodness of fit of a model."</data>
      <data key="d2">4f7b43545046f0e6f9b6fb3816da1d79</data>
    </node>
    <node id="&quot;HYPERPARAMETER OPTIMIZATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Hyperparameter Optimization is the process of finding the best hyperparameters for a machine learning model, which is facilitated by ReservoirPy."
"Hyperparameter Optimization is a process that involves finding the best hyperparameters for a machine learning model to improve its performance."
"Hyperparameter Optimization is the process of finding the best hyperparameters for a machine learning model, such as Echo State Networks, to improve its performance."
"Hyperparameter Optimization is the process of finding the best set of hyperparameters that minimize the loss function."
"Hyperparameter Optimization is the process of finding the best hyperparameters for a machine learning model."</data>
      <data key="d2">46913f0d73ba0b8cecfdf42bde9862f4,9abdbd696e340cb5dd8c66ac5cd30c67,bd4cf5e35045463b7f0d8da82debc122,d4684af3c445d312afe4d838abc45502,f70c7d3d89baaabbeaad57b58e379e08</data>
    </node>
    <node id="&quot;OBJECTIVE FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Objective Function is a function that calculates the loss or error of a machine learning model, which ReservoirPy uses to optimize hyperparameters."
"The Objective Function is the function that the Optimization Algorithm aims to optimize, measuring the performance of the forecasting task."
"The Objective Function is the function that is being optimized in hyperparameter optimization, which in this case is the Root Mean Squared Error (RMSE) loss function."
"The Objective Function is a function that defines the goal to be optimized, mentioned in the text."
"The objective function is a part of the provided code, used to evaluate and optimize the performance of a machine learning model."</data>
      <data key="d2">0753d4e507badadd900c522ee03ad28d,11749b7d0fdadf05ea29da6025618407,251a50c2ae8ceea4fd7da1127cc5f461,91704ce63f9ba41247fdc452a7a62ba6,d4684af3c445d312afe4d838abc45502</data>
    </node>
    <node id="&quot;CONFIG FILE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Config File is a JSON file that defines the hyperparameters and settings for the hyperparameter optimization process, which is used as input to ReservoirPy."</data>
      <data key="d2">d4684af3c445d312afe4d838abc45502</data>
    </node>
    <node id="&quot;CONFIGURATION FILE&quot;">
      <data key="d0">"FILE"</data>
      <data key="d1">"The Configuration File is a JSON file that defines the hyperparameters and settings for the Hyperparameter Optimization process."</data>
      <data key="d2">bd4cf5e35045463b7f0d8da82debc122</data>
    </node>
    <node id="&quot;RANDOM SEARCH ALGORITHM&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"Random Search Algorithm is a technique used in Hyperparameter Optimization to randomly choose parameters within a specified range, maximizing the chances of reaching a local minimum."
"Random Search Algorithm is a method mentioned for hyperparameter optimization, which involves randomly choosing parameters within a specified range."
"Random Search Algorithm is a method used to optimize parameters by randomly choosing values within a specified range."</data>
      <data key="d2">0a9b132ecb1c4b63fdbb0e144295362e,25743a99f36f3e56551ffafbba8d15c4,bd4cf5e35045463b7f0d8da82debc122</data>
    </node>
    <node id="&quot;GRID SEARCH&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"Grid Search is a technique used in Hyperparameter Optimization that involves a systematic search through a manually specified subset of the hyperparameter space of a learning algorithm."
"Grid search is a method used for hyperparameter tuning in machine learning, exploring different combinations of hyperparameters."
"Grid Search is a method mentioned for hyperparameter optimization, which involves systematically searching through a grid of parameters."
"Grid Search is a method mentioned as adding a bias during optimization, which could prevent finding a relevant minimum."</data>
      <data key="d2">0a9b132ecb1c4b63fdbb0e144295362e,25743a99f36f3e56551ffafbba8d15c4,b3361508c3e49b5bb3089f10e31d2c81,bd4cf5e35045463b7f0d8da82debc122</data>
    </node>
    <node id="&quot;PARAMETERS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Parameters are variables that can be adjusted to optimize a model or algorithm, mentioned in the text."
"Parameters are the configurable values used in the reservoir network, such as units, connectivity, sr, input_scaling, mu, sigma, warmup, learning_rate, epochs, W_dist, and Win_dist."</data>
      <data key="d2">388cc054a99cc5cadff33147f95d6156,6a6d88a8f9731e1ed05b786e0a9ba6dc</data>
    </node>
    <node id="&quot;LOSS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Loss is a measure of the error or discrepancy between the predicted and actual values in a model, mentioned in the text."
"Loss is a measure of the error or discrepancy between predicted and actual values in a machine learning model."
"Loss is a metric used to evaluate the performance of the machine learning model, representing the error between predicted and actual values."
"Loss is a measure of the error or discrepancy between the predicted and actual values in a model."</data>
      <data key="d2">136d135c710f6cf78a4c536d43276fe1,6a6d88a8f9731e1ed05b786e0a9ba6dc,716940af834825642e01a3cb59a7e006,75e530c1a04e30b373dc7cc68e3ad819</data>
    </node>
    <node id="&quot;EXPERIMENTATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Experimentation refers to the process of testing and optimizing parameters, mentioned in the text."
"Experimentation refers to the process of designing and conducting experiments to test hypotheses, gather data, and make decisions."</data>
      <data key="d2">251a50c2ae8ceea4fd7da1127cc5f461,6a6d88a8f9731e1ed05b786e0a9ba6dc</data>
    </node>
    <node id="&quot;HINAUT, X.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Hinaut, X. is an author of a guide on hyperparameter exploration mentioned in the text."
"Hinaut, X. is an author mentioned in the text, contributing to a guide on exploring hyper-parameters for Echo State Networks."</data>
      <data key="d2">088d2280349d652200861994c09d7dd5,6a6d88a8f9731e1ed05b786e0a9ba6dc</data>
    </node>
    <node id="&quot;TROUVAIN, N.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Trouvain, N. is a co-author of a guide on hyperparameter exploration mentioned in the text."
"Trouvain, N. is an author mentioned in the text, contributing to a guide on exploring hyper-parameters for Echo State Networks."</data>
      <data key="d2">088d2280349d652200861994c09d7dd5,6a6d88a8f9731e1ed05b786e0a9ba6dc</data>
    </node>
    <node id="&quot;ICANN 2021&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"ICANN 2021 is the conference where a guide on hyperparameter exploration was presented, mentioned in the text."
"ICANN 2021 is the event where the paper 'Which Hype for My New Task? Hints and Random Search for Echo State Networks Hyperparameters' was presented."
"ICANN 2021 is an event mentioned in the text, where papers on Canary Song Decoder and Echo State Networks Hyperparameters were presented."
"ICANN 2021 is an event mentioned where a guide on exploring hyper-parameters for a task was presented."</data>
      <data key="d2">20b16c2e1cb8813ade96fea5f9591631,25743a99f36f3e56551ffafbba8d15c4,46913f0d73ba0b8cecfdf42bde9862f4,6a6d88a8f9731e1ed05b786e0a9ba6dc</data>
    </node>
    <node id="&quot;XAVIER HINAUT&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Xavier Hinaut is an author of the paper, contributing to the research and development of hyperparameter optimization for Echo State Networks."
"Xavier Hinaut is a contact person for the library and a co-author on a paper about Reservoirpy."
"Xavier Hinaut is an author of ReservoirPy and a contributor to the library."
"Xavier Hinaut is a developer of ReservoirPy, a library for designing Echo State Networks."
"Xavier Hinaut is an author of a guide mentioned for exploring hyper-parameters for a task."
"Xavier Hinaut is a contributor to reservoirpy, working at Inria Bordeaux Sud-Ouest, IMN, LaBRI."
"Xavier Hinaut is an author of the paper, working at Inria Bordeaux Sud-Ouest."
"Xavier Hinaut is an author of a research paper on canary song decoder: transduction and implicit segmentation with esns and ltsms."</data>
      <data key="d2">136559fd2a1fbef4cc8a6b11abcb3eef,18910a60b2547ec3133340f42c45bb47,20b16c2e1cb8813ade96fea5f9591631,25743a99f36f3e56551ffafbba8d15c4,296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba,46913f0d73ba0b8cecfdf42bde9862f4,7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;NICOLAS TROUVAIN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Nicolas Trouvain is an author of the paper, contributing to the research and development of hyperparameter optimization for Echo State Networks."
"Nicolas Trouvain is an author of a guide mentioned for exploring hyper-parameters for a task."</data>
      <data key="d2">25743a99f36f3e56551ffafbba8d15c4,46913f0d73ba0b8cecfdf42bde9862f4</data>
    </node>
    <node id="&quot;HP_SPACE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"hp_space is a configuration parameter in the hyperopt file that defines the ranges of parameters explored."
"hp_space is a parameter exploration space used for hyperparameter optimization."
"hp_space is a parameter that specifies the ranges of parameters explored."</data>
      <data key="d2">5cea9edfd65fcfa25a081554300b28cc,80033e741d8e10abdcfe20dd17192152,adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </node>
    <node id="&quot;N&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"N is a parameter explored by hp_space that represents the number of neurons, fixed to 500 in the provided configuration."
"N is a parameter that specifies the number of neurons, which is fixed to 500."</data>
      <data key="d2">5cea9edfd65fcfa25a081554300b28cc,adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </node>
    <node id="&quot;SR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"sr is a parameter explored by hp_space that represents the spectral radius, log-uniformly distributed between 1e-2 and 10."
"sr is a parameter that specifies the spectral radius, which is log-uniformly distributed between 1e-2 and 10."
"SR likely refers to Support Vector Regression, a regression model that uses support vectors to find the best fit line."
"SR is a parameter in the ES2N class, likely representing a learning rate or a similar concept."
"SR is a parameter mentioned in the text, likely used in the configuration of the ES2N system."</data>
      <data key="d2">136d135c710f6cf78a4c536d43276fe1,5cea9edfd65fcfa25a081554300b28cc,9dd8e12c7acbe10cf34817ff14780d24,adfc38e9dc5e6fd0fe67ce83dfa1f154,cedd895dd1ddc09e70c8799effdf7427</data>
    </node>
    <node id="&quot;LR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"lr is a parameter explored by hp_space that represents the leaking rate, log-uniformly distributed between 1e-3 and 1."
"lr is a parameter that specifies the leaking rate, which is log-uniformly distributed between 1e-3 and 1."
"LR likely refers to Logistic Regression, a statistical model used for binary classification."</data>
      <data key="d2">136d135c710f6cf78a4c536d43276fe1,5cea9edfd65fcfa25a081554300b28cc,adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </node>
    <node id="&quot;JSAN.DUMP()&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"jsan.dump() is a function used to serialize a Python object and write it as a JSON-formatted stream to a file."</data>
      <data key="d2">80033e741d8e10abdcfe20dd17192152</data>
    </node>
    <node id="&quot;TRAINING SERIES&quot;">
      <data key="d0">"DATA"</data>
      <data key="d1">"training series is a subset of the dataset used for training an ESN on timeseries."
"training series is a subset of the dataset used for training an Echo State Network (ESN)."</data>
      <data key="d2">1db191f05801d40d5a346febd10d3352,80033e741d8e10abdcfe20dd17192152</data>
    </node>
    <node id="&quot;TESTING SERIES&quot;">
      <data key="d0">"DATA"</data>
      <data key="d1">"testing series is a subset of the dataset used for evaluating the performance of the trained ESN."
"testing series is a subset of the dataset used to evaluate the performance of an Echo State Network (ESN) after training."</data>
      <data key="d2">1db191f05801d40d5a346febd10d3352,80033e741d8e10abdcfe20dd17192152</data>
    </node>
    <node id="&quot;HYPEROPT_CONFIG&quot;">
      <data key="d0" />
      <data key="d1">
"hyperopt_config is an object that contains configuration settings for hyperparameter optimization."
"Hyperopt_config is a configuration file used to define the parameters for hyperparameter optimization."
"hyperopt_config is a configuration used for hyperparameter optimization."</data>
      <data key="d2">1db191f05801d40d5a346febd10d3352,75e530c1a04e30b373dc7cc68e3ad819,80033e741d8e10abdcfe20dd17192152,f70c7d3d89baaabbeaad57b58e379e08</data>
    </node>
    <node id="&quot;JSON.DUMP&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"json.dump is a Python function used to serialize a Python object and write it as a JSON-formatted stream to a file."</data>
      <data key="d2">1db191f05801d40d5a346febd10d3352</data>
    </node>
    <node id="&quot;OBJECTIVE&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"objective is a function that defines the objective to be optimized during hyperparameter optimization."
"objective is a function used to evaluate the performance of the machine learning model."
"Objective is a variable used to store the objective function for hyperparameter optimization, which is mentioned in the text."</data>
      <data key="d2">0982b8d1eb1e636b19fa2e9d9361e566,1db191f05801d40d5a346febd10d3352,f70c7d3d89baaabbeaad57b58e379e08</data>
    </node>
    <node id="&quot;RESEARCH&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"research is a function in ReservoirPy used to perform hyperparameter optimization and return the best configuration."</data>
      <data key="d2">1db191f05801d40d5a346febd10d3352</data>
    </node>
    <node id="&quot;FORCE LEARNING&quot;">
      <data key="d0">"METHOD"</data>
      <data key="d1">"FORCE learning is a method used for online training of an Echo State Network (ESN)."</data>
      <data key="d2">1db191f05801d40d5a346febd10d3352</data>
    </node>
    <node id="&quot;BACKPROPAGATION-DECORRELATION&quot;">
      <data key="d0">"METHOD"</data>
      <data key="d1">"Backpropagation-Decorrelation is a method used for training an Echo State Network (ESN) to minimize output error while preserving echo state properties."
"Backpropagation-Decorrelation is a technique used to adapt weights in the reservoir to minimize output error while preserving echo state properties."</data>
      <data key="d2">1db191f05801d40d5a346febd10d3352,9abdbd696e340cb5dd8c66ac5cd30c67</data>
    </node>
    <node id="&quot;RESERVOIR ADAPTATION&quot;">
      <data key="d0">"METHOD"</data>
      <data key="d1">"Reservoir Adaptation is a method used for training an Echo State Network (ESN) by modifying internal reservoir parameters based on performance metrics."
"Reservoir Adaptation is a technique used to modify internal reservoir parameters based on performance metrics."</data>
      <data key="d2">1db191f05801d40d5a346febd10d3352,9abdbd696e340cb5dd8c66ac5cd30c67</data>
    </node>
    <node id="&quot;TRAINING&quot;">
      <data key="d0" />
      <data key="d1">
"Training is the process of training the ESN network on a dataset, which happens offline and only once."
"Training is the process of fitting the ESN model to the input data and optimizing its parameters."
"Training is the process of training a neural network model, such as ESN, to learn patterns and make predictions."
"Training is the process of training the Echo State Network (ESN) model using a dataset, with the goal of learning patterns and making predictions."
"Training is the process of fitting the ESN node to the input data X and target data Y, optimizing the reservoir and readout weights."
"Training is mentioned in the text as a process that involves delivering targets to each readout using a dictionary."
"Training is the process of teaching the Ridge Readout to predict the next value in the Sine Wave sequence."
"Training is the process of fitting the ESN Model to input and output data, initializing nodes and training the Ridge readout."
"Training is the process of adjusting the reservoir's parameters to improve its performance in processing input data."</data>
      <data key="d2">1db191f05801d40d5a346febd10d3352,31ee481e47ac3a0b970199e72a0e0d31,324a8f3fb4d19b91457a99999e6d3d17,36e4df75a46fb977f9516f2d2f1f9bc2,59b469bdd618b3f36b3547f4f2b8a862,894d59d781535ca85389c4226715c007,8ade7819a5f8d1ec26e9bdbd059142e6,cc1fb6ca5695434ad0279c2606e928af,d0b9bbbd7257712eafd2eda5db1d0a8d,f2d5625f36aa4cb036089ce89ec607eb</data>
    </node>
    <node id="&quot;HYPER-PARAMETER EXPLORATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Hyper-parameter Exploration is the process of searching through a space of possible hyper-parameters to find the best combination for a given task."
"Hyper-parameter Exploration is an event where various parameters are explored to optimize a model's performance."</data>
      <data key="d2">136d135c710f6cf78a4c536d43276fe1,716940af834825642e01a3cb59a7e006</data>
    </node>
    <node id="&quot;REGRESSION TASK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Regression Task is a type of machine learning problem where the goal is to predict a continuous value based on input data."
"Regression Task is a machine learning task that involves predicting a continuous output variable based on input features."</data>
      <data key="d2">35631fbf2ad11c53d75cb9b42e2c39b4,716940af834825642e01a3cb59a7e006</data>
    </node>
    <node id="&quot;R^2 SCORE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"R^2 Score is a statistical measure that represents the proportion of the variance for a dependent variable that is explained by an independent variable or variables in a regression model."
"R^2 Score is a metric used to evaluate the performance of the Echo State Network, representing the proportion of the variance in the dependent variable that is predictable from the independent variable."</data>
      <data key="d2">716940af834825642e01a3cb59a7e006,f730c6800099724052a2d061f3cd8c2e</data>
    </node>
    <node id="&quot;LEARNING RATE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Learning Rate is a hyper-parameter in machine learning algorithms that determines how quickly the model learns from the training data."
"Learning Rate is a parameter in machine learning algorithms that determines how quickly the model updates its internal parameters based on the training data."
"Learning Rate is a parameter used in training machine learning models, which is being varied in the provided data."</data>
      <data key="d2">24b347e60cb01aea26f46f3067f5a0f0,716940af834825642e01a3cb59a7e006,8e1f4f13f617b982d272175296ec99d3</data>
    </node>
    <node id="&quot;RIDGE REGULARIZATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Ridge Regularization is a technique used to prevent overfitting in machine learning models by adding a penalty term to the loss function."
"Ridge Regularization is a technique mentioned in the text, used to avoid overfitting in the Ridge Readout."
"The Ridge Regularization is a fixed parameter with a log-uniform distribution between 1e-8 and 1e1."</data>
      <data key="d2">5d3baa9818a4e01fe1196c43378a2cea,65ba78d1f678e080bd930319c54234ef,716940af834825642e01a3cb59a7e006</data>
    </node>
    <node id="&quot;REGRESSION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Regression is a task that involves predicting a continuous value based on input data, modeling the relationship between input variables and a continuous output variable."
"Regression is a statistical technique used for approximating a function g, where g operates on the real numbers."</data>
      <data key="d2">1c462a6eef00aac37dc1ab33a689b930,9261efcc24379d9c0b2d35a2fde8275d</data>
    </node>
    <node id="&quot;CLASSIFICATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Classification is a task that involves assigning input data to one of several predefined categories or classes, predicting the category to which new data points belong."
"Classification is the process of categorizing data into distinct classes or categories."
"Classification is a method used in sequence-to-vector models to assign a single label to each input sequence."
"Classification is a problem where the codomain of a function g is a finite set, requiring the assignment of data points to specific categories."
"Classification is the process of assigning time series patterns to specific categories, such as identifying a word based on hand movements."</data>
      <data key="d2">1c462a6eef00aac37dc1ab33a689b930,423d3b5ec1acc9a4cb448a15d3b6b595,9261efcc24379d9c0b2d35a2fde8275d,a6f2502b5336ffc8606e1167b2813004,c05906c1f12c4edfc32a04aa9935067e</data>
    </node>
    <node id="&quot;REGRESSION MODELS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Regression Models are statistical methods used to predict a continuous variable, such as rainfall amount or sunlight intensity."</data>
      <data key="d2">d1617d5101da7c02e732e53860c49383</data>
    </node>
    <node id="&quot;LOGISTIC REGRESSION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Logistic Regression is a statistical method used to predict probabilities, often used as part of a classifier."</data>
      <data key="d2">d1617d5101da7c02e732e53860c49383</data>
    </node>
    <node id="&quot;SVM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"SVM (Support Vector Machine) is a true classification algorithm that predicts outcomes without providing probabilities."</data>
      <data key="d2">d1617d5101da7c02e732e53860c49383</data>
    </node>
    <node id="&quot;LINEAR PREDICTION COEFFICIENT (LPC)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Linear Prediction Coefficient (LPC) refers to the coefficients derived from the linear prediction model, used to predict the current sample of a signal based on its previous samples."</data>
      <data key="d2">d1617d5101da7c02e732e53860c49383</data>
    </node>
    <node id="&quot;LINEAR PREDICTIVE CODING (LPC)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Linear Predictive Coding (LPC) is a method that uses Linear Prediction Coefficients to represent the spectral envelope of a speech signal in a compressed form."
"LPC is a method used to represent the spectral envelope of a speech signal in a compressed form, using Linear Prediction Coefficients."</data>
      <data key="d2">c1ba6d7a4f4bd16c4fd25baf07c9747c,d1617d5101da7c02e732e53860c49383</data>
    </node>
    <node id="&quot;CEPSTRAL DOMAIN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The cepstral domain is a signal processing technique used to analyze signals, but its specific definition is not provided in the text."
"The cepstral domain is a representation of a signal that highlights periodic structures in the frequency domain, resulting from taking the inverse Fourier transform of the logarithm of the signal's spectrum."</data>
      <data key="d2">c1ba6d7a4f4bd16c4fd25baf07c9747c,d1617d5101da7c02e732e53860c49383</data>
    </node>
    <node id="&quot;CLASSIFICATION ALGORITHMS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">d1617d5101da7c02e732e53860c49383</data>
    </node>
    <node id="&quot;LINEAR PREDICTION COEFFICIENTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Linear Prediction Coefficients are the result of the Linear Predictive Coding process, used for signal representation and analysis."</data>
      <data key="d2">c1ba6d7a4f4bd16c4fd25baf07c9747c</data>
    </node>
    <node id="&quot;RESERVOIRPY LIBRARY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The ReservoirPy library is a collection of tools and functions used for data analysis and machine learning tasks, including the `japanese_vowels()` function."</data>
      <data key="d2">c1ba6d7a4f4bd16c4fd25baf07c9747c</data>
    </node>
    <node id="&quot;LINEAR PREDICTION COEFFICIENTS (LPCS)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Linear Prediction Coefficients (LPCs) are features used in the Japanese Vowels dataset, representing a method for analyzing speech signals."</data>
      <data key="d2">9f7337ee2d87543ced3b99dcae344b13</data>
    </node>
    <node id="&quot;SPEAKER IDENTIFIERS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Speaker identifiers are labels used in the Japanese Vowels dataset to classify spoken utterances to their respective speakers."</data>
      <data key="d2">9f7337ee2d87543ced3b99dcae344b13</data>
    </node>
    <node id="&quot;BOXPLOT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A boxplot is a graphical representation used to demonstrate the locality, spread, and skewness of numerical data through their quartiles."</data>
      <data key="d2">9f7337ee2d87543ced3b99dcae344b13</data>
    </node>
    <node id="&quot;RESERVOIRPY NODES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ReservoirPy Nodes are a software component used for processing sequences, such as audio data."</data>
      <data key="d2">a6f2502b5336ffc8606e1167b2813004</data>
    </node>
    <node id="&quot;SCIKITLEARNNODE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"ScikitLearnNode is a component in ReservoirPy that allows the integration of Scikit-Learn models into ReservoirPy workflows."
"ScikitLearnNode is a component in ReservoirPy that allows integration of Scikit-Learn models into ReservoirPy workflows."
"ScikitLearnNode is a tool used to implement machine learning models, allowing for the specification of model parameters using a dictionary."
"ScikitLearnNode is a technology used for offline readout nodes in machine learning, allowing for the training and prediction of models."
"ScikitLearnNode is a part of the ReservoirPy library, allowing the use of scikit-learn models as regular offline readout nodes."
"ScikitLearnNode is a component of the ReservoirPy library, used to integrate ScikitLearn models into a reservoir computing framework."
"ScikitLearnNode is a concept used in data analysis or modeling, likely referring to a node or component that interfaces with the scikit-learn library."
"ScikitLearnNode is a library that provides a simple interface to various machine learning models in scikit-learn."
"ScikitLearnNode is a node in a machine learning pipeline that wraps scikit-learn classifiers, allowing them to be used in the pipeline."
"ScikitLearnNode is a component used in the code to create a machine learning model for prediction."
"ScikitLearnNode is a library or module used for machine learning tasks, such as initializing nodes with specific models."</data>
      <data key="d2">2cba60e2f36479613bb0243a19f3a3b4,35631fbf2ad11c53d75cb9b42e2c39b4,84cacfea14ea9ff46a34150e77a0767a,861c28cb739722ddeb0babb7e1427409,8c66981c9d2009113219bbf2681f664c,b11a9f7777c0232bfa7323ae82ad139b,c05906c1f12c4edfc32a04aa9935067e,d58662ee42c14a0787d839ebfd0a6e9b,dc3bd3697a140b64d70e0e3ac6db6c7e,dddc79e4cd04d2d07e35930dd8458168,ee83abbbbc707d8131952b2b01ebc268</data>
    </node>
    <node id="&quot;LASSO REGRESSION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Lasso Regression is a method used for estimation and variable selection in high-dimensional contexts, compensating for the shortcomings of linear regression."
"LASSO Regression is a statistical method that builds on Linear Regression, with an advantage in carrying out variable selection, which is valuable in the presence of a large number of variables."
"Lasso Regression is a type of linear regression that uses shrinkage to reduce the complexity of the model and prevent overfitting."
"Lasso Regression is a machine learning algorithm used for regression tasks, which involves predicting a continuous output variable based on input variables."</data>
      <data key="d2">84cacfea14ea9ff46a34150e77a0767a,861c28cb739722ddeb0babb7e1427409,8c66981c9d2009113219bbf2681f664c,b11a9f7777c0232bfa7323ae82ad139b</data>
    </node>
    <node id="&quot;LINEAR_MODEL.LASSO&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"linear_model.Lasso is a specific machine learning model used for regression analysis, which is mentioned in the context of ScikitLearnNode."</data>
      <data key="d2">dddc79e4cd04d2d07e35930dd8458168</data>
    </node>
    <node id="&quot;STR()&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"str() is a built-in Python function used to convert a value into a string."</data>
      <data key="d2">dddc79e4cd04d2d07e35930dd8458168</data>
    </node>
    <node id="&quot;INSTANCE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"An instance refers to a single data point or observation in a dataset, which is used for training or prediction in machine learning models."
"An instance is a single data point in a dataset, consisting of a set of features or attributes that describe the data point."</data>
      <data key="d2">b130d3d59f0d3a2bb4feac9fdb85ed5b,dddc79e4cd04d2d07e35930dd8458168</data>
    </node>
    <node id="&quot;OUTPUT FEATURE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"An output feature is a measurable property or characteristic that a machine learning model is designed to predict."
"An output feature is a measurable property or characteristic that a machine learning model is designed to predict."</data>
      <data key="d2">b130d3d59f0d3a2bb4feac9fdb85ed5b,dddc79e4cd04d2d07e35930dd8458168</data>
    </node>
    <node id="&quot;REGRESSION METHODS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Regression Methods are statistical techniques used to model the relationship between a dependent variable and one or more independent variables."
"Regression Methods are statistical techniques used for predicting a continuous outcome variable based on one or more predictor variables."</data>
      <data key="d2">b130d3d59f0d3a2bb4feac9fdb85ed5b,dadca3c89b34dc48a60c53367ab55768</data>
    </node>
    <node id="&quot;MULTIPLE OUTPUT FEATURES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Multiple output features refer to the situation where a machine learning model is required to predict more than one dependent variable or target variable."</data>
      <data key="d2">b130d3d59f0d3a2bb4feac9fdb85ed5b</data>
    </node>
    <node id="&quot;CLASSIFICATION TASKS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Classification Tasks involve categorizing data into distinct classes or categories based on features or attributes."</data>
      <data key="d2">dadca3c89b34dc48a60c53367ab55768</data>
    </node>
    <node id="&quot;RIDGECLASSIFIER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"RidgeClassifier is a machine learning algorithm used for classification tasks, which applies a regularization term to the loss function to prevent overfitting."
"RidgeClassifier is a model from scikit-learn used for classification tasks."
"RidgeClassifier is a classifier implemented in the scikit-learn library, used for regression and classification tasks."
"RidgeClassifier is a model from Scikit-learn used for classification tasks."
"RidgeClassifier is a model from the ScikitLearn library used for classification tasks."
"RidgeClassifier is a machine learning algorithm used for classification tasks, provided by the Scikit-learn library."</data>
      <data key="d2">0036fb6f489e13c0db0f1c02bf3323be,2cba60e2f36479613bb0243a19f3a3b4,35631fbf2ad11c53d75cb9b42e2c39b4,d58662ee42c14a0787d839ebfd0a6e9b,dadca3c89b34dc48a60c53367ab55768,f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </node>
    <node id="&quot;LOGISTICREGRESSION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"LogisticRegression is a machine learning algorithm used for classification tasks, which models the probability of a data point belonging to a particular class."
"LogisticRegression is a model from scikit-learn used for classification tasks."
"LogisticRegression is a classifier implemented in the scikit-learn library, used for binary and multiclass classification tasks."
"LogisticRegression is a model from Scikit-learn used for classification tasks."
"LogisticRegression is a model from the ScikitLearn library used for classification tasks."
"LogisticRegression is a machine learning algorithm used for classification tasks, provided by the Scikit-learn library."</data>
      <data key="d2">0036fb6f489e13c0db0f1c02bf3323be,2cba60e2f36479613bb0243a19f3a3b4,35631fbf2ad11c53d75cb9b42e2c39b4,d58662ee42c14a0787d839ebfd0a6e9b,dadca3c89b34dc48a60c53367ab55768,f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </node>
    <node id="&quot;OUTLIER DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Outlier Data are data points that significantly differ from other observations and can have a significant impact on the decision boundary in regression tasks."</data>
      <data key="d2">dadca3c89b34dc48a60c53367ab55768</data>
    </node>
    <node id="&quot;DECISION BOUNDARY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Decision Boundary is the line or surface that separates different classes or categories in a classification task."</data>
      <data key="d2">dadca3c89b34dc48a60c53367ab55768</data>
    </node>
    <node id="&quot;JAPANESE_VOWELS() FUNCTION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"japanese_vowels() function is a function used for data processing, specifically for handling Japanese vowel data."</data>
      <data key="d2">dadca3c89b34dc48a60c53367ab55768</data>
    </node>
    <node id="&quot;REPEAT_TARGETS PARAMETER&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"repeat_targets parameter is a parameter used in the japanese_vowels() function to ensure that one label is obtained per timestep, not one label per utterance."</data>
      <data key="d2">dadca3c89b34dc48a60c53367ab55768</data>
    </node>
    <node id="&quot;ARGMAX() FUNCTION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"argmax() function is a function used to find the indices of the maximum values along an axis in a given array."</data>
      <data key="d2">dadca3c89b34dc48a60c53367ab55768</data>
    </node>
    <node id="&quot;JAPANESE_VOWELS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"japanese_vowels is a function used to obtain data for machine learning tasks."
"japanese_vowels is a dataset used for training and testing machine learning models."
"japanese_vowels is a dataset mentioned in the text, but its specific characteristics or purpose are not clearly defined."</data>
      <data key="d2">2cba60e2f36479613bb0243a19f3a3b4,9414efd266e7135a2cdd7461a888b045,b3361508c3e49b5bb3089f10e31d2c81</data>
    </node>
    <node id="&quot;NP.ARGMAX&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"np.argmax is a function from the NumPy library used to find the indices of the maximum values along an axis."</data>
      <data key="d2">b3361508c3e49b5bb3089f10e31d2c81</data>
    </node>
    <node id="&quot;KEEPDIMS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"keepdims is a parameter used to maintain the dimensions of the original array after performing the argmax operation."</data>
      <data key="d2">b3361508c3e49b5bb3089f10e31d2c81</data>
    </node>
    <node id="&quot;RANDOM SEARCH&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Random search is a method used for hyperparameter tuning in machine learning, sampling more efficiently than grid search."
"Random Search is a method for hyperparameter exploration that involves sampling more efficiently and not wasting evaluations on dimensions that do not significantly impact performance."
"Random Search is a method used by Hyperopt to choose different sets of parameters for exploration."</data>
      <data key="d2">4a9f33fa18891b67267b7615d61caaac,82ff270b1bbdfe0ee11e603de1e326c7,b3361508c3e49b5bb3089f10e31d2c81</data>
    </node>
    <node id="&quot;ECHO STATE PROPERTY (ESP)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Echo State Property (ESP) is a theoretical condition in reservoir computing that the spectral radius should be less than 1 to ensure a contracting system without inputs."
"Echo State Property (ESP) is a theoretical condition stating that the spectral radius should be less than 1 to ensure a contracting system without inputs. However, in practice, with non-linear reservoirs, the optimal spectral radius can be greater than 1."</data>
      <data key="d2">4a9f33fa18891b67267b7615d61caaac,b3361508c3e49b5bb3089f10e31d2c81</data>
    </node>
    <node id="&quot;HYPERPARAMETERS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Hyperparameters are configuration settings that are used during the training process of a machine learning model, and their values can significantly impact the model's performance."
"Hyperparameters are settings that can be adjusted to optimize the performance of a reservoir computing network, and ReservoirPy includes tools for exploring these."
"Hyperparameters are configuration settings that are not learned from data, but are set manually or through a hyperparameter optimization process."
"Hyperparameters are parameters that are set before the learning process begins, and their values can significantly impact the performance of the model."</data>
      <data key="d2">2d8ea1123f365fb047b024022ba4fdc4,4a9f33fa18891b67267b7615d61caaac,73e81fd6509a2ba400a8435793ade3c5,d1047d9e322054de394b880adaf6b536</data>
    </node>
    <node id="&quot;SPECTRAL RADIUS (SR)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Spectral Radius (SR) is a key hyperparameter in Reservoir Computing that refers to the maximum eigenvalue of the reservoir matrix."</data>
      <data key="d2">4a9f33fa18891b67267b7615d61caaac</data>
    </node>
    <node id="&quot;INPUT SCALING (IS)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input Scaling (IS) is a key hyperparameter in Reservoir Computing that refers to the scaling of input signals to the reservoir."</data>
      <data key="d2">4a9f33fa18891b67267b7615d61caaac</data>
    </node>
    <node id="&quot;LEAKING RATE (LR)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Leaking Rate (LR) is a key hyperparameter in Reservoir Computing that refers to the rate at which the reservoir state decays over time."</data>
      <data key="d2">4a9f33fa18891b67267b7615d61caaac</data>
    </node>
    <node id="&quot;FEEDBACK SCALING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Feedback Scaling is a hyperparameter in Reservoir Computing that refers to the scaling of feedback signals from readout units to the reservoir."</data>
      <data key="d2">4a9f33fa18891b67267b7615d61caaac</data>
    </node>
    <node id="&quot;HYPERPARAMETER EXPLORATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Hyperparameter Exploration is the process of finding the best hyperparameters for a machine learning model."</data>
      <data key="d2">1315547792fcb5d618913a4c7ac03511</data>
    </node>
    <node id="&quot;MACKEY-GLASS TIME SERIES PREDICTION&quot;">
      <data key="d0">"TASK"</data>
      <data key="d1">"Mackey-Glass Time Series Prediction is a task used to illustrate the proposed hyperparameter search method."</data>
      <data key="d2">1315547792fcb5d618913a4c7ac03511</data>
    </node>
    <node id="&quot;LORENZ TIME SERIES PREDICTION&quot;">
      <data key="d0">"TASK"</data>
      <data key="d1">"Lorenz Time Series Prediction is a task used to illustrate the proposed hyperparameter search method."</data>
      <data key="d2">1315547792fcb5d618913a4c7ac03511</data>
    </node>
    <node id="&quot;HYPERPARAMETER INTERDEPENDENCY PLOTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Hyperparameter Interdependency Plots are visual tools used to evaluate the loss as a function of all explored hyperparameters and their interactions."
"Hyperparameter Interdependency Plots are a visualization technique used to evaluate the loss as a function of all explored hyperparameters and their interactions."</data>
      <data key="d2">1315547792fcb5d618913a4c7ac03511,4ea4de00090795130d7ae12a57a729ec</data>
    </node>
    <node id="&quot;HYPERPARAMETER SEARCH METHOD&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Hyperparameter Search Method is a technique used to optimize the performance of machine learning models by finding the best combination of hyperparameters."</data>
      <data key="d2">4ea4de00090795130d7ae12a57a729ec</data>
    </node>
    <node id="&quot;MACKEY-GLASS TIME SERIES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Mackey-Glass Time Series is a type of chaotic time series used for prediction tasks."
"Mackey-Glass Time Series is a synthetic dataset used for testing and evaluating time series forecasting models."
"The Mackey-Glass Time Series is a non-linear, non-periodic, and non-random data set that resembles ECG rhythms, stocks, and weather."
"Mackey-Glass Time Series is a dataset used for demonstrating data preprocessing and analysis techniques, as seen in the provided code."
"Mackey-Glass Time Series is a chaotic time series dataset commonly used for testing and comparing the forecasting abilities of different models."</data>
      <data key="d2">4a7ca13b3f869961817e2aa723e67d24,4ea4de00090795130d7ae12a57a729ec,b2beacacc8c190393e4583a69518378c,eebc9d7d2b66e3898b7d068c38fd200f,fac681bdc38ae5829173c747ee6240fa</data>
    </node>
    <node id="&quot;LORENZ TIME SERIES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Lorenz Time Series is a type of chaotic time series used for prediction tasks."</data>
      <data key="d2">4ea4de00090795130d7ae12a57a729ec</data>
    </node>
    <node id="&quot;TEST DOCUMENT RECEPTION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Test Document Reception is a test to see if the recipients receive the documents."</data>
      <data key="d2">4ea4de00090795130d7ae12a57a729ec</data>
    </node>
    <node id="&quot;SCIKITLEARN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ScikitLearn is a machine learning library in Python that implements various methods for classification and regression."
"ScikitLearn is an organization that provides a machine learning library, including the Lasso Regression model used in the code."</data>
      <data key="d2">8c66981c9d2009113219bbf2681f664c,b11a9f7777c0232bfa7323ae82ad139b</data>
    </node>
    <node id="&quot;MACKEY-GLASS TASK&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Mackey-Glass task is a benchmark problem in time series prediction, used to test the performance of the created model."
"The Mackey-Glass task is a time series prediction problem used to test the model."</data>
      <data key="d2">b11a9f7777c0232bfa7323ae82ad139b,b3c8de6f33c2ebb84f0d2797933d0cad</data>
    </node>
    <node id="&quot;THE MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The model is a reservoir model created using ReservoirPy, which is trained on the Mackey-Glass task."</data>
      <data key="d2">b3c8de6f33c2ebb84f0d2797933d0cad</data>
    </node>
    <node id="&quot;THE AUTHOR&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"The author is the individual who creates the model and evaluates its performance."</data>
      <data key="d2">b3c8de6f33c2ebb84f0d2797933d0cad</data>
    </node>
    <node id="&quot;CASTING(MG, FORECAST=10, TEST_SIZE=0.2)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"casting(mg, forecast=10, test_size=0.2) is a data analysis or modeling process, likely involving a team or organization."</data>
      <data key="d2">ee83abbbbc707d8131952b2b01ebc268</data>
    </node>
    <node id="&quot;ESN PREDICTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"ESN prediction is a method used in data analysis or modeling, likely a part of the casting(mg, forecast=10, test_size=0.2) process."</data>
      <data key="d2">ee83abbbbc707d8131952b2b01ebc268</data>
    </node>
    <node id="&quot;TRUE VALUE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"True value is the actual or observed value in data analysis or modeling, compared to the ESN prediction."</data>
      <data key="d2">ee83abbbbc707d8131952b2b01ebc268</data>
    </node>
    <node id="&quot;LINEAR_MODEL.PASSIVEAGGRESSIVEREGRESSOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"linear_model.PassiveAggressiveRegressor is a machine learning model used in data analysis or modeling, likely a part of the ScikitLearnNode concept."</data>
      <data key="d2">ee83abbbbc707d8131952b2b01ebc268</data>
    </node>
    <node id="&quot;RSQUARE(Y_TEST, Y_PRED)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"rsquare(y_test, y_pred) is a statistical measure used in data analysis or modeling to evaluate the goodness of fit of a model."</data>
      <data key="d2">ee83abbbbc707d8131952b2b01ebc268</data>
    </node>
    <node id="&quot;NRMSE(Y_TEST, Y_PRED)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"nrmse(y_test, y_pred) is a statistical measure used in data analysis or modeling to evaluate the accuracy of a model."</data>
      <data key="d2">ee83abbbbc707d8131952b2b01ebc268</data>
    </node>
    <node id="&quot;PASSIVEAGGRESSIVEREGRESSOR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"PassiveAggressiveRegressor is a model from scikit-learn used for regression tasks."
"PassiveAggressiveRegressor is a regression model from scikit-learn, used for online learning."
"PassiveAggressiveRegressor is a model from the ScikitLearn library used for regression tasks."</data>
      <data key="d2">2cba60e2f36479613bb0243a19f3a3b4,35631fbf2ad11c53d75cb9b42e2c39b4,52d001cd1786e3d9f36e0c57538bc21e</data>
    </node>
    <node id="&quot;NP.CONCATENATE&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"np.concatenate is a function used to combine arrays along a specified axis."</data>
      <data key="d2">00648b24263129fdae8652f1a3339041</data>
    </node>
    <node id="&quot;NP.FLOAT64&quot;">
      <data key="d0">"DATA TYPE"</data>
      <data key="d1">"np.float64 is a data type used to represent 64-bit floating-point numbers."</data>
      <data key="d2">00648b24263129fdae8652f1a3339041</data>
    </node>
    <node id="&quot;PYTHON&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Python is the programming language used for ReservoirPy, and it is based on Python scientific modules."
"Python is a programming language used for data analysis and machine learning, including the implementation of Reservoir Computing and Ridge Regression models."
"Python is a programming language commonly used in Machine Learning and data analysis due to its simplicity, readability, and extensive libraries."
"Python is the programming language used to create ReservoirPy."
"Python is a programming language used for building the ReservoirPy library and for data analysis."</data>
      <data key="d2">41fa16855df7da666dc6fc38d2f8ee53,4d87a0d12ce76c7a493a24e1c4b06a83,6de297d888d10db4c987b5eafc6398b2,74c073137c970e32982756d008532cb8,ef9bf350e25daa8f123b0b5c4d60de5f</data>
    </node>
    <node id="&quot;PYTHON 3.8&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Python 3.8 is a version of Python that ReservoirPy supports."</data>
      <data key="d2">d1047d9e322054de394b880adaf6b536</data>
    </node>
    <node id="&quot;@RESERVOIRPY&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"@reservoirpy is a Twitter handle that shares updates and new releases about ReservoirPy."</data>
      <data key="d2">d1047d9e322054de394b880adaf6b536</data>
    </node>
    <node id="&quot;OFFICIAL DOCUMENTATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Official Documentation is a resource provided by ReservoirPy to learn more about its features, API, and installation process."</data>
      <data key="d2">d1047d9e322054de394b880adaf6b536</data>
    </node>
    <node id="&quot;USER GUIDE&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"User Guide is a resource provided by ReservoirPy that offers tutorials for learning how to use its features."
"User Guide is a resource that provides tutorials and instructions for using ReservoirPy."</data>
      <data key="d2">a2b183778107462d474c53e4ec0a9221,d1047d9e322054de394b880adaf6b536</data>
    </node>
    <node id="&quot;DEEP RESERVOIR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Deep Reservoir is a type of architecture that ReservoirPy supports, consisting of multiple reservoirs, readouts, and complex feedback loops."</data>
      <data key="d2">d1047d9e322054de394b880adaf6b536</data>
    </node>
    <node id="&quot;MACKEY-GLASS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Mackey-Glass is a chaotic timeseries used as an example in the ReservoirPy documentation for predicting chaotic behavior."
"Mackey-Glass is a chaotic system used as an example for timeseries prediction."
"Mackey-Glass is a time series data set used for testing and comparing the performance of different models."</data>
      <data key="d2">0b6c69085074b2cf23267eb149068b9f,90c1a399dd410f75f1f4bb03fe1f5f33,d1047d9e322054de394b880adaf6b536</data>
    </node>
    <node id="&quot;DEEP-ESN ARCHITECTURE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Deep-ESN Architecture is a type of Echo State Network that uses multiple layers to improve prediction accuracy."</data>
      <data key="d2">a2b183778107462d474c53e4ec0a9221</data>
    </node>
    <node id="&quot;PYTHON NOTEBOOKS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Python Notebooks are a web-based interactive computing platform used for data analysis and visualization."</data>
      <data key="d2">0b6c69085074b2cf23267eb149068b9f</data>
    </node>
    <node id="&quot;PIP&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Pip is a package manager for Python, used to install and manage Python packages and libraries."
"pip is a package installer for Python, used to install ReservoirPy and its dependencies."</data>
      <data key="d2">0b6c69085074b2cf23267eb149068b9f,c5413fef3b2d7e4d688c66e6046b56c7</data>
    </node>
    <node id="&quot;TIMESERIES PREDICTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Timeseries Prediction is the process of forecasting future values based on past observations, using techniques such as reservoir computing."</data>
      <data key="d2">0b6c69085074b2cf23267eb149068b9f</data>
    </node>
    <node id="&quot;REQUIREMENTS FILE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The requirements file lists the packages needed for running Python Notebooks in the tutorials folder."</data>
      <data key="d2">c5413fef3b2d7e4d688c66e6046b56c7</data>
    </node>
    <node id="&quot;TUTORIAL FOLDER&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Tutorial folder contains tutorials in Jupyter Notebooks, demonstrating the use of ReservoirPy."</data>
      <data key="d2">c5413fef3b2d7e4d688c66e6046b56c7</data>
    </node>
    <node id="&quot;EXAMPLES FOLDER&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Examples folder contains examples and papers with codes, also in Jupyter Notebooks, showcasing the applications of ReservoirPy."
"The examples folder contains examples of complex use cases from the literature, which can be used to learn more about the capabilities of ReservoirPy and to see how it can be used to solve complex problems."</data>
      <data key="d2">c5413fef3b2d7e4d688c66e6046b56c7,ead6383a44acd8ebd17907b85a910455</data>
    </node>
    <node id="&quot;HYPERPACKAGE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Hyperpackage is an optional feature in ReservoirPy that enables the use of Hyperopt for hyperparameter optimization."</data>
      <data key="d2">c5413fef3b2d7e4d688c66e6046b56c7</data>
    </node>
    <node id="&quot;TROUVAIN ET AL. (2020)&quot;">
      <data key="d0">"PUBLICATION"</data>
      <data key="d1">"Trouvain et al. (2020) is a paper that provides a tutorial for ReservoirPy (v0.2)."</data>
      <data key="d2">c5413fef3b2d7e4d688c66e6046b56c7</data>
    </node>
    <node id="&quot;HINAUT ET AL. (2021)&quot;">
      <data key="d0">"PUBLICATION"</data>
      <data key="d1">"Hinaut et al. (2021) is a paper that offers advice and a method for exploring hyperparameters for reservoirs."</data>
      <data key="d2">c5413fef3b2d7e4d688c66e6046b56c7</data>
    </node>
    <node id="&quot;TROUVAIN ET AL.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Trouvain et al. are authors of a tutorial on exploring hyperparameters with ReservoirPy and Hyperopt."</data>
      <data key="d2">280cbdf53022bbaed48ccb34ebe142bc</data>
    </node>
    <node id="&quot;HINAUT ET AL.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Hinaut et al. are authors of a paper providing advice and a method for exploring hyperparameters for reservoirs."</data>
      <data key="d2">280cbdf53022bbaed48ccb34ebe142bc</data>
    </node>
    <node id="&quot;LEGER ET AL.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Leger et al. are authors of a paper on evolving reservoirs for meta reinforcement learning."</data>
      <data key="d2">280cbdf53022bbaed48ccb34ebe142bc</data>
    </node>
    <node id="&quot;CHAIX-EICHEL ET AL.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Chaix-Eichel et al. are authors of a paper on implicit learning and explicit representations."</data>
      <data key="d2">280cbdf53022bbaed48ccb34ebe142bc</data>
    </node>
    <node id="&quot;TROUVAIN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Trouvain is an author mentioned in the tutorial on exploring hyperparameters with ReservoirPy and Hyperopt."</data>
      <data key="d2">280cbdf53022bbaed48ccb34ebe142bc</data>
    </node>
    <node id="&quot;HINAUT&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Hinaut is an author mentioned in the paper providing advice and a method for exploring hyperparameters for reservoirs."</data>
      <data key="d2">280cbdf53022bbaed48ccb34ebe142bc</data>
    </node>
    <node id="&quot;PAGLIARINI ET AL.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Pagliarini et al. are authors of papers on canary vocal sensorimotor models with RNN decoders and low-dimensional GAN generators."
"Pagliarini et al. are the authors of papers on Canary Vocal Sensorimotor Model with RNN Decoder and Low-dimensional GAN Generator, and What does the Canary Say? Low-Dimensional GAN Applied to Birdsong."</data>
      <data key="d2">20b16c2e1cb8813ade96fea5f9591631,280cbdf53022bbaed48ccb34ebe142bc</data>
    </node>
    <node id="&quot;TROUVAIN &amp; HINAUT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Trouvain &amp; Hinaut are the authors of a paper on Canary Song Decoder, Transduction, and Implicit Segmentation with ESNs and LTSMs."</data>
      <data key="d2">20b16c2e1cb8813ade96fea5f9591631</data>
    </node>
    <node id="&quot;ICDL 2021&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"ICDL 2021 is an event mentioned in the text, where a paper on Canary Vocal Sensorimotor Model with RNN Decoder and Low-dimensional GAN Generator was presented."</data>
      <data key="d2">20b16c2e1cb8813ade96fea5f9591631</data>
    </node>
    <node id="&quot;HAL PREPRINT&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">20b16c2e1cb8813ade96fea5f9591631</data>
    </node>
    <node id="&quot;MNEMOSYNE GROUP&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Mnemosyne group is a part of Inria that supports the development of ReservoirPy."
"Mnemosyne group is a part of Inria that supports the development of ReservoirPy."</data>
      <data key="d2">296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba</data>
    </node>
    <node id="&quot;BORDEAUX&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"Bordeaux is a city in France where Inria and the Mnemosyne group are located."
"Bordeaux is the location where Inria supports the development of ReservoirPy."</data>
      <data key="d2">296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba</data>
    </node>
    <node id="&quot;FRANCE&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"France is a country where Inria and the Mnemosyne group are located."</data>
      <data key="d2">2a197220a94bac0b44fc0b07712e45ba</data>
    </node>
    <node id="&quot;ICANN 2020&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"ICANN 2020 is an international conference where ReservoirPy was presented."
"ICANN 2020 is the conference where the ReservoirPy package was presented."</data>
      <data key="d2">296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba</data>
    </node>
    <node id="&quot;NATHAN TROUVAIN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Nathan Trouvain is an author of ReservoirPy and a contributor to the library."
"Nathan Trouvain is a developer of ReservoirPy, a library for designing Echo State Networks."
"Nathan Trouvain is a contributor to reservoirpy, working at Inria Bordeaux Sud-Ouest, IMN, LaBRI."
"Nathan Trouvain is an author of a research paper on canary song decoder: transduction and implicit segmentation with esns and ltsms."</data>
      <data key="d2">18910a60b2547ec3133340f42c45bb47,296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba,7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;LUCA PEDRELLI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Luca Pedrelli is an author of ReservoirPy and a contributor to the library."
"Luca Pedrelli is a developer of ReservoirPy, a library for designing Echo State Networks."</data>
      <data key="d2">296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba</data>
    </node>
    <node id="&quot;THANH TRUNG DINH&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Thanh Trung Dinh is an author of ReservoirPy and a contributor to the library."
"Thanh Trung Dinh is a developer of ReservoirPy, a library for designing Echo State Networks."</data>
      <data key="d2">296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba</data>
    </node>
    <node id="&quot;MACKEY-GLASS TIMES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Mackey-Glass Times are a time series data set commonly used for testing and benchmarking time series prediction methods."</data>
      <data key="d2">73e81fd6509a2ba400a8435793ade3c5</data>
    </node>
    <node id="&quot;AUTHOR&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"The author is not explicitly mentioned, but they present a basic example of optimizing hyperparameters using Hyperopt and ReservoirPy.hyper tools."
"The author is the person who wrote the text, discussing the use of Hyperopt and other concepts."
"The Author is the individual who wrote the code, which includes the development and use of the ES2N model."
"The author is the individual who wrote the code and performed the analysis, using the ESN model for time series prediction and forecasting."
"The author is the individual who developed the Reservoir Computing method, as indicated by the code and comments."
"The author is not explicitly mentioned, but they are the source of the text and the one performing the analysis."
"The author is the individual who wrote the text, demonstrating expertise in machine learning and data visualization."
"The author is the individual who is using ReservoirPy to build and train Echo State Networks for time series prediction and data analysis."
"The author is the person who wrote the code and is performing the ESN test, but their name is not explicitly mentioned."
"The author is the individual who wrote the text, demonstrating expertise in machine learning and time series analysis."
"The author is the person who wrote the code, using ReservoirPy to create and work with reservoir computing models."
"The author is the individual who wrote the text, not explicitly mentioned."
"The author discusses the Echo State Network (ESN) and its applications, comparing it to other neural network architectures and learning algorithms."</data>
      <data key="d2">069ae9388dfd52fec9c184c7168f64dd,11749b7d0fdadf05ea29da6025618407,2386633041e820b604fc4457264b5a33,29aad23ce67e778ac31d4fb287fd20c7,41fa16855df7da666dc6fc38d2f8ee53,684b1edf65b327cc06ceb69ca1279d74,73e81fd6509a2ba400a8435793ade3c5,76963fa19a9caab847e50167f71c86a2,854761a5b5b5b90af10bc6b6c76cc355,973d44d321c7ceee7add295c60b085d2,a4b801e70cf2ba3a3101d34899450087,e7d249cdab85dc69b631d43ac6b62915,fd81bdceb3e2b91ac2605a3d201d1eb4</data>
    </node>
    <node id="&quot;MATPLOTLIB.PYPLOT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"matplotlib.pyplot is a library used for creating visualizations in Python."</data>
      <data key="d2">94fd1ebf256db17e4ac2255b89caa473</data>
    </node>
    <node id="&quot;ECHO STATE PROPERTY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Echo State Property is a theoretical assumption that a spectral radius close to 1 allows the reservoir states to be less affected by their initial conditions, while having good memorization properties."
"Echo State Property is a property of a reservoir, which is supposed to allow the reservoir states to be less affected by their initial conditions while having good memorization properties."
"The Echo State Property is a concept that relates to the asymptotic properties of the excited reservoir dynamics and the driving signal."
"Echo State Property is a notion of stability that imposes an asymptotic fading of the memory of the input in Echo State Networks."</data>
      <data key="d2">1f30b86a46d4819603edc730df816c49,578045eb341c5e05d5a912f634854499,82f7e4647b9da5d5063fe92613f4fbcb,a9f53979e9dbe6b936ff3374c73006dd</data>
    </node>
    <node id="&quot;INPUT SCALING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input Scaling is a coefficient applied on W_(in) and adding a gain to the inputs of a reservoir, affecting its dynamics."
"Input Scaling is a coefficient applied on W_(in) and adding a gain to the inputs of the reservoir, influencing the reservoir's behavior."
"Input Scaling is a parameter used in Reservoir Computing to adjust the influence of each variable in a multivariate time-series."
"Input Scaling is a parameter in Reservoir Computing that adjusts the influence of each variable in a multivariate time-series."
"Input Scaling is a parameter mentioned in the text, which is fixed."
"Input Scaling is a technique used to normalize the input data to a specific range, often between -1 and 1, to improve the performance of machine learning models."
"Input Scaling is a parameter in the ES2N class, likely influencing the input data processing."
"Input Scaling is a parameter mentioned in the text, likely used in the configuration of the ES2N system."
"Input Scaling is a parameter used to control the magnitude of the input data in the Echo State Network (ESN) model."
"Input scaling is a process or technique used in the text, likely in the context of adjusting the magnitude or intensity of external inputs to the Reservoir system."
"Input Scaling is a technique used in neural networks to adjust the input data to improve the network's performance."
"Input Scaling is a parameter in Reservoir Computing that adjusts the strength of the input signal to the reservoir."
"The Input Scaling is a fixed parameter with a value of 1.0."</data>
      <data key="d2">0a9b132ecb1c4b63fdbb0e144295362e,1365a36c76afc697ac626fd0f784804a,1f30b86a46d4819603edc730df816c49,26d78bc91458f47d4053954505c45f92,65ba78d1f678e080bd930319c54234ef,82f7e4647b9da5d5063fe92613f4fbcb,8553a88d9aaf4f71d359c721a1f6fa70,8e1f4f13f617b982d272175296ec99d3,8ecf03267c90a64376f5040307d98195,9dd8e12c7acbe10cf34817ff14780d24,b957e1bf5bf175c7630222ca742c7933,bba680a0a7dd439bd5b0fe1547ffe040,cedd895dd1ddc09e70c8799effdf7427</data>
    </node>
    <node id="&quot;STABLE DYNAMICS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Stable Dynamics refers to a state where the reservoir's dynamics are predictable and consistent over time."</data>
      <data key="d2">1f30b86a46d4819603edc730df816c49</data>
    </node>
    <node id="&quot;CHAOTIC DYNAMICS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Chaotic Dynamics refers to a state where the reservoir's dynamics are unpredictable and highly sensitive to initial conditions."</data>
      <data key="d2">1f30b86a46d4819603edc730df816c49</data>
    </node>
    <node id="&quot;TIME-SERIES DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Time-series Data is a sequence of data points collected at regular time intervals, which is being processed using Reservoir Computing."
"Time-series Data refers to a sequence of data points collected at regular time intervals."
"Time-series Data is a sequence of data points collected at regular time intervals, which is being processed using Reservoir Computing."</data>
      <data key="d2">26d78bc91458f47d4053954505c45f92,8553a88d9aaf4f71d359c721a1f6fa70,8ecf03267c90a64376f5040307d98195</data>
    </node>
    <node id="&quot;MULTIVARIATE TIME-SERIES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Multivariate Time-series is a type of time-series data that consists of multiple variables or dimensions, each of which may have a different influence on the reservoir state."</data>
      <data key="d2">8553a88d9aaf4f71d359c721a1f6fa70</data>
    </node>
    <node id="&quot;RC CONNECTIVITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"RC Connectivity is a parameter in Reservoir Computing that determines the sparsity of the connections between the reservoir units."
"RC Connectivity is a term used in the context of a neural network, likely referring to the connectivity between reservoir and readout components."</data>
      <data key="d2">8ecf03267c90a64376f5040307d98195,b957e1bf5bf175c7630222ca742c7933</data>
    </node>
    <node id="&quot;INPUT CONNECTIVITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input Connectivity is a parameter in Reservoir Computing that determines the sparsity of the connections between the input variables and the reservoir units."
"Input Connectivity is a parameter used to control the connection between the input data and the reservoir in the Echo State Network (ESN) model."
"Input Connectivity is a parameter that determines the density of connections between input nodes and reservoir nodes."
"Input Connectivity is a term used in the context of a neural network, likely referring to the connections between input and reservoir components."
"Input Connectivity is a parameter in Reservoir Computing that determines the connection strength between the input signal and the reservoir neurons."</data>
      <data key="d2">1365a36c76afc697ac626fd0f784804a,26d78bc91458f47d4053954505c45f92,72e6eee633bcb5b1458c4cee3975cee1,8ecf03267c90a64376f5040307d98195,b957e1bf5bf175c7630222ca742c7933</data>
    </node>
    <node id="&quot;OPTIMIZATION TOOLS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Optimization Tools are used to find the best values for hyperparameters, improving the performance of the model."</data>
      <data key="d2">2d8ea1123f365fb047b024022ba4fdc4</data>
    </node>
    <node id="&quot;OPTIMIZATION ALGORITHM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Optimization Algorithm is a method used to find the best parameters for a given task, such as forecasting the Double Scroll Attractor."</data>
      <data key="d2">91704ce63f9ba41247fdc452a7a62ba6</data>
    </node>
    <node id="&quot;R&#178;&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"R&#178; is a metric used to measure the proportion of the variance in the dependent variable that is predictable from the independent variable(s)."
"R&#178; is a statistical metric used to measure the proportion of the variance in the dependent variable that is predictable from the independent variable(s), mentioned in the text."</data>
      <data key="d2">11749b7d0fdadf05ea29da6025618407,251a50c2ae8ceea4fd7da1127cc5f461</data>
    </node>
    <node id="&quot;LOSS FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Loss Function is a metric used to evaluate the performance of a model, mentioned in the text."</data>
      <data key="d2">11749b7d0fdadf05ea29da6025618407</data>
    </node>
    <node id="&quot;R2&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"R-squared (R2) is a metric used to evaluate the performance of the machine learning model, representing the proportion of the variance in the target variable that is predictable from the input variables."</data>
      <data key="d2">75e530c1a04e30b373dc7cc68e3ad819</data>
    </node>
    <node id="&quot;HYPEROPT-MULTISCROLL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"hyperopt-multiscroll is an experimentation name, likely a project or initiative."</data>
      <data key="d2">adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </node>
    <node id="&quot;HP_MAX_EVALS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"hp_max_evals is a parameter that specifies the number of different sets of parameters hyperopt has to try."</data>
      <data key="d2">adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </node>
    <node id="&quot;HP_METHOD&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"hp_method is a parameter that specifies the method used by hyperopt to choose sets of parameters."</data>
      <data key="d2">adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </node>
    <node id="&quot;INSTANCES_PER_TRIAL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"instances_per_trial is a parameter that specifies how many random ESN will be tried with each set of parameters."</data>
      <data key="d2">adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </node>
    <node id="&quot;REGULARIZATION PARAMETER&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Regularization Parameter is a parameter mentioned in the text, which is fixed."</data>
      <data key="d2">0a9b132ecb1c4b63fdbb0e144295362e</data>
    </node>
    <node id="&quot;RANDOM SEED&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Random Seed is a parameter mentioned in the text, which is used for the ESN initialization."
"The Random Seed is a parameter used for the ESN initialization, with a value of 1234."</data>
      <data key="d2">0a9b132ecb1c4b63fdbb0e144295362e,65ba78d1f678e080bd930319c54234ef</data>
    </node>
    <node id="&quot;ICANN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ICANN is mentioned as the conference where a guide on exploring hyper-parameters for Echo State Networks was presented."</data>
      <data key="d2">088d2280349d652200861994c09d7dd5</data>
    </node>
    <node id="&quot;Y&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Y is a variable or feature used in the time series forecasting process, often the target variable to be predicted."
"Y is a NumPy array of shape (test_timesteps, 20*33) that contains the output data for the dataset."
"Y is a NumPy array of shape (test_timesteps, 20*33), likely a data structure or component within the system."
"Y is a variable used in the provided code, which could represent a data structure or a mathematical object."
"y is the target data used for training and prediction in the ESN."
"Y is a variable used to represent output or target data in the context of data analysis and machine learning."</data>
      <data key="d2">09198e939639c229c2c97555f65b12a7,7b8e1f350eefb392053be12f35fe7daf,abd3ca2db22004a5de548dc22010c4d4,e396354e3a9be76616392af11f56e671,f3e58b69b1a93175e3094a2ba65c0429,f70c7d3d89baaabbeaad57b58e379e08</data>
    </node>
    <node id="&quot;TRAIN_LEN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"train_len is a parameter used to determine the length of the training data."
"Train_len is a variable used to set the length of the training data, which is mentioned in the text."</data>
      <data key="d2">0982b8d1eb1e636b19fa2e9d9361e566,f70c7d3d89baaabbeaad57b58e379e08</data>
    </node>
    <node id="&quot;FORECAST&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"forecast is a parameter used to determine the length of the forecasted data."
"Forecast is a variable used to set the forecasting horizon, which is mentioned in the text."</data>
      <data key="d2">0982b8d1eb1e636b19fa2e9d9361e566,f70c7d3d89baaabbeaad57b58e379e08</data>
    </node>
    <node id="&quot;R&#178; SCORE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"R&#178; Score is a statistical measure that represents the proportion of the variance for a dependent variable that is explained by an independent variable or variables in a regression model."</data>
      <data key="d2">136d135c710f6cf78a4c536d43276fe1</data>
    </node>
    <node id="&quot;IMPROVING RESERVOIRS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Improving Reservoirs is the main topic of the text, focusing on the use of intrinsic plasticity to enhance reservoir performance."</data>
      <data key="d2">414aba25cd4916c4a9e916beef385e81</data>
    </node>
    <node id="&quot;BENJAMIN SCHRAUWEN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Benjamin Schrauwen is a researcher who contributed to the study on improving reservoirs using intrinsic plasticity."
"Benjamin Schrauwen is an author of a research paper on oger: modular learning architectures for large-scale sequential processing."</data>
      <data key="d2">414aba25cd4916c4a9e916beef385e81,7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;MARION WARDERMANN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Marion Wardermann is a researcher who contributed to the study on improving reservoirs using intrinsic plasticity."</data>
      <data key="d2">414aba25cd4916c4a9e916beef385e81</data>
    </node>
    <node id="&quot;DAVID VERSTRAETEN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"David Verstraeten is a researcher who contributed to the study on improving reservoirs using intrinsic plasticity."
"David Verstraeten is an author of a research paper on oger: modular learning architectures for large-scale sequential processing."</data>
      <data key="d2">414aba25cd4916c4a9e916beef385e81,7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;JOCHEN J. STEIL&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Jochen J. Steil is a researcher who contributed to the study on improving reservoirs using intrinsic plasticity."</data>
      <data key="d2">414aba25cd4916c4a9e916beef385e81</data>
    </node>
    <node id="&quot;DIRK STROOBANDT&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Dirk Stroobandt is a researcher who contributed to the study on improving reservoirs using intrinsic plasticity."</data>
      <data key="d2">414aba25cd4916c4a9e916beef385e81</data>
    </node>
    <node id="&quot;NEUROCOMPUTING&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Neurocomputing is the journal where the study on improving reservoirs using intrinsic plasticity was published."</data>
      <data key="d2">414aba25cd4916c4a9e916beef385e81</data>
    </node>
    <node id="&quot;TRIESCH, J.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Triesch, J. is a researcher who published a related study on a gradient rule for the plasticity of a neuron's intrinsic excitability."</data>
      <data key="d2">414aba25cd4916c4a9e916beef385e81</data>
    </node>
    <node id="&quot;NARMA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"narma is a dataset used in the context of reservoir computing, a temporal processing technique."
"NARMA is a system used for generating time series data, often used in machine learning and neural network research."
"NARMA is a type of nonlinear autoregressive moving average model used for timeseries prediction."</data>
      <data key="d2">325bd631a690a34736918180b01f6917,53489f27f4a6a2b135fe6ea6fac9b479,8e2e87aa712be195790cf15483428d7f</data>
    </node>
    <node id="&quot;UNIFORM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"uniform is a type of probability distribution used in the context of generating matrices."
"uniform is a function from the reservoirpy.mat_gen module that creates a sparse matrix from a uniform distribution."
"uniform is a function used to generate sparse matrices from a uniform distribution."</data>
      <data key="d2">325bd631a690a34736918180b01f6917,3a3b7a67b23341dcd1b04ec5b61683f6,96c47d9b671ce319abe9c6ba2b8ae122</data>
    </node>
    <node id="&quot;BERNOULLI&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"bernoulli is a type of probability distribution used in the context of generating matrices."
"Bernoulli refers to a probability distribution that models a random variable that can take on two possible outcomes, such as success or failure, with a given probability of success."</data>
      <data key="d2">325bd631a690a34736918180b01f6917,7b9936d57ece8ba985947a7aca12e2c7</data>
    </node>
    <node id="&quot;IPRESERVOIR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"IPReservoir is a type of node used in reservoir computing, which implements an intrinsic plasticity adaptation rule."
"IPReservoir is a node in ReservoirPy that implements the method described in the paper Improving Reservoirs Using Intrinsic Plasticity by Schrauwen et al. It allows to train intrinsic plasticity parameters in an unsupervised way."
"IPReservoir is a type of reservoir used in the context of Intrinsic Plasticity, which includes parameters such as units, sr, mu, learning_rate, input_scaling, W_dist, and Win_dist."
"IPReservoir is a machine learning model used for time series prediction, involving units, sr, mu, sigma, learning_rate, input_scaling, W, Win, rc_connectivity, input_connectivity, activation, and epochs."</data>
      <data key="d2">325bd631a690a34736918180b01f6917,53489f27f4a6a2b135fe6ea6fac9b479,701ffa843b8d26f96c23dae69e683b58,eadceb9674dd1ce90473d99e0b58e141</data>
    </node>
    <node id="&quot;AUTHORS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"The Authors are the creators of the work, who have extended the ideas of intrinsic plasticity to a more commonly used non-linearity and a Gaussian output distribution."</data>
      <data key="d2">325bd631a690a34736918180b01f6917</data>
    </node>
    <node id="&quot;IP RULE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"IP Rule is a method described for improving reservoirs using intrinsic plasticity, as first described by Triesch."</data>
      <data key="d2">83ded1f13bd74b00694092be69a83870</data>
    </node>
    <node id="&quot;SCHRAUWEN ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Schrauwen et al. is the author group of the paper describing the implementation of the IP Rule."
"Schrauwen et al. is a group of authors who described a method to train intrinsic plasticity parameters in an unsupervised way in the paper Improving Reservoirs Using Intrinsic Plasticity."
"Schrauwen et al. are mentioned as the authors of a reservoir reformulation used in ReservoirPy."</data>
      <data key="d2">701ffa843b8d26f96c23dae69e683b58,83ded1f13bd74b00694092be69a83870,fcac967511cf2b019fd856e23d2e91d9</data>
    </node>
    <node id="&quot;TRIESCH&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Triesch is the author of the paper where the concept of intrinsic plasticity was first described."
"Triesch is an author who first described a gradient rule for the plasticity of a neuron&#8217;s intrinsic excitability in his paper A Gradient Rule for the Plasticity of a Neuron&#8217;s Intrinsic Excitability."</data>
      <data key="d2">701ffa843b8d26f96c23dae69e683b58,83ded1f13bd74b00694092be69a83870</data>
    </node>
    <node id="&quot;INTRINSIC PLASTICITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Intrinsic Plasticity is a concept mentioned in the text, which allows reservoirs to autonomously tune themselves to the dynamic regime which is optimal for a given task."

"Intrinsic Plasticity is a method used to adapt the parameters of a neural network during training, enhancing its learning capabilities."
"Intrinsic Plasticity is a technique used for NARMA timeseries, which involves the use of a sigmoid activation function and an exponential distribution."
"Intrinsic Plasticity is a mechanism in reservoirpy that allows for adaptive learning in reservoirs."</data>
      <data key="d2">53489f27f4a6a2b135fe6ea6fac9b479,701ffa843b8d26f96c23dae69e683b58,83ded1f13bd74b00694092be69a83870,8e2e87aa712be195790cf15483428d7f,b03e2cc6fe2648e792c1d5f1ec5773a3</data>
    </node>
    <node id="&quot;IMPROVING RESERVOIRS USING INTRINSIC PLASTICITY&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Improving Reservoirs Using Intrinsic Plasticity is the event of implementing a method described in a paper by Schrauwen et al. to train intrinsic plasticity parameters in an unsupervised way."</data>
      <data key="d2">701ffa843b8d26f96c23dae69e683b58</data>
    </node>
    <node id="&quot;SIGMOID ACTIVATION FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Sigmoid Activation Function is a mathematical function that maps input values to a range between 0 and 1, often used in neural networks."</data>
      <data key="d2">53489f27f4a6a2b135fe6ea6fac9b479</data>
    </node>
    <node id="&quot;EXPONENTIAL DISTRIBUTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Exponential Distribution is a probability distribution characterized by a single parameter, &#956;, which represents the mean rate of events."</data>
      <data key="d2">53489f27f4a6a2b135fe6ea6fac9b479</data>
    </node>
    <node id="&quot;MU&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Mu is a parameter in the Reservoir Computing model, representing the spectral radius of the reservoir."</data>
      <data key="d2">8e1f4f13f617b982d272175296ec99d3</data>
    </node>
    <node id="&quot;W&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"W is a parameter in the Reservoir Computing model, representing the weights of the connections between the reservoir nodes."
"W is a parameter in the ES2N class, likely representing the weights of the connections between neurons."</data>
      <data key="d2">8e1f4f13f617b982d272175296ec99d3,cedd895dd1ddc09e70c8799effdf7427</data>
    </node>
    <node id="&quot;WIN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Win is a parameter in the Reservoir Computing model, representing the weights of the connections between the input nodes and the reservoir nodes."
"Win is a parameter in the ES2N class, likely representing the input weights of the neural network node."</data>
      <data key="d2">8e1f4f13f617b982d272175296ec99d3,cedd895dd1ddc09e70c8799effdf7427</data>
    </node>
    <node id="&quot;CONNECTIVITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Connectivity is a parameter in the Reservoir Computing model, representing the probability of a connection between nodes in the reservoir."
"Connectivity is a property of a matrix mentioned in the context of creating random sparse matrices."
"connectivity is a parameter used to set the sparsity of the reservoir's weight matrix in the ESN."
"Connectivity is a parameter used to control the sparsity and structure of the reservoir in the Echo State Network (ESN) model."
"Connectivity is a parameter of the Reservoir component that determines the density of connections between nodes."</data>
      <data key="d2">09198e939639c229c2c97555f65b12a7,1365a36c76afc697ac626fd0f784804a,72e6eee633bcb5b1458c4cee3975cee1,8e1f4f13f617b982d272175296ec99d3,ff860bc63e3d697a6183c0b850689048</data>
    </node>
    <node id="&quot;ACTIVATION FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Activation Function is a mathematical function applied to the output of a node in a neural network to introduce non-linearity and enable the model to learn complex patterns."</data>
      <data key="d2">8e1f4f13f617b982d272175296ec99d3</data>
    </node>
    <node id="&quot;HYPERBOLIC TANGENT ACTIVATION FUNCTION&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"The Hyperbolic Tangent Activation Function is a mathematical function used in neural networks."
"Hyperbolic Tangent Activation Function is a mathematical function used in the context of the reservoir network."</data>
      <data key="d2">388cc054a99cc5cadff33147f95d6156,cb7823dcc9852e6a6f9e3607cb55134f</data>
    </node>
    <node id="&quot;PAPER&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Paper refers to a research publication discussing the effectiveness of reservoir activation distributions in neural networks."
"The Paper is a source of information about the most effective reservoir activation distribution for neurons equipped with a hyperbolic tangent activation function."</data>
      <data key="d2">388cc054a99cc5cadff33147f95d6156,cb7823dcc9852e6a6f9e3607cb55134f</data>
    </node>
    <node id="&quot;RESERVOIR NETWORK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Reservoir Network is a type of recurrent neural network used in the context of the paper."</data>
      <data key="d2">388cc054a99cc5cadff33147f95d6156</data>
    </node>
    <node id="&quot;RESERVOIR ACTIVATION DISTRIBUTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Reservoir Activation Distribution refers to the distribution of activation values in the reservoir network."</data>
      <data key="d2">388cc054a99cc5cadff33147f95d6156</data>
    </node>
    <node id="&quot;TIME SERIES PREDICTION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Time Series Prediction is the primary task performed by the IPReservoir model."
"Time Series Prediction is the process of forecasting future values based on past observations, which is a common application of reservoir computing models."
"Time Series Prediction is the process of forecasting future values based on past data points, a common application of Reservoir Computing and Ridge Regression models."
"Time Series Prediction is the task performed by the ESN and ES^2N models, which involve forecasting future values based on past data."
"Time Series Prediction is the process of forecasting future values based on past data points."
"Time Series Prediction is the task that the ESN network is used for, which involves forecasting future values based on past data."
"Time Series Prediction is the primary application of the Reservoir Computing method, as it is used for generating and comparing timeseries data."
"Time Series Prediction is the task of forecasting future values based on past observations, which is the primary application of the ESN model."
"Time Series Prediction is the task of forecasting future values based on past observations."
"Time Series Prediction is the task of forecasting future values based on past observations, a common application of reservoir computing models."
"Time Series Prediction refers to the use of a sequence of data points collected at regular time intervals to predict future values."
"Time Series Prediction is the process of forecasting future values based on past data points."
"Time Series Prediction is the task being performed using the ESN model, as indicated by the code and the context."
"Time Series Prediction is the task of forecasting future values based on past observations, which the ESN model is used for in the provided code."
"Time Series Prediction is the task of forecasting future values based on past observations, which is a common application of reservoir computing models."
"Time Series Prediction is the main task the model is designed for, involving the prediction of future values based on past data."
"Time Series Prediction is a task that involves forecasting future values based on past observations."</data>
      <data key="d2">0113164912437e96423379cb9c039f56,10112a11d47463e2aad7352c52922d61,2336a57d055095c6ffa9d156ddee0096,2386633041e820b604fc4457264b5a33,29f9b2e5fa311519b18e7aef31c68d0a,36e4df75a46fb977f9516f2d2f1f9bc2,41fa16855df7da666dc6fc38d2f8ee53,46dcc47b4358d3895c1eeb1182c6f997,5f841065cf74ef7bbc28efb775d5585e,6a7bea5f60347ea864c06adc327829dc,7b9936d57ece8ba985947a7aca12e2c7,cc1fb6ca5695434ad0279c2606e928af,eadceb9674dd1ce90473d99e0b58e141,eb7a223eeb120e3fcc45a96a6018707d,ef9bf350e25daa8f123b0b5c4d60de5f,f0c8d4d322d73f46464e3e9f6914f2ee,fd81bdceb3e2b91ac2605a3d201d1eb4</data>
    </node>
    <node id="&quot;GLOBAL ACTIVATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Global activation refers to the distribution of reservoir activations on a global scale."</data>
      <data key="d2">7f3ec3328c157265cc61021bd62b0ed7</data>
    </node>
    <node id="&quot;TARGET DISTRIBUTION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Target distribution refers to the desired probability density of reservoir activations."</data>
      <data key="d2">7f3ec3328c157265cc61021bd62b0ed7</data>
    </node>
    <node id="&quot;RESERVOIR ACTIVATIONS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Reservoir activations are a key concept in the text, referring to the activation of a reservoir in a system."</data>
      <data key="d2">7f3ec3328c157265cc61021bd62b0ed7</data>
    </node>
    <node id="&quot;THE SYSTEM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The system is an unspecified entity that includes reservoir activations and a target distribution."</data>
      <data key="d2">7f3ec3328c157265cc61021bd62b0ed7</data>
    </node>
    <node id="&quot;STATISTICS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Statistics is a branch of mathematics that deals with the collection, analysis, interpretation, presentation, and organization of data."
"Statistics is mentioned as a field where time series analysis is commonly used."
"Statistics is a field that focuses on data analysis and inference, including transferring knowledge about a sample to the whole population."</data>
      <data key="d2">423d3b5ec1acc9a4cb448a15d3b6b595,70c3a879c0f6e6b76a13d02d67bce1a8,a2b394d556da06b8c14dd2f5e106343b</data>
    </node>
    <node id="&quot;SIGNAL PROCESSING&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Signal Processing is a field of study that focuses on analyzing and manipulating signals, such as time series data, to extract meaningful information."
"Signal Processing is an application area where Echo State Networks have been found to be relevant and popular."
"Signal Processing is an area where ESNs have been widely used due to their ability to mix well with non-digital computer substrates."</data>
      <data key="d2">257d4cf08ffc32b99856b6e31fa4221e,4b89d9404fd683ecd03d5846ee2d86ce,70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </node>
    <node id="&quot;PATTERN RECOGNITION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Pattern Recognition is a field of study that focuses on the automatic discovery of patterns in data, including time series data."</data>
      <data key="d2">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </node>
    <node id="&quot;ECONOMETRICS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Econometrics is a branch of economics that uses statistical methods and econometric theory to analyze economic data, including time series data."
"Econometrics is mentioned as a field where time series analysis is used for forecasting."</data>
      <data key="d2">70c3a879c0f6e6b76a13d02d67bce1a8,a2b394d556da06b8c14dd2f5e106343b</data>
    </node>
    <node id="&quot;MATHEMATICAL FINANCE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Mathematical Finance is a field of study that applies mathematical and statistical methods to financial markets, including the analysis of time series data."</data>
      <data key="d2">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </node>
    <node id="&quot;WEATHER FORECASTING&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Weather Forecasting is the application of scientific methods and techniques to predict atmospheric conditions, including the analysis of time series data."</data>
      <data key="d2">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </node>
    <node id="&quot;EARTHQUAKE PREDICTION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Earthquake Prediction is the use of scientific methods and techniques to predict earthquakes, including the analysis of time series data."</data>
      <data key="d2">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </node>
    <node id="&quot;STOCHASTIC MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Stochastic Model is a mathematical representation of a system that accounts for uncertainty, often used in Time Series Analysis."</data>
      <data key="d2">8e69dad9d25c6b8f037f22592687e195</data>
    </node>
    <node id="&quot;FREQUENCY-DOMAIN METHODS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Frequency-domain Methods are techniques used in Time Series Analysis that involve analyzing data in the frequency domain, including Spectral Analysis and Wavelet Analysis."</data>
      <data key="d2">c7d17582a93a296eaaf9b9fca737ba51</data>
    </node>
    <node id="&quot;TIME-DOMAIN METHODS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Time-domain Methods are techniques used in Time Series Analysis that involve analyzing data in the time domain, including Auto-correlation and Cross-correlation Analysis."</data>
      <data key="d2">c7d17582a93a296eaaf9b9fca737ba51</data>
    </node>
    <node id="&quot;PARAMETRIC METHODS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Parametric Methods are approaches in Time Series Analysis that assume the underlying stationary stochastic process has a certain structure, described using a small number of parameters."
"Parametric Methods assume that the underlying stationary stochastic process has a certain structure, described using a small number of parameters."</data>
      <data key="d2">c087c124713c7ade4223617d95928cbf,c7d17582a93a296eaaf9b9fca737ba51</data>
    </node>
    <node id="&quot;NON-PARAMETRIC METHODS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Non-parametric Methods are approaches in Time Series Analysis that do not assume the underlying structure of the stochastic process, explicitly estimating the covariance or spectrum of the process."
"Non-Parametric Methods explicitly estimate the covariance or the spectrum of the process without assuming any particular structure."</data>
      <data key="d2">c087c124713c7ade4223617d95928cbf,c7d17582a93a296eaaf9b9fca737ba51</data>
    </node>
    <node id="&quot;LINEAR METHODS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Linear Methods are a type of time series analysis method."</data>
      <data key="d2">c087c124713c7ade4223617d95928cbf</data>
    </node>
    <node id="&quot;NON-LINEAR METHODS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Non-Linear Methods are a type of time series analysis method."</data>
      <data key="d2">c087c124713c7ade4223617d95928cbf</data>
    </node>
    <node id="&quot;UNIVARIATE METHODS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Univariate Methods are a type of time series analysis method."</data>
      <data key="d2">c087c124713c7ade4223617d95928cbf</data>
    </node>
    <node id="&quot;MULTIVARIATE METHODS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Multivariate Methods are a type of time series analysis method."</data>
      <data key="d2">c087c124713c7ade4223617d95928cbf</data>
    </node>
    <node id="&quot;PANEL DATA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Panel Data is a general class of multidimensional data set, which includes time series data."</data>
      <data key="d2">c087c124713c7ade4223617d95928cbf</data>
    </node>
    <node id="&quot;CROSS-SECTIONAL DATA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Cross-Sectional Data is a type of data set where unique records are determined by a non-time identifier."</data>
      <data key="d2">c087c124713c7ade4223617d95928cbf</data>
    </node>
    <node id="&quot;UNITED STATES&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"The United States is mentioned in the context of tuberculosis incidence."
"The United States is mentioned in the context of Tuberculosis incidence data."</data>
      <data key="d2">4b75a8a7637b05307e62f309c682d43b,a2b394d556da06b8c14dd2f5e106343b</data>
    </node>
    <node id="&quot;TUBERCULOSIS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Tuberculosis is mentioned as a disease with incidence rates being analyzed."
"Tuberculosis is a disease mentioned in the text, with incidence data being analyzed."</data>
      <data key="d2">4b75a8a7637b05307e62f309c682d43b,a2b394d556da06b8c14dd2f5e106343b</data>
    </node>
    <node id="&quot;QUANTITATIVE FINANCE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Quantitative Finance is mentioned as a field where time series analysis is used for forecasting."</data>
      <data key="d2">a2b394d556da06b8c14dd2f5e106343b</data>
    </node>
    <node id="&quot;CORPORATE DATA ANALYSTS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Corporate Data Analysts are mentioned in the text, with challenges related to exploratory time series analysis being discussed."</data>
      <data key="d2">4b75a8a7637b05307e62f309c682d43b</data>
    </node>
    <node id="&quot;SERIES ANALYSIS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Series Analysis is a technique used to discover patterns in data over time."</data>
      <data key="d2">630c86e110e2dabbe068f446b619cef3</data>
    </node>
    <node id="&quot;HEAT MAP MATRICES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Heat Map Matrices are visual tools used to represent time series data, helping to overcome challenges in data analysis."</data>
      <data key="d2">630c86e110e2dabbe068f446b619cef3</data>
    </node>
    <node id="&quot;CURVE FITTING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Curve Fitting is a technique used to construct a curve that best fits a series of data points, subject to constraints."
"Curve Fitting is a technique used to approximate a function when only a set of points is provided, and the function is an operation on the real numbers."</data>
      <data key="d2">630c86e110e2dabbe068f446b619cef3,9ec0dac4c72bcc2c78c7df43b9969fe7</data>
    </node>
    <node id="&quot;SMOOTHING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Smoothing is a method used in curve fitting to construct a 'smooth' function that approximately fits the data."</data>
      <data key="d2">630c86e110e2dabbe068f446b619cef3</data>
    </node>
    <node id="&quot;PROCESSES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Processes are the underlying activities or phenomena being analyzed or studied."</data>
      <data key="d2">630c86e110e2dabbe068f446b619cef3</data>
    </node>
    <node id="&quot;ECONOMIC TIME SERIES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Economic Time Series refers to the collection and analysis of data over time to understand economic trends and patterns."</data>
      <data key="d2">bde7c826c746ece93a512a0cf167fa3e</data>
    </node>
    <node id="&quot;FUNCTION APPROXIMATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Function Approximation is the process of selecting a function from a well-defined class that closely matches a target function."
"Function Approximation is the process of selecting a function from a well-defined class that closely matches a target function."</data>
      <data key="d2">472b44b36407c9a89cf5c51459188263,9ec0dac4c72bcc2c78c7df43b9969fe7</data>
    </node>
    <node id="&quot;POLYNOMIAL INTERPOLATION&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Polynomial Interpolation is a method that fits piecewise polynomial functions into time intervals to estimate values."</data>
      <data key="d2">472b44b36407c9a89cf5c51459188263</data>
    </node>
    <node id="&quot;SPLINE INTERPOLATION&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Spline Interpolation is a method that uses piecewise continuous functions composed of many polynomials to estimate values."</data>
      <data key="d2">472b44b36407c9a89cf5c51459188263</data>
    </node>
    <node id="&quot;POLYNOMIAL REGRESSION&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Polynomial Regression is a method that uses a single polynomial to model an entire data set."</data>
      <data key="d2">472b44b36407c9a89cf5c51459188263</data>
    </node>
    <node id="&quot;APPROXIMATION THEORY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Approximation Theory is a branch of numerical analysis that investigates how certain known functions can be approximated by a specific class of functions."</data>
      <data key="d2">9ec0dac4c72bcc2c78c7df43b9969fe7</data>
    </node>
    <node id="&quot;CLASSIFICATION PROBLEM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Classification Problem is a problem where the target function has a finite codomain, and the task is to categorize data points into these classes."</data>
      <data key="d2">9ec0dac4c72bcc2c78c7df43b9969fe7</data>
    </node>
    <node id="&quot;ONLINE TIME SERIES APPROXIMATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Online Time Series Approximation is a problem that requires summarizing data in one-pass and constructing an approximate representation of the time series."
"Online Time Series Approximation is a problem that involves summarizing data in one-pass and constructing an approximate representation to support various time series queries with error bounds."</data>
      <data key="d2">9261efcc24379d9c0b2d35a2fde8275d,9ec0dac4c72bcc2c78c7df43b9969fe7</data>
    </node>
    <node id="&quot;STATISTICAL LEARNING THEORY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Statistical Learning Theory is a unified framework that encompasses various statistical problems, including regression and classification."
"Statistical Learning Theory is a field that focuses on increasing the capacity of models by optimizing various biases, including manual experimentation."</data>
      <data key="d2">2a2a93486d6198ce228e77e120dc3c0c,9261efcc24379d9c0b2d35a2fde8275d</data>
    </node>
    <node id="&quot;HAND MOVEMENTS IN SIGN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Hand Movements in Sign is a technique used for identifying words based on a series of hand movements."</data>
      <data key="d2">9261efcc24379d9c0b2d35a2fde8275d</data>
    </node>
    <node id="&quot;WORLD WAR II&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"World War II is a significant historical event that accelerated the development of mathematical techniques and technologies."</data>
      <data key="d2">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </node>
    <node id="&quot;NORBERT WIENER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Norbert Wiener was a mathematician who made significant contributions to signal processing and filtering during World War II."</data>
      <data key="d2">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </node>
    <node id="&quot;RUDOLF E. K&#193;LM&#193;N&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Rudolf E. K&#225;lm&#225;n was an electrical engineer who developed the Kalman filter, a mathematical tool for signal estimation and prediction."</data>
      <data key="d2">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </node>
    <node id="&quot;DENNIS GABOR&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Dennis Gabor was an electrical engineer who contributed to the development of signal processing and spectral density estimation during World War II."</data>
      <data key="d2">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </node>
    <node id="&quot;SIGNAL ESTIMATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Signal Estimation is the approach of analyzing and filtering signals in the frequency domain using techniques like the Fourier transform and spectral density estimation."</data>
      <data key="d2">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </node>
    <node id="&quot;SEGMENTATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Segmentation is the process of splitting a time-series into a sequence of segments, each with its own characteristic properties."</data>
      <data key="d2">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </node>
    <node id="&quot;TIME-SERIES&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </node>
    <node id="&quot;DIGITAL SIGNAL PROCESSING&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Digital Signal Processing is a field that deals with processing and analyzing digital signals."</data>
      <data key="d2">5d6a266e9d567f013be17768c04cbc09</data>
    </node>
    <node id="&quot;TIME-SERIES SEGMENTATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Time-Series Segmentation is the process of dividing a time-series into individual segments, each with its own characteristics."</data>
      <data key="d2">5d6a266e9d567f013be17768c04cbc09</data>
    </node>
    <node id="&quot;TIME-SERIES CLUSTERING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Time-Series Clustering is the process of grouping time-series data into clusters based on their similarities."</data>
      <data key="d2">5d6a266e9d567f013be17768c04cbc09</data>
    </node>
    <node id="&quot;SUBSEQUENCE TIME-SERIES CLUSTERING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Subsequence Time-Series Clustering is a specific approach within Time-Series Clustering that focuses on subsequences."</data>
      <data key="d2">5d6a266e9d567f013be17768c04cbc09</data>
    </node>
    <node id="&quot;AUTOREGRESSIVE (AR) MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Autoregressive (AR) Models are statistical models that depend linearly on previous data points."</data>
      <data key="d2">a000a3fbf1f8fad62e4c25b495858c79</data>
    </node>
    <node id="&quot;INTEGRATED (I) MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Integrated (I) Models are statistical models used to model variations in the level of a process."</data>
      <data key="d2">a000a3fbf1f8fad62e4c25b495858c79</data>
    </node>
    <node id="&quot;MOVING-AVERAGE (MA) MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Moving-Average (MA) Models are statistical models that depend linearly on previous data points."</data>
      <data key="d2">a000a3fbf1f8fad62e4c25b495858c79</data>
    </node>
    <node id="&quot;AUTOREGRESSIVE MOVING-AVERAGE (ARMA) MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Autoregressive Moving-Average (ARMA) Models are statistical models that combine Autoregressive and Moving-Average concepts."</data>
      <data key="d2">a000a3fbf1f8fad62e4c25b495858c79</data>
    </node>
    <node id="&quot;AUTOREGRESSIVE INTEGRATED MOVING-AVERAGE (ARIMA) MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Autoregressive Integrated Moving-Average (ARIMA) Models are statistical models that combine Autoregressive and Integrated concepts with Moving-Average."</data>
      <data key="d2">a000a3fbf1f8fad62e4c25b495858c79</data>
    </node>
    <node id="&quot;AUTOREGRESSIVE FRACTIONALLY INTEGRATED MOVING-AVERAGE (ARFIMA) MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Autoregressive Fractionally Integrated Moving-Average (ARFIMA) Models are statistical models that generalize Autoregressive, Integrated, and Moving-Average concepts."</data>
      <data key="d2">a000a3fbf1f8fad62e4c25b495858c79</data>
    </node>
    <node id="&quot;VECTOR-VALUED DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Vector-Valued Data refers to data points that are multi-dimensional, allowing for more complex modeling."</data>
      <data key="d2">a000a3fbf1f8fad62e4c25b495858c79</data>
    </node>
    <node id="&quot;MULTIVARIATE TIME-SERIES MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Multivariate Time-Series Models are statistical models that extend univariate models to deal with Vector-Valued Data."
"Multivariate Time-Series Models are extensions of ARIMA and ARFIMA Models that can handle vector-valued data."</data>
      <data key="d2">a000a3fbf1f8fad62e4c25b495858c79,e5e350429a2ca41c6bfd4aa812e4c391</data>
    </node>
    <node id="&quot;ARIMA MODELS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ARIMA Models are a type of time series model that combines autoregressive and moving-average components."</data>
      <data key="d2">e5e350429a2ca41c6bfd4aa812e4c391</data>
    </node>
    <node id="&quot;ARFIMA MODELS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ARFIMA Models are an extension of ARIMA Models that generalizes them to include fractionally integrated components."</data>
      <data key="d2">e5e350429a2ca41c6bfd4aa812e4c391</data>
    </node>
    <node id="&quot;VAR MODELS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"VAR Models are a type of Multivariate Time-Series Model that stands for Vector Autoregression."</data>
      <data key="d2">e5e350429a2ca41c6bfd4aa812e4c391</data>
    </node>
    <node id="&quot;EXOGENOUS TIME-SERIES MODELS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Exogenous Time-Series Models are extensions of ARIMA and ARFIMA Models that account for the influence of a 'forcing' time-series."</data>
      <data key="d2">e5e350429a2ca41c6bfd4aa812e4c391</data>
    </node>
    <node id="&quot;KANTZ AND SCHREIBER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Kantz and Schreiber are mentioned as references for nonlinear time series analysis."</data>
      <data key="d2">e5e350429a2ca41c6bfd4aa812e4c391</data>
    </node>
    <node id="&quot;ABARBANEL&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Abarbanel is mentioned as a reference for nonlinear time series analysis."</data>
      <data key="d2">e5e350429a2ca41c6bfd4aa812e4c391</data>
    </node>
    <node id="&quot;NONLINEAR TIME SERIES ANALYSIS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Nonlinear Time Series Analysis is mentioned as a topic of interest due to its potential for producing chaotic time series and its advantage in empirical investigations."</data>
      <data key="d2">e5e350429a2ca41c6bfd4aa812e4c391</data>
    </node>
    <node id="&quot;NON-LINEAR TIME SERIES MODELS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Non-linear Time Series Models are a category of models used to analyze and predict time series data that deviate from linear patterns."</data>
      <data key="d2">7b54e70c4e190dec8a2da85292b3e4af</data>
    </node>
    <node id="&quot;AUTOREGRESSIVE CONDITIONAL HETEROSKEDASTICITY (ARCH)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Autoregressive Conditional Heteroskedasticity (ARCH) is a subcategory of Non-linear Time Series Models that focuses on modeling changes in variability over time."</data>
      <data key="d2">7b54e70c4e190dec8a2da85292b3e4af</data>
    </node>
    <node id="&quot;GARCH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"GARCH (Generalized Autoregressive Conditional Heteroskedasticity) is a specific model within Autoregressive Conditional Heteroskedasticity (ARCH) that predicts variability based on recent past values of the observed series."</data>
      <data key="d2">7b54e70c4e190dec8a2da85292b3e4af</data>
    </node>
    <node id="&quot;TARCH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"TARCH (Threshold Autoregressive Conditional Heteroskedasticity) is a variant of GARCH that incorporates a threshold to model asymmetric volatility patterns."</data>
      <data key="d2">7b54e70c4e190dec8a2da85292b3e4af</data>
    </node>
    <node id="&quot;EGARCH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"EGARCH (Exponential Generalized Autoregressive Conditional Heteroskedasticity) is a modification of GARCH that allows for the modeling of long-term memory effects in time series data."</data>
      <data key="d2">7b54e70c4e190dec8a2da85292b3e4af</data>
    </node>
    <node id="&quot;FIGARCH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"FIGARCH (Fractionally Integrated Generalized Autoregressive Conditional Heteroskedasticity) is an extension of GARCH that incorporates fractional integration to handle non-stationary data."</data>
      <data key="d2">7b54e70c4e190dec8a2da85292b3e4af</data>
    </node>
    <node id="&quot;CGARCH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"CGARCH (Conditional Generalized Autoregressive Conditional Heteroskedasticity) is a generalization of GARCH that allows for the modeling of conditional correlations and volatilities in multivariate time series data."</data>
      <data key="d2">7b54e70c4e190dec8a2da85292b3e4af</data>
    </node>
    <node id="&quot;WAVELET TRANSFORM BASED METHODS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Wavelet Transform Based Methods are a category of techniques used in model-free analyses to decompose time series data and illustrate time dependence at multiple scales."</data>
      <data key="d2">7b54e70c4e190dec8a2da85292b3e4af</data>
    </node>
    <node id="&quot;LOCALLY STATIONARY WAVELETS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Locally Stationary Wavelets are a subcategory of Wavelet Transform Based Methods that allow for the modeling of non-stationary data by adapting the wavelet basis to local characteristics."</data>
      <data key="d2">7b54e70c4e190dec8a2da85292b3e4af</data>
    </node>
    <node id="&quot;WAVELET DECOMPOSED NEURAL NETWORKS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Wavelet Decomposed Neural Networks are a combination of Wavelet Transform Based Methods and Neural Networks that decompose time series data at multiple scales and use neural networks for modeling and prediction."</data>
      <data key="d2">7b54e70c4e190dec8a2da85292b3e4af</data>
    </node>
    <node id="&quot;MARKOV SWITCHING MULTIFRACTAL (MSMF)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Markov Switching Multifractal (MSMF) techniques are a category of models used to model volatility in financial time series data, incorporating switching between multiple regimes and multifractal analysis."</data>
      <data key="d2">7b54e70c4e190dec8a2da85292b3e4af</data>
    </node>
    <node id="&quot;VOLATILITY MODELING&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">7b54e70c4e190dec8a2da85292b3e4af</data>
    </node>
    <node id="&quot;WAVELET TRANSFORM&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Wavelet Transform is a mathematical tool used for time-frequency analysis, gaining favor in recent work on model-free analyses."</data>
      <data key="d2">f5ca4e75341eb9da478381a489ae6058</data>
    </node>
    <node id="&quot;MULTISCALE TECHNIQUES&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Multiscale Techniques are used to decompose a given time series, illustrating time dependence at multiple scales."</data>
      <data key="d2">f5ca4e75341eb9da478381a489ae6058</data>
    </node>
    <node id="&quot;HIDDEN MARKOV MODEL&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Hidden Markov Model is a statistical model used for modeling time series data, such as speech recognition."</data>
      <data key="d2">f5ca4e75341eb9da478381a489ae6058</data>
    </node>
    <node id="&quot;SKTIME&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Sktime is a Python package that collects various time series models."</data>
      <data key="d2">f5ca4e75341eb9da478381a489ae6058</data>
    </node>
    <node id="&quot;ERGODICITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Ergodicity is a condition in time-series analysis that implies stationarity, but the converse is not necessarily the case."
"Ergodicity is a condition in time-series analysis that implies stationarity."</data>
      <data key="d2">cd814464801a2b4d72fc06b10de5e865,f5ca4e75341eb9da478381a489ae6058</data>
    </node>
    <node id="&quot;STATIONARITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Stationarity is a condition in time-series analysis that classifies into strict stationarity and wide-sense or second-order stationarity."
"Stationarity is a condition in time-series analysis that can be classified into strict and wide-sense or second-order stationarity."</data>
      <data key="d2">cd814464801a2b4d72fc06b10de5e865,f5ca4e75341eb9da478381a489ae6058</data>
    </node>
    <node id="&quot;TIME-SERIES ANALYSIS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Time-series Analysis is a field that deals with the analysis of time-series data, using various notations and conditions."</data>
      <data key="d2">cd814464801a2b4d72fc06b10de5e865</data>
    </node>
    <node id="&quot;TIME-FREQUENCY ANALYSIS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Time-frequency Analysis is a technique used to deal with situations where the amplitudes of frequency components change with time in time-series data."</data>
      <data key="d2">cd814464801a2b4d72fc06b10de5e865</data>
    </node>
    <node id="&quot;TOOLS FOR TIME-SERIES ANALYSIS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Tools for Time-series Analysis include measures, visualization techniques, and time-frequency analysis methods."</data>
      <data key="d2">cd814464801a2b4d72fc06b10de5e865</data>
    </node>
    <node id="&quot;TIME-SERIES METRICS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Time-series Metrics are features that can be used for time series classification or regression analysis."
"Time-series Metrics refers to the specific measures or features used for time series classification or regression analysis."</data>
      <data key="d2">a9c0a1c4d1c08c2c79806dc970186e79,cd814464801a2b4d72fc06b10de5e865</data>
    </node>
    <node id="&quot;VISUALIZATION TECHNIQUES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Visualization Techniques for time series include overlapping and separated charts, with overlapping charts displaying all time series on the same layout."</data>
      <data key="d2">cd814464801a2b4d72fc06b10de5e865</data>
    </node>
    <node id="&quot;MEASURES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Measures is a category of tools for investigating time-series data, which includes metrics and features for time series classification or regression analysis."</data>
      <data key="d2">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </node>
    <node id="&quot;VISUALIZATION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Visualization is a category of tools for investigating time-series data, which includes charts for displaying time series data."
"Visualization is the representation of data in a graphical format to facilitate understanding and interpretation."</data>
      <data key="d2">a9c0a1c4d1c08c2c79806dc970186e79,d15f6d075c072f0335b5332f11c00299</data>
    </node>
    <node id="&quot;OVERLAPPING CHARTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Overlapping Charts is a type of visualization that displays multiple time series on the same layout."</data>
      <data key="d2">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </node>
    <node id="&quot;SEPARATED CHARTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Separated Charts is a type of visualization that displays multiple time series on different layouts, but aligned for comparison."</data>
      <data key="d2">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </node>
    <node id="&quot;BRAIDED GRAPHS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Braided Graphs is a type of overlapping chart used for visualizing time series data."</data>
      <data key="d2">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </node>
    <node id="&quot;LINE CHARTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Line Charts is a type of overlapping chart used for visualizing time series data."</data>
      <data key="d2">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </node>
    <node id="&quot;SLOPE GRAPHS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Slope Graphs is a type of overlapping chart used for visualizing time series data."</data>
      <data key="d2">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </node>
    <node id="&quot;GAPCHART&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"GapChart is a type of overlapping chart used for visualizing time series data."</data>
      <data key="d2">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </node>
    <node id="&quot;HORIZON GRAPHS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Horizon Graphs is a type of separated chart used for visualizing time series data."</data>
      <data key="d2">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </node>
    <node id="&quot;REDUCED LINE CHART&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Reduced Line Chart, also known as Small Multiples, is a type of separated chart used for visualizing time series data."</data>
      <data key="d2">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </node>
    <node id="&quot;SILHOUETTE GRAPH&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Silhouette Graph is a type of separated chart used for visualizing time series data."</data>
      <data key="d2">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </node>
    <node id="&quot;CIRCULAR SILHOUETTE GRAPH&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Circular Silhouette Graph is a type of separated chart used for visualizing time series data, which is a variation of the Silhouette Graph."</data>
      <data key="d2">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </node>
    <node id="&quot;ECHO STATE NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Echo State Network is a type of recurrent neural network architecture developed for supervised learning."
"Echo State Network is a type of recurrent neural network that uses a high-dimensional dynamical system to reconstruct desired outputs."
"The Echo State Network is a type of recurrent neural network mentioned in the text, with control parameters and reservoir size as key components."
"Echo State Network is a type of recurrent neural network used for time series prediction and data analysis."
"Echo State Network is a type of recurrent neural network used for time series prediction and data analysis."
"Echo State Network is a type of recurrent neural network used for tasks such as time series prediction and function approximation."
"Echo State Network is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer."</data>
      <data key="d2">29f9b2e5fa311519b18e7aef31c68d0a,2dca9849c50a439b0637cf370afde7dd,688ebc7151bc148ac24dc7e2727d7afe,6daefaa8fbd5c1492f2d832d79841463,804bd76fa6f4950ef9a5cf8f0025fc1c,cc7c60d8e36838743d509a97c9ac3a4b,dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </node>
    <node id="&quot;HERBERT JAEGER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Herbert Jaeger is a researcher associated with the development of Echo State Networks."
"Herbert Jaeger is an author of a research paper on controlling recurrent neural networks by conceptors."
"Herbert Jaeger is a researcher who published a paper on controlling recurrent neural networks by conceptors."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3,804bd76fa6f4950ef9a5cf8f0025fc1c,c8b7bd13cf99920ecce56cb563910cb3</data>
    </node>
    <node id="&quot;JACOBS UNIVERSITY BREMEN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Jacobs University Bremen is the institution where Herbert Jaeger is affiliated."</data>
      <data key="d2">804bd76fa6f4950ef9a5cf8f0025fc1c</data>
    </node>
    <node id="&quot;BREMEN&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"Bremen is a city in Germany where Jacobs University is located."</data>
      <data key="d2">804bd76fa6f4950ef9a5cf8f0025fc1c</data>
    </node>
    <node id="&quot;GERMANY&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"Germany is the country where Bremen and Jacobs University are located."</data>
      <data key="d2">804bd76fa6f4950ef9a5cf8f0025fc1c</data>
    </node>
    <node id="&quot;LIQUID STATE MACHINES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Liquid State Machines are a type of recurrent neural network developed by Wolfgang Maass, which share similarities with Echo State Networks."
"LSM is a type of machine learning model developed independently by Wolfgang Maass, used for reservoir computing."</data>
      <data key="d2">804bd76fa6f4950ef9a5cf8f0025fc1c,b32958d42199d47252887dc7be40ab5a</data>
    </node>
    <node id="&quot;WOLFGANG MAASS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Wolfgang Maass is a researcher associated with the development of Liquid State Machines."
"Wolfgang Maass is a co-author of a scientific paper on a specific topic, but the relationship description and strength are not explicitly stated in the text."
"Wolfgang Maass is an author of a research paper on emergence of complex computational structures from chaotic neural networks."
"Wolfgang Maass is a researcher who independently developed Liquid State Machines, which are related to Echo State Networks and Reservoir Computing."
"Wolfgang Maass is a researcher who independently developed Liquid State Machines and Echo State Networks."</data>
      <data key="d2">158f53cd85edbb4f2e4c77b78c5e7acc,804bd76fa6f4950ef9a5cf8f0025fc1c,b32958d42199d47252887dc7be40ab5a,c8b7bd13cf99920ecce56cb563910cb3,ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;BACKPROPAGATION DECORRELATION LEARNING RULE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Backpropagation Decorrelation learning rule is a method for training recurrent neural networks, developed by Schiller and Steil."
"Backpropagation Decorrelation learning rule is a method used for training Recurrent Neural Networks, mentioned in the context of reservoir computing."</data>
      <data key="d2">804bd76fa6f4950ef9a5cf8f0025fc1c,b32958d42199d47252887dc7be40ab5a</data>
    </node>
    <node id="&quot;PETER F. DOMINEY&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Peter F. Dominey is a researcher who has investigated a related mechanism in the context of modeling sequence processing in mammalian brains, especially speech recognition in the human brain."
"Peter F. Dominey is a researcher in cognitive neuroscience who has investigated mechanisms related to reservoir computing, such as speech recognition in the human brain."
"Peter F. Dominey is a researcher who analyzed a process related to the modeling of sequence processing in the mammalian brain, including speech recognition in the human brain."</data>
      <data key="d2">804bd76fa6f4950ef9a5cf8f0025fc1c,88ff8a7687e01f40b2c9d151b6e83d64,b32958d42199d47252887dc7be40ab5a</data>
    </node>
    <node id="&quot;SCHILLER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Schiller is a researcher mentioned in the text, contributing to the understanding of reservoir computing."</data>
      <data key="d2">88ff8a7687e01f40b2c9d151b6e83d64</data>
    </node>
    <node id="&quot;STEIL&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Steil is a researcher mentioned in the text, contributing to the understanding of reservoir computing."
"Steil is mentioned as the author of the Intrinsic Plasticity mechanism for reservoirs in ReservoirPy."</data>
      <data key="d2">88ff8a7687e01f40b2c9d151b6e83d64,fcac967511cf2b019fd856e23d2e91d9</data>
    </node>
    <node id="&quot;K. KIRBY&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"K. Kirby is a researcher who exposed the concept of reservoir computing in a conference contribution."
"K. Kirby is a researcher who disclosed the concept of reservoir computing in a conference contribution."</data>
      <data key="d2">88ff8a7687e01f40b2c9d151b6e83d64,b32958d42199d47252887dc7be40ab5a</data>
    </node>
    <node id="&quot;L. SCHOMAKER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"L. Schomaker is a researcher who described a method of obtaining a desired target output from an RNN by learning to combine signals from a randomly configured ensemble of spiking neural oscillators."
"L. Schomaker is a researcher who contributed to the development of the reservoir computing idea."
"L. Schomaker is a researcher who described how a desired target output could be obtained from an RNN by learning to combine signals from a randomly configured ensemble of spiking neural oscillators."
"L. Schomaker is a researcher who described a formulation of the reservoir computing idea known today, involving the use of a randomly configured ensemble of spiking neural oscillators."</data>
      <data key="d2">88ff8a7687e01f40b2c9d151b6e83d64,a3368f9cab1f65643dba089af5a1f95e,a621b44739e0cb4379645a4a58f16697,b32958d42199d47252887dc7be40ab5a</data>
    </node>
    <node id="&quot;TRAINING METHODS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Training Methods refer to the processes used to adapt the weights of a reservoir computing network."</data>
      <data key="d2">88ff8a7687e01f40b2c9d151b6e83d64</data>
    </node>
    <node id="&quot;SEQUENCE PROCESSING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Sequence Processing is a task mentioned in the text, which has been modelled using reservoir computing in cognitive neuroscience."
</data>
      <data key="d2">88ff8a7687e01f40b2c9d151b6e83d64,b32958d42199d47252887dc7be40ab5a</data>
    </node>
    <node id="&quot;TEMPORAL INPUT DISCRIMINATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Temporal Input Discrimination is a task mentioned in the text, which has been modelled using reservoir computing in biological neural networks."</data>
      <data key="d2">88ff8a7687e01f40b2c9d151b6e83d64</data>
    </node>
    <node id="&quot;FREQUENCY GENERATOR&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Frequency Generator is a task mentioned in the text, which can be achieved using reservoir computing."</data>
      <data key="d2">88ff8a7687e01f40b2c9d151b6e83d64</data>
    </node>
    <node id="&quot;RNN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"RNN is an abbreviation for Recurrent Neural Network, a type of neural network used in the reservoir computing idea."
"RNN is a type of neural network mentioned in the text, which the Echo State Networks are designed to train."
"RNN stands for Recurrent Neural Network, a type of neural network used by Baidu and Google in their machine learning models."
"RNN is a type of neural network that corresponds to a linear chain, a special case of recursive neural networks."
"RNN is a type of neural network used in deep learning systems, such as Second-order RNNs and Long short-term memory (LSTM)."
"RNN is a type of neural network architecture that processes sequential data, with variants such as LSTM and IndRNN addressing the vanishing gradients problem."</data>
      <data key="d2">1aec5b03f663d1614b2ecbf97981a5c2,4470a7f7ad60a2866b31907a2a3ca96e,486e4b71bf02f756450ab727db88f821,7b6ff30ef255db2d2c68326d78cf0115,a621b44739e0cb4379645a4a58f16697,f0b3b2a88425b0563005400ea246528b</data>
    </node>
    <node id="&quot;TRAINING INPUT-OUTPUT SEQUENCE&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Training Input-Output Sequence is a set of data used to train the RNN to behave as a tunable frequency generator."</data>
      <data key="d2">a621b44739e0cb4379645a4a58f16697</data>
    </node>
    <node id="&quot;RESERVOIR COMPUTING IDEA&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">a621b44739e0cb4379645a4a58f16697</data>
    </node>
    <node id="&quot;SCHMIDHUBER ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Schmidhuber et al. is a group of researchers mentioned in the text, who have used margin-maximization criteria in the context of Echo State Networks."</data>
      <data key="d2">f0b3b2a88425b0563005400ea246528b</data>
    </node>
    <node id="&quot;SIGMOID UNIT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Sigmoid Unit is a type of neuron mentioned in the text, which is used in the basic discrete-time echo state network."</data>
      <data key="d2">f0b3b2a88425b0563005400ea246528b</data>
    </node>
    <node id="&quot;SIGMOID FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Sigmoid Function is a mathematical function used in the Echo State Network to map input values to a specific range, typically between 0 and 1."</data>
      <data key="d2">cc7c60d8e36838743d509a97c9ac3a4b</data>
    </node>
    <node id="&quot;RESERVOIR WEIGHT MATRIX&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Reservoir Weight Matrix is a matrix used in the Echo State Network to update the reservoir state."
"The Reservoir Weight Matrix is a component of a Recurrent Neural Network (RNN) that affects its behavior."</data>
      <data key="d2">3b592e5ac113a5c031925f91a182baa6,cc7c60d8e36838743d509a97c9ac3a4b</data>
    </node>
    <node id="&quot;INPUT WEIGHT MATRIX&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input Weight Matrix is a matrix used in the Echo State Network to map input signals to the reservoir state."</data>
      <data key="d2">cc7c60d8e36838743d509a97c9ac3a4b</data>
    </node>
    <node id="&quot;OUTPUT FEEDBACK MATRIX&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Output Feedback Matrix is a matrix used in the Echo State Network to provide feedback from the output signal to the reservoir state."</data>
      <data key="d2">cc7c60d8e36838743d509a97c9ac3a4b</data>
    </node>
    <node id="&quot;LOGISTIC SIGMOID&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Logistic Sigmoid is a type of sigmoid function that maps input values to a range between 0 and 1."</data>
      <data key="d2">cc7c60d8e36838743d509a97c9ac3a4b</data>
    </node>
    <node id="&quot;TANH FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Tanh Function is a type of sigmoid function that maps input values to a range between -1 and 1."</data>
      <data key="d2">cc7c60d8e36838743d509a97c9ac3a4b</data>
    </node>
    <node id="&quot;STATE COLLECTION MATRIX&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"State Collection Matrix is a data structure used to store extended system states during the training of the ESN."</data>
      <data key="d2">f18a060e6d2bb1da70432cbc71378770</data>
    </node>
    <node id="&quot;SYSTEM EQUATIONS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"System Equations are mathematical expressions used to describe the behavior of the ESN, including the reservoir and input states, and the output activation function."
"System Equations are mathematical representations used in the model to describe its behavior."</data>
      <data key="d2">6a4432cd530b28770e2b903fe242a0d1,f18a060e6d2bb1da70432cbc71378770</data>
    </node>
    <node id="&quot;OUTPUT FEEDBACK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Output Feedback is a feature of the model that involves writing correct outputs into the output units during the generation of system states."</data>
      <data key="d2">6a4432cd530b28770e2b903fe242a0d1</data>
    </node>
    <node id="&quot;DESIRED OUTPUTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Desired Outputs are the target values that the model aims to produce."</data>
      <data key="d2">6a4432cd530b28770e2b903fe242a0d1</data>
    </node>
    <node id="&quot;DESIRED OUTPUT WEIGHTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Desired Output Weights are the linear regression weights of the desired outputs on the harvested extended states."</data>
      <data key="d2">6a4432cd530b28770e2b903fe242a0d1</data>
    </node>
    <node id="&quot;PSEUDOINVERSE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Pseudoinverse is a mathematical operation used to compute the desired output weights in the model."
"The Pseudoinverse is a mathematical concept used in the computation of output weights in the context of a reservoir."</data>
      <data key="d2">6a4432cd530b28770e2b903fe242a0d1,a9f53979e9dbe6b936ff3374c73006dd</data>
    </node>
    <node id="&quot;LINEAR SIGNAL PROCESSING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Linear Signal Processing is a concept mentioned as a method for computing output weights."</data>
      <data key="d2">a9f53979e9dbe6b936ff3374c73006dd</data>
    </node>
    <node id="&quot;ADDITIVE-SIGMOID NEURON RESERVOIRS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Additive-Sigmoid Neuron Reservoirs are a type of reservoir mentioned in the context of the Echo State Property."</data>
      <data key="d2">a9f53979e9dbe6b936ff3374c73006dd</data>
    </node>
    <node id="&quot;JAEGER 2003&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Jaeger 2003 is a reference mentioned as a source for online adaptive methods used to compute output weights."</data>
      <data key="d2">a9f53979e9dbe6b936ff3374c73006dd</data>
    </node>
    <node id="&quot;OUTPUT WEIGHTS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">a9f53979e9dbe6b936ff3374c73006dd</data>
    </node>
    <node id="&quot;ESP&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"ESP stands for Echo State Property, which is a characteristic of a Reservoir Weight Matrix in the context of Recurrent Neural Networks."
"ESP is an abbreviation for Echo State Property."</data>
      <data key="d2">3b592e5ac113a5c031925f91a182baa6,578045eb341c5e05d5a912f634854499</data>
    </node>
    <node id="&quot;JAEGER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Jaeger is a researcher mentioned in the text, contributing to the understanding of Echo State Properties and Recurrent Neural Networks."
"Jaeger is a person mentioned in the text, likely a researcher or author."
"Jaeger is a researcher mentioned in the text, contributing to the theoretical research on Echo State Networks."
"Jaeger is a researcher mentioned in the text, contributing to the development of ESN Models and the concept of state noise."
"Jaeger is a person mentioned in the text who has contributed to the development of Echo State Networks."
"Jaeger is a person mentioned in the text, likely a researcher or author."
"Jaeger is not explicitly mentioned in the text, but the context suggests that he might be associated with Echo State Networks or Reservoir Computing."
"Jaeger is a reference to H. Jaeger, who is likely a researcher or author mentioned in the context of the timeseries model."
"Jaeger is a researcher who contributed to the development of Reservoir Computing and Echo State Networks."
"Jaeger is a person who firstly formulated the concept of RC networks within the ML community."
"Jaeger is mentioned as the author of Echo State Networks (ESNs), a popular flavor of RC."
"Jaeger is a researcher mentioned in the context of Echo State Networks, a type of recurrent neural network."
"Jaeger is a person who proposed a mechanism for controlling the dynamics of reservoirs."
"Jaeger is a researcher and developer of the Echo State Network (ESN) model, which is used in the provided code."</data>
      <data key="d2">257d4cf08ffc32b99856b6e31fa4221e,295606b4bc5d12929a913a3c79f93734,2dca9849c50a439b0637cf370afde7dd,3b592e5ac113a5c031925f91a182baa6,3edbc4fd903a173282dd592f5e8437d1,418f92b0dd08e03a20637ffec8193bfc,52d001cd1786e3d9f36e0c57538bc21e,5972cf7d440b1c3fdc0f05fca305f18d,5a9eaff8c67e594f49fae0318a502c6a,6de297d888d10db4c987b5eafc6398b2,a1adb5de4156f0a4a448caf79056e886,bc2d4d6bb706c3d06ffd2c9c2f362104,d4563a00dc04ebf7bcf01e5062fde46f,eafe89ad19a57846f953a1dfcf8571f8</data>
    </node>
    <node id="&quot;BUEHNER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Buehner is a researcher mentioned in the text, contributing to the algebraic conditions for additive-sigmoid neuron reservoirs."</data>
      <data key="d2">3b592e5ac113a5c031925f91a182baa6</data>
    </node>
    <node id="&quot;YILDIZ&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Yildiz is a researcher mentioned in the text, contributing to the algebraic conditions for a specific subclass of reservoirs."</data>
      <data key="d2">3b592e5ac113a5c031925f91a182baa6</data>
    </node>
    <node id="&quot;MANJUNATH&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Manjunath is a researcher mentioned in the text, exploring the relationship between input signal characteristics and the ESP."</data>
      <data key="d2">3b592e5ac113a5c031925f91a182baa6</data>
    </node>
    <node id="&quot;LEAKY INTEGRATOR NEURONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Leaky Integrator Neurons are a type of neuron mentioned in the text, and their algebraic conditions are spelled out in a research paper by Jaeger et al."</data>
      <data key="d2">3b592e5ac113a5c031925f91a182baa6</data>
    </node>
    <node id="&quot;MANJUNATH AND JAEGER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Manjunath and Jaeger are the authors of a study exploring the relationship between input signal characteristics and the ESP."</data>
      <data key="d2">d4563a00dc04ebf7bcf01e5062fde46f</data>
    </node>
    <node id="&quot;INPUT SIGNAL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input Signal refers to the signal used as input in the context of Echo State Networks and memory capacity."</data>
      <data key="d2">d4563a00dc04ebf7bcf01e5062fde46f</data>
    </node>
    <node id="&quot;OUTPUT SIGNAL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Output Signal refers to the signal produced by the Echo State Network as a result of processing the input signal."</data>
      <data key="d2">d4563a00dc04ebf7bcf01e5062fde46f</data>
    </node>
    <node id="&quot;MEMORY CAPACITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Memory Capacity is a term used to describe the ability of an Echo State Network to retain and recall information from past input signals."
"Memory Capacity is a measure of a reservoir computing model's ability to store and recall patterns from the input data."
"Memory Capacity refers to the ability of a system to store and retrieve information, which is being measured in the provided data."
"Memory Capacity is a concept used in the analysis, referring to the ability of the neural networks to store and recall information."</data>
      <data key="d2">24b347e60cb01aea26f46f3067f5a0f0,c53825a1ab5e01a794a428988435a7a7,d4563a00dc04ebf7bcf01e5062fde46f,f16792dcee6dab8ea8f8c8c6793bbc3d</data>
    </node>
    <node id="&quot;WHITE&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"White is a researcher mentioned in the text, contributing to the theoretical research on Echo State Networks."</data>
      <data key="d2">5a9eaff8c67e594f49fae0318a502c6a</data>
    </node>
    <node id="&quot;SOMPOLINSKY&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Sompolinsky is a researcher mentioned in the text, contributing to the theoretical research on Echo State Networks."</data>
      <data key="d2">5a9eaff8c67e594f49fae0318a502c6a</data>
    </node>
    <node id="&quot;HERMANS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Hermans is a researcher mentioned in the text, contributing to the theoretical research on Echo State Networks."</data>
      <data key="d2">5a9eaff8c67e594f49fae0318a502c6a</data>
    </node>
    <node id="&quot;SCHRAUWEN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Schrauwen is a researcher mentioned in the text, contributing to the theoretical research on Echo State Networks."</data>
      <data key="d2">5a9eaff8c67e594f49fae0318a502c6a</data>
    </node>
    <node id="&quot;SCHMIDHUBER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Schmidhuber is a researcher mentioned in the text, contributing to the theoretical research on Echo State Networks and their limitations."
"Schmidhuber is a researcher who contributed to the development of Long Short Term Memory (LSTM) cells for training Recurrent Neural Networks."
"Schmidhuber is a researcher who developed methods for training recurrent neural networks."
"Schmidhuber is a person who contributed to the development of recurrent neural networks in the 1980s and early 1990s."</data>
      <data key="d2">0bb54b1de8d2297293defe94addb8076,5a9eaff8c67e594f49fae0318a502c6a,6bbaf3df0fa2fac979f6d6a64abb2e91,6de297d888d10db4c987b5eafc6398b2</data>
    </node>
    <node id="&quot;MAASS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Maass is a researcher mentioned in the text, contributing to the theoretical research on Echo State Networks and their ability to realize unbounded memory spans."
"Maass is a researcher who contributed to the development of Echo State Networks and their applications in Machine Learning."</data>
      <data key="d2">5a9eaff8c67e594f49fae0318a502c6a,6de297d888d10db4c987b5eafc6398b2</data>
    </node>
    <node id="&quot;NATSCHLAEGER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Natschlaeger is a researcher mentioned in the text, contributing to the theoretical research on Echo State Networks."</data>
      <data key="d2">5a9eaff8c67e594f49fae0318a502c6a</data>
    </node>
    <node id="&quot;MARKRAM&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Markram is a researcher mentioned in the text, contributing to the theoretical research on Echo State Networks."</data>
      <data key="d2">5a9eaff8c67e594f49fae0318a502c6a</data>
    </node>
    <node id="&quot;BOUNDED MEMORY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Bounded Memory refers to the limitation of Echo State Networks in handling tasks that require unbounded-time memory."</data>
      <data key="d2">5a9eaff8c67e594f49fae0318a502c6a</data>
    </node>
    <node id="&quot;UNBOUNDED MEMORY SPANS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Unbounded Memory Spans refer to the ability of Echo State Networks to handle tasks that require memory spans beyond their initial size."</data>
      <data key="d2">5a9eaff8c67e594f49fae0318a502c6a</data>
    </node>
    <node id="&quot;MAASS, JOSHI &amp; SONTAG&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Maass, Joshi &amp; Sontag are the authors of a research paper on ESNs and their theoretical properties."</data>
      <data key="d2">e805d3f438bd9c485639f1c69f917ae5</data>
    </node>
    <node id="&quot;PASCANU &amp; JAEGER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Pascanu &amp; Jaeger are the authors of a research paper on an ESN-based model of working memory."</data>
      <data key="d2">e805d3f438bd9c485639f1c69f917ae5</data>
    </node>
    <node id="&quot;MAASS, NATSCHLAEGER &amp; MARKRAM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Maass, Natschlaeger &amp; Markram are the authors of a research paper on Liquid State Machines, a theoretical framework related to ESNs."</data>
      <data key="d2">e805d3f438bd9c485639f1c69f917ae5</data>
    </node>
    <node id="&quot;NONLINEAR MODELING TASKS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Nonlinear Modeling Tasks refer to the practical application of ESNs in complex, nonlinear systems."</data>
      <data key="d2">e805d3f438bd9c485639f1c69f917ae5</data>
    </node>
    <node id="&quot;TEST ERROR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Test Error refers to the performance metric used to evaluate the accuracy of a model on unseen data."</data>
      <data key="d2">e805d3f438bd9c485639f1c69f917ae5</data>
    </node>
    <node id="&quot;VALIDATION SET&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Validation Set is a portion of the original training data withheld for monitoring the performance of a model during training."
"The Validation Set is a portion of the Original Training Data withheld for evaluating the performance of the ESN Models."</data>
      <data key="d2">5972cf7d440b1c3fdc0f05fca305f18d,e805d3f438bd9c485639f1c69f917ae5</data>
    </node>
    <node id="&quot;ESN MODELS&quot;">
      <data key="d0" />
      <data key="d1">
"ESN Models are a type of machine learning model used for prediction and data analysis."</data>
      <data key="d2">5972cf7d440b1c3fdc0f05fca305f18d,e805d3f438bd9c485639f1c69f917ae5</data>
    </node>
    <node id="&quot;ORIGINAL TRAINING DATA&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Original Training Data is the initial dataset used to train the ESN Models."</data>
      <data key="d2">5972cf7d440b1c3fdc0f05fca305f18d</data>
    </node>
    <node id="&quot;STATE NOISE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"State Noise is a method used to stabilize solutions in ESN Models with output feedback, adding computational expense but potentially improving performance."</data>
      <data key="d2">5972cf7d440b1c3fdc0f05fca305f18d</data>
    </node>
    <node id="&quot;IDENTITY MATRIX&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Identity Matrix is a mathematical concept used in the text to describe a specific method."</data>
      <data key="d2">2dca9849c50a439b0637cf370afde7dd</data>
    </node>
    <node id="&quot;LUKO&#352;EVI&#268;IUS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Luko&#353;evi&#269;ius is a person mentioned in the text who has written about practical techniques for optimizing Echo State Networks."</data>
      <data key="d2">2dca9849c50a439b0637cf370afde7dd</data>
    </node>
    <node id="&quot;REAL-TIME RECURRENT LEARNING ALGORITHM&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Real-time Recurrent Learning Algorithm is an algorithm mentioned in the text for supervised training of RNNs."</data>
      <data key="d2">2dca9849c50a439b0637cf370afde7dd</data>
    </node>
    <node id="&quot;REAL-TIME RECURRENT LEARNING&quot;">
      <data key="d0">"ALGORITHM"</data>
      <data key="d1">"Real-time Recurrent Learning is a supervised training algorithm for RNNs that adapts all connections using gradient descent, known since the early 1990s."
"Real-Time Recurrent Learning is a method developed for training recurrent neural networks."
"Real-Time Recurrent Learning is a learning algorithm for Recurrent Neural Networks."</data>
      <data key="d2">2a2a93486d6198ce228e77e120dc3c0c,6bbaf3df0fa2fac979f6d6a64abb2e91,a4b801e70cf2ba3a3101d34899450087</data>
    </node>
    <node id="&quot;BACKPROPAGATION THROUGH TIME&quot;">
      <data key="d0">"ALGORITHM"</data>
      <data key="d1">"Backpropagation Through Time is a supervised training algorithm for RNNs that adapts all connections using gradient descent, introduced in 1990."
"Backpropagation Through Time (BPTT) is a method used in training recurrent neural networks to calculate gradients."
"Backpropagation through Time is a method developed for training recurrent neural networks."
"Backpropagation Through Time is a learning algorithm for Recurrent Neural Networks."</data>
      <data key="d2">0bb54b1de8d2297293defe94addb8076,2a2a93486d6198ce228e77e120dc3c0c,6bbaf3df0fa2fac979f6d6a64abb2e91,a4b801e70cf2ba3a3101d34899450087</data>
    </node>
    <node id="&quot;EXTENDED KALMAN FILTERING BASED METHODS&quot;">
      <data key="d0">"ALGORITHM"</data>
      <data key="d1">"Extended Kalman Filtering Based Methods is a supervised training algorithm for RNNs that adapts all connections, developed by Puskorius and Feldkamp in 2004."</data>
      <data key="d2">2a2a93486d6198ce228e77e120dc3c0c</data>
    </node>
    <node id="&quot;ATIYA-PARLOS ALGORITHM&quot;">
      <data key="d0">"ALGORITHM"</data>
      <data key="d1">"The Atiya-Parlos Algorithm is a supervised training algorithm for RNNs that adapts all connections using gradient descent, introduced by Atiya and Parlos in 2000."</data>
      <data key="d2">2a2a93486d6198ce228e77e120dc3c0c</data>
    </node>
    <node id="&quot;DEEP LEARNING&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Deep Learning is a subset of machine learning that involves training deep neural networks with multiple layers."</data>
      <data key="d2">eafe89ad19a57846f953a1dfcf8571f8</data>
    </node>
    <node id="&quot;HAAS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Haas is a person mentioned in the text, likely a researcher or author."
"Haas is a researcher and developer of the Echo State Network (ESN) model, which is used in the provided code."</data>
      <data key="d2">52d001cd1786e3d9f36e0c57538bc21e,eafe89ad19a57846f953a1dfcf8571f8</data>
    </node>
    <node id="&quot;DOYA&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Doya is a person mentioned in the text, likely a researcher or author."</data>
      <data key="d2">eafe89ad19a57846f953a1dfcf8571f8</data>
    </node>
    <node id="&quot;KUDITHIPUDI ET AL.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Kudithipudi et al. are a group of researchers mentioned in the text."</data>
      <data key="d2">eafe89ad19a57846f953a1dfcf8571f8</data>
    </node>
    <node id="&quot;ANTONELO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Antonelo is a person mentioned in the text, likely a researcher or author."</data>
      <data key="d2">eafe89ad19a57846f953a1dfcf8571f8</data>
    </node>
    <node id="&quot;2010&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"2010 is the year mentioned when Echo State Networks started to gain relevance and popularity."</data>
      <data key="d2">257d4cf08ffc32b99856b6e31fa4221e</data>
    </node>
    <node id="&quot;OPTICAL MICROCHIPS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Optical Microchips are a type of non-digital computational substrate that has been used in conjunction with Echo State Networks."
"Optical Microchips are a type of non-digital computer substrate that can be used as a reservoir in ESNs."
"Optical Microchips are a type of object used as a nonlinear reservoir."</data>
      <data key="d2">257d4cf08ffc32b99856b6e31fa4221e,4b89d9404fd683ecd03d5846ee2d86ce,7767d42e08c8eab856e8e3025c692309</data>
    </node>
    <node id="&quot;MECHANICAL NANO-OSCILLATORS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Mechanical Nano-oscillators are a type of non-digital computational substrate that has been used in conjunction with Echo State Networks."</data>
      <data key="d2">257d4cf08ffc32b99856b6e31fa4221e</data>
    </node>
    <node id="&quot;MEMRISTOR-BASED NEUROMORPHIC MICROCHIPS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Memristor-based Neuromorphic Microchips are a type of non-digital computational substrate that has been used in conjunction with Echo State Networks."</data>
      <data key="d2">257d4cf08ffc32b99856b6e31fa4221e</data>
    </node>
    <node id="&quot;CARBON-NANOTUBE / POLYMER MIXTURES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Carbon-nanotube / Polymer Mixtures are a type of non-digital computational substrate that has been used in conjunction with Echo State Networks."</data>
      <data key="d2">257d4cf08ffc32b99856b6e31fa4221e</data>
    </node>
    <node id="&quot;ARTIFICIAL SOFT LIMBS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Artificial Soft Limbs are a type of non-digital computational substrate that has been used in conjunction with Echo State Networks."
"Artificial Soft Limbs are a type of non-digital computer substrate that can be used as a reservoir in ESNs."
"Artificial Soft Limbs are a type of object used as a nonlinear reservoir."</data>
      <data key="d2">257d4cf08ffc32b99856b6e31fa4221e,4b89d9404fd683ecd03d5846ee2d86ce,7767d42e08c8eab856e8e3025c692309</data>
    </node>
    <node id="&quot;BIOSIGNAL PROCESSING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Biosignal Processing is an application area where Echo State Networks have been used, as mentioned by Kudithipudi et al. (2015)."</data>
      <data key="d2">257d4cf08ffc32b99856b6e31fa4221e</data>
    </node>
    <node id="&quot;REMOTE SENSING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Remote Sensing is an application area where Echo State Networks have been used, as mentioned by Antonelo (2017)."</data>
      <data key="d2">257d4cf08ffc32b99856b6e31fa4221e</data>
    </node>
    <node id="&quot;ROBOT MOTOR CONTROL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Robot Motor Control is an application area where Echo State Networks have been used, as mentioned by Polydoros et al. (2015)."</data>
      <data key="d2">257d4cf08ffc32b99856b6e31fa4221e</data>
    </node>
    <node id="&quot;B&#220;RGER ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"B&#252;rger et al. is a group of researchers mentioned in the text."</data>
      <data key="d2">dbca0570761b1698d32f0c0bfb593b1a</data>
    </node>
    <node id="&quot;DALE ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Dale et al. is a group of researchers mentioned in the text."</data>
      <data key="d2">dbca0570761b1698d32f0c0bfb593b1a</data>
    </node>
    <node id="&quot;NAKAJIMA, HAUSER AND PFEIFER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Nakajima, Hauser and Pfeifer is a group of researchers mentioned in the text."</data>
      <data key="d2">dbca0570761b1698d32f0c0bfb593b1a</data>
    </node>
    <node id="&quot;ESN METHODS&quot;">
      <data key="d0">"TECHNIQUE"</data>
      <data key="d1">"ESN methods are a type of computational method mentioned in the text."</data>
      <data key="d2">dbca0570761b1698d32f0c0bfb593b1a</data>
    </node>
    <node id="&quot;LIQUID STATE MACHINE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Liquid State Machine is a concept mentioned in the text."
"Liquid State Machine is a variant of Echo State Networks for spiking neurons."</data>
      <data key="d2">423cdb622c47fa8cec25f22eb9f9f01f,dbca0570761b1698d32f0c0bfb593b1a</data>
    </node>
    <node id="&quot;CENI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Ceni is an author of the paper who contributed to the development of the Edge of Stability Echo State Network."</data>
      <data key="d2">578045eb341c5e05d5a912f634854499</data>
    </node>
    <node id="&quot;GALLICCHIO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Gallicchio is an author of the paper who contributed to the development of the Edge of Stability Echo State Network."</data>
      <data key="d2">578045eb341c5e05d5a912f634854499</data>
    </node>
    <node id="&quot;EDGE OF STABILITY ECHO STATE NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Edge of Stability Echo State Network is a model developed by Ceni and Gallicchio, which is based on the Echo State Property principle and aims to balance the fading memory property with the ability to retain information."</data>
      <data key="d2">578045eb341c5e05d5a912f634854499</data>
    </node>
    <node id="&quot;ES2N&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"ES2N is an abbreviation for Edge of Stability Echo State Network."
"ES2N is a new architecture of Echo State Networks introduced to combine the fading memory property with the ability to retain as much memory as possible."
"ES2N is a class representing a neural network node with specific parameters and functions for forward propagation and initialization."
"ES2N is a model used for time series forecasting, which takes input data and predicts future values."
"ES2N is a reservoir model used in the analysis, which includes a mathematical analysis of its Jacobian matrix."
"ES2N is a system or model described in the text, used for a specific calculation or analysis."
"ES2N is a system or model mentioned in the text, which appears to be used for a specific calculation or analysis."
"ES2N is a machine learning model used in the provided code."
"ES2N is a model used for time series prediction, which is mentioned in the code."
"ES2N is a type of reservoir computing model that uses a sparse reservoir and a ridge regression output layer."
"ES2N is a variant of the ESN model that incorporates a proximity parameter to improve performance."
"ES2N is a variant of the ESN model, incorporating a proximity parameter to enhance its performance."
"ES2N is a machine learning model, likely a technology or method used for prediction and analysis."
"ES2N is a model mentioned in the text, potentially related to data processing or analysis."
"ES2N is a machine learning model used for time series forecasting, developed by the author of the code."
"ES2N is a machine learning model used for time series prediction, with specific hyperparameters and a noisy_tanh activation function."</data>
      <data key="d2">1298c65a923053e1de35aacddc13832c,24b347e60cb01aea26f46f3067f5a0f0,3a8ed31ef360d5587cc6411e9fce89d4,4a7ca13b3f869961817e2aa723e67d24,50d2bcd5726d191f4fee831a1c02fee1,578045eb341c5e05d5a912f634854499,5f841065cf74ef7bbc28efb775d5585e,854761a5b5b5b90af10bc6b6c76cc355,90c1a399dd410f75f1f4bb03fe1f5f33,96366d7c23d50de6294c54c3444eac86,97f5d2e9d34b3b50f8e922fc4bb7f824,9dd8e12c7acbe10cf34817ff14780d24,a614fa3f8de82d4078f220e5f373f03a,b49cfd8b041b59aa70d9ef614256eab2,c53825a1ab5e01a794a428988435a7a7,cedd895dd1ddc09e70c8799effdf7427</data>
    </node>
    <node id="&quot;STANDARD ESN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Standard ESN is a common architecture of Echo State Networks that uses a nonlinear reservoir layer."</data>
      <data key="d2">50d2bcd5726d191f4fee831a1c02fee1</data>
    </node>
    <node id="&quot;LINEAR RESERVOIR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Linear Reservoir is a component of ES2N that implements an orthogonal transformation."</data>
      <data key="d2">50d2bcd5726d191f4fee831a1c02fee1</data>
    </node>
    <node id="&quot;NONLINEAR RESERVOIR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Nonlinear Reservoir is a component of ES2N that is a convex combination of a nonlinear reservoir, similar to the standard ESN."
"Nonlinear Reservoir is a concept used to describe the objects mentioned, which are used in a nonlinear context."</data>
      <data key="d2">50d2bcd5726d191f4fee831a1c02fee1,7767d42e08c8eab856e8e3025c692309</data>
    </node>
    <node id="&quot;JACOBIAN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Jacobian refers to the matrix of all first-order partial derivatives of a vector-valued function."
"Jacobian is a mathematical concept used to represent the derivative of a vector-valued function."
"Jacobian is a mathematical concept used in the text, which is a matrix of first-order partial derivatives of a vector-valued function."</data>
      <data key="d2">50d2bcd5726d191f4fee831a1c02fee1,9dd8e12c7acbe10cf34817ff14780d24,b49cfd8b041b59aa70d9ef614256eab2</data>
    </node>
    <node id="&quot;EDGE-OF-CHAOS REGIME&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Edge-of-Chaos Regime refers to a state of dynamical systems that are on the edge of chaos, exhibiting complex and unpredictable behavior."</data>
      <data key="d2">50d2bcd5726d191f4fee831a1c02fee1</data>
    </node>
    <node id="&quot;AUTOREGRESSIVE NONLINEAR MODELING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Autoregressive Nonlinear Modeling is a technique used to predict future values based on past values of a time series."</data>
      <data key="d2">50d2bcd5726d191f4fee831a1c02fee1</data>
    </node>
    <node id="&quot;ES&#178;N&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ES&#178;N is a model proposed by Andrea Ceni and Claudio Gallicchio, which is an extension of the leaky echo state network (ESN) model."
"ES&#178;N is an organization mentioned in the context of optimizing a delay metric in a graph."</data>
      <data key="d2">7cb18067f7d75fd3cd20998c669a1741,ca59dc92d05a69002373e96e0a373215</data>
    </node>
    <node id="&quot;LEAKY ECHO STATE NETWORK (ESN)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Leaky Echo State Network (ESN) is a model used in the context of the ES&#178;N model, which is a reservoir model with a specific equation."</data>
      <data key="d2">ca59dc92d05a69002373e96e0a373215</data>
    </node>
    <node id="&quot;ANDREA CENI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Andrea Ceni is a co-author of the paper proposing the ES&#178;N model."</data>
      <data key="d2">ca59dc92d05a69002373e96e0a373215</data>
    </node>
    <node id="&quot;CLAUDIO GALLICCHIO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Claudio Gallicchio is a co-author of the paper proposing the ES&#178;N model."</data>
      <data key="d2">ca59dc92d05a69002373e96e0a373215</data>
    </node>
    <node id="&quot;ES&#178;N MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ES&#178;N Model is a mathematical model proposed in the text, consisting of a reservoir equation and a node creation process."</data>
      <data key="d2">5af3ee18603a7d8c50828262ddeee5eb</data>
    </node>
    <node id="&quot;LEAKY ECHO STATE NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Leaky Echo State Network is a type of recurrent neural network mentioned in the text, which shares a similar hyperparameter with the ES&#178;N Model."</data>
      <data key="d2">5af3ee18603a7d8c50828262ddeee5eb</data>
    </node>
    <node id="&quot;RANDOM ORTHOGONAL MATRIX GENERATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Random Orthogonal Matrix Generation is a process described in the text, involving the generation of a random matrix and its QR factorization."</data>
      <data key="d2">5af3ee18603a7d8c50828262ddeee5eb</data>
    </node>
    <node id="&quot;NODE CREATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Node Creation is a process described in the text, involving the calculation of a state and the application of an activation function."
"Node Creation is a process used to initialize nodes in a network, setting their input and output dimensions and parameters."</data>
      <data key="d2">5af3ee18603a7d8c50828262ddeee5eb,81b4162953007160e40ec76b84d045eb</data>
    </node>
    <node id="&quot;&#913;&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"&#945; is a hyperparameter in the ES&#178;N Model, representing the leak rate."</data>
      <data key="d2">5af3ee18603a7d8c50828262ddeee5eb</data>
    </node>
    <node id="&quot;&#914;&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"&#946; is a hyperparameter in the ES&#178;N Model, serving a role similar to that of a proximity parameter in the Leaky Echo State Network."</data>
      <data key="d2">5af3ee18603a7d8c50828262ddeee5eb</data>
    </node>
    <node id="&quot;QR FACTORIZATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"QR Factorization is a method used to decompose a matrix into the product of an orthogonal matrix and an upper triangular matrix."</data>
      <data key="d2">81b4162953007160e40ec76b84d045eb</data>
    </node>
    <node id="&quot;FORWARD FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Forward Function is a method used to calculate the output of a node based on its input, activation function, and proximity."</data>
      <data key="d2">81b4162953007160e40ec76b84d045eb</data>
    </node>
    <node id="&quot;INITIALIZE FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Initialize Function is used to set the input and output dimensions of a node and initialize its parameters, such as W and Win."</data>
      <data key="d2">81b4162953007160e40ec76b84d045eb</data>
    </node>
    <node id="&quot;RANDOM ORTHOGONAL FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Random Orthogonal Function generates a random orthogonal matrix, which is used in the QR Factorization method."</data>
      <data key="d2">81b4162953007160e40ec76b84d045eb</data>
    </node>
    <node id="&quot;NODE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Node is a class that ES2N inherits from, suggesting a hierarchical structure or relationship."
"A Node is a recurrent operator that can carry parameters and has a defined function for mapping internal state and input vector to the next state."
"Node is a class in reservoirpy that represents a unit in a network, which can be used for high-level object-oriented API or defined as functions for custom training/inference policies."</data>
      <data key="d2">b03e2cc6fe2648e792c1d5f1ec5773a3,cedd895dd1ddc09e70c8799effdf7427,dc46bcef51e88747b544f7efb111203a</data>
    </node>
    <node id="&quot;PROXIMITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Proximity is a parameter in the ES2N class, likely affecting the distance or similarity measure used in the network."
"Proximity is a parameter mentioned in the text, likely used in the configuration of the ES2N system."
"proximity is a parameter used in the ES2N machine learning model, likely representing a mathematical concept related to its architecture."</data>
      <data key="d2">96366d7c23d50de6294c54c3444eac86,9dd8e12c7acbe10cf34817ff14780d24,cedd895dd1ddc09e70c8799effdf7427</data>
    </node>
    <node id="&quot;ACTIVATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Activation is a parameter in the ES2N class, likely specifying the activation function used in the neural network node."</data>
      <data key="d2">cedd895dd1ddc09e70c8799effdf7427</data>
    </node>
    <node id="&quot;O&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"O is a parameter in the ES2N class, likely representing an orthogonal matrix used in the network."</data>
      <data key="d2">cedd895dd1ddc09e70c8799effdf7427</data>
    </node>
    <node id="&quot;ES2&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ES2 is a machine learning model mentioned in the text, which uses a kernel function and a Jacobian function."</data>
      <data key="d2">4faea507802b94c6fc0f3e4bf8bafb95</data>
    </node>
    <node id="&quot;TANH&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"Tanh is a mathematical function mentioned in the text, which is used as an activation function in the ES2 model."</data>
      <data key="d2">4faea507802b94c6fc0f3e4bf8bafb95</data>
    </node>
    <node id="&quot;KERNEL FUNCTION&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"Kernel Function is a mathematical function mentioned in the text, which is used in the ES2 model."</data>
      <data key="d2">4faea507802b94c6fc0f3e4bf8bafb95</data>
    </node>
    <node id="&quot;JACOBIAN FUNCTION&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"Jacobian Function is a mathematical function mentioned in the text, which is used in the ES2 model."</data>
      <data key="d2">4faea507802b94c6fc0f3e4bf8bafb95</data>
    </node>
    <node id="&quot;EIGENVALUES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Eigenvalues are a concept mentioned in the text, which are visualized in the context of the ES2 model."</data>
      <data key="d2">4faea507802b94c6fc0f3e4bf8bafb95</data>
    </node>
    <node id="&quot;LEAKY ESN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Leaky ESN is a variant of a system mentioned in the text, which seems to have additional parameters or configurations."
"Leaky ESN is a modification of the ESN model that introduces a leaking rate parameter to improve stability."</data>
      <data key="d2">5f841065cf74ef7bbc28efb775d5585e,b49cfd8b041b59aa70d9ef614256eab2</data>
    </node>
    <node id="&quot;ES2N MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ES2N Model is a machine learning model used for time series prediction and evaluation of memory capacity."</data>
      <data key="d2">ba1721161cafb25a7f10d8113f419612</data>
    </node>
    <node id="&quot;PEARSON CORRELATION COEFFICIENT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Pearson Correlation Coefficient is a statistical measure used to evaluate the linear relationship between two variables."
"Pearson Correlation Coefficient is a statistical measure used to evaluate the linear relationship between two variables."</data>
      <data key="d2">3edbc4fd903a173282dd592f5e8437d1,ba1721161cafb25a7f10d8113f419612</data>
    </node>
    <node id="&quot;INPUT SIGNAL U&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input Signal u is the time series data used to evaluate the memory capacity of the ES2N Model."</data>
      <data key="d2">ba1721161cafb25a7f10d8113f419612</data>
    </node>
    <node id="&quot;DELAY K&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Delay k is the time lag used to reproduce the input signal u with a delay of k (u[t-k])."</data>
      <data key="d2">ba1721161cafb25a7f10d8113f419612</data>
    </node>
    <node id="&quot;MEMORY CAPACITY SCORE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Memory Capacity Score is the sum of MC scores over all k, where MC score is defined as the squared Pearson correlation coefficient between the predicted output and the delayed input signal."</data>
      <data key="d2">ba1721161cafb25a7f10d8113f419612</data>
    </node>
    <node id="&quot;KTH_MEMORY_CAPACITY&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"kth_memory_capacity is a function defined in the provided code to calculate the kth memory capacity."</data>
      <data key="d2">a614fa3f8de82d4078f220e5f373f03a</data>
    </node>
    <node id="&quot;MEMORY_CAPACITY&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"memory_capacity is a function defined in the provided code to calculate memory capacities."</data>
      <data key="d2">a614fa3f8de82d4078f220e5f373f03a</data>
    </node>
    <node id="&quot;IDENTITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"identity is a mathematical concept mentioned in the code, but its specific role or meaning is not clear from the provided text."
"Identity is a mathematical concept mentioned in the text, potentially related to the analysis of data or models."</data>
      <data key="d2">3a8ed31ef360d5587cc6411e9fce89d4,97f5d2e9d34b3b50f8e922fc4bb7f824</data>
    </node>
    <node id="&quot;LINEAR ESN&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Linear ESN is a variant of the ESN model that uses the identity function as the activation function for the reservoir nodes."</data>
      <data key="d2">c53825a1ab5e01a794a428988435a7a7</data>
    </node>
    <node id="&quot;ORTHOGONAL ESN&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Orthogonal ESN is a variant of the ESN model that uses an orthogonal reservoir matrix to improve the model's performance."</data>
      <data key="d2">c53825a1ab5e01a794a428988435a7a7</data>
    </node>
    <node id="&quot;RANDOM ORTHOGONAL MATRIX&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Random Orthogonal Matrix is a matrix with orthogonal columns, used to initialize the reservoir matrix in the Orthogonal ESN model."</data>
      <data key="d2">c53825a1ab5e01a794a428988435a7a7</data>
    </node>
    <node id="&quot;LINEARESN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"linearESN is a type of memory capacity model used in the text, which includes specifications for its reservoir and ridge components."
"LinearESN is a variant of the ESN model that uses a linear activation function in the reservoir layer."</data>
      <data key="d2">385037cd24deb1be217b7e1db823ce9f,5f841065cf74ef7bbc28efb775d5585e</data>
    </node>
    <node id="&quot;MC_ORTHOESN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"mc_orthoESN is a variant of the memory capacity model, with a different reservoir matrix and input connectivity."</data>
      <data key="d2">385037cd24deb1be217b7e1db823ce9f</data>
    </node>
    <node id="&quot;MC_LINEARSCR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"mc_linearSCR is another variant of the memory capacity model, with a different reservoir matrix and input connectivity."</data>
      <data key="d2">385037cd24deb1be217b7e1db823ce9f</data>
    </node>
    <node id="&quot;MC_ESN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"mc_ESN is a type of memory capacity model mentioned in the text, but its specifications are not provided."</data>
      <data key="d2">385037cd24deb1be217b7e1db823ce9f</data>
    </node>
    <node id="&quot;MC_ES2N&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"mc_ES2N is a type of memory capacity model mentioned in the text, but its specifications are not provided."</data>
      <data key="d2">385037cd24deb1be217b7e1db823ce9f</data>
    </node>
    <node id="&quot;PLT&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"plt is a person or entity mentioned in the text, but no specific role or description is provided."
"plt is a library or module used for creating visualizations, as indicated by its use in the provided code."
"plt is a library in Python used for creating visualizations, such as graphs and charts."
"plt is a library in Python used for creating visualizations."</data>
      <data key="d2">12d680622df43439e6de83058b734953,385037cd24deb1be217b7e1db823ce9f,593306edfb8d4c7ef4b99d24fa009970,b496f2a8e7e239e6d3313a892de8e648</data>
    </node>
    <node id="&quot;ORTHOESN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"OrthoESN is a variant of the ESN model that uses an orthogonal initialization method for the reservoir weights."</data>
      <data key="d2">5f841065cf74ef7bbc28efb775d5585e</data>
    </node>
    <node id="&quot;LINEARSCR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"LinearSCR is a type of sparse coding-based reservoir computing model that uses a linear activation function."
"LinearSCR likely refers to a Linear State-space Closed-loop Reservoir, a type of reservoir computing system."</data>
      <data key="d2">5f841065cf74ef7bbc28efb775d5585e,6425e3620184116a3ee92d5690e4f891</data>
    </node>
    <node id="&quot;MATPLOTLIB&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Matplotlib is a library used for creating visualizations in Python, enabling the plotting of data and the generation of graphs and charts."
"Matplotlib is a popular data visualization library in Python, used to create visualizations such as heatmaps."
"Matplotlib is a library used for creating visualizations of the time series prediction results, as demonstrated in the provided code."
"Matplotlib is a library used for creating visualizations in Python, as mentioned in the text."
"Matplotlib is a library used for creating visualizations in Python, used to plot the sine wave data."
"Matplotlib is a library used for creating visualizations in Python, which is used in the provided code for plotting data."
"Matplotlib is a library used for creating visualizations, such as the sine wave plot."
"Matplotlib is a library used for creating static, animated, and interactive visualizations in Python."
"Matplotlib is a library used for creating static, animated, and interactive visualizations in Python."
"Matplotlib is a library used for creating visualizations in Python."
"Matplotlib is a popular data visualization library in Python used to create the timeseries and phase diagram plots."
"Matplotlib is a popular data visualization library in Python."
"Matplotlib is a library used for creating visualizations in the provided code."</data>
      <data key="d2">2f4c992d69812866e6fce6dbb52d8612,34b9ce80a22112b32e063179511af6e0,4073cafddb73621f26061385c5570659,50d4e4aab1823b8df6573ccf227f24d0,5f841065cf74ef7bbc28efb775d5585e,71f966d00b6d0eceb580d00b9cb86b1e,8294eed5fc10df1c118f9afa266910e4,8648b5740b93d805f139d9745e1171e8,a58317c7e13f27d513fc7671fd187ecb,c5c29ba06a5cc70a086c2c2c8858e5aa,d2cc4243ed9b1bb887527f7cc1153033,e396354e3a9be76616392af11f56e671,ef9bf350e25daa8f123b0b5c4d60de5f</data>
    </node>
    <node id="&quot;PROXIMITY PARAMETER&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Proximity Parameter is a parameter used in the ES2N model, which is being varied in the provided data."</data>
      <data key="d2">24b347e60cb01aea26f46f3067f5a0f0</data>
    </node>
    <node id="&quot;ES^2N&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ES^2N is a variant of ESN, used for comparison in the analysis, also focusing on memory capacity."
"ES^2N is a variant of the ESN model, with different hyperparameters such as spectral radius and input scaling."
"ES^2N is a variant of the ESN neural network, with modifications to enhance its performance."
"ES^2N is a variant of the ESN model, also used for time series prediction, as seen in the provided text."
"ES^2N is a variant of ESN that incorporates additional features for improved time series prediction, as demonstrated in the provided code."
"ES^2N is a model or algorithm used for prediction, as indicated by its use in the provided code."</data>
      <data key="d2">12d680622df43439e6de83058b734953,4fb25ea25c60216b307931b5edacc5cb,6a7bea5f60347ea864c06adc327829dc,857acb0c734bd5d7f1fec5f8f7aeb2cc,d2cc4243ed9b1bb887527f7cc1153033,f16792dcee6dab8ea8f8c8c6793bbc3d</data>
    </node>
    <node id="&quot;RESEARCHER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"The researcher is the author of the text, conducting an analysis using ESN and ES^2N."
"The researcher is the individual who is working on reconstructing the z coordinate of the Lorenz system from the x and y coordinates."
"Researcher is the individual analyzing the data and running the model."</data>
      <data key="d2">60639eb7c0f26a58e503c93e29c050b3,715720b663e85c5e16cbf8b1ef4ec208,f16792dcee6dab8ea8f8c8c6793bbc3d</data>
    </node>
    <node id="&quot;HYPER-PARAMETERS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Hyper-parameters are settings that are used in the analysis, such as the number of units, input scalings, spectral radii, and alphas."</data>
      <data key="d2">f16792dcee6dab8ea8f8c8c6793bbc3d</data>
    </node>
    <node id="&quot;ANALYSIS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">f16792dcee6dab8ea8f8c8c6793bbc3d</data>
    </node>
    <node id="&quot;RNG&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"rng is a random number generator used to generate various parameters for the dataset."</data>
      <data key="d2">f3e58b69b1a93175e3094a2ba65c0429</data>
    </node>
    <node id="&quot;U&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"u is a one-dimensional array of random numbers used as input data for the dataset."
"u is a variable used in the provided code, which could represent a data point or a mathematical object."</data>
      <data key="d2">7b8e1f350eefb392053be12f35fe7daf,f3e58b69b1a93175e3094a2ba65c0429</data>
    </node>
    <node id="&quot;TAUS&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"TAUS is an array of integers representing different time lags used in the dataset."</data>
      <data key="d2">f3e58b69b1a93175e3094a2ba65c0429</data>
    </node>
    <node id="&quot;NUS&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"NUS is an array of exponential values representing different frequencies used in the dataset."
"NUS is a parameter derived from an exponential function, potentially representing a sub-component or organization within the larger system."</data>
      <data key="d2">abd3ca2db22004a5de548dc22010c4d4,f3e58b69b1a93175e3094a2ba65c0429</data>
    </node>
    <node id="&quot;MAX_TAU&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"MAX_TAU is a parameter used in a mathematical model, likely a part of a larger system or organization."</data>
      <data key="d2">abd3ca2db22004a5de548dc22010c4d4</data>
    </node>
    <node id="&quot;LEAKYESN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"leakyESN is a model or algorithm used in the system, likely a component of the larger organization."</data>
      <data key="d2">abd3ca2db22004a5de548dc22010c4d4</data>
    </node>
    <node id="&quot;ALPHAS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ALPHAS is a parameter used in the machine learning model, likely representing a group or organization that contributes to its development."</data>
      <data key="d2">96366d7c23d50de6294c54c3444eac86</data>
    </node>
    <node id="&quot;ALPHA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"alpha is a parameter used in the machine learning model, likely representing a mathematical concept related to its architecture."
"Alpha is a mathematical concept mentioned in the text, potentially related to the analysis of data or models."</data>
      <data key="d2">96366d7c23d50de6294c54c3444eac86,97f5d2e9d34b3b50f8e922fc4bb7f824</data>
    </node>
    <node id="&quot;PHAS[I]&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"PHAS[i] is a model or system mentioned in the text, potentially related to data processing or analysis."</data>
      <data key="d2">97f5d2e9d34b3b50f8e922fc4bb7f824</data>
    </node>
    <node id="&quot;RING_MATRIX&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Ring matrix is a mathematical concept mentioned in the text, potentially related to the analysis of data or models."</data>
      <data key="d2">97f5d2e9d34b3b50f8e922fc4bb7f824</data>
    </node>
    <node id="&quot;POWERNORM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"PowerNorm is a normalization technique used to enhance the visualization of data."</data>
      <data key="d2">6425e3620184116a3ee92d5690e4f891</data>
    </node>
    <node id="&quot;NON-LINEARITY STRENGTH&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Non-linearity strength is a parameter used to adjust the complexity of the reservoir computing system."</data>
      <data key="d2">6425e3620184116a3ee92d5690e4f891</data>
    </node>
    <node id="&quot;DELAY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Delay is a parameter used to introduce a time lag in the reservoir computing system."</data>
      <data key="d2">6425e3620184116a3ee92d5690e4f891</data>
    </node>
    <node id="&quot;MULTIPLE SUPERIMPOSED OSCILLATORS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Multiple Superimposed Oscillators are a system consisting of multiple oscillators that interact with each other in an auto-regressive node."
"Multiple Superimposed Oscillators is a technique mentioned in the code, likely used for generating signals."</data>
      <data key="d2">7cb18067f7d75fd3cd20998c669a1741,c761a4e63ae52c3953bea0dde6eb3319</data>
    </node>
    <node id="&quot;AUTO-REGRESSIVE NODE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Auto-regressive Node is a system where the output of a process depends on its previous values."
"Auto-Regressive Node is a term used in the context of graph theory, likely referring to a node that depends on its own previous values."</data>
      <data key="d2">7cb18067f7d75fd3cd20998c669a1741,c761a4e63ae52c3953bea0dde6eb3319</data>
    </node>
    <node id="&quot;MSO8&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"MSO8 is a function mentioned in the code, likely generating a signal using the MSO technique with specific frequencies."</data>
      <data key="d2">7cb18067f7d75fd3cd20998c669a1741</data>
    </node>
    <node id="&quot;NOISY TANH&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Noisy Tanh is a function mentioned in the code, likely applying a noisy version of the hyperbolic tangent function."</data>
      <data key="d2">7cb18067f7d75fd3cd20998c669a1741</data>
    </node>
    <node id="&quot;GRAPH THEORY&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">7cb18067f7d75fd3cd20998c669a1741</data>
    </node>
    <node id="&quot;HYPERBOLIC TANGENT FUNCTION&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">7cb18067f7d75fd3cd20998c669a1741</data>
    </node>
    <node id="&quot;NP.TANH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"np.tanh is a hyperparameter used in the ESN and ES^2N models, representing the hyperbolic tangent activation function."
"np.tanh is a hyperbolic tangent function from the NumPy library, used as an activation function in machine learning models."
"np.tanh is a function from the NumPy library used to apply the hyperbolic tangent activation function to the ESN, as demonstrated in the provided code."</data>
      <data key="d2">1298c65a923053e1de35aacddc13832c,6a7bea5f60347ea864c06adc327829dc,d2cc4243ed9b1bb887527f7cc1153033</data>
    </node>
    <node id="&quot;NOISY_TANH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"noisy_tanh is a variant of the hyperbolic tangent activation function used in the ESN model, introducing noise to enhance the model's performance."
"noisy_tanh is an activation function used in machine learning models, introducing noise to improve performance."</data>
      <data key="d2">1298c65a923053e1de35aacddc13832c,6a7bea5f60347ea864c06adc327829dc</data>
    </node>
    <node id="&quot;MULTIPLE SUPERIMPOSED OSCILLATOR TASK&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Multiple Superimposed Oscillator Task is a time series prediction problem used to compare the performance of ESN and ES^2N."
"The Multiple Superimposed Oscillator Task is a task mentioned in the provided text, likely related to time series prediction."</data>
      <data key="d2">4fb25ea25c60216b307931b5edacc5cb,857acb0c734bd5d7f1fec5f8f7aeb2cc</data>
    </node>
    <node id="&quot;Y_PRED_ES2N&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"y_pred_es2n is the predicted output data from the ES2N machine learning model."
"y_pred_es2n is a variable representing the predicted test data using the ES^2N model."</data>
      <data key="d2">1298c65a923053e1de35aacddc13832c,12d680622df43439e6de83058b734953</data>
    </node>
    <node id="&quot;Y_PRED_ESN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"y_pred_esn is the predicted output data from the ESN machine learning model."
"y_pred_esn is a variable representing the predicted test data using the ESN model."</data>
      <data key="d2">1298c65a923053e1de35aacddc13832c,12d680622df43439e6de83058b734953</data>
    </node>
    <node id="&quot;TEST_STEPS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"TEST_STEPS is an event or process that is being visualized using the plt library."</data>
      <data key="d2">b496f2a8e7e239e6d3313a892de8e648</data>
    </node>
    <node id="&quot;INRIA BORDEAUX SUD-OUEST&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Inria Bordeaux Sud-Ouest is an organization where Nathan Trouvain and Xavier Hinaut work, contributing to reservoirpy."
"Inria Bordeaux Sud-Ouest is a research organization mentioned in the text, which is part of the IMN and LaBRI."</data>
      <data key="d2">136559fd2a1fbef4cc8a6b11abcb3eef,18910a60b2547ec3133340f42c45bb47</data>
    </node>
    <node id="&quot;IMN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"IMN is a department at Inria Bordeaux Sud-Ouest where Nathan Trouvain and Xavier Hinaut work, contributing to reservoirpy."</data>
      <data key="d2">18910a60b2547ec3133340f42c45bb47</data>
    </node>
    <node id="&quot;LABRI&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"LaBRI is a research unit at Inria Bordeaux Sud-Ouest where Nathan Trouvain and Xavier Hinaut work, contributing to reservoirpy."</data>
      <data key="d2">18910a60b2547ec3133340f42c45bb47</data>
    </node>
    <node id="&quot;HAL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents."</data>
      <data key="d2">18910a60b2547ec3133340f42c45bb47</data>
    </node>
    <node id="&quot;OFFLINE LEARNING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Offline Learning is a method in Machine Learning where the model is trained on a complete dataset before making predictions, allowing for batch processing."</data>
      <data key="d2">6de297d888d10db4c987b5eafc6398b2</data>
    </node>
    <node id="&quot;DOMINEY&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Dominey is a researcher who contributed to the development of Reservoir Computing and its applications in signal processing."
"Dominey is a person who contributed to the concept of RC networks in the Neuroscience field."</data>
      <data key="d2">418f92b0dd08e03a20637ffec8193bfc,6de297d888d10db4c987b5eafc6398b2</data>
    </node>
    <node id="&quot;BUONOMANO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Buonomano is a researcher who contributed to the development of Reservoir Computing and its applications in neuroscience."</data>
      <data key="d2">6de297d888d10db4c987b5eafc6398b2</data>
    </node>
    <node id="&quot;HOCHREITER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Hochreiter is a researcher who contributed to the development of Long Short Term Memory (LSTM) cells for training Recurrent Neural Networks."
"Hochreiter is a researcher who developed methods for training recurrent neural networks."
"Hochreiter is a person who contributed to the development of recurrent neural networks in the 1980s and early 1990s."</data>
      <data key="d2">0bb54b1de8d2297293defe94addb8076,6bbaf3df0fa2fac979f6d6a64abb2e91,6de297d888d10db4c987b5eafc6398b2</data>
    </node>
    <node id="&quot;LONG SHORT TERM MEMORY&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">6de297d888d10db4c987b5eafc6398b2</data>
    </node>
    <node id="&quot;RC NETWORKS&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"RC networks are a type of technology that use a reservoir of computations based on non-linear combinations of inputs."</data>
      <data key="d2">418f92b0dd08e03a20637ffec8193bfc</data>
    </node>
    <node id="&quot;MAASS ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Maass et al. is an organization that contributed to the formulation of RC networks within the ML community."</data>
      <data key="d2">418f92b0dd08e03a20637ffec8193bfc</data>
    </node>
    <node id="&quot;BUONOMANO AND MERZENICH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Buonomano and Merzenich is an organization that contributed to the concept of RC networks in the Neuroscience field."</data>
      <data key="d2">418f92b0dd08e03a20637ffec8193bfc</data>
    </node>
    <node id="&quot;DEEP LEARNING FRAMEWORKS&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Deep Learning frameworks are technologies that are commonly used to replicate RNN techniques."
"Deep Learning frameworks are mentioned as tools that can be used to easily replicate RNN techniques, but they are not commonly used for RC models."</data>
      <data key="d2">295606b4bc5d12929a913a3c79f93734,418f92b0dd08e03a20637ffec8193bfc</data>
    </node>
    <node id="&quot;LSTMS&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"LSTMs are a type of technology that competes with state-of-the-art RNN methods like RNNs."</data>
      <data key="d2">418f92b0dd08e03a20637ffec8193bfc</data>
    </node>
    <node id="&quot;TROUVAIN AND HINAUT&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Trouvain and Hinaut are mentioned as the authors of a paper comparing methods, including RNN techniques like LSTMs, with RC models."</data>
      <data key="d2">295606b4bc5d12929a913a3c79f93734</data>
    </node>
    <node id="&quot;RNN TECHNIQUES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"RNN techniques are mentioned as methods that can be easily replicated using popular Deep Learning frameworks, such as LSTMs."</data>
      <data key="d2">295606b4bc5d12929a913a3c79f93734</data>
    </node>
    <node id="&quot;RC MODELS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"RC models are mentioned as methods that are often implemented from scratch due to a lack of common libraries, and they do not benefit from the automatic differentiation features of Deep Learning tools."
"RC models are constructed using Reservoir Computing methods, which can be implemented using nodes from the reservoirpy library or by creating new ones."</data>
      <data key="d2">295606b4bc5d12929a913a3c79f93734,d622f95153798af8bb6f485db54aaea3</data>
    </node>
    <node id="&quot;ML TECHNIQUES&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">295606b4bc5d12929a913a3c79f93734</data>
    </node>
    <node id="&quot;COMPLEX NEURAL NETWORKS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">295606b4bc5d12929a913a3c79f93734</data>
    </node>
    <node id="&quot;RECURRENT OPERATOR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"A Recurrent Operator is a node that maps its internal state and input vector to the next state."</data>
      <data key="d2">dc46bcef51e88747b544f7efb111203a</data>
    </node>
    <node id="&quot;LMS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"LMS is a tool in reservoirpy that learns connections using Least Mean Square for a readout layer of neurons, allowing online learning."</data>
      <data key="d2">b03e2cc6fe2648e792c1d5f1ec5773a3</data>
    </node>
    <node id="&quot;RLS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"RLS is a tool in reservoirpy that learns connections using Recursive Least Square for a readout layer of neurons, allowing online learning."</data>
      <data key="d2">b03e2cc6fe2648e792c1d5f1ec5773a3</data>
    </node>
    <node id="&quot;NON-LINEAR VECTOR AUTOREGRESSIVE MACHINE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Non-Linear Vector Autoregressive machine is a recent reservoir reformulation in reservoirpy."</data>
      <data key="d2">b03e2cc6fe2648e792c1d5f1ec5773a3</data>
    </node>
    <node id="&quot;SUSSILLO AND ABBOTT&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Sussillo and Abbott are mentioned in the context of the Recursive Least Square learning algorithm used in ReservoirPy."</data>
      <data key="d2">fcac967511cf2b019fd856e23d2e91d9</data>
    </node>
    <node id="&quot;GAUTHIER ET AL.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Gauthier et al. are mentioned as the authors of the Non-Linear Vector Autoregressive machine used in ReservoirPy."
"Gauthier et al. is the author group of the paper 'Next Generation Reservoir Computing' that introduces the Nonlinear Vector Auto-regressive method."
"Gauthier et al. is an organization mentioned as the source of the figure and legend."
"Gauthier et al. is a group of researchers who contributed to the study of the Lorenz Attractor."</data>
      <data key="d2">244217eb4738aae272df8949bdaaf131,41ea479401d7ff7c83c9d38c91d76cd9,74dd26ac71a37c92f3eda8552701ca33,fcac967511cf2b019fd856e23d2e91d9</data>
    </node>
    <node id="&quot;PYRCN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"PyRCN is a related open-source software mentioned for comparison with ReservoirPy."
"PyRCN is an open-source library that defines a complete interface to train Extreme Learning Machine (ELM), a network similar to ESN where recurrence has been removed."
"PyRCN is a software that defines a complete interface to train Extreme Learning Machine (ELM) and allows the design of complex models."</data>
      <data key="d2">15e969cdc81fd313d389558850d0c8ec,a1adb5de4156f0a4a448caf79056e886,fcac967511cf2b019fd856e23d2e91d9</data>
    </node>
    <node id="&quot;ECHOTORCH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"EchoTorch is a related open-source software mentioned for comparison with ReservoirPy."
"EchoTorch is an open-source software that allows users to manipulate conceptors, a mechanism enabling to control the dynamics of reservoirs proposed by Jaeger (2014)."</data>
      <data key="d2">15e969cdc81fd313d389558850d0c8ec,fcac967511cf2b019fd856e23d2e91d9</data>
    </node>
    <node id="&quot;RESERVOIRCOMPUTING.JL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ReservoirComputing.jl is a related open-source software mentioned for comparison with ReservoirPy."
"ReservoirComputing.jl is an open-source library for reservoir computing in Julia, a high-level, high-performance dynamic programming language."
"ReservoirComputing.jl is a software for Reservoir Computing, developed by the Reservoir Computing Group at the University of Reading."
"ReservoirComputing.jl is a library mentioned in the text, potentially used for reservoir computing tasks."
"ReservoirComputing.jl is an efficient Julia-based implementation of various types of echo state networks."</data>
      <data key="d2">15e969cdc81fd313d389558850d0c8ec,a1adb5de4156f0a4a448caf79056e886,a4b801e70cf2ba3a3101d34899450087,d7ac2f6fb13af389417785f2f3152c52,fcac967511cf2b019fd856e23d2e91d9</data>
    </node>
    <node id="&quot;PYTORCH-ES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Pytorch-es is a related open-source software mentioned for comparison with ReservoirPy."</data>
      <data key="d2">fcac967511cf2b019fd856e23d2e91d9</data>
    </node>
    <node id="&quot;PYTORCH-ESN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Pytorch-esn is an open-source library for Echo State Networks (ESNs) implemented in PyTorch, a popular machine learning library."
"PyTorch-ESN is an open-source software for Reservoir Computing, developed by Stefano Lugli and available on GitHub."</data>
      <data key="d2">15e969cdc81fd313d389558850d0c8ec,a1adb5de4156f0a4a448caf79056e886</data>
    </node>
    <node id="&quot;DEEPESN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"DeepESN is an open-source library for deep Echo State Networks (ESNs), a type of recurrent neural network."</data>
      <data key="d2">15e969cdc81fd313d389558850d0c8ec</data>
    </node>
    <node id="&quot;RCNET&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"RCNet is a library for reservoir computing, providing features such as online learning and delayed connections."</data>
      <data key="d2">15e969cdc81fd313d389558850d0c8ec</data>
    </node>
    <node id="&quot;LSM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"LSM is an open-source library specializing in handling spiking neural networks."
"LSM specializes in handling spiking neural networks."</data>
      <data key="d2">15e969cdc81fd313d389558850d0c8ec,a1adb5de4156f0a4a448caf79056e886</data>
    </node>
    <node id="&quot;OGER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Oger is a historical package for reservoir computing, no longer maintained."
"Oger is the publication source for the research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d2">15e969cdc81fd313d389558850d0c8ec,b8d8b71875a9ccc508b40fe4aad8d796</data>
    </node>
    <node id="&quot;STEINER ET AL.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Steiner et al. are the authors of PyRCN, an open-source library for reservoir computing."</data>
      <data key="d2">15e969cdc81fd313d389558850d0c8ec</data>
    </node>
    <node id="&quot;SCHAETTI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Schaetti is the author of EchoTorch, an open-source software for reservoir computing."</data>
      <data key="d2">15e969cdc81fd313d389558850d0c8ec</data>
    </node>
    <node id="&quot;MARTINUZZI ET AL.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Martinuzzi et al. are the authors of ReservoirComputing.jl, an open-source library for reservoir computing in Julia."</data>
      <data key="d2">15e969cdc81fd313d389558850d0c8ec</data>
    </node>
    <node id="&quot;GALLICCHIO ET AL.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Gallicchio et al. are the authors of DeepESN, an open-source library for deep Echo State Networks (ESNs)."</data>
      <data key="d2">15e969cdc81fd313d389558850d0c8ec</data>
    </node>
    <node id="&quot;VERSTRAETEN ET AL.&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Verstraeten et al. are the authors of Oger, a historical package for reservoir computing."</data>
      <data key="d2">15e969cdc81fd313d389558850d0c8ec</data>
    </node>
    <node id="&quot;ELM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ELM is a network similar to ESN where recurrence has been removed, proposed by Huang et al. in 2011."</data>
      <data key="d2">a1adb5de4156f0a4a448caf79056e886</data>
    </node>
    <node id="&quot;NET&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"NET is an open-source software developed by Oleg Kozelsky and available on GitHub."</data>
      <data key="d2">a1adb5de4156f0a4a448caf79056e886</data>
    </node>
    <node id="&quot;SPIKING NEURAL NETWORKS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">a1adb5de4156f0a4a448caf79056e886</data>
    </node>
    <node id="&quot;OLEG KOZELSKY&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">a1adb5de4156f0a4a448caf79056e886</data>
    </node>
    <node id="&quot;UNIVERSITY OF READING&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">a1adb5de4156f0a4a448caf79056e886</data>
    </node>
    <node id="&quot;RIDGE LINEAR REGRESSION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Ridge linear regression is a method mentioned in the text, used as a readout node in the reservoir computing network."</data>
      <data key="d2">d7ac2f6fb13af389417785f2f3152c52</data>
    </node>
    <node id="&quot;JAMES BERGSTRA&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"James Bergstra is a contributor to reservoirpy, known for his work on optimizing machine learning algorithms."
"James Bergstra is an author of Hyperopt, a python library for optimizing the hyperparameters of machine learning algorithms."</data>
      <data key="d2">82de30f43839f4985de20a981b524af1,a3a74dc4754a8c8b0730f808285893e2</data>
    </node>
    <node id="&quot;DAN YAMINS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Dan Yamins is a contributor to reservoirpy, known for his work on optimizing machine learning algorithms."
"Dan Yamins is an author of Hyperopt, a python library for optimizing the hyperparameters of machine learning algorithms."</data>
      <data key="d2">82de30f43839f4985de20a981b524af1,a3a74dc4754a8c8b0730f808285893e2</data>
    </node>
    <node id="&quot;DAVID D COX&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"David D Cox is a contributor to reservoirpy, known for his work on optimizing machine learning algorithms."
"David D Cox is an author of Hyperopt, a python library for optimizing the hyperparameters of machine learning algorithms."</data>
      <data key="d2">82de30f43839f4985de20a981b524af1,a3a74dc4754a8c8b0730f808285893e2</data>
    </node>
    <node id="&quot;PROCEED&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Proceed is an event mentioned in the text, likely a conference or publication, where the Hyperopt library is presented."</data>
      <data key="d2">a3a74dc4754a8c8b0730f808285893e2</data>
    </node>
    <node id="&quot;LARS BUITINCK&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Lars Buitinck is an author of a paper on API design for machine learning software, highlighting experiences from the scikit-learn project."</data>
      <data key="d2">82de30f43839f4985de20a981b524af1</data>
    </node>
    <node id="&quot;DV BUONOMANO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"DV Buonomano is an author of a scientific paper on temporal information transformation into a spatial code by a neural network."
"DV Buonomano is an author of a scientific paper on transforming temporal information into a spatial code using a neural network."</data>
      <data key="d2">82de30f43839f4985de20a981b524af1,ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;M. M. MERZENICH&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"M. M. Merzenich is an author of a scientific paper on temporal information transformation into a spatial code by a neural network."
"M. M. Merzenich is a co-author of a scientific paper on transforming temporal information into a spatial code using a neural network."</data>
      <data key="d2">82de30f43839f4985de20a981b524af1,ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;P. DOMINEY&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"P. Dominey is an author of a scientific paper on complex sensory-motor systems."
"P. Dominey is an author of a scientific paper on complex sensory-motor sequence learning based on recurrent state representation and reinforcement learning."</data>
      <data key="d2">82de30f43839f4985de20a981b524af1,ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;API DESIGN FOR MACHINE LEARNING SOFTWARE&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"API Design for Machine Learning Software is a paper that highlights experiences from the scikit-learn project in designing APIs for machine learning software."</data>
      <data key="d2">82de30f43839f4985de20a981b524af1</data>
    </node>
    <node id="&quot;TEMPORAL INFORMATION TRANSFORMATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Temporal Information Transformation is a scientific concept explored in a paper by DV Buonomano and M. M. Merzenich, involving the transformation of temporal information into a spatial code by a neural network."</data>
      <data key="d2">82de30f43839f4985de20a981b524af1</data>
    </node>
    <node id="&quot;COMPLEX SENSORY-MOTOR SYSTEMS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Complex Sensory-Motor Systems is a scientific concept explored in a paper by P. Dominey."</data>
      <data key="d2">82de30f43839f4985de20a981b524af1</data>
    </node>
    <node id="&quot;C. GALLICCHIO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"C. Gallicchio is an author of a scientific paper on designing deep echo state networks."</data>
      <data key="d2">ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;A. MICHELI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"A. Micheli is a co-author of a scientific paper on designing deep echo state networks."</data>
      <data key="d2">ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;L. PEDRELLI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"L. Pedrelli is a co-author of a scientific paper on designing deep echo state networks."</data>
      <data key="d2">ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;SEPP HOCHREITER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Sepp Hochreiter is an author of a scientific paper on long short-term memory."
"Sepp Hochreiter is an author of a research paper on long short-term memory."</data>
      <data key="d2">c8b7bd13cf99920ecce56cb563910cb3,ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;J&#220;RGEN SCHMIDHUBER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"J&#252;rgen Schmidhuber is a co-author of a scientific paper on long short-term memory."
"J&#252;rgen Schmidhuber is an author of a research paper on long short-term memory."</data>
      <data key="d2">c8b7bd13cf99920ecce56cb563910cb3,ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;DANIEL J. GAUTHIER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Daniel J. Gauthier is an author of a scientific paper on next generation reservoir computing."
"Daniel J. Gauthier is an author of the paper on Next Generation Reservoir Computing."</data>
      <data key="d2">88b1448c89d0650dad09fe96cca86cee,ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;ERIK BOLLT&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Erik Bollt is a co-author of a scientific paper on next generation reservoir computing."
"Erik Bollt is an author of the paper on Next Generation Reservoir Computing."</data>
      <data key="d2">88b1448c89d0650dad09fe96cca86cee,ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;AARON GRIFFITH&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Aaron Griffith is a co-author of a scientific paper on next generation reservoir computing."
"Aaron Grif&#64257;th is an author of the paper on Next Generation Reservoir Computing."</data>
      <data key="d2">88b1448c89d0650dad09fe96cca86cee,ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;WENDSON A. S. BARBOSA&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Wendson A. S. Barbosa is a co-author of a scientific paper on next generation reservoir computing."
"Wendson A. S. Barbosa is an author of the paper on Next Generation Reservoir Computing."</data>
      <data key="d2">88b1448c89d0650dad09fe96cca86cee,ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;GREGOR M. HOERZER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Gregor M. Hoerzer is an author of a scientific paper on a specific topic, but the relationship description and strength are not explicitly stated in the text."
"Gregor M. Hoerzer is an author of a research paper on emergence of complex computational structures from chaotic neural networks."</data>
      <data key="d2">c8b7bd13cf99920ecce56cb563910cb3,ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;ROBERT LEGENSTEIN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Robert Legenstein is a co-author of a scientific paper on a specific topic, but the relationship description and strength are not explicitly stated in the text."
"Robert Legenstein is an author of a research paper on emergence of complex computational structures from chaotic neural networks."</data>
      <data key="d2">c8b7bd13cf99920ecce56cb563910cb3,ce7b58ffc7f43f36bc78154597d01903</data>
    </node>
    <node id="&quot;S. BARBOSA&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"S. Barbosa is an author of a research paper on next generation reservoir computing."</data>
      <data key="d2">c8b7bd13cf99920ecce56cb563910cb3</data>
    </node>
    <node id="&quot;GUANG-BIN HUANG&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Guang-Bin Huang is an author of a research paper on extreme learning machines."</data>
      <data key="d2">c8b7bd13cf99920ecce56cb563910cb3</data>
    </node>
    <node id="&quot;DIAN HUI WANG&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Dian Hui Wang is an author of a research paper on extreme learning machines."</data>
      <data key="d2">c8b7bd13cf99920ecce56cb563910cb3</data>
    </node>
    <node id="&quot;YUAN LAN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Yuan Lan is an author of a research paper on extreme learning machines."</data>
      <data key="d2">c8b7bd13cf99920ecce56cb563910cb3</data>
    </node>
    <node id="&quot;HINAUT H. TROUVAIN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Hinaut H. Trouvain is an author of a research paper on the echo state approach to analyzing and training recurrent neural networks."</data>
      <data key="d2">c8b7bd13cf99920ecce56cb563910cb3</data>
    </node>
    <node id="&quot;JACQUES KAISER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Jacques Kaiser is an author of a research paper mentioned in the text."
"Jacques Kaiser is a researcher who published a paper on scaling up liquid state machines to predict over address events from dynamic vision sensors."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3,c8b7bd13cf99920ecce56cb563910cb3</data>
    </node>
    <node id="&quot;RAINER STAL&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Rainer Stal is an author of a research paper mentioned in the text."
"Rainer Stal is a researcher who published a paper on scaling up liquid state machines."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3,c8b7bd13cf99920ecce56cb563910cb3</data>
    </node>
    <node id="&quot;NEURAL COMPUTATION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Neural Computation is a journal where a research paper on long short-term memory was published."</data>
      <data key="d2">c8b7bd13cf99920ecce56cb563910cb3</data>
    </node>
    <node id="&quot;GMD TECH. REPORT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"GMD Tech. Report is a research report published by the German National Research Center for Information Technology."</data>
      <data key="d2">c8b7bd13cf99920ecce56cb563910cb3</data>
    </node>
    <node id="&quot;CORR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"CoRR is a research archive where a paper on controlling recurrent neural networks by conceptors is published."</data>
      <data key="d2">c8b7bd13cf99920ecce56cb563910cb3</data>
    </node>
    <node id="&quot;INT. J. MACH. LEARN. &amp; CYBER.&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Int. J. Mach. Learn. &amp; Cyber. is a journal where a research paper on extreme learning machines is published."</data>
      <data key="d2">c8b7bd13cf99920ecce56cb563910cb3</data>
    </node>
    <node id="&quot;GERMAN NATIONAL RESEARCH CENTER FOR INFORMATION TECHNOLOGY GMD&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"GMD is a German research center that published a report on the 'echo state' approach for training recurrent neural networks."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;ANAND SUBRAMONEY&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Anand Subramoney is a researcher who published a paper on scaling up liquid state machines."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;ARNE ROENNAU&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Arne Roennau is a researcher who published a paper on scaling up liquid state machines."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;R&#220;DIGER DILL-MANN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"R&#252;diger Dill-mann is a researcher who published a paper on scaling up liquid state machines."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;W. MAASS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"W. Maass is a researcher who published a paper on real-time computing without stable states: A new framework for neural computation based on perturbations."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;T. NATSCHL&#168;AGER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"T. Natschl&#168;ager is a researcher who published a paper on real-time computing without stable states."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;H. MARKRAM&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"H. Markram is a researcher who published a paper on real-time computing without stable states."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;FRANCESCO MARTINUZZI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Francesco Martinuzzi is a researcher who published a paper on reservoircomputing.jl:"</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;CHRIS RACKAUCKAS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Chris Rackauckas is a researcher who published a paper on reservoircomputing.jl:"</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;ANAS ABDELREHIM&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Anas Abdelrehim is a researcher who published a paper on reservoircomputing.jl:"</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;MIGUEL D. MAHECHA&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Miguel D. Mahecha is a researcher who published a paper on reservoircomputing.jl:"</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;KARIN MORA&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Karin Mora is a researcher who published a paper on reservoircomputing.jl:"</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;BONN, GERMANY&quot;">
      <data key="d0">"GEO"</data>
      <data key="d1">"Bonn, Germany is the location where German National Research Center for Information Technology GMD is based."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;GMD TECH. REPORT, 148:34, 2001&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"GMD Tech. Report, 148:34, 2001 is a report published by German National Research Center for Information Technology GMD on the 'echo state' approach for training recurrent neural networks."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;CORR, ABS/1403.3369, 2014&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"CoRR, abs/1403.3369, 2014 is a paper published by Herbert Jaeger on controlling recurrent neural networks by conceptors."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;BIOINSPIRATION &amp; BIOMIMETICS, 12(5):055001, SEP 2017&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Bioinspiration &amp; Biomimetics, 12(5):055001, sep 2017 is a paper published by Jacques Kaiser and others on scaling up liquid state machines to predict over address events from dynamic vision sensors."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;NEURAL COMPUTATION, 14(11):2531&#8211;2560, 2002&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Neural computation, 14(11):2531&#8211;2560, 2002 is a paper published by W. Maass and others on real-time computing without stable states: A new framework for neural computation based on perturbations."</data>
      <data key="d2">546551fd625e354e9afe1245e060bed3</data>
    </node>
    <node id="&quot;W. MAASS, T. NATSCHL&#168;AGER, AND H. MARKRAM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"This entity is a group of authors who contributed to a research paper on real-time computing without stable states."</data>
      <data key="d2">ea62f994886333282704a19ebf0469ea</data>
    </node>
    <node id="&quot;FRANCESCO MARTINUZZI, CHRIS RACKAUCKAS, ANAS ABDELREHIM, MIGUEL D. MAHECHA, AND KARIN MORA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"This entity is a group of authors who developed a library for reservoir computing models, published in 2022."</data>
      <data key="d2">ea62f994886333282704a19ebf0469ea</data>
    </node>
    <node id="&quot;NILS SCHAETTI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Nils Schaetti is the developer of Echotorch, a reservoir computing library with pytorch, published in 2018."</data>
      <data key="d2">ea62f994886333282704a19ebf0469ea</data>
    </node>
    <node id="&quot;BENJAMIN SCHRAUWEN, MARION WARDERMANN, DAVID VERSTRAETEN, JOCHEN J. STEIL, AND DIRK STROOBANDT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"This entity is a group of authors who published a research paper on improving reservoirs using intrinsic plasticity in 2008."</data>
      <data key="d2">ea62f994886333282704a19ebf0469ea</data>
    </node>
    <node id="&quot;JOCHEN J STEIL&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Jochen J Steil is an author who published a research paper on online reservoir adaptation by intrinsic plasticity for backpropagation&#8211;decorrelation and echo state learning in 2007."
"Jochen J Steil is an author of a research paper on online reservoir adaptation using intrinsic plasticity."</data>
      <data key="d2">7751d435f46a6d4027a1c96edabb9626,ea62f994886333282704a19ebf0469ea</data>
    </node>
    <node id="&quot;PETER STEINER, AZARAKHSH JALALVAND, SIMON STONE, AND PETER BIRKHOLZ&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"This entity is a group of authors who developed Pyrcn, a toolbox for exploration and analysis of reservoir computing models."</data>
      <data key="d2">ea62f994886333282704a19ebf0469ea</data>
    </node>
    <node id="&quot;REAL-TIME COMPUTING WITHOUT STABLE STATES PAPER&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">ea62f994886333282704a19ebf0469ea</data>
    </node>
    <node id="&quot;RESERVOIRCOMPUTING.JL LIBRARY&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">ea62f994886333282704a19ebf0469ea</data>
    </node>
    <node id="&quot;ECHOTORCH LIBRARY&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">ea62f994886333282704a19ebf0469ea</data>
    </node>
    <node id="&quot;IMPROVING RESERVOIRS USING INTRINSIC PLASTICITY PAPER&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">ea62f994886333282704a19ebf0469ea</data>
    </node>
    <node id="&quot;ONLINE RESERVOIR ADAPTATION PAPER&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">ea62f994886333282704a19ebf0469ea</data>
    </node>
    <node id="&quot;PYRCN TOOLBOX&quot;">
      <data key="d0" />
      <data key="d1">
"Pyrcn Toolbox is a toolbox for exploration and application of reservoir computing networks, developed by a group of authors."</data>
      <data key="d2">7751d435f46a6d4027a1c96edabb9626,ea62f994886333282704a19ebf0469ea</data>
    </node>
    <node id="&quot;PETER STEINER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Peter Steiner is an author of a toolbox for exploration and application of reservoir computing networks."</data>
      <data key="d2">7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;AZARAKHSH JALALVAND&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Azarakhsh Jalalvand is an author of a toolbox for exploration and application of reservoir computing networks."</data>
      <data key="d2">7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;SIMON STONE&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Simon Stone is an author of a toolbox for exploration and application of reservoir computing networks."</data>
      <data key="d2">7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;PETER BIRKHOLZ&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Peter Birkholz is an author of a toolbox for exploration and application of reservoir computing networks."</data>
      <data key="d2">7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;DAVID SUSSILLO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"David Sussillo is an author of a research paper on generating coherent patterns of activity from chaotic neural networks."</data>
      <data key="d2">7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;L. F. ABBOTT&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"L. F. Abbott is an author of a research paper on generating coherent patterns of activity from chaotic neural networks."</data>
      <data key="d2">7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;SANDER DIELEMAN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Sander Dieleman is an author of a research paper on oger: modular learning architectures for large-scale sequential processing."</data>
      <data key="d2">7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;PHILEMON BRAKEL&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Philemon Brakel is an author of a research paper on oger: modular learning architectures for large-scale sequential processing."</data>
      <data key="d2">7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;PIETER BUTENEERS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Pieter Buteneers is an author of a research paper on oger: modular learning architectures for large-scale sequential processing."</data>
      <data key="d2">7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;DEJAN PECEVSKI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Dejan Pecevski is an author of a research paper on oger: modular learning architectures for large-scale sequential processing."</data>
      <data key="d2">7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;COHERENT PATTERNS RESEARCH&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;CANARY SONG DECODER RESEARCH&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;OGER RESEARCH&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">7751d435f46a6d4027a1c96edabb9626</data>
    </node>
    <node id="&quot;VERSTRAETEN, BENJAMIN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Verstraeten, Benjamin is an author of a research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d2">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </node>
    <node id="&quot;SCHRAUWEN, SANDER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Schrauwen, Sander is an author of a research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d2">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </node>
    <node id="&quot;DIELEMAN, PHILEMON&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Dieleman, Philemon is an author of a research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d2">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </node>
    <node id="&quot;BRAKEL, PIETER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Brakel, Pieter is an author of a research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d2">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </node>
    <node id="&quot;BUTE-NEERS, AND DEJAN PECEVSKI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Bute-neers, and Dejan Pecevski are additional authors of a research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d2">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </node>
    <node id="&quot;THE JOURNAL OF MACHINE LEARNING RESEARCH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Journal of Machine Learning Research is the publication source for the research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d2">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </node>
    <node id="&quot;MODULAR LEARNING ARCHITECTURES FOR LARGE-SCALE SEQUENTIAL PROCESSING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Modular Learning Architectures for Large-Scale Sequential Processing is the title of a research paper published in The Journal of Machine Learning Research."</data>
      <data key="d2">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </node>
    <node id="&quot;DIRECTED ACYCLIC GRAPH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Directed Acyclic Graph is a type of graph with no cycles, used in the context of neural networks."</data>
      <data key="d2">f59839daadfb1f3832bb9f8d201a7126</data>
    </node>
    <node id="&quot;STRICTLY FEEDFORWARD NEURAL NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Strictly Feedforward Neural Network is a type of neural network that processes information in a single direction, without loops."</data>
      <data key="d2">f59839daadfb1f3832bb9f8d201a7126</data>
    </node>
    <node id="&quot;INFINITE IMPULSE RECURRENT NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Infinite Impulse Recurrent Network is a type of neural network with a directed cyclic graph structure, which cannot be unrolled."</data>
      <data key="d2">f59839daadfb1f3832bb9f8d201a7126</data>
    </node>
    <node id="&quot;FINITE IMPULSE NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Finite Impulse Network is a type of neural network that may include additional stored states and controlled states."</data>
      <data key="d2">f59839daadfb1f3832bb9f8d201a7126</data>
    </node>
    <node id="&quot;FEEDBACK NEURAL NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Feedback Neural Network is a type of neural network that incorporates feedback loops or controlled states, such as Long Short-Term Memory networks and Gated Recurrent Units."</data>
      <data key="d2">f59839daadfb1f3832bb9f8d201a7126</data>
    </node>
    <node id="&quot;ISING MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Ising Model is a recurrent neural network architecture developed by Wilhelm Lenz and Ernst Ising in 1925, which was later made adaptive by Shun&#8217;ichi Amari in 1972."</data>
      <data key="d2">f59839daadfb1f3832bb9f8d201a7126</data>
    </node>
    <node id="&quot;DAVID RUMELHART&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"David Rumelhart is a researcher who contributed to the development of recurrent neural networks in 1986."
"David Rumelhart is a person who contributed to the development of neural networks in 1986."</data>
      <data key="d2">b31ca51b419f7270ee5f4910c90ea331,f59839daadfb1f3832bb9f8d201a7126</data>
    </node>
    <node id="&quot;NEURAL HISTORY COMPRESSOR SYSTEM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Neural History Compressor System is a system that used a recurrent neural network to solve a 'Very Deep Learning' task in 1993, requiring more than 1000 subsequent layers."</data>
      <data key="d2">f59839daadfb1f3832bb9f8d201a7126</data>
    </node>
    <node id="&quot;LONG SHORT-TERM MEMORY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Long Short-Term Memory is a type of recurrent neural network that addresses the vanishing gradient problem and is used in the context of Feedback Neural Networks."
"Long Short-Term Memory is an example of second-order RNNs that uses higher order weights and states, allowing for a direct mapping to a finite-state machine."
"Long Short-Term Memory is a type of Recurrent Neural Network that uses a mechanism to retain information over long sequences."</data>
      <data key="d2">2bdd28d9e151597072c8490db69b9941,b888c4ebe914c1dfa26682de69de9de9,f59839daadfb1f3832bb9f8d201a7126</data>
    </node>
    <node id="&quot;GATED RECURRENT UNITS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Gated Recurrent Units is a type of recurrent neural network that incorporates gated states, similar to Long Short-Term Memory networks."</data>
      <data key="d2">f59839daadfb1f3832bb9f8d201a7126</data>
    </node>
    <node id="&quot;SHUN&#8217;ICHI AMARI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Shun&#8217;ichi Amari is a person who made a neural network adaptive in 1972."</data>
      <data key="d2">b31ca51b419f7270ee5f4910c90ea331</data>
    </node>
    <node id="&quot;HOCHREITER AND SCHMIDHUBER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Hochreiter and Schmidhuber are a team of researchers who invented Long Short-Term Memory (LSTM) networks in 1997."</data>
      <data key="d2">b31ca51b419f7270ee5f4910c90ea331</data>
    </node>
    <node id="&quot;LSTM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Long Short-Term Memory (LSTM) networks are a type of neural network invented by Hochreiter and Schmidhuber in 1997."
"LSTM stands for Long Short-Term Memory, a type of RNN used by Google in their machine learning models."
"LSTM is a type of recurrent neural network (RNN) that can handle long delays between significant events and mix low and high-frequency components."
"LSTM is an abbreviation for Long Short-Term Memory, a type of Recurrent Neural Network that uses a mechanism to retain information over long sequences."
"LSTM is a variant of RNN that addresses the vanishing gradients problem by using a gating mechanism to selectively forget or remember information."
"LSTM is a type of recurrent neural network (RNN) that attempts to overcome problems such as the vanishing gradient problem."
"LSTM is a type of recurrent neural network architecture that addresses the vanishing gradient problem."
"LSTM is a type of recurrent neural network architecture that addresses some of the issues found in traditional RNNs."</data>
      <data key="d2">10112a11d47463e2aad7352c52922d61,1aec5b03f663d1614b2ecbf97981a5c2,2bdd28d9e151597072c8490db69b9941,486e4b71bf02f756450ab727db88f821,4b89d9404fd683ecd03d5846ee2d86ce,88405de18768775d8bce062ea467bd7f,b31ca51b419f7270ee5f4910c90ea331,b48d5212c060453a846e21cdb98dbd7d</data>
    </node>
    <node id="&quot;BAIDU&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Baidu is a Chinese company that used CTC-trained RNNs to break the 2S09 Switchboard Hub5&#8217;00 speech recognition dataset in 2014."
"Baidu is a Chinese company that used CTC-trained RNNs to improve speech recognition performance."</data>
      <data key="d2">486e4b71bf02f756450ab727db88f821,b31ca51b419f7270ee5f4910c90ea331</data>
    </node>
    <node id="&quot;GOOGLE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Google is a company that used LSTM networks in its speech recognition and Android systems."
"Google is a company that used CTC-trained LSTM for speech recognition and improved machine translation, language modeling, and multilingual language processing."</data>
      <data key="d2">486e4b71bf02f756450ab727db88f821,b31ca51b419f7270ee5f4910c90ea331</data>
    </node>
    <node id="&quot;CTC-TRAINED RNNS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">b31ca51b419f7270ee5f4910c90ea331</data>
    </node>
    <node id="&quot;2S09 SWITCHBOARD HUB5&#8217;00&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"2S09 Switchboard Hub5&#8217;00 is a speech recognition dataset that Baidu used to break a benchmark."</data>
      <data key="d2">486e4b71bf02f756450ab727db88f821</data>
    </node>
    <node id="&quot;CTC&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"CTC stands for Connectionist Temporal Classification, a method used by Baidu and Google in their machine learning models."

"CTC is a training method used for recurrent neural networks (RNNs) that achieves both alignment and recognition."</data>
      <data key="d2">4470a7f7ad60a2866b31907a2a3ca96e,486e4b71bf02f756450ab727db88f821,b48d5212c060453a846e21cdb98dbd7d</data>
    </node>
    <node id="&quot;CNN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"CNN stands for Convolutional Neural Network, a type of neural network used in combination with LSTM for automatic image captioning."</data>
      <data key="d2">486e4b71bf02f756450ab727db88f821</data>
    </node>
    <node id="&quot;ELMAN NETWORKS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Elman Networks are a type of neural network with additional context units, allowing them to maintain a sort of state and perform tasks such as sequence prediction."</data>
      <data key="d2">bf4dccb5096a917a6a71f0cc224e4d7c</data>
    </node>
    <node id="&quot;JORDAN NETWORKS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Jordan Networks are a type of neural network mentioned in the text, but no further details are provided about their characteristics or functions."</data>
      <data key="d2">bf4dccb5096a917a6a71f0cc224e4d7c</data>
    </node>
    <node id="&quot;SEQUENCE PREDICTION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Sequence Prediction is a task mentioned in the text, where neural networks are capable of predicting future elements in a sequence."</data>
      <data key="d2">bf4dccb5096a917a6a71f0cc224e4d7c</data>
    </node>
    <node id="&quot;ELMAN NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Elman Network is a type of recurrent neural network that uses context units to maintain a sort of state, allowing it to perform tasks such as sequence-prediction."</data>
      <data key="d2">fb7999a4b39733c28630293d3659d7eb</data>
    </node>
    <node id="&quot;JORDAN NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Jordan Network is a type of recurrent neural network that uses context units, also known as the state layer, which have a recurrent connection to themselves."
"Jordan Network is a type of recurrent neural network (RNN) that uses a specific structure and training method."</data>
      <data key="d2">8b12ccb4afc119b3357ceff10e04ce9f,fb7999a4b39733c28630293d3659d7eb</data>
    </node>
    <node id="&quot;GEOFFREY HINTON&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Geoffrey Hinton is not explicitly mentioned in the text, but he is a well-known figure in the field of neural networks and is often associated with the development of recurrent neural networks."</data>
      <data key="d2">fb7999a4b39733c28630293d3659d7eb</data>
    </node>
    <node id="&quot;JEFF ELMAN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Jeff Elman is the inventor of Elman Networks, a type of recurrent neural network that uses context units to maintain a sort of state."</data>
      <data key="d2">fb7999a4b39733c28630293d3659d7eb</data>
    </node>
    <node id="&quot;SIMPLE RECURRENT NETWORKS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Simple Recurrent Networks is a term used to refer to both Elman Networks and Jordan Networks, as they are similar in structure and function."</data>
      <data key="d2">fb7999a4b39733c28630293d3659d7eb</data>
    </node>
    <node id="&quot;XT&quot;">
      <data key="d0">"VARIABLE"</data>
      <data key="d1">"xt is a variable used in the equations of the Jordan Network and Hopfield Network."</data>
      <data key="d2">8b12ccb4afc119b3357ceff10e04ce9f</data>
    </node>
    <node id="&quot;HT&quot;">
      <data key="d0">"VARIABLE"</data>
      <data key="d1">"ht is a variable used in the equations of the Jordan Network and Hopfield Network."</data>
      <data key="d2">8b12ccb4afc119b3357ceff10e04ce9f</data>
    </node>
    <node id="&quot;YT&quot;">
      <data key="d0">"VARIABLE"</data>
      <data key="d1">"yt is a variable used in the equations of the Jordan Network and Hopfield Network."</data>
      <data key="d2">8b12ccb4afc119b3357ceff10e04ce9f</data>
    </node>
    <node id="&quot;WW&quot;">
      <data key="d0">"VARIABLE"</data>
      <data key="d1">"WW is a variable used in the equations of the Jordan Network and Hopfield Network."</data>
      <data key="d2">8b12ccb4afc119b3357ceff10e04ce9f</data>
    </node>
    <node id="&quot;UU&quot;">
      <data key="d0">"VARIABLE"</data>
      <data key="d1">"UU is a variable used in the equations of the Jordan Network and Hopfield Network."</data>
      <data key="d2">8b12ccb4afc119b3357ceff10e04ce9f</data>
    </node>
    <node id="&quot;BB&quot;">
      <data key="d0">"VARIABLE"</data>
      <data key="d1">"bb is a variable used in the equations of the Jordan Network and Hopfield Network."</data>
      <data key="d2">8b12ccb4afc119b3357ceff10e04ce9f</data>
    </node>
    <node id="&quot;&#931;H&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"&#963;h is a function used in the equations of the Jordan Network and Hopfield Network."</data>
      <data key="d2">8b12ccb4afc119b3357ceff10e04ce9f</data>
    </node>
    <node id="&quot;&#931;Y&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"&#963;y is a function used in the equations of the Jordan Network and Hopfield Network."</data>
      <data key="d2">8b12ccb4afc119b3357ceff10e04ce9f</data>
    </node>
    <node id="&quot;BIDIRECTIONAL ASSOCIATIVE MEMORY (BAM) NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"BAM Network is a variant of Hopfield Network that stores associative data as a vector, allowing for bi-directional information flow."</data>
      <data key="d2">a8c0edd2cdddb7d6d899284063b541f5</data>
    </node>
    <node id="&quot;BART KOSKO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Bart Kosko is the author of the Bidirectional Associative Memory (BAM) Network."</data>
      <data key="d2">a8c0edd2cdddb7d6d899284063b541f5</data>
    </node>
    <node id="&quot;RECENTLY&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Recently refers to a time period when stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications."</data>
      <data key="d2">a8c0edd2cdddb7d6d899284063b541f5</data>
    </node>
    <node id="&quot;BAM NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"BAM Network is a type of neural network with two layers that can be driven as inputs to recall an association and produce an output on the other layer."</data>
      <data key="d2">423cdb622c47fa8cec25f22eb9f9f01f</data>
    </node>
    <node id="&quot;INDEPENDENTLY RECURRENT NEURAL NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Independently Recurrent Neural Network is a type of neural network that addresses the gradient vanishing and exploding problems in the traditional fully connected RNN."</data>
      <data key="d2">423cdb622c47fa8cec25f22eb9f9f01f</data>
    </node>
    <node id="&quot;RECURSIVE NEURAL NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Recursive Neural Network is a type of neural network created by applying the same set of weights recursively over a differentiable graph-like structure by traversing."
"Recursive Neural Network is a type of neural network that applies the same set of weights recursively over a differentiable graph-like structure."</data>
      <data key="d2">423cdb622c47fa8cec25f22eb9f9f01f,7b6ff30ef255db2d2c68326d78cf0115</data>
    </node>
    <node id="&quot;NEURAL HISTORY COMPRESSOR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Neural History Compressor is an unsupervised stack of RNNs that learns to predict its next input from the previous inputs and compresses information for higher-level RNNs."
"The Neural History Compressor is a system composed of unsupervised RNNs that learns to predict and compress inputs, effectively minimizing the description length of data."</data>
      <data key="d2">7b6ff30ef255db2d2c68326d78cf0115,bd72a9eaf7c983a7512698f83535aa22</data>
    </node>
    <node id="&quot;AUTOMATIC DIFFERENTIATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Automatic Differentiation is a method used to compute derivatives of functions, often used in training recursive neural networks."</data>
      <data key="d2">7b6ff30ef255db2d2c68326d78cf0115</data>
    </node>
    <node id="&quot;LINEAR CHAIN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Linear Chain refers to the structure of a RNN, which is a sequence of nodes connected in a linear order."</data>
      <data key="d2">7b6ff30ef255db2d2c68326d78cf0115</data>
    </node>
    <node id="&quot;DISTRIBUTED REPRESENTATIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Distributed Representations refer to the way information is represented in a neural network, such as logical terms in the case of recursive neural networks."</data>
      <data key="d2">7b6ff30ef255db2d2c68326d78cf0115</data>
    </node>
    <node id="&quot;CONSCIOUS CHUNKER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Conscious Chunker is a higher-level RNN within the Neural History Compressor that studies a compressed representation of the information from the lower-level RNN, allowing for easy classification of deep sequences with long intervals between important events."</data>
      <data key="d2">bd72a9eaf7c983a7512698f83535aa22</data>
    </node>
    <node id="&quot;SUBCONSCIOUS AUTOMATIZER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Subconscious Automatizer is a lower-level RNN within the Neural History Compressor that learns to predict or imitate the hidden units of the Conscious Chunker, helping it to learn appropriate, rarely changing memories across long intervals."</data>
      <data key="d2">bd72a9eaf7c983a7512698f83535aa22</data>
    </node>
    <node id="&quot;CHUNKER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Chunker is a higher-level component in a system that learns to predict and compress inputs unpredictable by a lower-level automatizer."</data>
      <data key="d2">b888c4ebe914c1dfa26682de69de9de9</data>
    </node>
    <node id="&quot;AUTOMATIZER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Automatizer is a lower-level component in a system that learns to predict or imitate inputs based on the hidden units of the chunker."</data>
      <data key="d2">b888c4ebe914c1dfa26682de69de9de9</data>
    </node>
    <node id="&quot;GENERATIVE MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Generative Model is a system that partially overcame the vanishing gradient problem in neural networks and solved a 'Very Deep Learning' task in 1993."</data>
      <data key="d2">b888c4ebe914c1dfa26682de69de9de9</data>
    </node>
    <node id="&quot;SECOND-ORDER RNNS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Second-order RNNs use higher order weights and states, allowing a direct mapping to a finite-state machine, with examples including Long Short-Term Memory."
"Second-order RNNs are a type of deep learning system that uses higher order weights for mapping to a finite-state machine."</data>
      <data key="d2">4470a7f7ad60a2866b31907a2a3ca96e,b888c4ebe914c1dfa26682de69de9de9</data>
    </node>
    <node id="&quot;VANISHING GRADIENT PROBLEM&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">b888c4ebe914c1dfa26682de69de9de9</data>
    </node>
    <node id="&quot;LONG SHORT-TERM MEMORY (LSTM)&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem and can learn tasks requiring memories of events from thousands of time steps earlier."</data>
      <data key="d2">4470a7f7ad60a2866b31907a2a3ca96e</data>
    </node>
    <node id="&quot;FINITE-STATE MACHINE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Finite-state machine is a model of computation that can be used to describe the behavior of Second-order RNNs."</data>
      <data key="d2">4470a7f7ad60a2866b31907a2a3ca96e</data>
    </node>
    <node id="&quot;CONNECTIONIST TEMPORAL CLASSIFICATION (CTC)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Connectionist Temporal Classification (CTC) is a method used to train stacks of LSTM RNNs to find an RNN weight matrix that maximizes the probability of label sequences in a training set."</data>
      <data key="d2">4470a7f7ad60a2866b31907a2a3ca96e</data>
    </node>
    <node id="&quot;HMM&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"HMM is a statistical model that assumes the underlying system is a Markov process with hidden states."</data>
      <data key="d2">b48d5212c060453a846e21cdb98dbd7d</data>
    </node>
    <node id="&quot;GRUS&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"GRUs are a type of gating mechanism in recurrent neural networks (RNNs) introduced in 2014, similar to LSTM but with fewer parameters."</data>
      <data key="d2">b48d5212c060453a846e21cdb98dbd7d</data>
    </node>
    <node id="&quot;BI-DIRECTIONAL RNNS&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Bi-directional RNNs are a type of recurrent neural network (RNN) that uses a finite sequence to predict or label each element based on its past and future contexts."
"Bi-directional RNNs are a type of Recurrent Neural Network that use a finite sequence to predict or label each element of the sequence based on the element&#8217;s past and future contexts."</data>
      <data key="d2">2bdd28d9e151597072c8490db69b9941,b48d5212c060453a846e21cdb98dbd7d</data>
    </node>
    <node id="&quot;CONTINUOUS-TIME RECURRENT NEURAL NETWORK&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"A Continuous-time Recurrent Neural Network is a type of neural network that uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs."</data>
      <data key="d2">2bdd28d9e151597072c8490db69b9941</data>
    </node>
    <node id="&quot;CTRNN&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"CTRNN is an abbreviation for Continuous-time Recurrent Neural Network, a type of neural network that uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs."</data>
      <data key="d2">2bdd28d9e151597072c8490db69b9941</data>
    </node>
    <node id="&quot;EVOLUTIONARY ROBOTICS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Evolutionary Robotics is a field that applies principles of evolution and natural selection to the design and control of robots."</data>
      <data key="d2">2bdd28d9e151597072c8490db69b9941</data>
    </node>
    <node id="&quot;HIERARCHICAL RECURRENT NEURAL NETWORK&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Hierarchical Recurrent Neural Network is a type of Recurrent Neural Network that uses multiple layers to process data at different levels of abstraction."</data>
      <data key="d2">2bdd28d9e151597072c8490db69b9941</data>
    </node>
    <node id="&quot;HRNN&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"HRNN is an abbreviation for Hierarchical Recurrent Neural Network, a type of Recurrent Neural Network that uses multiple layers to process data at different levels of abstraction."</data>
      <data key="d2">2bdd28d9e151597072c8490db69b9941</data>
    </node>
    <node id="&quot;SHANNON SAMPLING THEOREM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Shannon sampling theorem is a principle in signal processing that determines the minimum sampling rate required to accurately represent a continuous-time signal."</data>
      <data key="d2">9e61c22432d6984a19da5840f64d417d</data>
    </node>
    <node id="&quot;HIERARCHICAL RECURRENT NEURAL NETWORKS&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Hierarchical recurrent neural networks are a type of recurrent neural network that decompose hierarchical behavior into useful subprograms, inspired by theories of memory presented by philosopher Henri Bergson."</data>
      <data key="d2">9e61c22432d6984a19da5840f64d417d</data>
    </node>
    <node id="&quot;CONSUMER PRICE INDEX&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The consumer price index (CPI) is a measure that examines the weighted average of price changes of a basket of consumer goods and services, with applications in forecasting and inflation prediction."</data>
      <data key="d2">9e61c22432d6984a19da5840f64d417d</data>
    </node>
    <node id="&quot;RECURRENT MULTILAYER PERCEPTRON NETWORK&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"A recurrent multilayer perceptron network (RMLP network) is a type of artificial neural network that consists of cascaded subnetworks, each containing multiple layers of nodes, with feedback connections in the last layer."</data>
      <data key="d2">9e61c22432d6984a19da5840f64d417d</data>
    </node>
    <node id="&quot;US CPI-U INDEX&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The US CPI-U index is a dataset used for evaluating inflation prediction methods."</data>
      <data key="d2">c7ca22e82a3823afda793ad30077348e</data>
    </node>
    <node id="&quot;HRNN MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The HRNN model is a prediction method that outperforms established methods in evaluating the US CPI-U index."</data>
      <data key="d2">c7ca22e82a3823afda793ad30077348e</data>
    </node>
    <node id="&quot;RMLP NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The RMLP network is a type of neural network consisting of cascaded subnetworks with feed-forward and feedback connections."</data>
      <data key="d2">c7ca22e82a3823afda793ad30077348e</data>
    </node>
    <node id="&quot;MTRNN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The MTRNN is a neural-based computational model that simulates the functional hierarchy of the brain through self-organization."</data>
      <data key="d2">c7ca22e82a3823afda793ad30077348e</data>
    </node>
    <node id="&quot;NT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"NT is a shorthand for Neural Turing machines, a type of neural network model that can perform complex computations."</data>
      <data key="d2">c7ca22e82a3823afda793ad30077348e</data>
    </node>
    <node id="&quot;HAWKINS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Hawkins is a researcher mentioned in the text, known for his work on the memory-prediction theory of brain function."</data>
      <data key="d2">0f59288ce2aaf33e468cdc3877cefd85</data>
    </node>
    <node id="&quot;NEURAL TURING MACHINES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Neural Turing machines are a method for extending recurrent neural networks, allowing them to interact with external memory resources."</data>
      <data key="d2">0f59288ce2aaf33e468cdc3877cefd85</data>
    </node>
    <node id="&quot;DIFFERENTIABLE NEURAL COMPUTERS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Differentiable neural computers are an extension of Neural Turing machines, allowing for the usage of fuzzy amounts of each memory address and a record of chronology."</data>
      <data key="d2">0f59288ce2aaf33e468cdc3877cefd85</data>
    </node>
    <node id="&quot;NEURAL NETWORK PUSHDOWN AUTOMATA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Neural network pushdown automata are similar to Neural Turing machines, but tapes are replaced by analog stacks that are differentiable and trained."
"Neural Network Pushdown Automata are a type of machine learning model that uses differentiable and trainable analog stacks, similar in complexity to recognizers of context-free grammars."</data>
      <data key="d2">0f59288ce2aaf33e468cdc3877cefd85,671755b49cf8893c9fcf9c9c05777ea6</data>
    </node>
    <node id="&quot;MEMRISTIVE NETWORKS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Memristive networks are described as a system of cortical neural networks that use memristive devices for memory storage and processing."
"Memristive Networks are a particular type of physical neural network that have been described as having similar properties to Hopfield networks, with continuous dynamics, a limited memory capacity, and natural relaxation via the minimization of a function which is asymptotic to the Ising model."
"Memristive Networks are a type of physical neural network with similar properties to Little-Hopfield networks, characterized by continuous dynamics, limited memory capacity, and natural relaxation."</data>
      <data key="d2">0f59288ce2aaf33e468cdc3877cefd85,5445391448d4ac43471e2bce5eb41a70,671755b49cf8893c9fcf9c9c05777ea6</data>
    </node>
    <node id="&quot;GREG SNIDER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Greg Snider is a researcher mentioned in the text, working at HP Labs and describing a system of cortical neural networks."
"Greg Snider is a researcher at HP Labs who has described a system of cortical computing with memristive nanodevices."</data>
      <data key="d2">0f59288ce2aaf33e468cdc3877cefd85,671755b49cf8893c9fcf9c9c05777ea6</data>
    </node>
    <node id="&quot;HP LABS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"HP Labs is an organization mentioned in the text, where Greg Snider works on neural network systems."
"HP Labs is a research organization that has been mentioned in the context of developing systems with memristive nanodevices."</data>
      <data key="d2">0f59288ce2aaf33e468cdc3877cefd85,671755b49cf8893c9fcf9c9c05777ea6</data>
    </node>
    <node id="&quot;MEMORY-PREDICTION THEORY&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0f59288ce2aaf33e468cdc3877cefd85</data>
    </node>
    <node id="&quot;EXTERNAL MEMORY RESOURCES&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0f59288ce2aaf33e468cdc3877cefd85</data>
    </node>
    <node id="&quot;CORTICAL NEURAL NETWORKS&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">0f59288ce2aaf33e468cdc3877cefd85</data>
    </node>
    <node id="&quot;DARPA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"DARPA is a research organization that has funded projects in collaboration with IBM Research, HP Labs, and the Boston University Department of Cognitive and Neural Systems (CNS) to develop neuromorphic architectures based on memristive systems."</data>
      <data key="d2">671755b49cf8893c9fcf9c9c05777ea6</data>
    </node>
    <node id="&quot;IBM RESEARCH&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"IBM Research is a research organization that has been mentioned in the context of collaborating with DARPA on neuromorphic architectures based on memristive systems."</data>
      <data key="d2">671755b49cf8893c9fcf9c9c05777ea6</data>
    </node>
    <node id="&quot;BOSTON UNIVERSITY DEPARTMENT OF COGNITIVE AND NEURAL SYSTEMS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Boston University Department of Cognitive and Neural Systems (CNS) is a research organization that has been mentioned in the context of collaborating with DARPA on neuromorphic architectures based on memristive systems."</data>
      <data key="d2">671755b49cf8893c9fcf9c9c05777ea6</data>
    </node>
    <node id="&quot;NEUROMORPHIC ARCHITECTURES&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">671755b49cf8893c9fcf9c9c05777ea6</data>
    </node>
    <node id="&quot;LITTLE-HOPFIELD NETWORKS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Little-Hopfield Networks are a type of neural network that Memristive Networks are compared to, known for their continuous dynamics and limited memory capacity."</data>
      <data key="d2">5445391448d4ac43471e2bce5eb41a70</data>
    </node>
    <node id="&quot;RESISTOR-CAPACITOR NETWORK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Resistor-Capacitor Network is mentioned as a type of network that Memristive Networks have a more interesting non-linear behavior compared to."</data>
      <data key="d2">5445391448d4ac43471e2bce5eb41a70</data>
    </node>
    <node id="&quot;NEUROMORPHIC ENGINEERING&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Neuromorphic Engineering is a field mentioned in the context of engineering analog memristive networks, where the device behavior depends on the circuit wiring or topology."</data>
      <data key="d2">5445391448d4ac43471e2bce5eb41a70</data>
    </node>
    <node id="&quot;CARAVELLI-TRAVERSA-DI VENTRA EQUATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Caravelli-Traversa-Di Ventra Equation is mentioned as a method used to study the evolution of memristive networks analytically."</data>
      <data key="d2">5445391448d4ac43471e2bce5eb41a70</data>
    </node>
    <node id="&quot;MODERN LIBRARIES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Modern Libraries are mentioned as providers of runtime-optimized implementations for recurrent neural networks."</data>
      <data key="d2">5445391448d4ac43471e2bce5eb41a70</data>
    </node>
    <node id="&quot;WERBOS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Werbos is a researcher who developed methods for training recurrent neural networks."
"Werbos is a person who contributed to the development of recurrent neural networks in the 1980s and early 1990s."</data>
      <data key="d2">0bb54b1de8d2297293defe94addb8076,6bbaf3df0fa2fac979f6d6a64abb2e91</data>
    </node>
    <node id="&quot;WILLIAMS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Williams is a researcher who developed methods for training recurrent neural networks."
"Williams is a person who contributed to the development of recurrent neural networks in the 1980s and early 1990s."</data>
      <data key="d2">0bb54b1de8d2297293defe94addb8076,6bbaf3df0fa2fac979f6d6a64abb2e91</data>
    </node>
    <node id="&quot;ROBINSON&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Robinson is a researcher who developed methods for training recurrent neural networks."
"Robinson is a person who contributed to the development of recurrent neural networks in the 1980s and early 1990s."</data>
      <data key="d2">0bb54b1de8d2297293defe94addb8076,6bbaf3df0fa2fac979f6d6a64abb2e91</data>
    </node>
    <node id="&quot;PEARLMUTTER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Pearlmutter is a researcher who developed methods for training recurrent neural networks."
"Pearlmutter is a person who contributed to the development of recurrent neural networks in the 1980s and early 1990s."</data>
      <data key="d2">0bb54b1de8d2297293defe94addb8076,6bbaf3df0fa2fac979f6d6a64abb2e91</data>
    </node>
    <node id="&quot;BPTT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"BPTT is a method for training RNNs that uses backpropagation through time, with a time-complexity of O(number of weights) per time step."
"BPTT is a backpropagation through time algorithm used in training recurrent neural networks."</data>
      <data key="d2">1aec5b03f663d1614b2ecbf97981a5c2,88405de18768775d8bce062ea467bd7f</data>
    </node>
    <node id="&quot;RTRL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"RTRL is a method for training RNNs that uses real-time recursive learning, with a time-complexity of O(number of hidden x number of weights) per time step for computing Jacobian matrices."
"RTRL is a real-time recurrent learning algorithm used in training recurrent neural networks."</data>
      <data key="d2">1aec5b03f663d1614b2ecbf97981a5c2,88405de18768775d8bce062ea467bd7f</data>
    </node>
    <node id="&quot;INDRNN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"IndRNN is a variant of RNN that reduces the context of a neuron to its own past state, allowing for the exploration of cross-neuron information in following layers."
"IndRNN is an independently recurrent neural network that reduces the context of a neuron to its own past state, allowing for the learning of memories of different ranges."</data>
      <data key="d2">1aec5b03f663d1614b2ecbf97981a5c2,88405de18768775d8bce062ea467bd7f</data>
    </node>
    <node id="&quot;CRBP&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"CRBP is an on-line algorithm that implements and combines BPTT and RTRL paradigms for locally recurrent networks, minimizing the global error term."</data>
      <data key="d2">88405de18768775d8bce062ea467bd7f</data>
    </node>
    <node id="&quot;SIGNAL-FLOW GRAPHS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Signal-Flow Graphs is a method used for gradient information computation in RNNs with arbitrary architectures, based on diagrammatic derivation."</data>
      <data key="d2">88405de18768775d8bce062ea467bd7f</data>
    </node>
    <node id="&quot;LEE&#8217;S THEOREM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Lee&#8217;s Theorem is a mathematical concept used in network sensitivity calculations."</data>
      <data key="d2">88405de18768775d8bce062ea467bd7f</data>
    </node>
    <node id="&quot;GLOBAL OPTIMIZATION METHODS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Global Optimization Methods are techniques used to train the weights in a neural network, modeled as a non-linear optimization problem."</data>
      <data key="d2">88405de18768775d8bce062ea467bd7f</data>
    </node>
    <node id="&quot;WAN&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Wan is a person mentioned as a contributor to the development of RNNs."</data>
      <data key="d2">7ede01f521333d9e39fc34a245103242</data>
    </node>
    <node id="&quot;BEAUFAYS&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Beaufays is a person mentioned as a contributor to the development of RNNs."</data>
      <data key="d2">7ede01f521333d9e39fc34a245103242</data>
    </node>
    <node id="&quot;CAMPOLUCCI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Campolucci is a person mentioned as a contributor to the development of RNNs."</data>
      <data key="d2">7ede01f521333d9e39fc34a245103242</data>
    </node>
    <node id="&quot;UNCINI&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Uncini is a person mentioned as a contributor to the development of RNNs."</data>
      <data key="d2">7ede01f521333d9e39fc34a245103242</data>
    </node>
    <node id="&quot;PIAZZA&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Piazza is a person mentioned as a contributor to the development of RNNs."</data>
      <data key="d2">7ede01f521333d9e39fc34a245103242</data>
    </node>
    <node id="&quot;GENETIC ALGORITHMS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Genetic Algorithms are a global optimization method mentioned for training RNNs."
"Genetic Algorithms are a global optimization method used for training RNNs, evolving multiple neural networks to minimize the mean-squared error."</data>
      <data key="d2">797480b3d8c00dbb7f02fccb2ab8256a,7ede01f521333d9e39fc34a245103242</data>
    </node>
    <node id="&quot;SIMULATED ANNEALING&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Simulated Annealing is a global optimization technique that may be used to seek a good set of weights for RNNs."</data>
      <data key="d2">797480b3d8c00dbb7f02fccb2ab8256a</data>
    </node>
    <node id="&quot;PARTICLE SWARM OPTIMIZATION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Particle Swarm Optimization is a global optimization technique that may be used for training RNNs, seeking a good set of weights."</data>
      <data key="d2">797480b3d8c00dbb7f02fccb2ab8256a</data>
    </node>
    <node id="&quot;DYNAMICAL SYSTEMS THEORY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Dynamical Systems Theory is a field that may be used for analyzing the behavior of RNNs, which can appear chaotic."
"Dynamical Systems Theory is a field of mathematics used to analyze chaotic behavior in systems."</data>
      <data key="d2">2f1161d1f711d264529aa7bddf81959b,797480b3d8c00dbb7f02fccb2ab8256a</data>
    </node>
    <node id="&quot;RECURSIVE NEURAL NETWORKS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Recursive Neural Networks are a type of neural network that operate on any hierarchical structure, combining child representations into parent representations."
"Recursive Neural Networks are a type of neural network that operates on hierarchical structures, combining child representations into parent representations."</data>
      <data key="d2">2f1161d1f711d264529aa7bddf81959b,797480b3d8c00dbb7f02fccb2ab8256a</data>
    </node>
    <node id="&quot;FINITE IMPULSE RESPONSE FILTERS&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Finite Impulse Response Filters are a type of filter that can be implemented as a nonlinear version of Recurrent Neural Networks."</data>
      <data key="d2">2f1161d1f711d264529aa7bddf81959b</data>
    </node>
    <node id="&quot;INFINITE IMPULSE RESPONSE FILTERS&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Infinite Impulse Response Filters are a type of filter that can be implemented as a nonlinear version of Recurrent Neural Networks."</data>
      <data key="d2">2f1161d1f711d264529aa7bddf81959b</data>
    </node>
    <node id="&quot;NONLINEAR AUTOREGRESSIVE EXOGENOUS MODEL (NARX)&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Nonlinear Autoregressive Exogenous Model (NARX) is a type of model that can be implemented as a nonlinear version of Recurrent Neural Networks."</data>
      <data key="d2">2f1161d1f711d264529aa7bddf81959b</data>
    </node>
    <node id="&quot;SILENCING MECHANISM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Silencing Mechanism is a biological mechanism exhibited in neurons with a relatively high frequency spiking activity, used in a more biological-based model for memory-based learning."</data>
      <data key="d2">2f1161d1f711d264529aa7bddf81959b</data>
    </node>
    <node id="&quot;LIBRARIES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Libraries are mentioned in the text, but no specific library is named."
"Libraries are mentioned as a source of external links, but no further details are provided about their role or activities.")&lt;|COMPLETE|&gt;</data>
      <data key="d2">07d8f04f437c25358d3df4e745af77d4,2f1161d1f711d264529aa7bddf81959b</data>
    </node>
    <node id="&quot;APPLICATIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Applications of recurrent neural networks are mentioned, but no specific applications are named."</data>
      <data key="d2">2f1161d1f711d264529aa7bddf81959b</data>
    </node>
    <node id="&quot;REFERENCES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"References are mentioned in the text, but no specific references are named."</data>
      <data key="d2">2f1161d1f711d264529aa7bddf81959b</data>
    </node>
    <node id="&quot;MACKEY-GLASS EQUATIONS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Mackey-Glass Equations are a set of delayed differential equations used to describe the temporal behavior of physiological signals."
"Mackey-Glass Equations are a set of delayed differential equations used to describe the temporal behavior of physiological signals, such as the quantity of mature blood cells over time."</data>
      <data key="d2">238049de5f28dca3e857a46a8b1bed03,2f4c992d69812866e6fce6dbb52d8612</data>
    </node>
    <node id="&quot;PHYSIOLOGICAL SIGNALS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Physiological Signals are biological signals used to describe the behavior of different physiological systems, such as the relative quantity of mature blood cells over time."</data>
      <data key="d2">2f4c992d69812866e6fce6dbb52d8612</data>
    </node>
    <node id="&quot;TAU&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Tau is a parameter in the Mackey-Glass Equations that controls the chaotic behavior of the equations, with higher values leading to more chaotic timeseries."
"Tau is a time delay parameter used in the Mackey-Glass Equation and the Phase Diagram."</data>
      <data key="d2">238049de5f28dca3e857a46a8b1bed03,518f1e492b92054cf2f5c5289444da02</data>
    </node>
    <node id="&quot;PHASE DIAGRAM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Phase Diagram is a graphical representation of the behavior of a system, showing the relationship between two variables."</data>
      <data key="d2">518f1e492b92054cf2f5c5289444da02</data>
    </node>
    <node id="&quot;TASK 1: 10 TIMESTEPS AHEAD FORECAST&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Task 1 involves predicting P(t+10) given P(t) in the Mackey-Glass Time Series using Reservoir Computing."</data>
      <data key="d2">fac681bdc38ae5829173c747ee6240fa</data>
    </node>
    <node id="&quot;DATA PREPROCESSING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Data Preprocessing is the step of preparing the Mackey-Glass Time Series data for use in Task 1, which includes visualizing the training and testing data."
"Data Preprocessing is the step of loading and processing the data to prepare it for analysis."
"Data Preprocessing is the process of cleaning, transforming, and preparing data for analysis."
"Data Preprocessing is the process of transforming raw data into a format that can be used for analysis, as shown in the provided code."
"Data Preprocessing refers to the steps taken to clean and prepare the data for use in the Echo State Network (ESN) model."</data>
      <data key="d2">1365a36c76afc697ac626fd0f784804a,8677349b328abac82fa1cfc91c856a6c,b2beacacc8c190393e4583a69518378c,c5c29ba06a5cc70a086c2c2c8858e5aa,fac681bdc38ae5829173c747ee6240fa</data>
    </node>
    <node id="&quot;X_TRAIN1&quot;">
      <data key="d0">"DATA"</data>
      <data key="d1">"X_train1 is a dataset used for training the ESN neural network."
"X_train1 is a subset of the input data used for training the ESN."</data>
      <data key="d2">09198e939639c229c2c97555f65b12a7,993a69efae014a8f8d6ec0c235104d46</data>
    </node>
    <node id="&quot;Y_TRAIN1&quot;">
      <data key="d0">"DATA"</data>
      <data key="d1">"y_train1 is a dataset used for training the ESN neural network, representing the target output."
"y_train1 is a subset of the target data used for training the ESN."</data>
      <data key="d2">09198e939639c229c2c97555f65b12a7,993a69efae014a8f8d6ec0c235104d46</data>
    </node>
    <node id="&quot;TEST&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Test is the process of evaluating the performance of the trained ESN network on new data."</data>
      <data key="d2">cc1fb6ca5695434ad0279c2606e928af</data>
    </node>
    <node id="&quot;NP&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"np is a library in Python used for numerical computations."</data>
      <data key="d2">593306edfb8d4c7ef4b99d24fa009970</data>
    </node>
    <node id="&quot;X_T&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"X_t is a variable representing the true values of the input data in the time series."</data>
      <data key="d2">593306edfb8d4c7ef4b99d24fa009970</data>
    </node>
    <node id="&quot;X_GEN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"X_gen is a variable representing the generated values of the input data in the time series."</data>
      <data key="d2">593306edfb8d4c7ef4b99d24fa009970</data>
    </node>
    <node id="&quot;ONE-TIMESTEP-AHEAD FORECAST&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"One-timestep-ahead forecast is a prediction method where the next value in the time series is predicted based on the current value."
"One-timestep-ahead Forecast is the task of predicting the next data point in a time series."</data>
      <data key="d2">29aad23ce67e778ac31d4fb287fd20c7,593306edfb8d4c7ef4b99d24fa009970</data>
    </node>
    <node id="&quot;GENERATIVE MODE&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Generative mode is a method used to generate new data points in the time series based on the learned model."
"Generative Mode is a process in which the ESN model generates new time series data without external inputs."
"Generative Mode is a concept used in the context of data analysis and machine learning, referring to a mode of operation that generates new data based on learned patterns."
"Generative Mode is a method used in the text, which involves generating timeseries data."
"Generative Mode is a task where the model generates new data points based on learned patterns."</data>
      <data key="d2">29aad23ce67e778ac31d4fb287fd20c7,424bf7c7b82dc966139c25f7c9ccffb7,593306edfb8d4c7ef4b99d24fa009970,70db98fabc82fc96ecf8cc2c023b586b,e396354e3a9be76616392af11f56e671</data>
    </node>
    <node id="&quot;SUSSILLO AND ABOTT&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Sussillo and Abott are the authors of the FORCE Algorithm, which is used in the ESN model for online learning."</data>
      <data key="d2">424bf7c7b82dc966139c25f7c9ccffb7</data>
    </node>
    <node id="&quot;SUSSILLO&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Sussillo is a contributor to the FORCE Algorithm, developed with Abott in 2009."</data>
      <data key="d2">1b9bc5f1bd54d2b0c90359b6ed022bb6</data>
    </node>
    <node id="&quot;ABOTT&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Abott is a contributor to the FORCE Algorithm, developed with Sussillo in 2009."</data>
      <data key="d2">1b9bc5f1bd54d2b0c90359b6ed022bb6</data>
    </node>
    <node id="&quot;ROBOT FALLING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Robot Falling is a use case mentioned in the text, where data from a robot falling is used to demonstrate the use of Echo State Networks (ESNs)."
"Robot falling is an event mentioned in the text, which involves the use of data from a robot that has fallen to analyze its behavior."</data>
      <data key="d2">af2db1cc5ab6b16acae2c93d3facb668,b483c6bbce54156c724905b340aa2e85</data>
    </node>
    <node id="&quot;ZENODO&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Zenodo is a platform mentioned in the text, where data for the Robot Falling use case can be found."
"Zenodo is a platform mentioned in the text, where data for the robot falling use case can be found."
"Zenodo is a platform for sharing research data, with a record available for the canary song decoding use case."
"Zenodo is a platform where the data is found, hosting various research outputs and data."</data>
      <data key="d2">8677349b328abac82fa1cfc91c856a6c,af2db1cc5ab6b16acae2c93d3facb668,b483c6bbce54156c724905b340aa2e85,d15f6d075c072f0335b5332f11c00299</data>
    </node>
    <node id="&quot;ROBOT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The robot is the subject of the data analysis, with features such as 'left_ankle_pitch' and 'right_hip_yaw' being monitored."</data>
      <data key="d2">90fa1052aec4e6374867e9a2951fb3c4</data>
    </node>
    <node id="&quot;FALL INDICATOR&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The fall indicator is an event that the robot is being monitored for, with the objective of preventing falls."</data>
      <data key="d2">90fa1052aec4e6374867e9a2951fb3c4</data>
    </node>
    <node id="&quot;FORCE MAGNITUDE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Force magnitude is a concept that is being measured and analyzed in the context of the robot's data."</data>
      <data key="d2">90fa1052aec4e6374867e9a2951fb3c4</data>
    </node>
    <node id="&quot;PYTHON CODE&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Python Code is used to analyze and visualize data, perform calculations, and display results."</data>
      <data key="d2">d15f6d075c072f0335b5332f11c00299</data>
    </node>
    <node id="&quot;CANARY SONG DECODING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Canary Song Decoding is the process of analyzing and classifying temporal motifs in canary songs to identify different phrases and silence."
"Canary Song Decoding is a use case that involves analyzing a canary's song to extract information."
"Canary Song Decoding is the process of analyzing and interpreting a canary song, as discussed in Chapter 5."</data>
      <data key="d2">1e8ee805d22cd143d2372d300997d253,d15f6d075c072f0335b5332f11c00299,e7d249cdab85dc69b631d43ac6b62915</data>
    </node>
    <node id="&quot;THE DATA&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Data refers to the temporal motifs to classify found on Zenodo, which includes phrases and silence."</data>
      <data key="d2">8677349b328abac82fa1cfc91c856a6c</data>
    </node>
    <node id="&quot;LIBRISPEECH DATASET&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Librispeech Dataset is a collection of audio files and annotations used for speech recognition and analysis."</data>
      <data key="d2">adfade0d7bc85c6420e61ecd1ce7095c</data>
    </node>
    <node id="&quot;MFCC&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"MFCC (Mel-Frequency Cepstral Coefficients) is a feature extraction technique used to represent the short-term power spectrum of a sound."
"MFCC (Mel-frequency cepstral coefficients) is a feature extraction technique used in the data analysis process."</data>
      <data key="d2">9a54cf00618f7dcfb151c8dc8f7471bd,adfade0d7bc85c6420e61ecd1ce7095c</data>
    </node>
    <node id="&quot;DELTA-DELTA&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Delta-Delta is a feature extraction technique used to capture the rate of change of Delta coefficients over time."</data>
      <data key="d2">adfade0d7bc85c6420e61ecd1ce7095c</data>
    </node>
    <node id="&quot;LOAD_DATA&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"load_data is a function used to load audio files and annotations from the Librispeech Dataset, extracting MFCC, Delta, and Delta-Delta features."
"load_data is a function used to load and preprocess audio data, which is a key step in the data analysis process."</data>
      <data key="d2">9a54cf00618f7dcfb151c8dc8f7471bd,adfade0d7bc85c6420e61ecd1ce7095c</data>
    </node>
    <node id="&quot;SPECIAL NODE E&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Special Node E is a unique node within the ESN network, playing a significant role in the training process."</data>
      <data key="d2">894d59d781535ca85389c4226715c007</data>
    </node>
    <node id="&quot;SPECIAL NODE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Special Node is a component of the Echo State Network (ESN) that allows parallelization of states computations, improving training efficiency."</data>
      <data key="d2">d0b9bbbd7257712eafd2eda5db1d0a8d</data>
    </node>
    <node id="&quot;SKLEARN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Sklearn is a library used for importing metrics, which may be used for evaluating the performance of the trained ESN model."
"sklearn is a popular machine learning library that provides tools for data analysis and model training."
"Sklearn is a machine learning library in Python, used for preprocessing data and encoding labels in the text."
"Sklearn is a Python library for machine learning, mentioned in the text."</data>
      <data key="d2">1e8ee805d22cd143d2372d300997d253,870f29520f7a1c42eecb0c4ff855f09e,9fdaabd6c7e893a275a3848c10007477,d0b9bbbd7257712eafd2eda5db1d0a8d</data>
    </node>
    <node id="&quot;ONE_HOT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"one_hot is a variable representing the one-hot encoding used to transform the target data into a format suitable for training the ESN system."
"one_hot is a technique used for encoding categorical variables, converting them into a binary matrix representation."</data>
      <data key="d2">1db5e6cd356c6066227de5e273de1abe,80c9f51870e239404ed671ef0374f191</data>
    </node>
    <node id="&quot;VOCAB&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"vocab is a variable representing the vocabulary used to map the predicted output to the corresponding target value."
"vocab is a vocabulary or dictionary used for mapping between numerical and categorical representations of data."</data>
      <data key="d2">1db5e6cd356c6066227de5e273de1abe,80c9f51870e239404ed671ef0374f191</data>
    </node>
    <node id="&quot;AVERAGE ACCURACY&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Average Accuracy is a metric that measures the overall performance of a model, calculated as the mean accuracy."</data>
      <data key="d2">eb4cbfc924325a7ec01e566ffac75ac3</data>
    </node>
    <node id="&quot;STANDARD DEVIATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Standard Deviation is a metric that measures the amount of variation or dispersion of a set of values."</data>
      <data key="d2">eb4cbfc924325a7ec01e566ffac75ac3</data>
    </node>
    <node id="&quot;ADVANCED FEATURES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Advanced Features refers to the capabilities of ReservoirPy beyond basic usage, such as input-to-readout connections and custom weight matrices.""Advanced Features refers to the capabilities of ReservoirPy beyond basic usage, such as input-to-readout connections, feedback connections, custom weight matrices, parallelization, and 'deep' architectures."</data>
      <data key="d2">ead6383a44acd8ebd17907b85a910455</data>
      <data key="d3">"CONCEPT"</data>
    </node>
    <node id="&quot;DIRECT CONNECTIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Direct Connections refer to the presence of input-to-readout connections in more advanced ESNs."</data>
      <data key="d2">ead6383a44acd8ebd17907b85a910455</data>
    </node>
    <node id="&quot;LONG TERM FORECASTING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Long term forecasting refers to the ability to predict data over long periods of time using feedback connections."</data>
      <data key="d2">ead6383a44acd8ebd17907b85a910455</data>
    </node>
    <node id="&quot;CUSTOM WEIGHT MATRICES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Custom weight matrices are matrices used to create more complex ESNs, which can be useful for tasks such as image classification or modeling complex systems."
"Custom Weight Matrices are parameters that can be initialized using custom initializer functions, allowing for customization of the reservoir and readout matrices."</data>
      <data key="d2">ead6383a44acd8ebd17907b85a910455,ff860bc63e3d697a6183c0b850689048</data>
    </node>
    <node id="&quot;'DEEP' ARCHITECTURES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"'Deep' architectures are architectures created by stacking multiple ESNs together, which can be useful for tasks such as image classification or modeling complex systems."</data>
      <data key="d2">ead6383a44acd8ebd17907b85a910455</data>
    </node>
    <node id="&quot;MODEL CREATION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Model creation is the process of building a neural network model, including defining nodes, connections, and parameters."</data>
      <data key="d2">f1fc6fbc8158d3da070d55544041a2ca</data>
    </node>
    <node id="&quot;CONNECTION CHAINING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Connection chaining is the process of connecting nodes in a sequential manner, using operators such as &gt;&gt;."</data>
      <data key="d2">f1fc6fbc8158d3da070d55544041a2ca</data>
    </node>
    <node id="&quot;CONNECTION MERGE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Connection merge is the process of combining connections from multiple nodes to a single node, using operators such as &amp;."</data>
      <data key="d2">f1fc6fbc8158d3da070d55544041a2ca</data>
    </node>
    <node id="&quot;CONCAT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Concat is a concept used in the model, which is a special node that concatenates all incoming vectors before feeding them to the receiver node, the readout in this case."
"Concat is a function used to combine multiple data streams into a single stream."</data>
      <data key="d2">c82c9d05b211ff65131f70eb8cb13513,cf15a09e77b695a117e1cca05461aea2</data>
    </node>
    <node id="&quot;FEEDBACK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Feedback is a mechanism used in reservoir computing models to influence the internal state of the reservoir."
"Feedback refers to the process of using the output of a system as input to the same system, influencing its behavior."</data>
      <data key="d2">3ff318aebcb07ca141d0a40730d96c7c,b338d2dcc1fe6ccf42407444c02cad7c</data>
    </node>
    <node id="&quot;MODEL.RUN()&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Model.run() is a method used to make predictions using a trained model, such as an Echo State Network (ESN), on new input data."</data>
      <data key="d2">b338d2dcc1fe6ccf42407444c02cad7c</data>
    </node>
    <node id="&quot;FEEDBACK TIMESERIES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Feedback Timeseries is a sequence of data used to influence the behavior of a system, such as an Echo State Network (ESN), during the prediction phase."</data>
      <data key="d2">b338d2dcc1fe6ccf42407444c02cad7c</data>
    </node>
    <node id="&quot;TEACHER VALUES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Teacher Values are the target values used during the training phase of a model, such as an Echo State Network (ESN), to guide its learning."</data>
      <data key="d2">b338d2dcc1fe6ccf42407444c02cad7c</data>
    </node>
    <node id="&quot;INITIALIZER FUNCTIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Initializer Functions are used to initialize parameters in the ESN model, taking the shape of the parameter matrix as input and returning an array or a Scipy matrix."
"Initializer Functions are used to initialize the parameters of the reservoir and readout matrices at the first run of the node."</data>
      <data key="d2">38b3e8ea0ec280360770513327b0d9d3,ff860bc63e3d697a6183c0b850689048</data>
    </node>
    <node id="&quot;WEIGHT MATRICES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Weight Matrices are parameters in the ESN model that hold connections between nodes, influencing the model's behavior."</data>
      <data key="d2">38b3e8ea0ec280360770513327b0d9d3</data>
    </node>
    <node id="&quot;READOUTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Readouts are components of the ESN model that output predictions based on the internal state of the model."
"Readouts are a component of the Reservoirpy library, which holds parameters stored as Numpy arrays or Scipy sparse matrices."</data>
      <data key="d2">38b3e8ea0ec280360770513327b0d9d3,ff860bc63e3d697a6183c0b850689048</data>
    </node>
    <node id="&quot;RESERVOIRS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Reservoirs are components of the ESN model that store and process information, influencing the model's ability to learn and forecast."</data>
      <data key="d2">38b3e8ea0ec280360770513327b0d9d3</data>
    </node>
    <node id="&quot;GENERATIVE TASK&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Generative Task refers to the process of performing a task using a for-loop and an ESN call method, which is described in the text."</data>
      <data key="d2">ff860bc63e3d697a6183c0b850689048</data>
    </node>
    <node id="&quot;ESN CALL METHOD&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"ESN Call Method is a method mentioned in the context of performing a generative task using a for-loop."</data>
      <data key="d2">ff860bc63e3d697a6183c0b850689048</data>
    </node>
    <node id="&quot;RANDOM SPARSE MATRICES&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">ff860bc63e3d697a6183c0b850689048</data>
    </node>
    <node id="&quot;SCIPY.STATS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"scipy.stats is a module that provides functions for statistical analysis and probability distributions."</data>
      <data key="d2">96c47d9b671ce319abe9c6ba2b8ae122</data>
    </node>
    <node id="&quot;NORMAL&quot;">
      <data key="d0">"FUNCTION"</data>
      <data key="d1">"normal is a function from the reservoirpy.mat_gen module that creates a dense matrix from a Gaussian distribution."
"normal is a function used to generate dense matrices from a normal distribution."</data>
      <data key="d2">3a3b7a67b23341dcd1b04ec5b61683f6,96c47d9b671ce319abe9c6ba2b8ae122</data>
    </node>
    <node id="&quot;BERNOULLI DISTRIBUTION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Bernoulli Distribution is a type of probability distribution used to generate matrices."</data>
      <data key="d2">d5e39e29b61f6ea0ffe0c868ba7a4252</data>
    </node>
    <node id="&quot;DATA PROCESSING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Data Processing is the task performed by nodes such as ESN in ReservoirPy, which involves processing input sequences to produce target sequences."</data>
      <data key="d2">3bee7b78d0ab9582cc9bffe9e305df2e</data>
    </node>
    <node id="&quot;SEQUENTIAL BACKEND&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Sequential Backend is a component of the ESN model that processes data in a sequential manner."</data>
      <data key="d2">f0c8d4d322d73f46464e3e9f6914f2ee</data>
    </node>
    <node id="&quot;MULTIPROCESSING BACKEND&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Multiprocessing Backend is a component of the ESN model that processes data in parallel using multiple cores."</data>
      <data key="d2">f0c8d4d322d73f46464e3e9f6914f2ee</data>
    </node>
    <node id="&quot;DEEP ECHO STATE NETWORKS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Deep Echo State Networks are a type of model mentioned in the text, which contain nodes and connections."</data>
      <data key="d2">59b469bdd618b3f36b3547f4f2b8a862</data>
    </node>
    <node id="&quot;RESERVOIR1&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"reservoir1 is a component in a system, likely a part of a larger organization or system."
"Reservoir1 is a component in a reservoir model, used for data processing and analysis."</data>
      <data key="d2">2fa8f7c2f23059f64f6d2452b6e7af49,8648b5740b93d805f139d9745e1171e8</data>
    </node>
    <node id="&quot;RESERVOIR2&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"reservoir2 is a component in a system, likely a part of a larger organization or system."
"Reservoir2 is a component in a reservoir model, used for data processing and analysis."</data>
      <data key="d2">2fa8f7c2f23059f64f6d2452b6e7af49,8648b5740b93d805f139d9745e1171e8</data>
    </node>
    <node id="&quot;RESERVOIR3&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"reservoir3 is a component in a system, likely a part of a larger organization or system."
"Reservoir3 is a component in a reservoir model, used for data processing and analysis."</data>
      <data key="d2">2fa8f7c2f23059f64f6d2452b6e7af49,8648b5740b93d805f139d9745e1171e8</data>
    </node>
    <node id="&quot;READOUT1&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"readout1 is a component in a system, likely a part of a larger organization or system."</data>
      <data key="d2">2fa8f7c2f23059f64f6d2452b6e7af49</data>
    </node>
    <node id="&quot;READOUT2&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"readout2 is a component in a system, likely a part of a larger organization or system."</data>
      <data key="d2">2fa8f7c2f23059f64f6d2452b6e7af49</data>
    </node>
    <node id="&quot;NEXT GENERATION RESERVOIR COMPUTING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Next Generation Reservoir Computing is a machine learning algorithm for processing information generated by dynamical systems, which requires small training data sets and minimal computing resources."</data>
      <data key="d2">88b1448c89d0650dad09fe96cca86cee</data>
    </node>
    <node id="&quot;NATURE COMMUNICATIONS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Nature Communications is the journal where the paper on Next Generation Reservoir Computing was published."</data>
      <data key="d2">88b1448c89d0650dad09fe96cca86cee</data>
    </node>
    <node id="&quot;DOI: 10.1038/S41467-021-25801-2&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"DOI: 10.1038/s41467-021-25801-2 is the Digital Object Identifier for the paper on Next Generation Reservoir Computing."</data>
      <data key="d2">88b1448c89d0650dad09fe96cca86cee</data>
    </node>
    <node id="&quot;NONLINEAR VECTOR AUTO-REGRESSION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Nonlinear Vector Auto-regression is a method that excels at reservoir computing benchmark tasks, requiring fewer meta-parameters and providing interpretable results."</data>
      <data key="d2">244217eb4738aae272df8949bdaaf131</data>
    </node>
    <node id="&quot;NVAR MACHINE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"NVAR Machine is a system that implements equations to process input data and construct feature vectors."</data>
      <data key="d2">5430aac4404b66ea20503e5cac4d4328</data>
    </node>
    <node id="&quot;LINEAR FEATURES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Linear Features are a part of the feature vector constructed by the NVAR Machine, made of input data concatenated with delayed inputs."</data>
      <data key="d2">5430aac4404b66ea20503e5cac4d4328</data>
    </node>
    <node id="&quot;NONLINEAR REPRESENTATIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Nonlinear Representations are a part of the feature vector constructed by the NVAR Machine, using unique monomials of order n of the inputs."</data>
      <data key="d2">5430aac4404b66ea20503e5cac4d4328</data>
    </node>
    <node id="&quot;FEATURE VECTOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Feature Vector is the final output of the NVAR Machine, composed of Linear Features and Nonlinear Representations."</data>
      <data key="d2">5430aac4404b66ea20503e5cac4d4328</data>
    </node>
    <node id="&quot;STRANGE ATTRACTOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Strange Attractor is a mathematical concept used to describe the behavior of a system over time, which is processed by a traditional RC and used as input for an NG-RC."</data>
      <data key="d2">399d166d41a7d3d575222d30699a29e4</data>
    </node>
    <node id="&quot;TRADITIONAL RC&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Traditional RC is a system that processes time-series data associated with a strange attractor using an artificial recurrent neural network."</data>
      <data key="d2">399d166d41a7d3d575222d30699a29e4</data>
    </node>
    <node id="&quot;NG-RC&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"NG-RC is a next-generation system that performs a forecast using a linear weight of time-delay states and nonlinear functionals of the time-series data, which is an improvement over traditional RC systems."</data>
      <data key="d2">399d166d41a7d3d575222d30699a29e4</data>
    </node>
    <node id="&quot;NVAR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"NVAR is an organization or method mentioned for Lorenz Attractor forecasting."
"NVAR is a type of model that combines linear and nonlinear components to capture complex dynamics."
"NVAR is a model used for time series prediction, connected to a readout layer with offline learning using regularized linear regression."
"NVAR is a model with relevant features that are learned as soon as all the delayed signals are non-zeros."
"nvar is a variable used in the provided code, which could represent a function or a process."
"NVAR is a model used for analyzing and predicting time series data, such as the dynamics of a system."
"NVAR is a method or technique used for forecasting, as mentioned in the context of the Double Scroll Attractor."
"NVAR is a model or technique mentioned in the text, used for analyzing the dynamics of the Double Scroll Attractor."</data>
      <data key="d2">2035514bf3ab5b7f12ae1321972551f1,41ea479401d7ff7c83c9d38c91d76cd9,4f0156c4eb24a5c168fff8417c6f046f,59c163f6fc13814d6ae0ff1b04d22653,684b1edf65b327cc06ceb69ca1279d74,770691846086629ac7d541f51760552c,7b8e1f350eefb392053be12f35fe7daf,ac2eb4232eaa7c1adbf00d4a0be3d799</data>
    </node>
    <node id="&quot;RESERVOIR COMPUTING MACHINE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Reservoir Computing Machine is a traditional method for processing time-series data, mentioned in the context of a figure."</data>
      <data key="d2">41ea479401d7ff7c83c9d38c91d76cd9</data>
    </node>
    <node id="&quot;NEXT GENERATION RESERVOIR COMPUTING MACHINE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Next Generation Reservoir Computing Machine is a method that performs forecasting using linear weights and nonlinear functionals, mentioned in the context of a figure."</data>
      <data key="d2">41ea479401d7ff7c83c9d38c91d76cd9</data>
    </node>
    <node id="&quot;LORENZ (1963)&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Lorenz (1963) is the event when Edward Lorenz introduced the Lorenz Attractor."</data>
      <data key="d2">74dd26ac71a37c92f3eda8552701ca33</data>
    </node>
    <node id="&quot;REGULARIZED LINEAR REGRESSION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Regularized Linear Regression is a method used for training the model to infer relationships, with a regularization parameter set to 2.5 &#215; 10^-6."</data>
      <data key="d2">ac2eb4232eaa7c1adbf00d4a0be3d799</data>
    </node>
    <node id="&quot;MATHEMATICAL MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Mathematical Model is a representation of a system or process using mathematical equations."
"Mathematical Model refers to the use of mathematical equations to represent and understand complex systems, such as the Double Scroll Attractor."</data>
      <data key="d2">770691846086629ac7d541f51760552c,ac2eb4232eaa7c1adbf00d4a0be3d799</data>
    </node>
    <node id="&quot;CHAOTIC SYSTEM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Chaotic System is a type of system that exhibits sensitive dependence on initial conditions, making it difficult to predict its long-term behavior."</data>
      <data key="d2">ac2eb4232eaa7c1adbf00d4a0be3d799</data>
    </node>
    <node id="&quot;DIFFERENTIAL EQUATIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Differential Equations are mathematical equations that describe the relationship between a function and its derivatives."
"Differential Equations are mathematical equations that describe the relationship between a function and its derivatives."</data>
      <data key="d2">10112a11d47463e2aad7352c52922d61,ac2eb4232eaa7c1adbf00d4a0be3d799</data>
    </node>
    <node id="&quot;LINEAR COMPONENT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Linear Component refers to the part of the NVAR model that captures linear relationships."</data>
      <data key="d2">ac2eb4232eaa7c1adbf00d4a0be3d799</data>
    </node>
    <node id="&quot;NONLINEAR COMPONENT&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Nonlinear Component refers to the part of the NVAR model that captures nonlinear relationships."</data>
      <data key="d2">ac2eb4232eaa7c1adbf00d4a0be3d799</data>
    </node>
    <node id="&quot;ONE-STEP-AHEAD PREDICTION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"One-step-ahead Prediction is the process of predicting the next value in a time series based on the current value."</data>
      <data key="d2">2035514bf3ab5b7f12ae1321972551f1</data>
    </node>
    <node id="&quot;LINEAR COEFFICIENTS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Linear Coefficients are learned by the Model and displayed in a plot."</data>
      <data key="d2">4f0156c4eb24a5c168fff8417c6f046f</data>
    </node>
    <node id="&quot;RES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"res is a variable used in the provided code, which could represent a data structure or a mathematical object to store results."</data>
      <data key="d2">7b8e1f350eefb392053be12f35fe7daf</data>
    </node>
    <node id="&quot;FIG&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"fig is a variable used in the provided code, which could represent a figure or a plot object."</data>
      <data key="d2">7b8e1f350eefb392053be12f35fe7daf</data>
    </node>
    <node id="&quot;AX&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ax is a variable used in the provided code, which could represent an axis object in a plot."</data>
      <data key="d2">7b8e1f350eefb392053be12f35fe7daf</data>
    </node>
    <node id="&quot;AX1&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ax1 is a variable used in the provided code, which could represent an axis object in a plot."</data>
      <data key="d2">7b8e1f350eefb392053be12f35fe7daf</data>
    </node>
    <node id="&quot;AX2&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"ax2 is a variable used in the provided code, which could represent an axis object in a plot."</data>
      <data key="d2">7b8e1f350eefb392053be12f35fe7daf</data>
    </node>
    <node id="&quot;ATTRACTOR&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Attractor is a mathematical concept that describes the long-term behavior of a system, in this case, a model."</data>
      <data key="d2">29ce72a8f609c311ebb852cc96aee54d</data>
    </node>
    <node id="&quot;DX&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"dX is the derivative of X, representing the rate of change of the variables in the model."</data>
      <data key="d2">29ce72a8f609c311ebb852cc96aee54d</data>
    </node>
    <node id="&quot;TRAIN STEPS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Train Steps refer to the number of iterations or steps used to train the model."</data>
      <data key="d2">29ce72a8f609c311ebb852cc96aee54d</data>
    </node>
    <node id="&quot;WARM STEPS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Warm Steps refer to the number of initial steps used to initialize the model before training."</data>
      <data key="d2">29ce72a8f609c311ebb852cc96aee54d</data>
    </node>
    <node id="&quot;TEST STEPS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Test Steps refer to the number of steps used to evaluate the model's performance after training."</data>
      <data key="d2">29ce72a8f609c311ebb852cc96aee54d</data>
    </node>
    <node id="&quot;X-COORDINATE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The x-coordinate is one of the three variables in the Lorenz System, representing the flow's deviation from the mean flow in the x-direction."</data>
      <data key="d2">ac3456a3574f6939fcb6d5242c202810</data>
    </node>
    <node id="&quot;Y-COORDINATE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The y-coordinate is one of the three variables in the Lorenz System, representing the flow's deviation from the mean flow in the y-direction."</data>
      <data key="d2">ac3456a3574f6939fcb6d5242c202810</data>
    </node>
    <node id="&quot;Z-COORDINATE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The z-coordinate is one of the three variables in the Lorenz System, representing the flow's deviation from the mean flow in the z-direction."</data>
      <data key="d2">ac3456a3574f6939fcb6d5242c202810</data>
    </node>
    <node id="&quot;RECONSTRUCTION&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Reconstruction is the process of predicting the z-coordinate at time t of a point in the Lorenz Attractor given the x-coordinate and y-coordinate at time t-1."</data>
      <data key="d2">ac3456a3574f6939fcb6d5242c202810</data>
    </node>
    <node id="&quot;PREDICTION OF Z COORDINATE&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Prediction of z coordinate is the event of estimating the z coordinate of a point in the Lorenz attractor given the x and y coordinates at a previous time step."</data>
      <data key="d2">715720b663e85c5e16cbf8b1ef4ec208</data>
    </node>
    <node id="&quot;NVAR MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The NVAR Model is a nonlinear autoregressive model used for time series prediction and analysis, with parameters such as delay, order, and strides."</data>
      <data key="d2">c838b1b4744bc0f400abf85f791950cf</data>
    </node>
    <node id="&quot;TRAINER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"The Trainer is the individual who sets up and trains the models, including the Lorenz Model, NVAR Model, and Ridge Regression."</data>
      <data key="d2">c838b1b4744bc0f400abf85f791950cf</data>
    </node>
    <node id="&quot;TUTORIAL&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The text describes a tutorial on using ReservoirPy to create and train an Echo State Network (ESN) on sequential data."</data>
      <data key="d2">74c073137c970e32982756d008532cb8</data>
    </node>
    <node id="&quot;ARTIFICIAL RATE NEURONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Artificial Rate Neurons are a type of neuron used in the reservoir of an Echo State Network, which are randomly connected to their inputs and to themselves."</data>
      <data key="d2">711ec1b4879d910d0df0a477c9e240ba</data>
    </node>
    <node id="&quot;REGULARIZED RIDGE REGRESSION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Regularized Ridge Regression is a learning algorithm used to train the readout of an ESN."</data>
      <data key="d2">cdc64af0dde941250d89b191d0666c9b</data>
    </node>
    <node id="&quot;RESERVOIR CLASS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Reservoir class is a component of the ReservoirPy library used to create a reservoir for an ESN."</data>
      <data key="d2">9b360c6a33aafa6827417de5bd4faa82</data>
    </node>
    <node id="&quot;SINE WAVE&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"A sine wave is a mathematical curve that describes a smooth periodic oscillation, used as input data for the reservoir."
"Sine Wave is a mathematical function that generates a continuous wave-like pattern, used as an example in the text."
"Sine Wave is a mathematical function used as an example in the text, with a blue wave representing the current values and a red wave representing the future values."
"A sine wave is a mathematical curve that describes a smooth periodic oscillation, which is used as input data in the provided code."
"Sine Wave is a mathematical function that describes a smooth, continuous oscillation."
"A Sine Wave is a mathematical curve that describes a smooth periodic oscillation."</data>
      <data key="d2">2bcc39da2ecef3011cc3da428fca5dd5,34b9ce80a22112b32e063179511af6e0,5366a81a025c098744b5d6f1432c2fbc,71f966d00b6d0eceb580d00b9cb86b1e,a58317c7e13f27d513fc7671fd187ecb,f2d5625f36aa4cb036089ce89ec607eb</data>
    </node>
    <node id="&quot;TIMESTEPS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Timesteps refer to the discrete time intervals at which data is processed or analyzed."</data>
      <data key="d2">9e84667b4aeb0789808517f0912043ce</data>
    </node>
    <node id="&quot;INPUT TIMESERIES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Input Timeseries is a data series used as input for training a machine learning model."</data>
      <data key="d2">5366a81a025c098744b5d6f1432c2fbc</data>
    </node>
    <node id="&quot;TARGET TIMESERIES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Target Timeseries is a data series that the machine learning model is trained to predict based on the Input Timeseries."</data>
      <data key="d2">5366a81a025c098744b5d6f1432c2fbc</data>
    </node>
    <node id="&quot;ONE-TIMESTEP-AHEAD PREDICTION TASK&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"One-timestep-ahead Prediction Task is a machine learning task where the goal is to predict the next timestep of a timeseries based on its current timestep."</data>
      <data key="d2">5366a81a025c098744b5d6f1432c2fbc</data>
    </node>
    <node id="&quot;PREDICTIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Predictions refer to the output of the ESN model, which attempts to forecast the next value in a timeseries."</data>
      <data key="d2">7b294b788fe5ee385d08c4aabe2ca71d</data>
    </node>
    <node id="&quot;RUNNING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Running is the process of using the trained ESN Model to make predictions on unseen data."</data>
      <data key="d2">8ade7819a5f8d1ec26e9bdbd059142e6</data>
    </node>
    <node id="&quot;RESERVOIR COMPUTING MODEL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"A Reservoir Computing Model is a type of recurrent neural network used for time series prediction and other tasks."</data>
      <data key="d2">71f966d00b6d0eceb580d00b9cb86b1e</data>
    </node>
    <node id="&quot;FORCED FEEDBACKS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Forced Feedbacks are a technique used in reservoir computing models to control the internal dynamics of the model."</data>
      <data key="d2">c82c9d05b211ff65131f70eb8cb13513</data>
    </node>
    <node id="&quot;FITTING&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Fitting is the process of training a reservoir computing model to learn patterns and relationships in the data."</data>
      <data key="d2">c82c9d05b211ff65131f70eb8cb13513</data>
    </node>
    <node id="&quot;FIT METHOD&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Fit Method is used to train the echo state network model by optimizing the parameters of the readout layer."</data>
      <data key="d2">ed28ba3543e07641536ff1eb5e0749dd</data>
    </node>
    <node id="&quot;RUN METHOD&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Run Method is used to generate predictions or forecasts using the trained echo state network model."</data>
      <data key="d2">ed28ba3543e07641536ff1eb5e0749dd</data>
    </node>
    <node id="&quot;WARMUP&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Warmup is a technique used to initialize the reservoir with a sequence of input data before making predictions or forecasts."
"Warmup is a phase in the Generative Mode method, where initial data is used to prepare the system before generating new data."</data>
      <data key="d2">70db98fabc82fc96ecf8cc2c023b586b,ed28ba3543e07641536ff1eb5e0749dd</data>
    </node>
    <node id="&quot;ESN_MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"esn_model is a machine learning model constructed using the Reservoir and Ridge organizations."</data>
      <data key="d2">751b176a8d6149a853e597c65a6fe0cf</data>
    </node>
    <node id="&quot;NORMAL_W&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"normal_w is a concept or variable used in the provided code, likely representing a function for generating normal distribution weights."</data>
      <data key="d2">751b176a8d6149a853e597c65a6fe0cf</data>
    </node>
    <node id="&quot;RESERVOIRPY.NODES.RESERVOIR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"reservoirpy.nodes.Reservoir is an organization or module used in the provided code, likely a part of a larger machine learning framework."</data>
      <data key="d2">751b176a8d6149a853e597c65a6fe0cf</data>
    </node>
    <node id="&quot;RESERVOIRPY.MAT&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"reservoirpy.mat is an organization or module used in the provided code, likely a part of a larger machine learning framework."</data>
      <data key="d2">751b176a8d6149a853e597c65a6fe0cf</data>
    </node>
    <node id="&quot;HIERARCHICAL ESN&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Hierarchical ESN is a type of echo state network (ESN) that uses multiple reservoirs and readouts to improve performance."
"Hierarchical ESN is a type of reservoir computing model mentioned in the text, consisting of multiple reservoirs and readouts."</data>
      <data key="d2">00d22666fe697ffb66c2392939f45b39,e39809b687cd044a7918eca37727a188</data>
    </node>
    <node id="&quot;SEQUENTIAL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Sequential is a library or framework mentioned in the text, potentially used for data processing."</data>
      <data key="d2">e39809b687cd044a7918eca37727a188</data>
    </node>
    <node id="&quot;MULTI-INPUTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Multi-inputs is a term used in the context of reservoir computing, referring to the use of multiple data streams or inputs in the model."</data>
      <data key="d2">e39809b687cd044a7918eca37727a188</data>
    </node>
    <node id="&quot;PLOTTING FUNCTION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The Plotting Function is used to visualize data, such as timeseries and phase diagrams."</data>
      <data key="d2">c5c29ba06a5cc70a086c2c2c8858e5aa</data>
    </node>
    <node id="&quot;JUPYTER NOTEBOOK&quot;">
      <data key="d0">"TECHNOLOGY"</data>
      <data key="d1">"Jupyter Notebook is an interactive environment used for data analysis and building the Echo State Network."</data>
      <data key="d2">41fa16855df7da666dc6fc38d2f8ee53</data>
    </node>
    <node id="&quot;CHAPTER 2&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Chapter 2 is a section of the text that discusses the use of a generative mode in the context of data analysis."</data>
      <data key="d2">e396354e3a9be76616392af11f56e671</data>
    </node>
    <node id="&quot;X_TRAIN3&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"X_train3 is a dataset used for training a model."</data>
      <data key="d2">0c5a253fb2bcebe8674581a5dc12fd96</data>
    </node>
    <node id="&quot;Y_TRAIN3&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"y_train3 is a dataset used for training a model."</data>
      <data key="d2">0c5a253fb2bcebe8674581a5dc12fd96</data>
    </node>
    <node id="&quot;X_TEST3&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"X_test3 is a dataset used for testing a model."</data>
      <data key="d2">0c5a253fb2bcebe8674581a5dc12fd96</data>
    </node>
    <node id="&quot;Y_TEST3&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"y_test3 is a dataset used for testing a model."</data>
      <data key="d2">0c5a253fb2bcebe8674581a5dc12fd96</data>
    </node>
    <node id="&quot;CHAPTER 3&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Chapter 3 is a section discussing online learning."</data>
      <data key="d2">0c5a253fb2bcebe8674581a5dc12fd96</data>
    </node>
    <node id="&quot;FORCE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"FORCE is a node used in the reservoir network."
"FORCE is a readout algorithm used in reservoir computing networks."
"FORCE is a type of readout algorithm used in the Echo State Network (ESN) model."</data>
      <data key="d2">0c5a253fb2bcebe8674581a5dc12fd96,1365a36c76afc697ac626fd0f784804a,324a8f3fb4d19b91457a99999e6d3d17</data>
    </node>
    <node id="&quot;RESERVOIR COMPUTING NETWORKS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Reservoir Computing Networks are a type of recurrent neural network that use a reservoir to process input data."</data>
      <data key="d2">324a8f3fb4d19b91457a99999e6d3d17</data>
    </node>
    <node id="&quot;TESTING DATA&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Testing Data refers to the data used to evaluate the performance of the trained Echo State Network (ESN) model."</data>
      <data key="d2">1365a36c76afc697ac626fd0f784804a</data>
    </node>
    <node id="&quot;LEAK RATE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Leak Rate is a parameter used to control the rate at which information is lost from the reservoir in the Echo State Network (ESN) model."
"Leak Rate is a parameter of the Reservoir component that controls the rate at which the current state is forgotten."
"Leak Rate is a parameter used in the design of recurrent neural networks, which is mentioned in the text."</data>
      <data key="d2">1365a36c76afc697ac626fd0f784804a,72e6eee633bcb5b1458c4cee3975cee1,b957e1bf5bf175c7630222ca742c7933</data>
    </node>
    <node id="&quot;TQDM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"tqdm is a library that provides a progress bar for loops, used to monitor the progress of data processing."</data>
      <data key="d2">9fdaabd6c7e893a275a3848c10007477</data>
    </node>
    <node id="&quot;DATA SCIENTIST&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"The data scientist is the person who wrote the provided code, utilizing libraries such as sklearn, joblib, and tqdm for data analysis and model training."</data>
      <data key="d2">9fdaabd6c7e893a275a3848c10007477</data>
    </node>
    <node id="&quot;TRAINING THE ESN&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Training the ESN refers to the process of fitting the ESN model to the training data, allowing it to learn patterns and make predictions."</data>
      <data key="d2">eb7a223eeb120e3fcc45a96a6018707d</data>
    </node>
    <node id="&quot;ROBOT PERFORMANCE EVALUATION&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Robot Performance Evaluation is a process that involves comparing predicted and actual outcomes to assess the model's accuracy."</data>
      <data key="d2">e7d249cdab85dc69b631d43ac6b62915</data>
    </node>
    <node id="&quot;ROBOT PERFORMANCE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Robot Performance refers to the accuracy and effectiveness of a robot in its tasks."</data>
      <data key="d2">e7d249cdab85dc69b631d43ac6b62915</data>
    </node>
    <node id="&quot;CANARY SONG&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Canary Song refers to the song produced by a canary, which may contain hidden information."</data>
      <data key="d2">e7d249cdab85dc69b631d43ac6b62915</data>
    </node>
    <node id="&quot;CHAPTER 5&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Chapter 5 is a section of the text that discusses a use case in the wild, specifically decoding a canary song."</data>
      <data key="d2">1e8ee805d22cd143d2372d300997d253</data>
    </node>
    <node id="&quot;IPYTHON&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"IPython is an open-source project that provides a rich environment for interactive computing and data visualization."</data>
      <data key="d2">1e8ee805d22cd143d2372d300997d253</data>
    </node>
    <node id="&quot;PANDAS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Pandas is a software library for data manipulation and analysis, used for handling and processing data in the text."</data>
      <data key="d2">1e8ee805d22cd143d2372d300997d253</data>
    </node>
    <node id="&quot;LIFTER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"lifter is a parameter used in the MFCC feature extraction process, indicating its role in the data analysis."</data>
      <data key="d2">9a54cf00618f7dcfb151c8dc8f7471bd</data>
    </node>
    <node id="&quot;N_MFCC&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"n_mfcc is a parameter used in the MFCC feature extraction process, indicating its role in the data analysis."</data>
      <data key="d2">9a54cf00618f7dcfb151c8dc8f7471bd</data>
    </node>
    <node id="&quot;LBR.FEATURE.DELTA&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"lbr.feature.delta is a function used to calculate the delta features of a signal, which are used for feature extraction."</data>
      <data key="d2">71366a4c7e791080872ba783d3787bd7</data>
    </node>
    <node id="&quot;NP.VSTACK&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"np.vstack is a function from the NumPy library used to vertically stack arrays."
"np.vstack is a NumPy function used to vertically stack arrays, which is used in the context of one-hot encoding and inverse transformations."</data>
      <data key="d2">1db5e6cd356c6066227de5e273de1abe,71366a4c7e791080872ba783d3787bd7</data>
    </node>
    <node id="&quot;NP.ARRAY&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"np.array is a function from the NumPy library used to create a NumPy array from an input."</data>
      <data key="d2">71366a4c7e791080872ba783d3787bd7</data>
    </node>
    <node id="&quot;INPUTS SCALING&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Inputs Scaling is a parameter that scales the input data to improve the performance of the ESN model."</data>
      <data key="d2">72e6eee633bcb5b1458c4cee3975cee1</data>
    </node>
    <node id="&quot;OUTPUTS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"outputs are the predicted results generated by a model, which are compared to the actual targets for evaluation."</data>
      <data key="d2">1db5e6cd356c6066227de5e273de1abe</data>
    </node>
    <node id="&quot;RESERVOIR CONNECTIVITY&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Reservoir Connectivity is a parameter in Reservoir Computing that determines the interconnectivity among the neurons in the reservoir."</data>
      <data key="d2">26d78bc91458f47d4053954505c45f92</data>
    </node>
    <node id="&quot;LEAKING RATES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Leaking Rates are parameters used in the simulations run by the Reservoir organization."</data>
      <data key="d2">548c454b31f852543b600df173bd44ab</data>
    </node>
    <node id="&quot;DOUBLESCROLL&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Doublescroll is a mathematical concept used to generate data for simulations, with parameters such as timesteps and initial conditions."</data>
      <data key="d2">548c454b31f852543b600df173bd44ab</data>
    </node>
    <node id="&quot;OPTIMIZE HYPERPARAMETERS&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"Optimize Hyperparameters is an event or process where the parameters of a system, such as the Reservoir organization's simulations, are adjusted to improve performance or accuracy."</data>
      <data key="d2">548c454b31f852543b600df173bd44ab</data>
    </node>
    <node id="&quot;OBJECTIVE FUNCTIONS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Objective Functions are used to evaluate the performance of reservoir computing models and guide the search for optimal parameters."</data>
      <data key="d2">0113164912437e96423379cb9c039f56</data>
    </node>
    <node id="&quot;RESERVOIR COMPUTING MODELS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Reservoir Computing Models are a type of recurrent neural network used for time series prediction and analysis, created using the ReservoirPy library."</data>
      <data key="d2">0113164912437e96423379cb9c039f56</data>
    </node>
    <node id="&quot;R-SQUARED&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"R-squared is a metric used to evaluate the performance of a model, representing the proportion of the variance in the dependent variable that is predictable from the independent variable."</data>
      <data key="d2">82ff270b1bbdfe0ee11e603de1e326c7</data>
    </node>
    <node id="&quot;RANDOM METHOD&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The Random Method is the method used by Hyperopt to choose different sets of parameters."</data>
      <data key="d2">65ba78d1f678e080bd930319c54234ef</data>
    </node>
    <node id="&quot;CHOICE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Choice is a parameter in the Hyperopt configuration, which is mentioned in the text."</data>
      <data key="d2">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </node>
    <node id="&quot;LOGUNIFORM&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Loguniform is a parameter in the Hyperopt configuration, which is mentioned in the text."</data>
      <data key="d2">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </node>
    <node id="&quot;BEST&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Best is a variable used to store the best hyperparameters found by the hyperparameter optimization process, which is mentioned in the text."</data>
      <data key="d2">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </node>
    <node id="&quot;RESEARCH PAPER&quot;">
      <data key="d0">"EVENT"</data>
      <data key="d1">"The text appears to be a research paper or a documentation, as it discusses methods and results."</data>
      <data key="d2">870f29520f7a1c42eecb0c4ff855f09e</data>
    </node>
    <node id="&quot;SKLEARN.METRICS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"sklearn.metrics is a library in Python used for evaluating machine learning models."</data>
      <data key="d2">9414efd266e7135a2cdd7461a888b045</data>
    </node>
    <node id="&quot;LPC (CEPSTRA)&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"LPC (cepstra) is a feature extraction method used in speech processing, which is used in the task of analyzing speaker data."</data>
      <data key="d2">688ebc7151bc148ac24dc7e2727d7afe</data>
    </node>
    <node id="&quot;SPEAKER DATA&quot;">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">688ebc7151bc148ac24dc7e2727d7afe</data>
    </node>
    <node id="&quot;LINEAR MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Linear Model is a component of ScikitLearnNode used to create a machine learning model for prediction."
"Linear Model is a type of predictive model that assumes a linear relationship between the input variables and the output variable."</data>
      <data key="d2">dc3bd3697a140b64d70e0e3ac6db6c7e,eebc9d7d2b66e3898b7d068c38fd200f</data>
    </node>
    <node id="&quot;LASSO&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Lasso is a type of linear model used in the code for prediction."
"Lasso is a type of linear model that uses shrinkage to reduce the complexity of the model and prevent overfitting."</data>
      <data key="d2">dc3bd3697a140b64d70e0e3ac6db6c7e,eebc9d7d2b66e3898b7d068c38fd200f</data>
    </node>
    <node id="&quot;ACCURACY SCORE&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Accuracy Score is a metric used in the code to evaluate the performance of the prediction model."</data>
      <data key="d2">dc3bd3697a140b64d70e0e3ac6db6c7e</data>
    </node>
    <node id="&quot;SCIKIT-LEARN NODE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Scikit-learn Node is a wrapper for using scikit-learn models in a node-based system."</data>
      <data key="d2">52d001cd1786e3d9f36e0c57538bc21e</data>
    </node>
    <node id="&quot;RESERVOIR MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"The Reservoir Model is a computing model that uses a reservoir of neurons to process data."</data>
      <data key="d2">0970cd32ce54f6ee1180ab237fdcefe1</data>
    </node>
    <node id="&quot;SK RIDGE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"SK Ridge is a model used in the Reservoir Model, which is a type of regression algorithm."</data>
      <data key="d2">0970cd32ce54f6ee1180ab237fdcefe1</data>
    </node>
    <node id="&quot;SK LOGISTIC&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"SK Logistic is a model used in the Reservoir Model, which is a type of classification algorithm."</data>
      <data key="d2">0970cd32ce54f6ee1180ab237fdcefe1</data>
    </node>
    <node id="&quot;SK PERCEPTRON&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"SK Perceptron is a model used in the Reservoir Model, which is a type of binary classification algorithm."</data>
      <data key="d2">0970cd32ce54f6ee1180ab237fdcefe1</data>
    </node>
    <node id="&quot;SPEAKER&quot;">
      <data key="d0">"PERSON"</data>
      <data key="d1">"Speaker is a role or entity that is being predicted or classified by the Reservoir Model."</data>
      <data key="d2">0970cd32ce54f6ee1180ab237fdcefe1</data>
    </node>
    <node id="&quot;RESERVOIR COMPUTER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Reservoir Computer is a type of computer system that uses a recurrent neural network with a sparsely connected hidden layer."</data>
      <data key="d2">dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </node>
    <node id="&quot;SPARSELY CONNECTED HIDDEN LAYER&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Sparsely Connected Hidden Layer is a component of the Echo State Network with typically 1% connectivity."</data>
      <data key="d2">dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </node>
    <node id="&quot;OUTPUT NEURONS&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Output Neurons are the final layer of neurons in the Echo State Network that can produce or reproduce specific temporal patterns."</data>
      <data key="d2">dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </node>
    <node id="&quot;SYNAPSES&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Synapses are the connections between neurons in the Echo State Network that transmit signals."</data>
      <data key="d2">dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </node>
    <node id="&quot;GAUSSIAN PROCESS MODEL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Gaussian Process Model is a probabilistic model used in the context of prediction generation, given the training data."
"Gaussian Process Model is a statistical model that uses Gaussian priors and an ESN-driven kernel function."</data>
      <data key="d2">a4b801e70cf2ba3a3101d34899450087,dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </node>
    <node id="&quot;AURESERVOIR&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Aureservoir is an efficient C++ library for various kinds of echo state networks with python/numpy bindings."
"aureservoir is an efficient C++ library for various kinds of echo state networks with python/numpy bindings."</data>
      <data key="d2">a4b801e70cf2ba3a3101d34899450087,dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </node>
    <node id="&quot;MATLAB CODE&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Matlab code is an efficient implementation of an echo state network in Matlab."</data>
      <data key="d2">a4b801e70cf2ba3a3101d34899450087</data>
    </node>
    <node id="&quot;PYESN&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"pyESN is a simple echo state networks implementation in Python."</data>
      <data key="d2">a4b801e70cf2ba3a3101d34899450087</data>
    </node>
    <node id="&quot;SCHILLER AND STEIL&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"Schiller and Steil are researchers who demonstrated the use of the Backpropagation Decorrelation learning rule for RNNs."
"Schiller and Steil are a research team that demonstrated the dominance of output weight changes in conventional training approaches for RNNs."</data>
      <data key="d2">158f53cd85edbb4f2e4c77b78c5e7acc,b32958d42199d47252887dc7be40ab5a</data>
    </node>
    <node id="&quot;RANDOM, NONLINEAR MEDIUM&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"The fixed RNN acts as a random, nonlinear medium whose dynamic response is used as a signal base."</data>
      <data key="d2">10112a11d47463e2aad7352c52922d61</data>
    </node>
    <node id="&quot;AUTODIFFERENTIATION&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Autodifferentiation is a technique used in deep learning libraries to compute gradients automatically."
"Autodifferentiation is a technique used in deep learning libraries to automatically compute gradients, which has improved the efficiency and stability of neural network training."</data>
      <data key="d2">10112a11d47463e2aad7352c52922d61,4b89d9404fd683ecd03d5846ee2d86ce</data>
    </node>
    <node id="&quot;GRU&quot;">
      <data key="d0">"ORGANIZATION"</data>
      <data key="d1">"GRU is a type of recurrent neural network architecture that is similar to LSTM but with fewer gates."
"GRU is a type of recurrent neural network architecture that is similar to LSTM and is also used to address issues found in traditional RNNs."</data>
      <data key="d2">10112a11d47463e2aad7352c52922d61,4b89d9404fd683ecd03d5846ee2d86ce</data>
    </node>
    <node id="&quot;MECHANICAL NANOOSCILLATORS&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Mechanical Nanooscillators are a type of non-digital computer substrate that can be used as a reservoir in ESNs."
"Mechanical Nanooscillators are a type of object used as a nonlinear reservoir."</data>
      <data key="d2">4b89d9404fd683ecd03d5846ee2d86ce,7767d42e08c8eab856e8e3025c692309</data>
    </node>
    <node id="&quot;POLYMER MIXTURES&quot;">
      <data key="d0">"CONCEPT"</data>
      <data key="d1">"Polymer Mixtures are a type of non-digital computer substrate that can be used as a reservoir in ESNs."
"Polymer Mixtures are a type of object used as a nonlinear reservoir."</data>
      <data key="d2">4b89d9404fd683ecd03d5846ee2d86ce,7767d42e08c8eab856e8e3025c692309</data>
    </node>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;CLASSIFICATION TASK&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Reservoir Computing is well-suited to Classification Tasks, as mentioned in the text."
"Reservoir Computing is also well-suited for classification tasks, as it captures the dynamic behavior of input sequences, providing rich representations for categorizing inputs into discrete classes."</data>
      <data key="d6">716940af834825642e01a3cb59a7e006,86a136730a696f1a817bd530dbff778d</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;ECHO STATE NETWORKS&quot;">
      <data key="d4">4.0</data>
      <data key="d5">"Echo State Networks are a type of Reservoir Computing architecture."
"Echo State Networks are a type of Recurrent Neural Network that is based on the principles of Reservoir Computing."
"Echo State Networks are a type of neural network that falls under the concept of Reservoir Computing, which also includes Liquid State Machines and the Backpropagation Decorrelation learning rule for RNNs."
"Reservoir Computing is the underlying idea used in the construction of Echo State Networks, which are a variant of reservoir computing."</data>
      <data key="d6">158f53cd85edbb4f2e4c77b78c5e7acc,6de297d888d10db4c987b5eafc6398b2,8e16fc97c32c39d7961b52e21b99dc53,a3368f9cab1f65643dba089af5a1f95e</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;RECURRENT NEURAL NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is a type of Recurrent Neural Network architecture."</data>
      <data key="d6">8e16fc97c32c39d7961b52e21b99dc53</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;SUPPORT VECTOR MACHINES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is similar to Support Vector Machines in transforming inputs into dynamic, non-linear, high-dimensional representations."</data>
      <data key="d6">8e16fc97c32c39d7961b52e21b99dc53</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;TIME SERIES FORECASTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is used in Time Series Forecasting tasks."</data>
      <data key="d6">8e16fc97c32c39d7961b52e21b99dc53</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;SEQUENCE GENERATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is used in Sequence Generation tasks."</data>
      <data key="d6">8e16fc97c32c39d7961b52e21b99dc53</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;INRIA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Inria has published a document on Reservoir Computing and maintains a reservoirPy page and a github repository."</data>
      <data key="d6">8e16fc97c32c39d7961b52e21b99dc53</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;FEEDBACK CONNECTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feedback Connections in Reservoir Computing architectures help stabilize and control the activity of neurons in the reservoir."</data>
      <data key="d6">b5b73413fbe4ab8b61c4a939fe6c6a2b</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;RESERVOIR NEURONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Neurons in Reservoir Computing architectures process input signals, and their connections do not need to be trained as they are predefined."</data>
      <data key="d6">b5b73413fbe4ab8b61c4a939fe6c6a2b</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;RANDOM HIGH-DIMENSIONAL VECTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing uses a reservoir to generate a Random High-Dimensional Vector, which captures intricate patterns and dynamics of the input data."</data>
      <data key="d6">82a734e7c7ada95b1c99783140dd7168</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;TIMESTEP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing processes data in discrete timesteps to make predictions or analyze time series data."</data>
      <data key="d6">56cde5dc9d350498c1544cd57733ca8f</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;INPUT DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input Data is used as the source of data for the Reservoir Computing model to make predictions or analyze time series data."</data>
      <data key="d6">56cde5dc9d350498c1544cd57733ca8f</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;STATE VECTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir Computing model maintains an internal representation of its current state in the form of a State Vector, which is updated after processing each timestep of data."</data>
      <data key="d6">56cde5dc9d350498c1544cd57733ca8f</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;NULL VECTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing often initializes the internal state of the reservoir to a null vector."</data>
      <data key="d6">baeb61b8c35e75d37a338fafd6a417fa</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;SHAPE ATTRIBUTE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The shape attribute is used to determine the size and structure of arrays in reservoir computing, such as the state vector."</data>
      <data key="d6">baeb61b8c35e75d37a338fafd6a417fa</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;EMPTY FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The empty function is used to create a new array without initializing the entries in reservoir computing, allowing for later data filling."</data>
      <data key="d6">baeb61b8c35e75d37a338fafd6a417fa</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;OUTPUT DIMENSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The output dimension is a key concept in reservoir computing, specifying the size of the output and used to determine the size of the state vector."</data>
      <data key="d6">baeb61b8c35e75d37a338fafd6a417fa</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;FITTING PROCESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Fitting Process is a common technique used in Reservoir Computing to train models and learn the appropriate connections from the reservoir to the readout neurons."</data>
      <data key="d6">87757855658e1d198ec49a3290760dd5</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;DEEP ARCHITECTURE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep Architecture in reservoir computing refers to a model that contains multiple layers of reservoirs, connected in a hierarchical manner."</data>
      <data key="d6">a16039f06e545c915f8e7668c39c3e5c</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;MACHINE LEARNING&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Reservoir Computing is a type of machine learning model that falls under the broader field of Machine Learning."
"Reservoir Computing is a field of Machine Learning that focuses on training Recurrent Neural Networks efficiently."</data>
      <data key="d6">6de297d888d10db4c987b5eafc6398b2,a16039f06e545c915f8e7668c39c3e5c</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;COMPLEX MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Complex Models are a subset of Reservoir Computing, focusing on more sophisticated and intricate models."</data>
      <data key="d6">22499cd4a0b7216dad5b05eb109fcb73</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;REGRESSION TASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is well-suited for regression tasks, as it efficiently handles temporal and sequential data, making it ideal for predicting continuous outputs."</data>
      <data key="d6">716940af834825642e01a3cb59a7e006</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;REGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is well-suited for regression tasks, as it efficiently handles temporal and sequential data, making it ideal for predicting continuous outputs."</data>
      <data key="d6">1c462a6eef00aac37dc1ab33a689b930</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;CLASSIFICATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is also well-suited for classification tasks, as it captures the dynamic behavior of input sequences, providing rich representations for categorizing inputs into discrete classes."</data>
      <data key="d6">1c462a6eef00aac37dc1ab33a689b930</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;HYPERPARAMETERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing involves the exploration and optimization of hyperparameters to improve the performance of a task."</data>
      <data key="d6">4a9f33fa18891b67267b7615d61caaac</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;SPECTRAL RADIUS (SR)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Spectral Radius (SR) is a key hyperparameter in Reservoir Computing that can significantly impact the performance of a task."</data>
      <data key="d6">4a9f33fa18891b67267b7615d61caaac</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;INPUT SCALING (IS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input Scaling (IS) is a key hyperparameter in Reservoir Computing that can significantly impact the performance of a task."</data>
      <data key="d6">4a9f33fa18891b67267b7615d61caaac</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;LEAKING RATE (LR)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Leaking Rate (LR) is a key hyperparameter in Reservoir Computing that can significantly impact the performance of a task."</data>
      <data key="d6">4a9f33fa18891b67267b7615d61caaac</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;FEEDBACK SCALING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feedback Scaling is a hyperparameter in Reservoir Computing that can significantly impact the performance of a task."</data>
      <data key="d6">4a9f33fa18891b67267b7615d61caaac</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;ESN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ESN is a type of reservoir used in the context of Reservoir Computing."
"ESN is a type of reservoir computing model."</data>
      <data key="d6">069ae9388dfd52fec9c184c7168f64dd,f730c6800099724052a2d061f3cd8c2e</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;INPUT SCALING&quot;">
      <data key="d4">4.0</data>
      <data key="d5">"Input Scaling is a parameter used in Reservoir Computing to adjust the influence of each variable in a multivariate time-series."
"Input Scaling is a parameter used in Reservoir Computing to adjust the influence of each variable in a multivariate time-series."
"Input Scaling is a technique used in the Reservoir Computing model."
"Input Scaling is a parameter used in Reservoir Computing to adjust the strength of the input signal to the reservoir."</data>
      <data key="d6">26d78bc91458f47d4053954505c45f92,8553a88d9aaf4f71d359c721a1f6fa70,8e1f4f13f617b982d272175296ec99d3,8ecf03267c90a64376f5040307d98195</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;LEAKING RATE&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"Leaking Rate is a parameter used in Reservoir Computing to control the rate at which the reservoir state decays over time."
"Leaking Rate is a parameter used in Reservoir Computing to control the decay of the reservoir's state over time."
"Leaking Rate is a parameter used in Reservoir Computing to control the rate at which information is lost from the reservoir."</data>
      <data key="d6">26d78bc91458f47d4053954505c45f92,8553a88d9aaf4f71d359c721a1f6fa70,8ecf03267c90a64376f5040307d98195</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;CORRELATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Correlation is a measure used in Reservoir Computing to determine the relationship between reservoir states and inputs."</data>
      <data key="d6">8553a88d9aaf4f71d359c721a1f6fa70</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;TIME-SERIES DATA&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"Reservoir Computing is being used to process Time-series Data."
"Reservoir Computing is used for processing Time-series Data."
"Time-series Data is being processed using Reservoir Computing to analyze and understand the underlying patterns and dynamics."</data>
      <data key="d6">26d78bc91458f47d4053954505c45f92,8553a88d9aaf4f71d359c721a1f6fa70,8ecf03267c90a64376f5040307d98195</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;MULTIVARIATE TIME-SERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is being used to process Multivariate Time-series Data, which consists of multiple variables or dimensions."</data>
      <data key="d6">8553a88d9aaf4f71d359c721a1f6fa70</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;SPECTRAL RADIUS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Spectral Radius is a parameter used in Reservoir Computing to determine the stability and speed of the reservoir's dynamics."
"Spectral Radius is a parameter used in Reservoir Computing to determine the stability and dynamics of the reservoir."</data>
      <data key="d6">26d78bc91458f47d4053954505c45f92,8ecf03267c90a64376f5040307d98195</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;UNITS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Units refer to the number of processing elements in the reservoir of Reservoir Computing."
"Units are a component of the Reservoir Computing model."</data>
      <data key="d6">8e1f4f13f617b982d272175296ec99d3,8ecf03267c90a64376f5040307d98195</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;RC CONNECTIVITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RC Connectivity is a parameter used in Reservoir Computing to determine the sparsity of the connections between the reservoir units."</data>
      <data key="d6">8ecf03267c90a64376f5040307d98195</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;INPUT CONNECTIVITY&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Input Connectivity is a parameter used in Reservoir Computing to determine the sparsity of the connections between the input variables and the reservoir units."
"Input Connectivity is a parameter used in Reservoir Computing to determine the connection strength between the input signal and the reservoir neurons."</data>
      <data key="d6">26d78bc91458f47d4053954505c45f92,8ecf03267c90a64376f5040307d98195</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;IP RULE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The IP Rule is used in the context of Reservoir Computing, where it allows the internal dynamics to autonomously tune themselves to the optimal regime for a given task."</data>
      <data key="d6">83ded1f13bd74b00694092be69a83870</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;MU&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Mu is a parameter used in the Reservoir Computing model."</data>
      <data key="d6">8e1f4f13f617b982d272175296ec99d3</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;LEARNING RATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Learning Rate is a parameter used in the Reservoir Computing model."</data>
      <data key="d6">8e1f4f13f617b982d272175296ec99d3</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;W&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"W is a parameter used in the Reservoir Computing model."</data>
      <data key="d6">8e1f4f13f617b982d272175296ec99d3</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;WIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Win is a parameter used in the Reservoir Computing model."</data>
      <data key="d6">8e1f4f13f617b982d272175296ec99d3</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;CONNECTIVITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Connectivity is a parameter used in the Reservoir Computing model."</data>
      <data key="d6">8e1f4f13f617b982d272175296ec99d3</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;ACTIVATION FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Activation Function is a component of the Reservoir Computing model."</data>
      <data key="d6">8e1f4f13f617b982d272175296ec99d3</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;ECHO STATE NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks are a type of recurrent neural network architecture that falls under the umbrella term of Reservoir Computing."</data>
      <data key="d6">804bd76fa6f4950ef9a5cf8f0025fc1c</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;LIQUID STATE MACHINES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Liquid State Machines are a type of recurrent neural network architecture that falls under the umbrella term of Reservoir Computing."</data>
      <data key="d6">804bd76fa6f4950ef9a5cf8f0025fc1c</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;SCHILLER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Schiller contributes to the understanding of reservoir computing by investigating traditional training methods for RNNs."</data>
      <data key="d6">88ff8a7687e01f40b2c9d151b6e83d64</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;STEIL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Steil contributes to the understanding of reservoir computing by investigating traditional training methods for RNNs."</data>
      <data key="d6">88ff8a7687e01f40b2c9d151b6e83d64</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;PETER F. DOMINEY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Peter F. Dominey has investigated mechanisms related to reservoir computing in cognitive neuroscience."</data>
      <data key="d6">88ff8a7687e01f40b2c9d151b6e83d64</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;K. KIRBY&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"K. Kirby exposed the concept of reservoir computing in a conference contribution."
"K. Kirby disclosed the concept of reservoir computing in a conference contribution."</data>
      <data key="d6">88ff8a7687e01f40b2c9d151b6e83d64,b32958d42199d47252887dc7be40ab5a</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;L. SCHOMAKER&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"L. Schomaker described a method of obtaining a desired target output from an RNN using a randomly configured ensemble of spiking neural oscillators."
"L. Schomaker described a formulation of the reservoir computing idea known today, which involves the use of a randomly configured ensemble of spiking neural oscillators."</data>
      <data key="d6">88ff8a7687e01f40b2c9d151b6e83d64,a3368f9cab1f65643dba089af5a1f95e</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;TRAINING METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Training Methods are mentioned in the context of reservoir computing, indicating their importance in the field."</data>
      <data key="d6">88ff8a7687e01f40b2c9d151b6e83d64</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;SEQUENCE PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sequence Processing has been modelled using reservoir computing in cognitive neuroscience."</data>
      <data key="d6">88ff8a7687e01f40b2c9d151b6e83d64</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;TEMPORAL INPUT DISCRIMINATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Temporal Input Discrimination has been modelled using reservoir computing in biological neural networks."</data>
      <data key="d6">88ff8a7687e01f40b2c9d151b6e83d64</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;FREQUENCY GENERATOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Frequency Generator is a task that can be achieved using reservoir computing."</data>
      <data key="d6">88ff8a7687e01f40b2c9d151b6e83d64</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;RIDGE REGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is used in conjunction with Ridge Regression to improve prediction accuracy and prevent overfitting in time series prediction tasks."</data>
      <data key="d6">ef9bf350e25daa8f123b0b5c4d60de5f</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;TIME SERIES PREDICTION&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Reservoir Computing is a machine learning model commonly used for time series prediction tasks, such as forecasting future values based on past data points."
"The Reservoir Computing method is used for generating and comparing timeseries data, which is a key application of time series prediction."</data>
      <data key="d6">2386633041e820b604fc4457264b5a33,ef9bf350e25daa8f123b0b5c4d60de5f</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;RECURRENT NEURAL NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is a paradigm for training Recurrent Neural Networks while keeping the recurrent layer untrained."</data>
      <data key="d6">6de297d888d10db4c987b5eafc6398b2</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;JAEGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jaeger is a researcher who made significant contributions to the development of Reservoir Computing and Echo State Networks."</data>
      <data key="d6">6de297d888d10db4c987b5eafc6398b2</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;DOMINEY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dominey is a researcher who made significant contributions to the development of Reservoir Computing and its applications in signal processing."</data>
      <data key="d6">6de297d888d10db4c987b5eafc6398b2</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;BUONOMANO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Buonomano is a researcher who made significant contributions to the development of Reservoir Computing and its applications in neuroscience."</data>
      <data key="d6">6de297d888d10db4c987b5eafc6398b2</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;RC MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is the method used to construct RC models."</data>
      <data key="d6">d622f95153798af8bb6f485db54aaea3</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;MACKEY-GLASS TIMESERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is used for chaotic timeseries forecasting, specifically mentioned to be used with Mackey-Glass Timeseries."</data>
      <data key="d6">2f4c992d69812866e6fce6dbb52d8612</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;NUMPY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Numpy is a library used in the context of reservoir computing, as mentioned in the text."</data>
      <data key="d6">2f4c992d69812866e6fce6dbb52d8612</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;MATPLOTLIB&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Matplotlib is a library used in the context of reservoir computing, as mentioned in the text."</data>
      <data key="d6">2f4c992d69812866e6fce6dbb52d8612</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;RESERVOIRPY&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Reservoirpy is a library specifically used for reservoir computing."
"ReservoirPy is a library for generating matrices used in reservoir computing."</data>
      <data key="d6">2f4c992d69812866e6fce6dbb52d8612,ef85a7b1ca82dc1446ea71964d607a73</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;TASK 1: 10 TIMESTEPS AHEAD FORECAST&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is the method used in Task 1 to predict 10 timesteps ahead in the Mackey-Glass Time Series."</data>
      <data key="d6">fac681bdc38ae5829173c747ee6240fa</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;AUTHOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author developed the Reservoir Computing method for time series prediction."</data>
      <data key="d6">2386633041e820b604fc4457264b5a33</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;NONLINEAR VECTOR AUTO-REGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Nonlinear Vector Auto-regression is demonstrated to excel at reservoir computing benchmark tasks, outperforming Reservoir Computing in terms of meta-parameters, training data sets, and training time."</data>
      <data key="d6">244217eb4738aae272df8949bdaaf131</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;TIMESTEPS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing processes data in discrete timesteps."</data>
      <data key="d6">9e84667b4aeb0789808517f0912043ce</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;FEATURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing takes arrays of shape (timesteps, features) as input."</data>
      <data key="d6">9e84667b4aeb0789808517f0912043ce</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;STATES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing returns an array of shape (timesteps, states), representing the internal representations or memory of a reservoir node."</data>
      <data key="d6">9e84667b4aeb0789808517f0912043ce</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;RIDGE READOUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing is the method used to train the Ridge Readout."</data>
      <data key="d6">f2d5625f36aa4cb036089ce89ec607eb</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;RIDGE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge is used as a regression model in the reservoir computing model."</data>
      <data key="d6">069ae9388dfd52fec9c184c7168f64dd</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;RESERVOIR CONNECTIVITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Connectivity is a parameter used in Reservoir Computing to determine the interconnectivity among the neurons in the reservoir."</data>
      <data key="d6">26d78bc91458f47d4053954505c45f92</data>
    </edge>
    <edge source="&quot;RESERVOIR COMPUTING&quot;" target="&quot;WOLFGANG MAASS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Wolfgang Maass is a researcher who independently developed Liquid State Machines, which are related to Reservoir Computing."</data>
      <data key="d6">158f53cd85edbb4f2e4c77b78c5e7acc</data>
    </edge>
    <edge source="&quot;JAPANESE VOWEL DATASET&quot;" target="&quot;MALE SPEAKERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Japanese Vowel Dataset is composed of utterances from 9 different male speakers."</data>
      <data key="d6">86a136730a696f1a817bd530dbff778d</data>
    </edge>
    <edge source="&quot;JAPANESE VOWEL DATASET&quot;" target="&quot;M. KUDO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"M. Kudo is a reference mentioned in the text regarding the Japanese Vowel Dataset."</data>
      <data key="d6">86a136730a696f1a817bd530dbff778d</data>
    </edge>
    <edge source="&quot;JAPANESE VOWEL DATASET&quot;" target="&quot;J. TOYAMA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"J. Toyama is a reference mentioned in the text regarding the Japanese Vowel Dataset."</data>
      <data key="d6">86a136730a696f1a817bd530dbff778d</data>
    </edge>
    <edge source="&quot;JAPANESE VOWEL DATASET&quot;" target="&quot;M. SHIMBO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"M. Shimbo is a reference mentioned in the text regarding the Japanese Vowel Dataset."</data>
      <data key="d6">86a136730a696f1a817bd530dbff778d</data>
    </edge>
    <edge source="&quot;M. KUDO&quot;" target="&quot;MULTIDIMENSIONAL CURVE CLASSIFICATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"M. Kudo is a co-author of a reference that discusses the technique of multidimensional curve classification."</data>
      <data key="d6">7cd25eb11825d9b9c2978d248997c3fe</data>
    </edge>
    <edge source="&quot;J. TOYAMA&quot;" target="&quot;MULTIDIMENSIONAL CURVE CLASSIFICATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"J. Toyama is a co-author of a reference that discusses the technique of multidimensional curve classification."</data>
      <data key="d6">7cd25eb11825d9b9c2978d248997c3fe</data>
    </edge>
    <edge source="&quot;M. SHIMBO&quot;" target="&quot;MULTIDIMENSIONAL CURVE CLASSIFICATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"M. Shimbo is a co-author of a reference that discusses the technique of multidimensional curve classification."</data>
      <data key="d6">7cd25eb11825d9b9c2978d248997c3fe</data>
    </edge>
    <edge source="&quot;CLASSIFICATION TASK&quot;" target="&quot;RIDGECLASSIFIER&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The RidgeClassifier model is used for classification tasks."
"RidgeClassifier is a model used for the machine learning task of Classification."</data>
      <data key="d6">35631fbf2ad11c53d75cb9b42e2c39b4,f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </edge>
    <edge source="&quot;CLASSIFICATION TASK&quot;" target="&quot;LOGISTICREGRESSION&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The LogisticRegression model is used for classification tasks."
"LogisticRegression is a model used for the machine learning task of Classification."</data>
      <data key="d6">35631fbf2ad11c53d75cb9b42e2c39b4,f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </edge>
    <edge source="&quot;CLASSIFICATION TASK&quot;" target="&quot;PERCEPTRON&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The Perceptron model is used for classification tasks."
"Perceptron is a model used for the machine learning task of Classification."</data>
      <data key="d6">35631fbf2ad11c53d75cb9b42e2c39b4,f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </edge>
    <edge source="&quot;CLASSIFICATION TASK&quot;" target="&quot;RESERVOIR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir concept is used in the machine learning task of Classification."</data>
      <data key="d6">f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </edge>
    <edge source="&quot;UCI MACHINE LEARNING REPOSITORY&quot;" target="&quot;JAPANESE VOWELS DATASET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"UCI Machine Learning Repository is the source of the Japanese Vowels Dataset, providing the audio signals for analysis."</data>
      <data key="d6">7cd25eb11825d9b9c2978d248997c3fe</data>
    </edge>
    <edge source="&quot;JAPANESE VOWELS DATASET&quot;" target="&quot;RESERVOIRPY LIBRARY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Japanese Vowels Dataset is used in the ReservoirPy Library, specifically in the `japanese_vowels()` function for classification tasks."</data>
      <data key="d6">c1ba6d7a4f4bd16c4fd25baf07c9747c</data>
    </edge>
    <edge source="&quot;JAPANESE VOWELS DATASET&quot;" target="&quot;RESERVOIRPY&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy provides the Japanese Vowels dataset, which is used for classification tasks involving spoken utterances."
"ReservoirPy is used to analyze the Japanese Vowels Dataset for time series analysis and classification."</data>
      <data key="d6">870f29520f7a1c42eecb0c4ff855f09e,9f7337ee2d87543ced3b99dcae344b13</data>
    </edge>
    <edge source="&quot;JAPANESE VOWELS DATASET&quot;" target="&quot;LINEAR PREDICTION COEFFICIENTS (LPCS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Japanese Vowels dataset uses Linear Prediction Coefficients (LPCs) as features to analyze speech signals."</data>
      <data key="d6">9f7337ee2d87543ced3b99dcae344b13</data>
    </edge>
    <edge source="&quot;JAPANESE VOWELS DATASET&quot;" target="&quot;SPEAKER IDENTIFIERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Japanese Vowels dataset uses speaker identifiers as labels to classify spoken utterances to their respective speakers."</data>
      <data key="d6">9f7337ee2d87543ced3b99dcae344b13</data>
    </edge>
    <edge source="&quot;JAPANESE VOWELS DATASET&quot;" target="&quot;BOXPLOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A boxplot is a graphical representation used to visualize and analyze the distribution of data in the Japanese Vowels dataset."</data>
      <data key="d6">9f7337ee2d87543ced3b99dcae344b13</data>
    </edge>
    <edge source="&quot;JAPANESE VOWELS DATASET&quot;" target="&quot;SEQUENCE-TO-SEQUENCE ENCODING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sequence-to-sequence encoding is a method used to solve tasks involving the Japanese Vowels dataset, such as classifying spoken utterances."</data>
      <data key="d6">9f7337ee2d87543ced3b99dcae344b13</data>
    </edge>
    <edge source="&quot;JAPANESE VOWELS DATASET&quot;" target="&quot;SCIKITLEARNNODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ScikitLearnNode is used to demonstrate machine learning models on the Japanese Vowels Dataset."</data>
      <data key="d6">35631fbf2ad11c53d75cb9b42e2c39b4</data>
    </edge>
    <edge source="&quot;JAPANESE VOWELS DATASET&quot;" target="&quot;SKLEARN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sklearn is used to calculate accuracy score for the Japanese Vowels Dataset in the classification process."</data>
      <data key="d6">870f29520f7a1c42eecb0c4ff855f09e</data>
    </edge>
    <edge source="&quot;MULTIDIMENSIONAL CURVE CLASSIFICATION&quot;" target="&quot;PATTERN RECOGNITION LETTERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multidimensional Curve Classification is a technique discussed in a reference published in Pattern Recognition Letters."</data>
      <data key="d6">7cd25eb11825d9b9c2978d248997c3fe</data>
    </edge>
    <edge source="&quot;CEPSTRA&quot;" target="&quot;AUDIO PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"cepstra is a feature extraction technique used in Audio Processing to convert audio signals into sequences of cepstral coefficients."</data>
      <data key="d6">79d5959f3f6471cad55498ab4a8a3176</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;TRANSDUCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to build and train models that implement the Transduction technique for sequence-to-sequence encoding."</data>
      <data key="d6">79d5959f3f6471cad55498ab4a8a3176</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;SIMPLE ECHO STATE NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to train a Simple Echo State Network."</data>
      <data key="d6">75c1234e634cf2c009a116e4ee6c053e</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;SEQUENCE-TO-VECTOR MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create Sequence-to-Vector Models for data analysis and prediction."</data>
      <data key="d6">64b0ff9558a0f4794c16619aa76354c4</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;DATA ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used for creating reservoir computing models, which are used for data analysis."</data>
      <data key="d6">64b0ff9558a0f4794c16619aa76354c4</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;PREDICTION&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy is used for creating reservoir computing models, which are used for prediction."
"ReservoirPy is used to make predictions about future data using trained reservoir computing models."</data>
      <data key="d6">64b0ff9558a0f4794c16619aa76354c4,c82c9d05b211ff65131f70eb8cb13513</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RESERVOIR&quot;">
      <data key="d4">17.0</data>
      <data key="d5">"ReservoirPy is used to create and train reservoir computing models, which include the Reservoir component."
"ReservoirPy is a library that creates and works with reservoir computing networks, including reservoirs."
"ReservoirPy is used to create a reservoir node, which includes the Reservoir component."
"ReservoirPy provides the Reservoir node, which implements the reservoir computing concept."
"Reservoirpy is a library that provides the Reservoir component for reservoir computing models."
"Reservoir is a tool implemented in reservoirpy for high dimensional embedding of timeseries."
"reservoirpy is a toolbox that contains different architectures of reservoirs, which are used in Reservoir Computing techniques."
"ReservoirPy is a library that provides the functionality to create and work with reservoirs in reservoir computing models."
"ReservoirPy provides the Reservoir node, which can be triggered on single timesteps or complete timeseries."
"ReservoirPy contains a node for creating Reservoirs, which are a component of ESNs."
"ReservoirPy is a library that includes the Reservoir class, which is used to create the Echo State Network (ESN) model."
"ReservoirPy is used to create and work with reservoir computing models, which include the Reservoir component."
"Reservoirpy is used to generate matrices for a concept called Reservoir."
"ReservoirPy is used to create and configure reservoirs, which are a core component of echo state networks (ESNs)."
"ReservoirPy is used for creating reservoir computing models, which include reservoirs as components."
"ReservoirPy is used to create and configure reservoir computing networks, including the reservoir component."
"ReservoirPy is used to create and work with reservoirs, which are instances of the Reservoir concept."</data>
      <data key="d6">00d22666fe697ffb66c2392939f45b39,09ea760dd2f000c961d1cfd4ea795da5,0e0afab060f214d46062c9886e762002,1ea13fb2c1fff4954b699a8e2377f99f,2336a57d055095c6ffa9d156ddee0096,324a8f3fb4d19b91457a99999e6d3d17,3a3b7a67b23341dcd1b04ec5b61683f6,3ae4ccee74392bfe317d8132e99a3aa9,4073cafddb73621f26061385c5570659,6daefaa8fbd5c1492f2d832d79841463,94fd1ebf256db17e4ac2255b89caa473,b03e2cc6fe2648e792c1d5f1ec5773a3,b11a9f7777c0232bfa7323ae82ad139b,c82c9d05b211ff65131f70eb8cb13513,cb71a9bc3b00e7abcd1a53004abdea69,e39809b687cd044a7918eca37727a188,f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RIDGE&quot;">
      <data key="d4">12.0</data>
      <data key="d5">"ReservoirPy is used to create and train reservoir computing models, which include the Ridge component for training the readout."
"Reservoirpy is a library that provides the Ridge component for reservoir computing models."
"Ridge is a tool implemented in reservoirpy that learns connections through Tikhonov linear regression."
"ReservoirPy is a library that provides the functionality to create and work with Ridge regression models."
"ReservoirPy contains a node for creating Ridge readouts, which are used for regularization in machine learning."
"ReservoirPy is a library that includes the Ridge class, which is used to create a readout in the ESN model."
"ReservoirPy is used to create and work with reservoir computing models that include the Ridge component for data prediction and analysis."
"Ridge is a type of linear regression implemented in ReservoirPy, which is used for training and running ESNs."
"ReservoirPy is used to create and configure ridge readouts, which are used in the readout stage of echo state networks (ESNs)."
"ReservoirPy is used for creating reservoir computing models, which include Ridge as a type of readout or output layer."
"ReservoirPy is used to create and work with Ridge, a component in a reservoir model."
"ReservoirPy is used to create and work with ridge regularization, which is an instance of the Ridge concept."</data>
      <data key="d6">00d22666fe697ffb66c2392939f45b39,09ea760dd2f000c961d1cfd4ea795da5,1ea13fb2c1fff4954b699a8e2377f99f,2336a57d055095c6ffa9d156ddee0096,3ae4ccee74392bfe317d8132e99a3aa9,4073cafddb73621f26061385c5570659,7b9936d57ece8ba985947a7aca12e2c7,8648b5740b93d805f139d9745e1171e8,94fd1ebf256db17e4ac2255b89caa473,b03e2cc6fe2648e792c1d5f1ec5773a3,c82c9d05b211ff65131f70eb8cb13513,e39809b687cd044a7918eca37727a188</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;INPUT&quot;">
      <data key="d4">4.0</data>
      <data key="d5">"ReservoirPy is used to create and train reservoir computing models, which include the Input component for feeding input data into the model."
"ReservoirPy is used to create and work with reservoir computing models that include the Input component for providing data to be processed and analyzed."
"ReservoirPy is used for creating reservoir computing models, which include input data or signals as components."
"ReservoirPy is used to create and work with Input, a component in a reservoir model."</data>
      <data key="d6">1ea13fb2c1fff4954b699a8e2377f99f,8648b5740b93d805f139d9745e1171e8,c82c9d05b211ff65131f70eb8cb13513,e39809b687cd044a7918eca37727a188</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;ECHO STATE NETWORKS&quot;">
      <data key="d4">4.0</data>
      <data key="d5">"ReservoirPy is a library that includes the development of Echo State Networks."
"ReservoirPy is a library used for creating and analyzing reservoir computing systems, including Echo State Networks."
"ReservoirPy is a library used for creating and working with echo state networks."
"ReservoirPy is a library used for building and training Echo State Networks."</data>
      <data key="d6">41fa16855df7da666dc6fc38d2f8ee53,73e81fd6509a2ba400a8435793ade3c5,83fafb2423a01afae7e522917d79ace9,ed28ba3543e07641536ff1eb5e0749dd</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RESERVOIRPY DOCUMENTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy documentation is a resource where more information about the library and its components can be found."</data>
      <data key="d6">83fafb2423a01afae7e522917d79ace9</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;ESNS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy is used for creating Echo State Networks."
"ReservoirPy is a library for creating and training Echo State Networks (ESNs)."</data>
      <data key="d6">6be085e79e86abc5b1a7eaff6bda1ec5,7f2d69f9a9baca70ffd25a6865189206</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;REAL-VALUED CONTINUOUS DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy requires Real-Valued Continuous Data to be formatted as NumPy arrays of shape (timesteps, features) to avoid unexpected results or errors."</data>
      <data key="d6">e94f386a2ed7de2156b4864797cc199e</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;DISCRETE NUMERIC DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy requires Discrete Numeric Data to be formatted as NumPy arrays of shape (timesteps, features) to avoid unexpected results or errors."</data>
      <data key="d6">e94f386a2ed7de2156b4864797cc199e</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;DISCRETE SYMBOLIC DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy requires Discrete Symbolic Data to be formatted as NumPy arrays of shape (timesteps, features) to avoid unexpected results or errors."</data>
      <data key="d6">e94f386a2ed7de2156b4864797cc199e</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;NUMPY ARRAY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy stores data in Numpy arrays, which are powerful n-dimensional array objects."</data>
      <data key="d6">f838f4cbb7060f4409ba2d174a396fb1</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;NUMPY&quot;">
      <data key="d4">14.0</data>
      <data key="d5">"ReservoirPy uses Numpy for all computations, which provides a powerful N-dimensional array object and various derived objects."
"NumPy is a fundamental package used by ReservoirPy for scientific computing and for storing data in N-dimensional arrays."
"numpy is a library used in reservoirpy for numerical computing."
"numpy and reservoirpy are libraries used in the code for numerical computations and reservoir computing, respectively, showing their relationship in this context."
"Numpy is a library used for numerical computing in Python, which is mentioned in the context of storing parameters in Reservoirpy."
"ReservoirPy allows parameters to be initialized using Numpy arrays or sparse matrices of correct shape."
"ReservoirPy uses Numpy for data storage and computations."
"Numpy is a library used for numerical computing in ReservoirPy, and it is used to store and manipulate data."
"ReservoirPy uses Numpy for handling and processing the sine wave data."
"ReservoirPy uses Numpy for numerical computations and handling arrays."
"ReservoirPy uses Numpy to generate and manipulate matrices."
"ReservoirPy uses Numpy for numerical computing, such as creating and manipulating arrays and matrices."
"Numpy is used in ReservoirPy for numerical computing."
"NumPy is used in the ReservoirPy library for numerical computing tasks."</data>
      <data key="d6">325bd631a690a34736918180b01f6917,34b9ce80a22112b32e063179511af6e0,37549a8af907ce182bd36eec43002a7d,3a8ed31ef360d5587cc6411e9fce89d4,50d4e4aab1823b8df6573ccf227f24d0,5732296d26c7a572dc90d4af1172626a,74c073137c970e32982756d008532cb8,7b9936d57ece8ba985947a7aca12e2c7,8648b5740b93d805f139d9745e1171e8,a58317c7e13f27d513fc7671fd187ecb,ef85a7b1ca82dc1446ea71964d607a73,f838f4cbb7060f4409ba2d174a396fb1,fe90abb0dde126fafbf44782aeb6738c,ff860bc63e3d697a6183c0b850689048</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;SCIPY&quot;">
      <data key="d4">7.0</data>
      <data key="d5">"SciPy is used by ReservoirPy for scientific and technical computing, providing algorithms and functions for optimization and integration."
"Scipy is a library used for scientific computing in Python, which is mentioned in the context of storing parameters as sparse matrices in Reservoirpy."
"ReservoirPy uses Scipy for mathematical functions and convenience functions, such as for optimization and statistics."
"ReservoirPy uses Scipy for computations."
"Scipy is a library used for scientific computing in ReservoirPy, and it is used in computations within the library."
"ReservoirPy uses Scipy to generate sparse matrices."
"ReservoirPy uses Scipy for scientific computing, such as mathematical functions and optimization."</data>
      <data key="d6">37549a8af907ce182bd36eec43002a7d,5732296d26c7a572dc90d4af1172626a,74c073137c970e32982756d008532cb8,7b9936d57ece8ba985947a7aca12e2c7,ef85a7b1ca82dc1446ea71964d607a73,fe90abb0dde126fafbf44782aeb6738c,ff860bc63e3d697a6183c0b850689048</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;GITHUB&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy is a Python library for reservoir computing, and its documentation can be found on GitHub."
"ReservoirPy encourages users to propose new implementations, which are added to the GitHub repository."</data>
      <data key="d6">77c3759b4ed32509aaf1403c6fa8030f,fcac967511cf2b019fd856e23d2e91d9</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;ECHO STATE NETWORKS (ESNS)&quot;">
      <data key="d4">5.0</data>
      <data key="d5">"ReservoirPy is a library used for creating and analyzing Echo State Networks (ESNs)."
"ReservoirPy is used to create and work with Echo State Networks (ESNs)."
"ReservoirPy is a library used for training and running Echo State Networks (ESNs)."
"ReservoirPy supports the creation and training of Echo State Networks (ESNs)."
"ReservoirPy is a library used to create Echo State Networks (ESNs)."</data>
      <data key="d6">01f8dd8235ba0d4cf0837b5ea958ec95,37549a8af907ce182bd36eec43002a7d,74c073137c970e32982756d008532cb8,a2b183778107462d474c53e4ec0a9221,af2db1cc5ab6b16acae2c93d3facb668</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;NP.PI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.pi is a mathematical constant used in a line of code within the context of the ReservoirPy library."</data>
      <data key="d6">01f8dd8235ba0d4cf0837b5ea958ec95</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;CONTEXT MANAGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Context Manager is a concept used in the ReservoirPy library, allowing for temporary modifications of the reservoir's state without permanently altering it."</data>
      <data key="d6">58330f62da357197950f63388e4ceaff</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;FROM_STATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The 'from_state' parameter in ReservoirPy is used to initialize the reservoir with a specific state at the start of a simulation or training process."</data>
      <data key="d6">58330f62da357197950f63388e4ceaff</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;WITH_STATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The 'with_state' parameter in ReservoirPy is used to temporarily change the state of the reservoir for operations inside a block of code."</data>
      <data key="d6">58330f62da357197950f63388e4ceaff</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;PARALLELIZATION&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy supports the use of parallelization to train and run multiple reservoirs or nodes simultaneously, enhancing computational efficiency."
"ReservoirPy supports parallelization, allowing for faster training of ESNs."</data>
      <data key="d6">a16039f06e545c915f8e7668c39c3e5c,ead6383a44acd8ebd17907b85a910455</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;DEEP ARCHITECTURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep Architectures are models that can be created and worked with using the ReservoirPy library."</data>
      <data key="d6">8965403859beb43a6ab7e5c8c916b857</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;INPUT-TO-READOUT CONNECTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input-to-readout connections are a feature implemented in the ReservoirPy library for Echo State Networks."</data>
      <data key="d6">8965403859beb43a6ab7e5c8c916b857</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;ECHO STATE NETWORK (ESN)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is a software library used for creating and working with Echo State Networks (ESNs), which include the Input Data, Reservoir, and Readout Layer."</data>
      <data key="d6">4da651284dbab3f68dc3cae41e6e0311</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RESERVOIRPY.MAT_GEN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is the parent organization of the reservoirpy.mat_gen submodule."</data>
      <data key="d6">8f2f2cfd667a304a288723de779c9bee</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;ESN&quot;">
      <data key="d4">11.0</data>
      <data key="d5">"ESN is a type of recurrent neural network that can be created and worked with using the ReservoirPy library."
"ReservoirPy is a library that includes the Echo State Network (ESN) for timeseries prediction."
"ReservoirPy is used to create and train ESN models."
"ReservoirPy is used to create and work with Echo State Networks (ESNs), which are used for prediction and generation tasks."
"ReservoirPy provides a special node for Echo State Networks (ESN), which can be used for offline training with ridge regression in a distributed way."
"ReservoirPy includes the ESN node, which is used for data processing tasks."
"ReservoirPy is mentioned for creating and working with ESNs, including the Reservoir and Ridge nodes."
"ReservoirPy is used for creating and training ESNs, which are a type of recurrent neural network."
"ESN is a type of network implemented in ReservoirPy, which is used for time series prediction and data analysis."
"ReservoirPy is used to create and train echo state networks (ESNs), which are a type of recurrent neural network."
"ESN is a machine learning model developed by the ReservoirPy library."</data>
      <data key="d6">00d22666fe697ffb66c2392939f45b39,0b6c69085074b2cf23267eb149068b9f,3ae4ccee74392bfe317d8132e99a3aa9,3bee7b78d0ab9582cc9bffe9e305df2e,593080a95ef7640b3925b07cad1bedd4,7b9936d57ece8ba985947a7aca12e2c7,7f70879016c133fe58e4838172a69613,c53825a1ab5e01a794a428988435a7a7,cdc64af0dde941250d89b191d0666c9b,eb7a223eeb120e3fcc45a96a6018707d,fe90abb0dde126fafbf44782aeb6738c</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;UNITS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy uses the UNITS hyperparameter to configure the number of neurons inside the reservoir."</data>
      <data key="d6">a8df60a94e25d863b436f47f4f8e6a6d</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;SPECTRAL_RADIUS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy uses the SPECTRAL_RADIUS hyperparameter to influence the dynamics' stability and chaos."</data>
      <data key="d6">a8df60a94e25d863b436f47f4f8e6a6d</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;INPUT_SCALING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy uses the INPUT_SCALING hyperparameter to influence the correlation between states and inputs."</data>
      <data key="d6">a8df60a94e25d863b436f47f4f8e6a6d</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;OPTIMIZATION ALGORITHMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is a tool that accepts objective functions, which are often used by optimization algorithms to find the minimum of a function."</data>
      <data key="d6">4f7b43545046f0e6f9b6fb3816da1d79</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;HYPERPARAMETER OPTIMIZATION&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"ReservoirPy is used for the process of Hyperparameter Optimization."
"ReservoirPy is designed for hyperparameter optimization, providing functions for conducting optimization processes."
"ReservoirPy is used for the process of Hyperparameter Optimization."</data>
      <data key="d6">9abdbd696e340cb5dd8c66ac5cd30c67,d4684af3c445d312afe4d838abc45502,f70c7d3d89baaabbeaad57b58e379e08</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;OBJECTIVE FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy accepts Objective Functions with certain conventions, such as requiring a 'loss' key in the output dictionary."</data>
      <data key="d6">d4684af3c445d312afe4d838abc45502</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;HYPEROPT&quot;">
      <data key="d4">5.0</data>
      <data key="d5">"ReservoirPy utilizes Hyperopt for hyperparameter optimization."
"ReservoirPy can be used in conjunction with Hyperopt for hyperparameter optimization."
"reservoirpy is mentioned in the context of Hyperopt, a Python library for optimizing machine learning algorithms."
"ReservoirPy is mentioned in the context of using Hyperopt for hyperparameter optimization."
"ReservoirPy is used in conjunction with Hyperopt for hyperparameter optimization."</data>
      <data key="d6">0982b8d1eb1e636b19fa2e9d9361e566,870f29520f7a1c42eecb0c4ff855f09e,9abdbd696e340cb5dd8c66ac5cd30c67,a3a74dc4754a8c8b0730f808285893e2,c5413fef3b2d7e4d688c66e6046b56c7</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;BACKPROPAGATION-DECORRELATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy implements the Backpropagation-Decorrelation technique for weight adaptation."</data>
      <data key="d6">9abdbd696e340cb5dd8c66ac5cd30c67</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RESERVOIR ADAPTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy supports Reservoir Adaptation, allowing for the modification of internal reservoir parameters."</data>
      <data key="d6">9abdbd696e340cb5dd8c66ac5cd30c67</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;HYPER-PARAMETER EXPLORATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used for hyper-parameter exploration, which involves searching through a space of possible hyper-parameters to find the best combination for a given task."</data>
      <data key="d6">716940af834825642e01a3cb59a7e006</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;SCIKITLEARNNODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy includes the ScikitLearnNode component, which allows for the integration of Scikit-Learn models into ReservoirPy workflows."</data>
      <data key="d6">c05906c1f12c4edfc32a04aa9935067e</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;THE AUTHOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author uses the ReservoirPy library to create and train the model."</data>
      <data key="d6">b3c8de6f33c2ebb84f0d2797933d0cad</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;SCIKIT-LEARN&quot;">
      <data key="d4">4.0</data>
      <data key="d5">"ReservoirPy is used in conjunction with Scikit-learn to create machine learning models."
"reservoirpy and scikit-learn both offer high-level APIs for machine learning tasks, but reservoirpy is specifically designed for Reservoir Computing."
"scikit-learn tools are integrated within reservoirpy in a transparent way, indicating a relationship between the two organizations."
"ReservoirPy is used to integrate with Scikit-Learn, allowing the use of Scikit-Learn's machine learning models within ReservoirPy's framework."</data>
      <data key="d6">82de30f43839f4985de20a981b524af1,d622f95153798af8bb6f485db54aaea3,eebc9d7d2b66e3898b7d068c38fd200f,f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;PYTHON 3.8&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy supports Python 3.8 and higher."</data>
      <data key="d6">d1047d9e322054de394b880adaf6b536</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;@RESERVOIRPY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"@reservoirpy shares updates and new releases about ReservoirPy."</data>
      <data key="d6">d1047d9e322054de394b880adaf6b536</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;OFFICIAL DOCUMENTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Official Documentation is a resource provided by ReservoirPy to learn more about its features, API, and installation process."</data>
      <data key="d6">d1047d9e322054de394b880adaf6b536</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;USER GUIDE&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"User Guide is a resource provided by ReservoirPy that offers tutorials for learning how to use its features."
"ReservoirPy comes with a User Guide that provides tutorials and instructions for using the library."</data>
      <data key="d6">a2b183778107462d474c53e4ec0a9221,d1047d9e322054de394b880adaf6b536</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;DEEP RESERVOIR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy supports the creation of Deep Reservoir architectures."</data>
      <data key="d6">d1047d9e322054de394b880adaf6b536</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;HYPERPARAMETERS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy includes tools for exploring and optimizing Hyperparameters."
"ReservoirPy provides tools for optimizing hyperparameters, such as the Leaking Rate, to improve the performance of the model."</data>
      <data key="d6">2d8ea1123f365fb047b024022ba4fdc4,d1047d9e322054de394b880adaf6b536</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;MACKEY-GLASS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Mackey-Glass is used as an example in the ReservoirPy documentation for predicting chaotic behavior."</data>
      <data key="d6">d1047d9e322054de394b880adaf6b536</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;DEEP-ESN ARCHITECTURE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create and work with Deep-ESN Architectures."</data>
      <data key="d6">a2b183778107462d474c53e4ec0a9221</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;MACKEY-GLASS TIMESERIES&quot;">
      <data key="d4">4.0</data>
      <data key="d5">"ReservoirPy provides a dataset generator for creating Mackey-Glass Timeseries."
"reservoirpy is used for chaotic timeseries forecasting, specifically mentioned with the Mackey-Glass timeseries."
"reservoirpy is used to predict the Mackey-Glass timeseries, which is a mathematical function used as a benchmark for time series prediction and data analysis."
"ReservoirPy is used to analyze and model Mackey-Glass Timeseries data."</data>
      <data key="d6">238049de5f28dca3e857a46a8b1bed03,6daefaa8fbd5c1492f2d832d79841463,a1adb5de4156f0a4a448caf79056e886,a2b183778107462d474c53e4ec0a9221</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;TIMESERIES PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used for timeseries prediction, which involves forecasting future values based on past observations."</data>
      <data key="d6">0b6c69085074b2cf23267eb149068b9f</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;PYTHON NOTEBOOKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Python Notebooks are used in the tutorials folder of ReservoirPy for data analysis and visualization, demonstrating reservoir computing techniques."</data>
      <data key="d6">0b6c69085074b2cf23267eb149068b9f</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;PIP&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Pip is used to install and manage ReservoirPy, a Python library for reservoir computing."
"ReservoirPy is installed using pip, a package installer for Python."</data>
      <data key="d6">0b6c69085074b2cf23267eb149068b9f,c5413fef3b2d7e4d688c66e6046b56c7</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;REQUIREMENTS FILE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The requirements file lists the packages needed for running Python Notebooks in the tutorials folder, including ReservoirPy."</data>
      <data key="d6">c5413fef3b2d7e4d688c66e6046b56c7</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;TUTORIAL FOLDER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Tutorial folder contains tutorials in Jupyter Notebooks, demonstrating the use of ReservoirPy."</data>
      <data key="d6">c5413fef3b2d7e4d688c66e6046b56c7</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;EXAMPLES FOLDER&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The Examples folder contains examples and papers with codes, also in Jupyter Notebooks, showcasing the applications of ReservoirPy."
"The examples folder contains examples of complex use cases from the literature, which can be used to learn more about the capabilities of ReservoirPy."</data>
      <data key="d6">c5413fef3b2d7e4d688c66e6046b56c7,ead6383a44acd8ebd17907b85a910455</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;HYPERPACKAGE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Hyperpackage is an optional feature in ReservoirPy that enables the use of Hyperopt for hyperparameter optimization."</data>
      <data key="d6">c5413fef3b2d7e4d688c66e6046b56c7</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;TROUVAIN ET AL. (2020)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Trouvain et al. (2020) provides a tutorial for ReservoirPy (v0.2)."</data>
      <data key="d6">c5413fef3b2d7e4d688c66e6046b56c7</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;HINAUT ET AL. (2021)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hinaut et al. (2021) offers advice and a method for exploring hyperparameters for reservoirs, which can be applied in the context of ReservoirPy."</data>
      <data key="d6">c5413fef3b2d7e4d688c66e6046b56c7</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;TROUVAIN ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Trouvain et al. are authors of a tutorial that uses ReservoirPy for hyperparameter exploration."</data>
      <data key="d6">280cbdf53022bbaed48ccb34ebe142bc</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;HINAUT ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hinaut et al. are authors of a paper that provides advice and a method for exploring hyperparameters for reservoirs using ReservoirPy."</data>
      <data key="d6">280cbdf53022bbaed48ccb34ebe142bc</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;LEGER ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Leger et al. are authors of a paper that uses ReservoirPy for evolving reservoirs for meta reinforcement learning."</data>
      <data key="d6">280cbdf53022bbaed48ccb34ebe142bc</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;CHAIX-EICHEL ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chaix-Eichel et al. are authors of a paper that uses ReservoirPy for implicit learning and explicit representations."</data>
      <data key="d6">280cbdf53022bbaed48ccb34ebe142bc</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;PAGLIARINI ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Pagliarini et al. are authors of papers that use ReservoirPy for canary vocal sensorimotor models."</data>
      <data key="d6">280cbdf53022bbaed48ccb34ebe142bc</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;TROUVAIN &amp; HINAUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Trouvain &amp; Hinaut are mentioned as the authors of a paper that cites ReservoirPy."</data>
      <data key="d6">20b16c2e1cb8813ade96fea5f9591631</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;INRIA&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy is developed and supported by Inria."
"ReservoirPy is developed and supported by Inria."</data>
      <data key="d6">296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;MNEMOSYNE GROUP&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy is developed and supported by the Mnemosyne group within Inria."
"ReservoirPy is developed in the Mnemosyne group at Inria."</data>
      <data key="d6">296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;ICANN 2020&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy was presented at ICANN 2020."
"ReservoirPy is presented at the ICANN 2020 conference."</data>
      <data key="d6">296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;NATHAN TROUVAIN&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"Nathan Trouvain is an author and contributor to ReservoirPy."
"Nathan Trouvain is a developer of ReservoirPy."
"Nathan Trouvain is a contributor to reservoirpy, working at Inria Bordeaux Sud-Ouest, IMN, LaBRI."</data>
      <data key="d6">18910a60b2547ec3133340f42c45bb47,296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;LUCA PEDRELLI&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Luca Pedrelli is an author and contributor to ReservoirPy."
"Luca Pedrelli is a developer of ReservoirPy."</data>
      <data key="d6">296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;THANH TRUNG DINH&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Thanh Trung Dinh is an author and contributor to ReservoirPy."
"Thanh Trung Dinh is a developer of ReservoirPy."</data>
      <data key="d6">296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;XAVIER HINAUT&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"Xavier Hinaut is an author and contributor to ReservoirPy."
"Xavier Hinaut is a developer of ReservoirPy."
"Xavier Hinaut is a contributor to reservoirpy, working at Inria Bordeaux Sud-Ouest, IMN, LaBRI."</data>
      <data key="d6">18910a60b2547ec3133340f42c45bb47,296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;LEAKING RATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy uses the Leaking Rate parameter to control the time constant of the ESN."</data>
      <data key="d6">2d8ea1123f365fb047b024022ba4fdc4</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;DOUBLE SCROLL ATTRACTOR&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The example demonstrates the use of ReservoirPy to forecast the Double Scroll Attractor."
"ReservoirPy provides the Double scroll attractor as a test case for dynamical systems."</data>
      <data key="d6">91704ce63f9ba41247fdc452a7a62ba6,b483c6bbce54156c724905b340aa2e85</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;TIME SERIES FORECASTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used for the process of Time Series Forecasting."</data>
      <data key="d6">f70c7d3d89baaabbeaad57b58e379e08</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;HYPEROPT_CONFIG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"hyperopt_config is a configuration used for hyperparameter optimization in ReservoirPy."</data>
      <data key="d6">f70c7d3d89baaabbeaad57b58e379e08</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;IMPROVING RESERVOIRS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The study on improving reservoirs is mentioned in the context of the ReservoirPy library."</data>
      <data key="d6">414aba25cd4916c4a9e916beef385e81</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;NARMA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"narma is a dataset used in reservoirpy for reservoir computing."</data>
      <data key="d6">325bd631a690a34736918180b01f6917</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;UNIFORM&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"uniform is a type of probability distribution used in reservoirpy for generating matrices."
"uniform is a function provided by reservoirpy that creates a sparse matrix from a uniform distribution."
"uniform is a function provided by Reservoirpy to generate sparse matrices from a uniform distribution."</data>
      <data key="d6">325bd631a690a34736918180b01f6917,3a3b7a67b23341dcd1b04ec5b61683f6,96c47d9b671ce319abe9c6ba2b8ae122</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;BERNOULLI&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"bernoulli is a type of probability distribution used in reservoirpy for generating matrices."
"Bernoulli distribution is used in ReservoirPy to generate random matrices."</data>
      <data key="d6">325bd631a690a34736918180b01f6917,7b9936d57ece8ba985947a7aca12e2c7</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;IPRESERVOIR&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"IPReservoir is a type of node used in reservoirpy, which implements an intrinsic plasticity adaptation rule."
"IPReservoir is a node in ReservoirPy, a library used for implementing reservoir computing algorithms."</data>
      <data key="d6">325bd631a690a34736918180b01f6917,701ffa843b8d26f96c23dae69e683b58</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;IP RULE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is a library used to implement the IP Rule, as mentioned in the text."</data>
      <data key="d6">83ded1f13bd74b00694092be69a83870</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;ES&#178;N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES&#178;N model is implemented using the ReservoirPy library."</data>
      <data key="d6">ca59dc92d05a69002373e96e0a373215</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;IDENTITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"identity is mentioned in the code in the context of reservoirpy, but its specific role or meaning is not clear from the provided text."</data>
      <data key="d6">3a8ed31ef360d5587cc6411e9fce89d4</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create and train ES2N models."</data>
      <data key="d6">c53825a1ab5e01a794a428988435a7a7</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;LINEAR ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create and train Linear ESN models."</data>
      <data key="d6">c53825a1ab5e01a794a428988435a7a7</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;ORTHOGONAL ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create and train Orthogonal ESN models."</data>
      <data key="d6">c53825a1ab5e01a794a428988435a7a7</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;INRIA BORDEAUX SUD-OUEST&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Inria Bordeaux Sud-Ouest is an organization where Nathan Trouvain and Xavier Hinaut work, contributing to reservoirpy."</data>
      <data key="d6">18910a60b2547ec3133340f42c45bb47</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;IMN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IMN is a department at Inria Bordeaux Sud-Ouest where Nathan Trouvain and Xavier Hinaut work, contributing to reservoirpy."</data>
      <data key="d6">18910a60b2547ec3133340f42c45bb47</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;LABRI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LaBRI is a research unit at Inria Bordeaux Sud-Ouest where Nathan Trouvain and Xavier Hinaut work, contributing to reservoirpy."</data>
      <data key="d6">18910a60b2547ec3133340f42c45bb47</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;PYTHON&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"ReservoirPy is a Python library that provides tools and algorithms for implementing Reservoir Computing."
"ReservoirPy is implemented in Python."
"Python is the programming language used to build the ReservoirPy library."</data>
      <data key="d6">41fa16855df7da666dc6fc38d2f8ee53,6de297d888d10db4c987b5eafc6398b2,74c073137c970e32982756d008532cb8</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;JAEGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoirpy is tailored for RC networks design, with a focus on Echo State Networks (ESNs), which were developed by Jaeger."</data>
      <data key="d6">295606b4bc5d12929a913a3c79f93734</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RC MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"reservoirpy contains various implementations of Reservoir Computing tools that can be used to construct RC models."</data>
      <data key="d6">d622f95153798af8bb6f485db54aaea3</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;NODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Node is a class within reservoirpy that represents a unit in a network, used for various implementations of RC tools."</data>
      <data key="d6">b03e2cc6fe2648e792c1d5f1ec5773a3</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;LMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LMS is a tool implemented in reservoirpy that learns connections using Least Mean Square, allowing online learning."</data>
      <data key="d6">b03e2cc6fe2648e792c1d5f1ec5773a3</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RLS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RLS is a tool implemented in reservoirpy that learns connections using Recursive Least Square, allowing online learning."</data>
      <data key="d6">b03e2cc6fe2648e792c1d5f1ec5773a3</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;INTRINSIC PLASTICITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Intrinsic Plasticity is a mechanism implemented in reservoirpy that allows for adaptive learning in reservoirs."</data>
      <data key="d6">b03e2cc6fe2648e792c1d5f1ec5773a3</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;NON-LINEAR VECTOR AUTOREGRESSIVE MACHINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Non-Linear Vector Autoregressive machine is a recent reservoir reformulation implemented in reservoirpy."</data>
      <data key="d6">b03e2cc6fe2648e792c1d5f1ec5773a3</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;SUSSILLO AND ABBOTT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy uses the Recursive Least Square learning algorithm developed by Sussillo and Abbott."</data>
      <data key="d6">fcac967511cf2b019fd856e23d2e91d9</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;STEIL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy includes nodes implementing the Intrinsic Plasticity mechanism developed by Steil."</data>
      <data key="d6">fcac967511cf2b019fd856e23d2e91d9</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;SCHRAUWEN ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy includes a reservoir reformulation developed by Schrauwen et al."</data>
      <data key="d6">fcac967511cf2b019fd856e23d2e91d9</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;GAUTHIER ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy includes the Non-Linear Vector Autoregressive machine developed by Gauthier et al."</data>
      <data key="d6">fcac967511cf2b019fd856e23d2e91d9</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;PYRCN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy is compared to PyRCN, another open-source software for reservoir computing."
"Reservoirpy is compared to PyRCN, which defines a complete interface to train Extreme Learning Machine (ELM)."</data>
      <data key="d6">15e969cdc81fd313d389558850d0c8ec,fcac967511cf2b019fd856e23d2e91d9</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;ECHOTORCH&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy is compared to EchoTorch, another open-source software for reservoir computing."
"Reservoirpy is compared to EchoTorch, which allows users to manipulate conceptors for controlling the dynamics of reservoirs."</data>
      <data key="d6">15e969cdc81fd313d389558850d0c8ec,fcac967511cf2b019fd856e23d2e91d9</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RESERVOIRCOMPUTING.JL&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy is compared to ReservoirComputing.jl, another open-source software for reservoir computing."
"Reservoirpy is compared to ReservoirComputing.jl, which is a library for reservoir computing in Julia."</data>
      <data key="d6">15e969cdc81fd313d389558850d0c8ec,fcac967511cf2b019fd856e23d2e91d9</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;PYTORCH-ES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is compared to Pytorch-es, another open-source software for reservoir computing."</data>
      <data key="d6">fcac967511cf2b019fd856e23d2e91d9</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;PYTORCH-ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoirpy is compared to Pytorch-esn, which is an implementation of Echo State Networks (ESNs) in PyTorch."</data>
      <data key="d6">15e969cdc81fd313d389558850d0c8ec</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;DEEPESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoirpy is compared to DeepESN, which is an open-source library for deep Echo State Networks (ESNs)."</data>
      <data key="d6">15e969cdc81fd313d389558850d0c8ec</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RCNET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoirpy is compared to RCNet, which is a library for reservoir computing providing features such as online learning and delayed connections."</data>
      <data key="d6">15e969cdc81fd313d389558850d0c8ec</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;LSM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoirpy is compared to LSM, which is an open-source library specializing in handling spiking neural networks."</data>
      <data key="d6">15e969cdc81fd313d389558850d0c8ec</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;OGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoirpy is compared to Oger, which is a historical package for reservoir computing, no longer maintained."</data>
      <data key="d6">15e969cdc81fd313d389558850d0c8ec</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;JAMES BERGSTRA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"James Bergstra is a contributor to reservoirpy."</data>
      <data key="d6">a3a74dc4754a8c8b0730f808285893e2</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;DAN YAMINS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dan Yamins is a contributor to reservoirpy."</data>
      <data key="d6">a3a74dc4754a8c8b0730f808285893e2</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;DAVID D COX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"David D Cox is a contributor to reservoirpy."</data>
      <data key="d6">a3a74dc4754a8c8b0730f808285893e2</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;ECHO STATE NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is a library that provides tools for building and training Echo State Networks."</data>
      <data key="d6">29f9b2e5fa311519b18e7aef31c68d0a</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;LORENZ CHAOTIC ATTRACTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy provides the Lorenz chaotic attractor as a test case for dynamical systems."</data>
      <data key="d6">b483c6bbce54156c724905b340aa2e85</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;H&#201;NON MAP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy provides the H&#233;non map as a test case for dynamical systems."</data>
      <data key="d6">b483c6bbce54156c724905b340aa2e85</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;LOGISTIC MAP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy provides the Logistic map as a test case for dynamical systems."</data>
      <data key="d6">b483c6bbce54156c724905b340aa2e85</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;ADVANCED FEATURES&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy offers advanced features such as input-to-readout connections, feedback connections, custom weight matrices, parallelization, and 'deep' architectures.""ReservoirPy offers advanced features such as input-to-readout connections and custom weight matrices."</data>
      <data key="d6">ead6383a44acd8ebd17907b85a910455</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;GENERATIVE TASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoirpy is used in the process of performing a generative task using a for-loop and an ESN call method."</data>
      <data key="d6">ff860bc63e3d697a6183c0b850689048</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;CUSTOM WEIGHT MATRICES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoirpy allows for the creation of custom weight matrices using initializer functions."</data>
      <data key="d6">ff860bc63e3d697a6183c0b850689048</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;SCIPY.STATS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"reservoirpy uses functions from scipy.stats to create weights from probability distributions."</data>
      <data key="d6">96c47d9b671ce319abe9c6ba2b8ae122</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RANDOM_SPARSE&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"random_sparse is a function provided by reservoirpy that creates a random sparse matrix initializer."
"random_sparse is a function provided by Reservoirpy to generate random sparse matrices."</data>
      <data key="d6">3a3b7a67b23341dcd1b04ec5b61683f6,96c47d9b671ce319abe9c6ba2b8ae122</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;NORMAL&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"normal is a function provided by reservoirpy that creates a dense matrix from a Gaussian distribution."
"normal is a function provided by Reservoirpy to generate dense matrices from a normal distribution."</data>
      <data key="d6">3a3b7a67b23341dcd1b04ec5b61683f6,96c47d9b671ce319abe9c6ba2b8ae122</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;NORMAL DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy uses a function from Numpy to generate matrices from a Normal Distribution."</data>
      <data key="d6">d5e39e29b61f6ea0ffe0c868ba7a4252</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;UNIFORM DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy uses a function from the library to generate matrices from a Uniform Distribution."</data>
      <data key="d6">d5e39e29b61f6ea0ffe0c868ba7a4252</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;BERNOULLI DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy uses a function from the library to generate matrices from a Bernoulli Distribution."</data>
      <data key="d6">d5e39e29b61f6ea0ffe0c868ba7a4252</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;MODEL&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ReservoirPy is used to create models, which are structures that can include nodes such as ESN and Ridge."
"ReservoirPy is used to create and work with Model, a component in a reservoir model."</data>
      <data key="d6">3bee7b78d0ab9582cc9bffe9e305df2e,8648b5740b93d805f139d9745e1171e8</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;DEEP ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep ESN models are created and worked with using the ReservoirPy library."</data>
      <data key="d6">2336a57d055095c6ffa9d156ddee0096</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;NONLINEAR VECTOR AUTO-REGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to implement the NVAR node, demonstrating the Nonlinear Vector Auto-regressive method in a provided notebook."</data>
      <data key="d6">244217eb4738aae272df8949bdaaf131</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;NONLINEAR REPRESENTATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is mentioned as a tool that assists in computing outer products and selecting unique monomials for Nonlinear Representations."</data>
      <data key="d6">5430aac4404b66ea20503e5cac4d4328</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;TUTORIAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The tutorial introduces the use of ReservoirPy to create and train an Echo State Network (ESN) on sequential data."</data>
      <data key="d6">74c073137c970e32982756d008532cb8</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RESERVOIR CLASS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy includes the Reservoir class, which is used to create reservoirs for ESNs."</data>
      <data key="d6">9b360c6a33aafa6827417de5bd4faa82</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;MATPLOTLIB&quot;">
      <data key="d4">4.0</data>
      <data key="d5">"ReservoirPy uses Matplotlib for visualizing the activation of the reservoir."
"ReservoirPy uses Matplotlib for creating visualizations of reservoir neuron activation."
"Matplotlib is used in ReservoirPy for creating visualizations."
"Matplotlib is used in the ReservoirPy library for creating visualizations of the Mackey-Glass Equation."</data>
      <data key="d6">34b9ce80a22112b32e063179511af6e0,50d4e4aab1823b8df6573ccf227f24d0,8648b5740b93d805f139d9745e1171e8,a58317c7e13f27d513fc7671fd187ecb</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;SINE WAVE&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The reservoir is run on a sine wave as input data."
"A sine wave is used as input data for the reservoir created using ReservoirPy."</data>
      <data key="d6">34b9ce80a22112b32e063179511af6e0,a58317c7e13f27d513fc7671fd187ecb</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RESERVOIR NEURONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir neurons are components of the reservoir created using ReservoirPy, which process and store information from the input data."</data>
      <data key="d6">a58317c7e13f27d513fc7671fd187ecb</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;ESN MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create the ESN Model, which includes components like Reservoir and Ridge."</data>
      <data key="d6">8294eed5fc10df1c118f9afa266910e4</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RESERVOIR COMPUTING MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create and work with Reservoir Computing Models."</data>
      <data key="d6">71f966d00b6d0eceb580d00b9cb86b1e</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;CONCAT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create and work with reservoir computing models that include the Concat function for combining multiple data streams into a single stream."</data>
      <data key="d6">c82c9d05b211ff65131f70eb8cb13513</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;FEEDBACK CONNECTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create and work with reservoir computing models that utilize Feedback Connections to improve data processing and prediction."</data>
      <data key="d6">c82c9d05b211ff65131f70eb8cb13513</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;FORCED FEEDBACKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create and work with reservoir computing models that utilize Forced Feedbacks to control the internal dynamics of the model."</data>
      <data key="d6">c82c9d05b211ff65131f70eb8cb13513</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;FITTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to train reservoir computing models, which is a process known as Fitting."</data>
      <data key="d6">c82c9d05b211ff65131f70eb8cb13513</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;NP.RANDOM.NORMAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoirpy uses np.random.normal to generate random numbers from a normal distribution."</data>
      <data key="d6">3a3b7a67b23341dcd1b04ec5b61683f6</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;PLT.HIST&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"plt.hist is used to plot a histogram of the weights distribution in the Reservoir matrix generated by Reservoirpy."</data>
      <data key="d6">3a3b7a67b23341dcd1b04ec5b61683f6</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;SEQUENTIAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sequential is mentioned in the context of using the ReservoirPy library for data processing."</data>
      <data key="d6">e39809b687cd044a7918eca37727a188</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RESERVOIR1&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create and work with Reservoir1, a component in a reservoir model."</data>
      <data key="d6">8648b5740b93d805f139d9745e1171e8</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RESERVOIR2&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create and work with Reservoir2, a component in a reservoir model."</data>
      <data key="d6">8648b5740b93d805f139d9745e1171e8</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RESERVOIR3&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create and work with Reservoir3, a component in a reservoir model."</data>
      <data key="d6">8648b5740b93d805f139d9745e1171e8</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create and work with Data, a component in a reservoir model."</data>
      <data key="d6">8648b5740b93d805f139d9745e1171e8</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;MACKEY-GLASS EQUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used for timeseries forecasting of the Mackey-Glass Equation."</data>
      <data key="d6">50d4e4aab1823b8df6573ccf227f24d0</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;DATA PREPROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used for data preprocessing, such as converting the Mackey-Glass Time Series dataset into a forecasting format."</data>
      <data key="d6">b2beacacc8c190393e4583a69518378c</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;FORECASTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used for forecasting, as demonstrated in the provided code using the ReservoirPy library to convert the Mackey-Glass Time Series dataset into a forecasting format."</data>
      <data key="d6">b2beacacc8c190393e4583a69518378c</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;AUTHOR&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The author is using ReservoirPy to build and train Echo State Networks."
"The author uses the ReservoirPy library to create and work with reservoir computing models."</data>
      <data key="d6">069ae9388dfd52fec9c184c7168f64dd,41fa16855df7da666dc6fc38d2f8ee53</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;FORCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy supports the use of the FORCE readout algorithm in reservoir computing networks."</data>
      <data key="d6">324a8f3fb4d19b91457a99999e6d3d17</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RESERVOIR COMPUTING MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create and train Reservoir Computing Models, which are used for time series prediction and analysis."</data>
      <data key="d6">0113164912437e96423379cb9c039f56</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;MACKEY-GLASS TIME SERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy's datasets include the Mackey-Glass Time Series, which is used for testing and comparing the forecasting abilities of different models."</data>
      <data key="d6">eebc9d7d2b66e3898b7d068c38fd200f</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;RIDGECLASSIFIER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create a reservoir computing model that includes the RidgeClassifier algorithm for classification tasks."</data>
      <data key="d6">0036fb6f489e13c0db0f1c02bf3323be</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;LOGISTICREGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create a reservoir computing model that includes the LogisticRegression algorithm for classification tasks."</data>
      <data key="d6">0036fb6f489e13c0db0f1c02bf3323be</data>
    </edge>
    <edge source="&quot;RESERVOIRPY&quot;" target="&quot;PERCEPTRON&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy is used to create a reservoir computing model that includes the Perceptron algorithm for classification tasks."</data>
      <data key="d6">0036fb6f489e13c0db0f1c02bf3323be</data>
    </edge>
    <edge source="&quot;TRANSDUCTION&quot;" target="&quot;SEQUENCE-TO-SEQUENCE MODEL&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Transduction is a sequence-to-sequence encoding technique that is used to convert input sequences into output sequences in a Sequence-to-Sequence Model."
"Transduction is the process of transforming input data into output data in the context of the Sequence-to-Sequence Model."</data>
      <data key="d6">688ebc7151bc148ac24dc7e2727d7afe,79d5959f3f6471cad55498ab4a8a3176</data>
    </edge>
    <edge source="&quot;TRANSDUCTION&quot;" target="&quot;CLASSIFICATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Transduction and Classification are two different methods used in sequence modeling, with Transduction generating a sequence of output labels and Classification assigning a single label to each input sequence."</data>
      <data key="d6">c05906c1f12c4edfc32a04aa9935067e</data>
    </edge>
    <edge source="&quot;TRANSDUCTION&quot;" target="&quot;JAPANESE_VOWELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The japanese_vowels dataset is used for training and testing a machine learning model called Transduction, which is a sequence-to-sequence model."</data>
      <data key="d6">9414efd266e7135a2cdd7461a888b045</data>
    </edge>
    <edge source="&quot;SEQUENCE-TO-SEQUENCE MODEL&quot;" target="&quot;AUDIO PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A Sequence-to-Sequence Model is used in this context to convert sequences of audio signals into sequences of labels, which is a part of the Audio Processing task."</data>
      <data key="d6">79d5959f3f6471cad55498ab4a8a3176</data>
    </edge>
    <edge source="&quot;SEQUENCE-TO-SEQUENCE MODEL&quot;" target="&quot;JAPANESE VOWELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Sequence-to-Sequence Model is used to solve a task involving the prediction of Japanese vowels."</data>
      <data key="d6">688ebc7151bc148ac24dc7e2727d7afe</data>
    </edge>
    <edge source="&quot;SIMPLE ECHO STATE NETWORK&quot;" target="&quot;SEQUENCE-TO-SEQUENCE ENCODING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Simple Echo State Network is used for sequence-to-sequence encoding, also known as transduction."</data>
      <data key="d6">75c1234e634cf2c009a116e4ee6c053e</data>
    </edge>
    <edge source="&quot;SIMPLE ECHO STATE NETWORK&quot;" target="&quot;INPUT SEQUENCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input Sequence is used as input for the Simple Echo State Network."</data>
      <data key="d6">75c1234e634cf2c009a116e4ee6c053e</data>
    </edge>
    <edge source="&quot;SIMPLE ECHO STATE NETWORK&quot;" target="&quot;OUTPUT SEQUENCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Simple Echo State Network produces an Output Sequence."</data>
      <data key="d6">75c1234e634cf2c009a116e4ee6c053e</data>
    </edge>
    <edge source="&quot;SIMPLE ECHO STATE NETWORK&quot;" target="&quot;TRAINING DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Simple Echo State Network is trained using Training Data."</data>
      <data key="d6">75c1234e634cf2c009a116e4ee6c053e</data>
    </edge>
    <edge source="&quot;SIMPLE ECHO STATE NETWORK&quot;" target="&quot;TEST DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Simple Echo State Network is evaluated using Test Data."</data>
      <data key="d6">75c1234e634cf2c009a116e4ee6c053e</data>
    </edge>
    <edge source="&quot;SIMPLE ECHO STATE NETWORK&quot;" target="&quot;RIDGE REGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge Regression is used for the readout of the Simple Echo State Network."</data>
      <data key="d6">75c1234e634cf2c009a116e4ee6c053e</data>
    </edge>
    <edge source="&quot;SEQUENCE-TO-SEQUENCE ENCODING&quot;" target="&quot;RESERVOIRPY NODES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy Nodes use Sequence-to-Sequence Encoding for tasks such as converting a sequence of audio data into a sequence of labels."</data>
      <data key="d6">a6f2502b5336ffc8606e1167b2813004</data>
    </edge>
    <edge source="&quot;SEQUENCE-TO-SEQUENCE ENCODING&quot;" target="&quot;CLASSIFICATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sequence-to-Sequence Encoding is not typically used for classification tasks, as it is designed for generating sequences of output labels."</data>
      <data key="d6">a6f2502b5336ffc8606e1167b2813004</data>
    </edge>
    <edge source="&quot;INPUT SEQUENCE&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN is driven by an Input Sequence during the state harvesting stage of training."</data>
      <data key="d6">f18a060e6d2bb1da70432cbc71378770</data>
    </edge>
    <edge source="&quot;OUTPUT SEQUENCE&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN generates an Output Sequence during the training stage, which may involve teacher forcing."</data>
      <data key="d6">f18a060e6d2bb1da70432cbc71378770</data>
    </edge>
    <edge source="&quot;TRAINING DATA&quot;" target="&quot;ECHO STATE NETWORKS (ESNS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESNs are trained using historical data, allowing them to learn patterns and dynamics."</data>
      <data key="d6">4b78fdc153f982e64291112395c316c7</data>
    </edge>
    <edge source="&quot;TRAINING DATA&quot;" target="&quot;TIMESTEP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Timestep is a single point in time used to train a machine learning model with Training Data."</data>
      <data key="d6">af2db1cc5ab6b16acae2c93d3facb668</data>
    </edge>
    <edge source="&quot;TRAINING DATA&quot;" target="&quot;ECHO STATE NETWORK (ESN)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network (ESN) model is trained using Training Data."</data>
      <data key="d6">1365a36c76afc697ac626fd0f784804a</data>
    </edge>
    <edge source="&quot;TRAINING DATA&quot;" target="&quot;DATA PREPROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Data Preprocessing is applied to the Training Data to clean and prepare it for use in the Echo State Network (ESN) model."</data>
      <data key="d6">1365a36c76afc697ac626fd0f784804a</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;MULTICOLLINEARITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge Regression is used to address multicollinearity in data, which can lead to unreliable coefficients in traditional linear regression."</data>
      <data key="d6">b5b73413fbe4ab8b61c4a939fe6c6a2b</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;IBM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IBM provides information about Ridge Regression, including detailed explanations and resources."</data>
      <data key="d6">b5b73413fbe4ab8b61c4a939fe6c6a2b</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;OVERFITTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge Regression is a technique used to prevent overfitting in machine learning models."</data>
      <data key="d6">77c3759b4ed32509aaf1403c6fa8030f</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;ESN&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"Ridge Regression is mentioned as a regularization technique used during the offline training of Echo State Networks."
"ESN models use Ridge Regression in their output layer."
"ESN can be trained offline using ridge regression, which helps to prevent overfitting by adding a penalty term to the loss function."</data>
      <data key="d6">593080a95ef7640b3925b07cad1bedd4,c53825a1ab5e01a794a428988435a7a7,fe90abb0dde126fafbf44782aeb6738c</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;ESN OFFLINE TRAINING WITH RIDGE REGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge Regression is used during the training of ESN for offline tasks."</data>
      <data key="d6">18e4624a6da9e8e6d9b9b2ed260bf9b2</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;TRAINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge regression is a method used for offline training of an Echo State Network (ESN)."</data>
      <data key="d6">1db191f05801d40d5a346febd10d3352</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ES2N models use Ridge Regression in their output layer."</data>
      <data key="d6">c53825a1ab5e01a794a428988435a7a7</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;LINEAR ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Linear ESN models use Ridge Regression in their output layer."</data>
      <data key="d6">c53825a1ab5e01a794a428988435a7a7</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;ORTHOGONAL ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Orthogonal ESN models use Ridge Regression in their output layer."</data>
      <data key="d6">c53825a1ab5e01a794a428988435a7a7</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;DATA ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge Regression is a linear regression model used in data analysis tasks to prevent overfitting and improve prediction accuracy."</data>
      <data key="d6">ef9bf350e25daa8f123b0b5c4d60de5f</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge Regression is used to estimate the coefficients of the Model, helping to avoid overfitting."</data>
      <data key="d6">29ce72a8f609c311ebb852cc96aee54d</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;NVAR MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge Regression is used as a statistical method for regression analysis in the setup of the NVAR Model."</data>
      <data key="d6">c838b1b4744bc0f400abf85f791950cf</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;TRAINER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Trainer uses Ridge Regression as a statistical method for regression analysis in the model setup."</data>
      <data key="d6">c838b1b4744bc0f400abf85f791950cf</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;ECHO STATE NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge Regression is a method used in echo state networks for regression analysis to prevent overfitting."</data>
      <data key="d6">ed28ba3543e07641536ff1eb5e0749dd</data>
    </edge>
    <edge source="&quot;RIDGE REGRESSION&quot;" target="&quot;ECHO STATE NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network uses Ridge Regression as a regularization technique in the readout layer."</data>
      <data key="d6">688ebc7151bc148ac24dc7e2727d7afe</data>
    </edge>
    <edge source="&quot;SPEAKER LABELING&quot;" target="&quot;SEQUENCE-TO-VECTOR MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sequence-to-Vector Models are used for Speaker Labeling, which is a part of data analysis and classification of sequential patterns."</data>
      <data key="d6">64b0ff9558a0f4794c16619aa76354c4</data>
    </edge>
    <edge source="&quot;SEQUENCE-TO-VECTOR MODEL&quot;" target="&quot;RESERVOIRPY NODES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy Nodes can also use Sequence-to-Vector Model for tasks such as classifying sequential patterns."</data>
      <data key="d6">a6f2502b5336ffc8606e1167b2813004</data>
    </edge>
    <edge source="&quot;SEQUENCE-TO-VECTOR MODEL&quot;" target="&quot;CLASSIFICATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sequence-to-Vector Model is commonly used for classification tasks, as it allows for the assignment of a single label to each input sequence."</data>
      <data key="d6">a6f2502b5336ffc8606e1167b2813004</data>
    </edge>
    <edge source="&quot;DATA ANALYSIS&quot;" target="&quot;ECHO STATE NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks are also used for Data Analysis tasks, such as pattern recognition and classification."</data>
      <data key="d6">29f9b2e5fa311519b18e7aef31c68d0a</data>
    </edge>
    <edge source="&quot;DATA ANALYSIS&quot;" target="&quot;PYTHON CODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Python Code is used to analyze data in the context of data analysis tasks."</data>
      <data key="d6">d15f6d075c072f0335b5332f11c00299</data>
    </edge>
    <edge source="&quot;DATA ANALYSIS&quot;" target="&quot;RMSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RMSE is a measure used in data analysis to evaluate the accuracy of predictions."</data>
      <data key="d6">d15f6d075c072f0335b5332f11c00299</data>
    </edge>
    <edge source="&quot;DATA ANALYSIS&quot;" target="&quot;CANARY SONG DECODING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Canary Song Decoding involves the analysis and classification of temporal motifs in canary songs."</data>
      <data key="d6">d15f6d075c072f0335b5332f11c00299</data>
    </edge>
    <edge source="&quot;DATA ANALYSIS&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN is a type of network used for data analysis."</data>
      <data key="d6">7b9936d57ece8ba985947a7aca12e2c7</data>
    </edge>
    <edge source="&quot;DATA ANALYSIS&quot;" target="&quot;AUTHOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author is using Echo State Networks for data analysis."</data>
      <data key="d6">41fa16855df7da666dc6fc38d2f8ee53</data>
    </edge>
    <edge source="&quot;DATA ANALYSIS&quot;" target="&quot;JUPYTER NOTEBOOK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jupyter Notebook is an interactive environment used for data analysis."</data>
      <data key="d6">41fa16855df7da666dc6fc38d2f8ee53</data>
    </edge>
    <edge source="&quot;PREDICTION&quot;" target="&quot;TIME SERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prediction is mentioned in the context of Time Series, referring to forecasting future data points."</data>
      <data key="d6">14bccd672d2f8dd2cd7300581c8844fb</data>
    </edge>
    <edge source="&quot;PREDICTION&quot;" target="&quot;SEED&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Seed is mentioned in the context of making predictions reproducible, as it initializes a random number generator."</data>
      <data key="d6">14bccd672d2f8dd2cd7300581c8844fb</data>
    </edge>
    <edge source="&quot;PREDICTION&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN generates Predictions by using the learned patterns and dynamics of the Input Data to generate future values of the timeseries."</data>
      <data key="d6">693e4d1e43289f46866236c10207a17e</data>
    </edge>
    <edge source="&quot;PREDICTION&quot;" target="&quot;FORECASTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Prediction is a broader concept that includes forecasting, which involves making specific predictions about future data points."</data>
      <data key="d6">9261efcc24379d9c0b2d35a2fde8275d</data>
    </edge>
    <edge source="&quot;PREDICTION&quot;" target="&quot;ESN MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN model is used for predicting the next 100 steps of a timeseries, based on its last 10 steps."</data>
      <data key="d6">8c520b4037fe01ffce62d46b67175e67</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;JAPANESE VOWELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Ridge component is trained on the Japanese Vowels dataset to classify sequential patterns."</data>
      <data key="d6">1ea13fb2c1fff4954b699a8e2377f99f</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;HP_SPACE&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"hp_space is the configuration parameter that explores the ridge parameter, but its configuration is not explicitly mentioned in the given text."
"hp_space is used to explore the ridge parameter."
"hp_space is associated with the parameter ridge, which specifies a regularization term."</data>
      <data key="d6">5cea9edfd65fcfa25a081554300b28cc,80033e741d8e10abdcfe20dd17192152,adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;ESN&quot;">
      <data key="d4">12.0</data>
      <data key="d5">"The ESN uses a Ridge readout layer to read out the desired output from the activations of the reservoir."
"The ESN model uses Ridge regularization to prevent overfitting."
"Ridge is used as a regularization technique in the ESN machine learning model."
"ESN is composed of a Ridge component for readout and training."
"ESN uses Ridge regularization to prevent overfitting and improve generalization."
"Ridge is used as a regularization technique in the Echo State Network for regression tasks."
"The Ridge is a type of readout used in the Echo State Network (ESN) system for making predictions."
"Ridge is a regularization technique used in Echo State Networks (ESNs) to prevent overfitting."
"The ESN node in ReservoirPy is created from the Ridge node."
"Ridge is a type of readout used in the ESN model for time series prediction."
"ESN uses a Ridge readout for training and prediction."
"ESN uses a Ridge component for regularization to prevent overfitting."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7,1298c65a923053e1de35aacddc13832c,36e4df75a46fb977f9516f2d2f1f9bc2,3bee7b78d0ab9582cc9bffe9e305df2e,6a7bea5f60347ea864c06adc327829dc,72e6eee633bcb5b1458c4cee3975cee1,7f70879016c133fe58e4838172a69613,80c9f51870e239404ed671ef0374f191,993a69efae014a8f8d6ec0c235104d46,bf4eaad93f89884d02cdad6a50f145a6,f0c8d4d322d73f46464e3e9f6914f2ee,f7f7dbc1e69b3b0e801bc5ba9c0cabca</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;RESERVOIR&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"Reservoir and Ridge are components used together in the provided code, likely as part of a machine learning model."
"The Reservoir is used in conjunction with Ridge, a regularization technique, to handle the processing of Time Series Data."
"Ridge is used as a readout algorithm in combination with a Reservoir in the provided code."</data>
      <data key="d6">0753d4e507badadd900c522ee03ad28d,2bcc39da2ecef3011cc3da428fca5dd5,82ff270b1bbdfe0ee11e603de1e326c7</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;HYPER-PARAMETER EXPLORATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge is a regularization technique used in Hyper-parameter Exploration to prevent overfitting."</data>
      <data key="d6">136d135c710f6cf78a4c536d43276fe1</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;ES2N&quot;">
      <data key="d4">5.0</data>
      <data key="d5">"ES2N and Ridge are used in combination to build a model for time series forecasting."
"ES2N and Ridge are used together in the provided code to build a machine learning model."
"ES2N and Ridge are used together in the code for time series prediction, indicating their relationship in this context."
"ES2N is mentioned in the context of the Ridge model."
"Ridge is used as a regularization technique in the ES2N machine learning model."</data>
      <data key="d6">1298c65a923053e1de35aacddc13832c,3a8ed31ef360d5587cc6411e9fce89d4,4a7ca13b3f869961817e2aa723e67d24,97f5d2e9d34b3b50f8e922fc4bb7f824,a614fa3f8de82d4078f220e5f373f03a</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;REGRESSION ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge is a model used for regression analysis, which is used as a readout in the combination with ES2N."</data>
      <data key="d6">4a7ca13b3f869961817e2aa723e67d24</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;MACKEY-GLASS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Ridge model is used to analyze the Mackey-Glass time series data set."</data>
      <data key="d6">90c1a399dd410f75f1f4bb03fe1f5f33</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;LEAKYESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The leakyESN model includes a Ridge component."</data>
      <data key="d6">abd3ca2db22004a5de548dc22010c4d4</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;MACHINE LEARNING MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge is a regularization technique used in the machine learning model, helping to prevent overfitting and improve its performance."</data>
      <data key="d6">96366d7c23d50de6294c54c3444eac86</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;PHAS[I]&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"PHAS[i] is mentioned in the context of the Ridge model."</data>
      <data key="d6">97f5d2e9d34b3b50f8e922fc4bb7f824</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;ES^2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES^2N model uses Ridge regularization to prevent overfitting."</data>
      <data key="d6">6a7bea5f60347ea864c06adc327829dc</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;ESNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge is a type of readout layer used in ESNs, which estimates the output based on the reservoir's state."</data>
      <data key="d6">f1fc6fbc8158d3da070d55544041a2ca</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;FEEDBACK CONNECTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Ridge component of ESNs can be connected through feedback connections."</data>
      <data key="d6">7f2d69f9a9baca70ffd25a6865189206</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;Y_TRAIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Ridge component of the ESN model is trained using the training target data (Y_train)."</data>
      <data key="d6">7f2d69f9a9baca70ffd25a6865189206</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;ESN MODEL&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The Ridge is a component of the ESN model, with a ridge parameter of 1e-7."
"The ESN Model contains a Ridge node, which is used as a readout for making predictions."</data>
      <data key="d6">8ade7819a5f8d1ec26e9bdbd059142e6,8c520b4037fe01ffce62d46b67175e67</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;DEEP ECHO STATE NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep Echo State Networks are constructed using components of type Ridge."</data>
      <data key="d6">59b469bdd618b3f36b3547f4f2b8a862</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;DEEP ESN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Deep ESN models use Ridge regression models for making predictions."
"Deep ESN models include a Ridge type readout or output layer."</data>
      <data key="d6">2336a57d055095c6ffa9d156ddee0096,e39809b687cd044a7918eca37727a188</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;NVAR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NVAR is connected to a readout layer with offline learning using Ridge for regularized linear regression."</data>
      <data key="d6">2035514bf3ab5b7f12ae1321972551f1</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;DOUBLE SCROLL ATTRACTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge is a model used to analyze the dynamics of the Double Scroll Attractor."</data>
      <data key="d6">684b1edf65b327cc06ceb69ca1279d74</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;AUTHOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author is using the Ridge model to analyze the Double Scroll Attractor."</data>
      <data key="d6">684b1edf65b327cc06ceb69ca1279d74</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;INPUT TIMESERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge is trained using an Input Timeseries as input."</data>
      <data key="d6">5366a81a025c098744b5d6f1432c2fbc</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;TARGET TIMESERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge is trained to create a mapping from Input Timeseries to Target Timeseries."</data>
      <data key="d6">5366a81a025c098744b5d6f1432c2fbc</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;ONE-TIMESTEP-AHEAD PREDICTION TASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge is used to solve a One-timestep-ahead Prediction Task, predicting the next timestep of a timeseries based on its current timestep."</data>
      <data key="d6">5366a81a025c098744b5d6f1432c2fbc</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;ESN_MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Ridge organization is used to construct the esn_model."</data>
      <data key="d6">751b176a8d6149a853e597c65a6fe0cf</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;HIERARCHICAL ESN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Ridge readouts are used in the construction of hierarchical echo state networks (ESNs)."
"Hierarchical ESN models consist of multiple readouts, which are Ridge type output layers."</data>
      <data key="d6">00d22666fe697ffb66c2392939f45b39,e39809b687cd044a7918eca37727a188</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;REGULARIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The amount of regularization in the Ridge readout is controlled using the regularization parameter in the ESN."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;MODEL&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The Ridge component is a part of the Model, contributing to its regularization and prediction capabilities."
"The Model uses the Ridge component for output prediction."</data>
      <data key="d6">46dcc47b4358d3895c1eeb1182c6f997,a0feae89e52a4291db0a512a3a102d8e</data>
    </edge>
    <edge source="&quot;RIDGE&quot;" target="&quot;HYPEROPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge is a parameter mentioned in the Hyperopt configuration."</data>
      <data key="d6">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;JAPANESE VOWELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir component generates internal states based on input sequences from the Japanese Vowels dataset."</data>
      <data key="d6">1ea13fb2c1fff4954b699a8e2377f99f</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;STATES_TRAIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"states_train are generated using the reservoir component, potentially representing a relationship between input data and states."</data>
      <data key="d6">b101b38a87b2fcac0ff450a4e3f22143</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;X_TEST&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"X_test is used to generate states using the reservoir component, potentially representing a relationship between input data and states."
"The Reservoir component processes the testing input data."</data>
      <data key="d6">b101b38a87b2fcac0ff450a4e3f22143,dc3bd3697a140b64d70e0e3ac6db6c7e</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;ECHO STATE NETWORKS&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"Echo State Networks consist of a reservoir component, which is a pool of randomly connected neurons."
"Echo State Networks consist of a Reservoir that receives input signals and transforms them into high-dimensional representations."
"Reservoir is a component of an echo state network that stores and processes information."</data>
      <data key="d6">0e6f0f7cd882a638ecb571ef36068868,83fafb2423a01afae7e522917d79ace9,ed28ba3543e07641536ff1eb5e0749dd</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;RANDOM HIGH-DIMENSIONAL VECTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Random High-Dimensional Vector refers to the activations of the reservoir in Echo State Networks, which capture intricate patterns and dynamics of the input data."</data>
      <data key="d6">0e6f0f7cd882a638ecb571ef36068868</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;FROM_STATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"from_state is a parameter used to initialize the reservoir with a specific state at the start of a simulation or training process."</data>
      <data key="d6">cb71a9bc3b00e7abcd1a53004abdea69</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;WITH_STATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"with_state is a parameter used to continue the simulation or training process from a specific state, updating the reservoir's state during its operation."</data>
      <data key="d6">cb71a9bc3b00e7abcd1a53004abdea69</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;RUN(X)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"run(X) is a method that processes the entire timeseries X through the reservoir node, updating the reservoir's state at each timestep based on the input data."</data>
      <data key="d6">cb71a9bc3b00e7abcd1a53004abdea69</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;RECURRENT NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recurrent network is a type of artificial neural network characterized by bi-directional flow of information, which is a characteristic of reservoirs."</data>
      <data key="d6">cb71a9bc3b00e7abcd1a53004abdea69</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;READOUT NODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir provides internal states to the Readout Node, which uses them to make predictions."</data>
      <data key="d6">fa082948fa919150e9c06c6f5c1b53b0</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;INPUT DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input Data is processed by the Reservoir, generating dynamic representations that can be utilized by the model."</data>
      <data key="d6">4da651284dbab3f68dc3cae41e6e0311</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;READOUT LAYER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir processes the input data and generates dynamic representations that are then utilized by the Readout Layer."</data>
      <data key="d6">4da651284dbab3f68dc3cae41e6e0311</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;ONE-TO-MANY CONNECTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir component can be connected to multiple subsequent nodes using One-to-Many Connections, allowing the same input data to be processed in different ways, enhancing the model's ability to capture various aspects of the data."</data>
      <data key="d6">a35f6cae32a3d24b18ee17ec0471a9d4</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;FEEDBACK CONNECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Feedback Connection mechanism allows the Reservoir to access the state of the Readout with a one-timestep delay."</data>
      <data key="d6">333ecf478bfbd4291de9f193bbf0443a</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;IN-PLACE FEEDBACK CONNECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The In-place Feedback Connection variant allows the Reservoir to directly hold the reference to the Readout node's state without creating a copy."</data>
      <data key="d6">333ecf478bfbd4291de9f193bbf0443a</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;READOUT&quot;">
      <data key="d4">7.0</data>
      <data key="d5">"The Reservoir receives feedback from the Readout, using the most recent output information for current processing.""The Readout sends its state to the Reservoir for feedback, allowing the Reservoir to remember and incorporate past decisions or predictions."
"The Reservoir's output is used by the Readout to generate predictions in Echo State Networks (ESNs)."
"The output of the reservoir component is fed into the readout component of the model."
"The reservoir node sends its processed output to the readout node."
"The Reservoir provides data to the Readout, which learns connections from the reservoir to the readout neurons."
"The reservoir processes input data, and the readout algorithm is used to extract meaningful information from the reservoir's output."</data>
      <data key="d6">0d922ae20673124fc4588949e3863ed0,324a8f3fb4d19b91457a99999e6d3d17,333ecf478bfbd4291de9f193bbf0443a,58115d5a63315a84d9c8d4e6ddc98ffd,cf15a09e77b695a117e1cca05461aea2,d25cd385546ec6a033287e75d65a551a</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;FORCED FEEDBACK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"In Forced Feedback technique, Teacher Vectors are provided as feedback to the Reservoir in Echo State Networks (ESNs)."</data>
      <data key="d6">0d922ae20673124fc4588949e3863ed0</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;ECHO STATE NETWORKS (ESNS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir in an ESN helps preserve information and capture temporal dynamics, contributing to accurate predictions."</data>
      <data key="d6">4b78fdc153f982e64291112395c316c7</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;MACKEY-GLASS TASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir component is used in the Mackey-Glass task to create a reservoir node for time series prediction."</data>
      <data key="d6">b11a9f7777c0232bfa7323ae82ad139b</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;SCIKITLEARNNODE&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The Reservoir is used in conjunction with ScikitLearnNode to wrap scikit-learn classifiers and use them in a machine learning pipeline."
"ScikitLearnNode is mentioned in the context of using a Reservoir component, but the nature of their relationship is not explicitly stated."</data>
      <data key="d6">2cba60e2f36479613bb0243a19f3a3b4,d58662ee42c14a0787d839ebfd0a6e9b</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;ESN&quot;">
      <data key="d4">16.0</data>
      <data key="d5">"The ESN is made of a Reservoir, which is a random recurrent network used to encode inputs in a high-dimensional space."
"The ESN system relies on the reservoir having the echo state property for its correct functioning."
"Reservoir is a component of the ESN machine learning model."
"ESN is composed of a Reservoir component for processing input data."
"ESN is composed of a Reservoir component that stores and processes information."
"ESN is composed of a Reservoir component that processes input data and generates a high-dimensional state space."
"Echo State Networks utilize a Reservoir component to process and store input data."
"The Reservoir is a component of the Echo State Network (ESN) system, used for processing input data."
"Reservoir is a component of Echo State Networks (ESNs), responsible for processing input data."
"The ESN node in ReservoirPy is created from the standard Reservoir node."
"Reservoir is a component of the ESN model that stores and processes data."
"ESN is composed of a Reservoir component."
"The Reservoir is a component used in the creation of an ESN."
"The ESN model is composed of a reservoir and a readout, with the reservoir processing input data."
"ESN is composed of a Reservoir component that processes input data."
"ESN is composed of a Reservoir component that processes input data."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7,1298c65a923053e1de35aacddc13832c,36e4df75a46fb977f9516f2d2f1f9bc2,3bee7b78d0ab9582cc9bffe9e305df2e,6a4432cd530b28770e2b903fe242a0d1,72e6eee633bcb5b1458c4cee3975cee1,7b294b788fe5ee385d08c4aabe2ca71d,7f70879016c133fe58e4838172a69613,80c9f51870e239404ed671ef0374f191,993a69efae014a8f8d6ec0c235104d46,9b360c6a33aafa6827417de5bd4faa82,bf4eaad93f89884d02cdad6a50f145a6,cc1fb6ca5695434ad0279c2606e928af,cdc64af0dde941250d89b191d0666c9b,f0c8d4d322d73f46464e3e9f6914f2ee,f7f7dbc1e69b3b0e801bc5ba9c0cabca</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;SPECTRAL RADIUS&quot;">
      <data key="d4">6.0</data>
      <data key="d5">"The spectral radius is a property of the Reservoir component in the reservoir computing model."
"Spectral Radius is a property of the Reservoir, with a value close to 1 having theoretical implications for the reservoir states."
"The Spectral Radius parameter is used to control the dynamics of the reservoir in the Echo State Network (ESN) model."
"Spectral Radius is a parameter of the Reservoir component that determines the stability and speed of the system."
"The Spectral Radius concept is used to describe the maximum eigenvalue of a reservoir matrix."
"The Reservoir component's Spectral Radius is mentioned, indicating its role in the network."</data>
      <data key="d6">1365a36c76afc697ac626fd0f784804a,4073cafddb73621f26061385c5570659,72e6eee633bcb5b1458c4cee3975cee1,82f7e4647b9da5d5063fe92613f4fbcb,94fd1ebf256db17e4ac2255b89caa473,b957e1bf5bf175c7630222ca742c7933</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;MACKEY-GLASS TIMESERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Mackey-Glass timeseries is used as inputs for the Reservoir component in the reservoir computing model."</data>
      <data key="d6">94fd1ebf256db17e4ac2255b89caa473</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;INPUT SCALING&quot;">
      <data key="d4">4.0</data>
      <data key="d5">"Input Scaling is applied to a reservoir, affecting its dynamics and potentially influencing its ability to process input data."
"Input Scaling is a parameter of the Reservoir that influences the reservoir's behavior."
"The Reservoir system undergoes Input scaling as a process."
"The Reservoir component is adjusted using Input Scaling to improve the network's performance."</data>
      <data key="d6">1f30b86a46d4819603edc730df816c49,82f7e4647b9da5d5063fe92613f4fbcb,b957e1bf5bf175c7630222ca742c7933,bba680a0a7dd439bd5b0fe1547ffe040</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;ECHO STATE PROPERTY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Property is a property of the Reservoir, which is supposed to allow the reservoir states to be less affected by their initial conditions while having good memorization properties."</data>
      <data key="d6">82f7e4647b9da5d5063fe92613f4fbcb</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;IPRESERVOIR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The IPReservoir model uses a Reservoir to process input data."</data>
      <data key="d6">eadceb9674dd1ce90473d99e0b58e141</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;LEAKYESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The leakyESN model includes a Reservoir component."</data>
      <data key="d6">abd3ca2db22004a5de548dc22010c4d4</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;MACHINE LEARNING MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir is a component of the machine learning model, contributing to its architecture and functionality."</data>
      <data key="d6">96366d7c23d50de6294c54c3444eac86</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;PHAS[I]&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"PHAS[i] is mentioned in the context of the Reservoir model."</data>
      <data key="d6">97f5d2e9d34b3b50f8e922fc4bb7f824</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ES2N is mentioned in the context of the Reservoir model."</data>
      <data key="d6">97f5d2e9d34b3b50f8e922fc4bb7f824</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;RING_MATRIX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir is mentioned in the context of the ring_matrix concept."</data>
      <data key="d6">97f5d2e9d34b3b50f8e922fc4bb7f824</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;IDENTITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir is mentioned in the context of the identity concept."</data>
      <data key="d6">97f5d2e9d34b3b50f8e922fc4bb7f824</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;ESNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir is a component of ESNs that processes input data and generates a rich representation for the readout layer."</data>
      <data key="d6">f1fc6fbc8158d3da070d55544041a2ca</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;INPUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The input data is processed by the reservoir component of the model."</data>
      <data key="d6">58115d5a63315a84d9c8d4e6ddc98ffd</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;ESN MODEL&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"The ESN Model includes the reservoir component for processing the input data."
"The Reservoir is a component of the ESN model, with a learning rate of 0.5 and a spectral radius of 0.9."
"The ESN Model contains a Reservoir node, which processes input data."</data>
      <data key="d6">58115d5a63315a84d9c8d4e6ddc98ffd,8ade7819a5f8d1ec26e9bdbd059142e6,8c520b4037fe01ffce62d46b67175e67</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Data is input into the reservoir node, which processes it and sends it to the readout node."</data>
      <data key="d6">cf15a09e77b695a117e1cca05461aea2</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;FEEDBACK CONNECTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir component of ESNs can be connected through feedback connections."</data>
      <data key="d6">7f2d69f9a9baca70ffd25a6865189206</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;X_TRAIN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The Reservoir component of the ESN model processes the training input data (X_train)."
"The Reservoir component processes the training input data."</data>
      <data key="d6">7f2d69f9a9baca70ffd25a6865189206,dc3bd3697a140b64d70e0e3ac6db6c7e</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;X&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir component of the ESN model processes the input data (X)."</data>
      <data key="d6">7f2d69f9a9baca70ffd25a6865189206</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;INITIALIZER FUNCTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Initializer functions are used to initialize the parameters of the reservoir matrices."</data>
      <data key="d6">ff860bc63e3d697a6183c0b850689048</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;READOUTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir and readouts are components of the Reservoirpy library that hold parameters stored as Numpy arrays or Scipy sparse matrices."</data>
      <data key="d6">ff860bc63e3d697a6183c0b850689048</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;DEEP ECHO STATE NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep Echo State Networks are constructed using components of type Reservoir."</data>
      <data key="d6">59b469bdd618b3f36b3547f4f2b8a862</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;DEEP ESN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Deep ESN models use multiple reservoirs to process input data."
"Deep ESN models consist of multiple interconnected reservoirs."</data>
      <data key="d6">2336a57d055095c6ffa9d156ddee0096,e39809b687cd044a7918eca37727a188</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;TIMESTEP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir can be triggered on a single timestep of data, which is used as input for the node."</data>
      <data key="d6">0e0afab060f214d46062c9886e762002</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;TIMESERIES&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Reservoir can also be triggered on complete timeseries, which is another form of input for the node."
"The Reservoir processes the Timeseries to gather the activations of its neurons."</data>
      <data key="d6">0e0afab060f214d46062c9886e762002,c4f5a27caf9dd9c1d972492c1147efa0</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;NEURONS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Reservoir is a recurrent neural network consisting of individual neurons, which can be triggered and activated."
"The Reservoir is composed of Neurons, which evolve in time following the input timeseries."</data>
      <data key="d6">0e0afab060f214d46062c9886e762002,c4f5a27caf9dd9c1d972492c1147efa0</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;RIDGE READOUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir is connected to the Ridge Readout, as the input timeseries for training the readout."</data>
      <data key="d6">5d3baa9818a4e01fe1196c43378a2cea</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;TIME SERIES DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir is used to process Time Series Data, such as a Sine Wave."</data>
      <data key="d6">2bcc39da2ecef3011cc3da428fca5dd5</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;ESN_MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir organization is used to construct the esn_model."</data>
      <data key="d6">751b176a8d6149a853e597c65a6fe0cf</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;HIERARCHICAL ESN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Reservoirs are used in the construction of hierarchical echo state networks (ESNs)."
"Hierarchical ESN models consist of multiple reservoirs, which are components of the model."</data>
      <data key="d6">00d22666fe697ffb66c2392939f45b39,e39809b687cd044a7918eca37727a188</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;UNITS&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"The Reservoir component of the ESN has a specified number of units or neurons."
"Units refer to the number of nodes in the Reservoir component of the ESN model."
"The Reservoir system is composed of a specific number of UNITS."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7,72e6eee633bcb5b1458c4cee3975cee1,bba680a0a7dd439bd5b0fe1547ffe040</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;INPUT_SCALING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input data is scaled using the input_scaling parameter in the Reservoir component of the ESN."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;SPECTRAL_RADIUS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The spectral radius of the reservoir's weight matrix is set using the spectral_radius parameter in the ESN."
"The Reservoir system is characterized by the SPECTRAL_RADIUS."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7,bba680a0a7dd439bd5b0fe1547ffe040</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;LEAK_RATE&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The leakage of the reservoir's neurons is controlled using the leak_rate parameter in the ESN."
"The Reservoir system experiences information loss or decay at the LEAK_RATE."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7,bba680a0a7dd439bd5b0fe1547ffe040</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;CONNECTIVITY&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"The sparsity of the reservoir's weight matrix is set using the connectivity parameter in the ESN."
"The Connectivity parameter is used to control the sparsity and structure of the reservoir in the Echo State Network (ESN) model."
"Connectivity is a parameter of the Reservoir component that determines the density of connections between nodes."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7,1365a36c76afc697ac626fd0f784804a,72e6eee633bcb5b1458c4cee3975cee1</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;INPUT_CONNECTIVITY&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The sparsity of the input-to-reservoir weight matrix is set using the input_connectivity parameter in the ESN."
"The Reservoir system interacts with external inputs at the INPUT_CONNECTIVITY level."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7,bba680a0a7dd439bd5b0fe1547ffe040</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;SEED&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The random number generator in the ESN is initialized using the seed parameter."
"The Reservoir system is initialized with a specific SEED value."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7,bba680a0a7dd439bd5b0fe1547ffe040</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;GENERATIVE MODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Generative Mode utilizes a Reservoir to generate timeseries data."</data>
      <data key="d6">70db98fabc82fc96ecf8cc2c023b586b</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;FORCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"FORCE is a node used in the Reservoir network."</data>
      <data key="d6">0c5a253fb2bcebe8674581a5dc12fd96</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;TRAINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Training is the process of adjusting the reservoir's parameters to improve its performance in processing input data."</data>
      <data key="d6">324a8f3fb4d19b91457a99999e6d3d17</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;RESERVOIR COMPUTING NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing Networks are a type of recurrent neural network that use a reservoir to process input data."</data>
      <data key="d6">324a8f3fb4d19b91457a99999e6d3d17</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;ECHO STATE NETWORK (ESN)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network (ESN) model consists of a reservoir component that stores and processes information from the input data."</data>
      <data key="d6">1365a36c76afc697ac626fd0f784804a</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;LEAK RATE&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"The Leak Rate parameter is used to control the rate at which information is lost from the reservoir in the Echo State Network (ESN) model."
"Leak Rate is a parameter of the Reservoir component that controls the rate at which the current state is forgotten."
"The Reservoir component's Leak Rate is mentioned, indicating its role in the network."</data>
      <data key="d6">1365a36c76afc697ac626fd0f784804a,72e6eee633bcb5b1458c4cee3975cee1,b957e1bf5bf175c7630222ca742c7933</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;INPUT CONNECTIVITY&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"The Input Connectivity parameter is used to control the connection between the input data and the reservoir in the Echo State Network (ESN) model."
"Input Connectivity is a parameter that determines the density of connections between input nodes and reservoir nodes."
"The Reservoir component is connected to the input through Input Connectivity."</data>
      <data key="d6">1365a36c76afc697ac626fd0f784804a,72e6eee633bcb5b1458c4cee3975cee1,b957e1bf5bf175c7630222ca742c7933</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;RC_CONNECTIVITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir system's units are interconnected at the RC_CONNECTIVITY level."</data>
      <data key="d6">bba680a0a7dd439bd5b0fe1547ffe040</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;RC CONNECTIVITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir component is connected to other components through RC Connectivity."</data>
      <data key="d6">b957e1bf5bf175c7630222ca742c7933</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;LEAKING RATES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir organization uses Leaking Rates as parameters in their simulations."</data>
      <data key="d6">548c454b31f852543b600df173bd44ab</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;DOUBLESCROLL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir organization uses the Doublescroll concept to generate data for their simulations."</data>
      <data key="d6">548c454b31f852543b600df173bd44ab</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;OPTIMIZE HYPERPARAMETERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir organization is involved in the process of Optimizing Hyperparameters, likely to improve the accuracy or performance of their simulations."</data>
      <data key="d6">548c454b31f852543b600df173bd44ab</data>
    </edge>
    <edge source="&quot;RESERVOIR&quot;" target="&quot;MODEL&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The Reservoir component is a part of the Model, contributing to its time series prediction capabilities."
"The Model uses the Reservoir component for processing input data."</data>
      <data key="d6">46dcc47b4358d3895c1eeb1182c6f997,a0feae89e52a4291db0a512a3a102d8e</data>
    </edge>
    <edge source="&quot;INPUT&quot;" target="&quot;ESNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input is a node in ESNs that represents the input data to be processed."</data>
      <data key="d6">f1fc6fbc8158d3da070d55544041a2ca</data>
    </edge>
    <edge source="&quot;INPUT&quot;" target="&quot;READOUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The input data is also directly fed into the readout component of the model."</data>
      <data key="d6">58115d5a63315a84d9c8d4e6ddc98ffd</data>
    </edge>
    <edge source="&quot;INPUT&quot;" target="&quot;ESN MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN Model takes the input data as one of its components."</data>
      <data key="d6">58115d5a63315a84d9c8d4e6ddc98ffd</data>
    </edge>
    <edge source="&quot;INPUT&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input is a component of the ESN model that provides data to the reservoir."</data>
      <data key="d6">f0c8d4d322d73f46464e3e9f6914f2ee</data>
    </edge>
    <edge source="&quot;INPUT&quot;" target="&quot;DEEP ECHO STATE NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep Echo State Networks are constructed using components of type Input."</data>
      <data key="d6">59b469bdd618b3f36b3547f4f2b8a862</data>
    </edge>
    <edge source="&quot;INPUT&quot;" target="&quot;NUMPY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Numpy is used to create and manipulate input data for echo state networks (ESNs)."</data>
      <data key="d6">00d22666fe697ffb66c2392939f45b39</data>
    </edge>
    <edge source="&quot;INPUT&quot;" target="&quot;HIERARCHICAL ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input data is fed into hierarchical echo state networks (ESNs) for processing and prediction."</data>
      <data key="d6">00d22666fe697ffb66c2392939f45b39</data>
    </edge>
    <edge source="&quot;INPUT&quot;" target="&quot;MULTI-INPUTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multi-inputs in reservoir computing models refer to the use of multiple data streams or inputs."</data>
      <data key="d6">e39809b687cd044a7918eca37727a188</data>
    </edge>
    <edge source="&quot;JAPANESE VOWELS&quot;" target="&quot;MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Japanese Vowels dataset is used for training and testing the Model."</data>
      <data key="d6">a0feae89e52a4291db0a512a3a102d8e</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;STATES_TRAIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Y_train and states_train are used together in the training process, potentially representing a relationship between labels and states."</data>
      <data key="d6">b101b38a87b2fcac0ff450a4e3f22143</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;X_TRAIN&quot;">
      <data key="d4">5.0</data>
      <data key="d5">"X_train and Y_train are components of a machine learning model used for training data, with X_train containing input data and Y_train containing corresponding labels or targets."
"X_train and Y_train are subsets of the X and Y variables used for training the machine learning model."
"X_train and y_train are subsets of the input and output data used for training the ESN."
"X_train and y_train are subsets of the input and target data used for training the ESN model."
"X_train and Y_train are datasets used together for training a machine learning model, with X_train containing input features and Y_train containing corresponding target labels."</data>
      <data key="d6">0036fb6f489e13c0db0f1c02bf3323be,71366a4c7e791080872ba783d3787bd7,72e6eee633bcb5b1458c4cee3975cee1,b3361508c3e49b5bb3089f10e31d2c81,f70c7d3d89baaabbeaad57b58e379e08</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;NP.ARGMAX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The np.argmax function is used to find the indices of the maximum values along an axis in Y_train, which contains labels or targets for training data."</data>
      <data key="d6">b3361508c3e49b5bb3089f10e31d2c81</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;MODEL&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The machine learning model is trained using the y_train dataset, which contains the target values."
"Y_train is used for training the Model."</data>
      <data key="d6">75e530c1a04e30b373dc7cc68e3ad819,a0feae89e52a4291db0a512a3a102d8e</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"y_train is used for training the ES2N machine learning model."</data>
      <data key="d6">1298c65a923053e1de35aacddc13832c</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;ESN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"y_train is used for training the ESN machine learning model."
"The Echo State Network (ESN) system is trained using the training target data (y_train)."</data>
      <data key="d6">1298c65a923053e1de35aacddc13832c,80c9f51870e239404ed671ef0374f191</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;X&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The sine wave (X) is used as input data, and its future values (Y_train) are used as target data for training the ESN model."</data>
      <data key="d6">09ea760dd2f000c961d1cfd4ea795da5</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;Y_PRED&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The predicted future values of the sine wave (Y_pred) are compared to the actual future values (Y_train) to evaluate the performance of the trained ESN model."</data>
      <data key="d6">09ea760dd2f000c961d1cfd4ea795da5</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;ESN_MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The esn_model is trained using the Y_train data output."</data>
      <data key="d6">751b176a8d6149a853e597c65a6fe0cf</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;NP.ARRAY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.array is used to create a NumPy array from the output data for training the ESN."</data>
      <data key="d6">71366a4c7e791080872ba783d3787bd7</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;ONEHOTENCODER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"OneHotEncoder is used for one-hot encoding of categorical variables in the output data for training the ESN."</data>
      <data key="d6">71366a4c7e791080872ba783d3787bd7</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;DATASET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Y_train is a variable used to store the training target data in the dataset."</data>
      <data key="d6">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;SCIKITLEARNNODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ScikitLearnNode component is trained using the training output data."</data>
      <data key="d6">dc3bd3697a140b64d70e0e3ac6db6c7e</data>
    </edge>
    <edge source="&quot;Y_TRAIN&quot;" target="&quot;RESERVOIR MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir Model is trained using the Y_train dataset, which contains the target values."</data>
      <data key="d6">0970cd32ce54f6ee1180ab237fdcefe1</data>
    </edge>
    <edge source="&quot;STATES_TRAIN&quot;" target="&quot;READOUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"states_train are used to train the readout component, potentially representing a relationship between states and predictions."</data>
      <data key="d6">b101b38a87b2fcac0ff450a4e3f22143</data>
    </edge>
    <edge source="&quot;STATES_TRAIN&quot;" target="&quot;Y_PRED&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"states_train are used to generate predictions using the readout component, potentially representing a relationship between states and predictions."</data>
      <data key="d6">b101b38a87b2fcac0ff450a4e3f22143</data>
    </edge>
    <edge source="&quot;READOUT&quot;" target="&quot;ECHO STATE NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks consist of a readout component, which is a single layer of neurons that decodes the reservoir's activations to perform a task."</data>
      <data key="d6">83fafb2423a01afae7e522917d79ace9</data>
    </edge>
    <edge source="&quot;READOUT&quot;" target="&quot;ESN&quot;">
      <data key="d4">4.0</data>
      <data key="d5">"ESN uses a Readout component to produce the final output prediction."
"Echo State Networks utilize a Readout component to transform the internal state of the network into output predictions."
"ESN is composed of a Readout component."
"The ESN model is composed of a reservoir and a readout, with the readout making predictions based on the output of the reservoir."</data>
      <data key="d6">7b294b788fe5ee385d08c4aabe2ca71d,bf4eaad93f89884d02cdad6a50f145a6,cc1fb6ca5695434ad0279c2606e928af,cdc64af0dde941250d89b191d0666c9b</data>
    </edge>
    <edge source="&quot;READOUT&quot;" target="&quot;ESN MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN Model uses the readout component to produce the output data."</data>
      <data key="d6">58115d5a63315a84d9c8d4e6ddc98ffd</data>
    </edge>
    <edge source="&quot;READOUT&quot;" target="&quot;CONCAT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Concat node concatenates incoming vectors and sends them to the readout node."</data>
      <data key="d6">cf15a09e77b695a117e1cca05461aea2</data>
    </edge>
    <edge source="&quot;READOUT&quot;" target="&quot;RES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The variable readout is used to determine the size of the variable res."</data>
      <data key="d6">7b8e1f350eefb392053be12f35fe7daf</data>
    </edge>
    <edge source="&quot;READOUT&quot;" target="&quot;REGULARIZED RIDGE REGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Readout component of an ESN is trained using Regularized Ridge Regression."</data>
      <data key="d6">cdc64af0dde941250d89b191d0666c9b</data>
    </edge>
    <edge source="&quot;READOUT&quot;" target="&quot;ECHO STATE NETWORK (ESN)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network (ESN) model consists of a readout component that maps the reservoir's output to the desired output."</data>
      <data key="d6">1365a36c76afc697ac626fd0f784804a</data>
    </edge>
    <edge source="&quot;READOUT&quot;" target="&quot;FORCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The FORCE algorithm is used as the readout algorithm in the Echo State Network (ESN) model."</data>
      <data key="d6">1365a36c76afc697ac626fd0f784804a</data>
    </edge>
    <edge source="&quot;X_TEST&quot;" target="&quot;Y_TEST&quot;">
      <data key="d4">5.0</data>
      <data key="d5">"X_test and Y_test are components of a machine learning model used for test data, with X_test containing input data and Y_test containing corresponding labels or targets."
"X_test and Y_test are subsets of the X and Y variables used for testing the performance of the trained machine learning model."
"X_test and y_test are subsets of the input and output data used for testing the ESN."
"X_test and y_test are subsets of the input and target data used for testing the trained ESN model."
"X_test and Y_test are datasets used together for testing the performance of a trained machine learning model, with X_test containing input features and Y_test containing corresponding target labels."</data>
      <data key="d6">0036fb6f489e13c0db0f1c02bf3323be,71366a4c7e791080872ba783d3787bd7,72e6eee633bcb5b1458c4cee3975cee1,b3361508c3e49b5bb3089f10e31d2c81,f70c7d3d89baaabbeaad57b58e379e08</data>
    </edge>
    <edge source="&quot;X_TEST&quot;" target="&quot;JAPANESE_VOWELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The japanese_vowels function is used to obtain data for machine learning tasks, providing input data for X_test."</data>
      <data key="d6">b3361508c3e49b5bb3089f10e31d2c81</data>
    </edge>
    <edge source="&quot;X_TEST&quot;" target="&quot;MODEL&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The machine learning model is tested using the x_test dataset."
"X_test is used for testing the Model's performance."</data>
      <data key="d6">75e530c1a04e30b373dc7cc68e3ad819,a0feae89e52a4291db0a512a3a102d8e</data>
    </edge>
    <edge source="&quot;X_TEST&quot;" target="&quot;U&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"x_test is a subset of the input data array u."</data>
      <data key="d6">f3e58b69b1a93175e3094a2ba65c0429</data>
    </edge>
    <edge source="&quot;X_TEST&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"x_test is used for testing the ES2N machine learning model."</data>
      <data key="d6">1298c65a923053e1de35aacddc13832c</data>
    </edge>
    <edge source="&quot;X_TEST&quot;" target="&quot;ESN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"x_test is used for testing the ESN machine learning model."
"The Echo State Network (ESN) system is evaluated using the testing input data (X_test)."</data>
      <data key="d6">1298c65a923053e1de35aacddc13832c,80c9f51870e239404ed671ef0374f191</data>
    </edge>
    <edge source="&quot;X_TEST&quot;" target="&quot;DATASET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"X_test is a variable used to store the testing input data in the dataset."</data>
      <data key="d6">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </edge>
    <edge source="&quot;X_TEST&quot;" target="&quot;SCIKITLEARNNODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ScikitLearnNode component is used to predict the output data for the testing input data."</data>
      <data key="d6">dc3bd3697a140b64d70e0e3ac6db6c7e</data>
    </edge>
    <edge source="&quot;X_TEST&quot;" target="&quot;RESERVOIR MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir Model is tested using the X_test dataset."</data>
      <data key="d6">0970cd32ce54f6ee1180ab237fdcefe1</data>
    </edge>
    <edge source="&quot;Y_PRED&quot;" target="&quot;Y_TEST&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"Y_pred and Y_test are compared using the accuracy_score function, potentially representing a relationship between predicted labels and true labels."
"The testing output data is compared to the predicted output data to evaluate the accuracy of the prediction model."
"Y_pred and Y_test are datasets used together to evaluate the performance of a machine learning model, with Y_pred containing predicted labels for the input features in X_test and Y_test containing the actual labels."</data>
      <data key="d6">0036fb6f489e13c0db0f1c02bf3323be,b101b38a87b2fcac0ff450a4e3f22143,dc3bd3697a140b64d70e0e3ac6db6c7e</data>
    </edge>
    <edge source="&quot;Y_PRED&quot;" target="&quot;NP.ABS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.abs is used to compute the absolute difference between the 'y_test' and 'y_pred' arrays."</data>
      <data key="d6">9078b0f36522f21a9e8e1aadac48ed9c</data>
    </edge>
    <edge source="&quot;Y_PRED&quot;" target="&quot;NP.CONCATENATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.concatenate is used to combine arrays containing the predicted values generated by a model."</data>
      <data key="d6">00648b24263129fdae8652f1a3339041</data>
    </edge>
    <edge source="&quot;Y_PRED&quot;" target="&quot;NP.FLOAT64&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.float64 is used to represent the predicted values generated by a model as 64-bit floating-point numbers."</data>
      <data key="d6">00648b24263129fdae8652f1a3339041</data>
    </edge>
    <edge source="&quot;Y_PRED&quot;" target="&quot;SCIKITLEARNNODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ScikitLearnNode component generates the predicted output data."</data>
      <data key="d6">dc3bd3697a140b64d70e0e3ac6db6c7e</data>
    </edge>
    <edge source="&quot;Y_PRED&quot;" target="&quot;ACCURACY SCORE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Accuracy Score metric is calculated using the predicted output data."</data>
      <data key="d6">dc3bd3697a140b64d70e0e3ac6db6c7e</data>
    </edge>
    <edge source="&quot;Y_PRED&quot;" target="&quot;ACCURACY_SCORE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"accuracy_score is a metric used to compare the predicted labels in Y_pred to the actual labels in Y_test, providing a measure of the machine learning model's performance."</data>
      <data key="d6">0036fb6f489e13c0db0f1c02bf3323be</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;NP.ABS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.abs is used to compute the absolute difference between the 'y_test' and 'y_pred' arrays."</data>
      <data key="d6">9078b0f36522f21a9e8e1aadac48ed9c</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;NP.ARGMAX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The np.argmax function is used to find the indices of the maximum values along an axis in Y_test, which contains labels or targets for test data."</data>
      <data key="d6">b3361508c3e49b5bb3089f10e31d2c81</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;NP.CONCATENATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.concatenate is used to combine arrays containing the actual values used for testing the performance of a model."</data>
      <data key="d6">00648b24263129fdae8652f1a3339041</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;NP.FLOAT64&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.float64 is used to represent the actual values used for testing the performance of a model as 64-bit floating-point numbers."</data>
      <data key="d6">00648b24263129fdae8652f1a3339041</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;MODEL&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The machine learning model is tested using the y_test dataset, which contains the actual target values."
"Y_test is used for testing the Model's performance."</data>
      <data key="d6">75e530c1a04e30b373dc7cc68e3ad819,a0feae89e52a4291db0a512a3a102d8e</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;Y_PRED_ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"y_test is compared to y_pred_es2n to evaluate the accuracy of the ES^2N model."</data>
      <data key="d6">12d680622df43439e6de83058b734953</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;Y_PRED_ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"y_test is compared to y_pred_esn to evaluate the accuracy of the ESN model."</data>
      <data key="d6">12d680622df43439e6de83058b734953</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;PLT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"plt is used to visualize y_test, which represents the actual test data."</data>
      <data key="d6">12d680622df43439e6de83058b734953</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network (ESN) system is evaluated using the testing target data (y_test)."</data>
      <data key="d6">80c9f51870e239404ed671ef0374f191</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;ONE_HOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The testing target data (y_test) is transformed using one-hot encoding (one_hot) for evaluation."</data>
      <data key="d6">80c9f51870e239404ed671ef0374f191</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;OUTPUTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"y_test is the actual target data compared to the predicted outputs to evaluate the performance of the ESN model."</data>
      <data key="d6">1db5e6cd356c6066227de5e273de1abe</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;DATASET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Y_test is a variable used to store the testing target data in the dataset."</data>
      <data key="d6">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;ACCURACY SCORE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Accuracy Score metric is calculated using the testing output data."</data>
      <data key="d6">dc3bd3697a140b64d70e0e3ac6db6c7e</data>
    </edge>
    <edge source="&quot;Y_TEST&quot;" target="&quot;RESERVOIR MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir Model is tested using the Y_test dataset, which contains the actual target values."</data>
      <data key="d6">0970cd32ce54f6ee1180ab237fdcefe1</data>
    </edge>
    <edge source="&quot;ACCURACY_SCORE&quot;" target="&quot;MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"accuracy_score is used to evaluate the performance of a model by comparing its predictions to the actual values."</data>
      <data key="d6">00648b24263129fdae8652f1a3339041</data>
    </edge>
    <edge source="&quot;ACCURACY_SCORE&quot;" target="&quot;ESN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ESN's performance is evaluated using the accuracy_score function."
"accuracy_score is used to evaluate the performance of the ESN model, comparing predicted outputs to actual targets."</data>
      <data key="d6">1db5e6cd356c6066227de5e273de1abe,72e6eee633bcb5b1458c4cee3975cee1</data>
    </edge>
    <edge source="&quot;ACCURACY_SCORE&quot;" target="&quot;SKLEARN.METRICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"sklearn.metrics is a library that provides the accuracy_score metric for evaluating machine learning models."</data>
      <data key="d6">9414efd266e7135a2cdd7461a888b045</data>
    </edge>
    <edge source="&quot;DR. STEPHEN GROSSBERG&quot;" target="&quot;BOSTON UNIVERSITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dr. Stephen Grossberg is a researcher at Boston University."</data>
      <data key="d6">5e20e3cd48e20db98711d9948014c2d8</data>
    </edge>
    <edge source="&quot;DR. STEPHEN GROSSBERG&quot;" target="&quot;RECURRENT NEURAL NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dr. Stephen Grossberg specializes in the study of Recurrent Neural Networks, particularly biological recurrent neural networks."</data>
      <data key="d6">5e20e3cd48e20db98711d9948014c2d8</data>
    </edge>
    <edge source="&quot;BOSTON UNIVERSITY&quot;" target="&quot;MA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Boston University is located in the geographical location of MA, likely referring to Massachusetts."</data>
      <data key="d6">5e20e3cd48e20db98711d9948014c2d8</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;ESN METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN methods are mentioned in the same context as Recurrent Neural Networks."</data>
      <data key="d6">dbca0570761b1698d32f0c0bfb593b1a</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;DAVID RUMELHART&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"David Rumelhart contributed to the development of recurrent neural networks in 1986."</data>
      <data key="d6">f59839daadfb1f3832bb9f8d201a7126</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;NEURAL HISTORY COMPRESSOR SYSTEM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Neural History Compressor System used a recurrent neural network to solve a 'Very Deep Learning' task in 1993."</data>
      <data key="d6">f59839daadfb1f3832bb9f8d201a7126</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;LONG SHORT-TERM MEMORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Long Short-Term Memory is a type of Recurrent Neural Network that uses a mechanism to retain information over long sequences."</data>
      <data key="d6">2bdd28d9e151597072c8490db69b9941</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;BI-DIRECTIONAL RNNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Bi-directional RNNs are a type of Recurrent Neural Network that use a finite sequence to predict or label each element of the sequence based on the element&#8217;s past and future contexts."</data>
      <data key="d6">2bdd28d9e151597072c8490db69b9941</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;CONTINUOUS-TIME RECURRENT NEURAL NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A Continuous-time Recurrent Neural Network is a type of neural network that uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs."</data>
      <data key="d6">2bdd28d9e151597072c8490db69b9941</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;HIERARCHICAL RECURRENT NEURAL NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hierarchical Recurrent Neural Network is a type of Recurrent Neural Network that uses multiple layers to process data at different levels of abstraction."</data>
      <data key="d6">2bdd28d9e151597072c8490db69b9941</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;SHANNON SAMPLING THEOREM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Shannon sampling theorem is mentioned in the context of recurrent neural networks, suggesting a connection between the principles of signal processing and the structure of artificial neural networks."</data>
      <data key="d6">9e61c22432d6984a19da5840f64d417d</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;RECURRENT MULTILAYER PERCEPTRON NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A recurrent multilayer perceptron network is a type of recurrent neural network that consists of cascaded subnetworks, each containing multiple layers of nodes, with feedback connections in the last layer."</data>
      <data key="d6">9e61c22432d6984a19da5840f64d417d</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;GRADIENT DESCENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gradient Descent is used to minimize the error term in Recurrent Neural Networks."</data>
      <data key="d6">0bb54b1de8d2297293defe94addb8076</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;BACKPROPAGATION THROUGH TIME&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Backpropagation Through Time is a method used in training Recurrent Neural Networks to calculate gradients."</data>
      <data key="d6">0bb54b1de8d2297293defe94addb8076</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;RECURSIVE NEURAL NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recursive Neural Networks and Recurrent Neural Networks are both types of neural networks, and their similarities are mentioned in the text."</data>
      <data key="d6">2f1161d1f711d264529aa7bddf81959b</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;FINITE IMPULSE RESPONSE FILTERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recurrent Neural Networks are mentioned as a nonlinear version of Finite Impulse Response Filters."</data>
      <data key="d6">2f1161d1f711d264529aa7bddf81959b</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;INFINITE IMPULSE RESPONSE FILTERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recurrent Neural Networks are mentioned as a nonlinear version of Infinite Impulse Response Filters."</data>
      <data key="d6">2f1161d1f711d264529aa7bddf81959b</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;NONLINEAR AUTOREGRESSIVE EXOGENOUS MODEL (NARX)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recurrent Neural Networks are mentioned as a nonlinear version of Nonlinear Autoregressive Exogenous Model (NARX)."</data>
      <data key="d6">2f1161d1f711d264529aa7bddf81959b</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;ECHO STATE NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks are a type of Recurrent Neural Network that operates a random, large, fixed, recurring network with the input signal."</data>
      <data key="d6">158f53cd85edbb4f2e4c77b78c5e7acc</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS&quot;" target="&quot;SCHILLER AND STEIL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Schiller and Steil demonstrated the use of the Backpropagation Decorrelation learning rule for RNNs, which is a learning algorithm used for training Recurrent Neural Networks."</data>
      <data key="d6">158f53cd85edbb4f2e4c77b78c5e7acc</data>
    </edge>
    <edge source="&quot;MCCULLOCH-PITTS MODEL&quot;" target="&quot;BINARY SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The McCulloch-Pitts Model is a classical model that describes the behavior of Binary Systems, which are a type of recurrent neural network."</data>
      <data key="d6">5838adba6968ede203f6820ddc368bc4</data>
    </edge>
    <edge source="&quot;MCCULLOCH-PITTS MODEL&quot;" target="&quot;SHORT-TERM MEMORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Short-Term Memory is a concept that is described by the McCulloch-Pitts Model."</data>
      <data key="d6">5838adba6968ede203f6820ddc368bc4</data>
    </edge>
    <edge source="&quot;MCCULLOCH-PITTS MODEL&quot;" target="&quot;JOHN VON NEUMANN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The McCulloch-Pitts Model had a significant influence on John von Neumann's development of the digital computer."</data>
      <data key="d6">25eb64dbb12dee61a753085741ee91d4</data>
    </edge>
    <edge source="&quot;MCCULLOCH-PITTS MODEL&quot;" target="&quot;WARREN MCCULLOCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Warren McCulloch, along with Walter Pitts, developed the McCulloch-Pitts Model."</data>
      <data key="d6">25eb64dbb12dee61a753085741ee91d4</data>
    </edge>
    <edge source="&quot;MCCULLOCH-PITTS MODEL&quot;" target="&quot;WALTER PITTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Walter Pitts, along with Warren McCulloch, developed the McCulloch-Pitts Model."</data>
      <data key="d6">25eb64dbb12dee61a753085741ee91d4</data>
    </edge>
    <edge source="&quot;CONTINUOUS-NONLINEAR SYSTEMS&quot;" target="&quot;GROSSBERG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg has made contributions to the field of Continuous-Nonlinear Systems, which are a type of recurrent neural network."</data>
      <data key="d6">5838adba6968ede203f6820ddc368bc4</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;ADDITIVE MODEL&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"Grossberg discovered the Additive Model, which is an equation used to determine the rate of change of neuronal activities."
"Grossberg is a prominent figure in neural network research who has made significant contributions to the development of the Additive Model."
"Grossberg is mentioned as an author of the development of the Additive Model."</data>
      <data key="d6">2e76149ff772441e6627913bc1df5000,3235445917507f02710bd66cc8368194,df77d35da87a38cae0984a42b9a1d41c</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;NEURAL NETWORKS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Grossberg is a researcher contributing to the development of Neural Networks, contributing to the development of the Additive Model and the STM Equation."
"Grossberg is mentioned in the context of reviews related to Neural Networks, indicating his expertise and contributions in the field."</data>
      <data key="d6">47d1d12642cdab6e9a5d21c184f83c9c,4c7e78f7237cb3420e70c7749bb259f9</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;STM&quot;">
      <data key="d4">4.0</data>
      <data key="d5">"Grossberg introduced the generalized STM equation, which includes terms such as LTM and MTM that are part of the STM system."
"Grossberg contributed to the development of Short-Term Memory, a component that stores input patterns persistently."
"Grossberg's theorems showed how the choice of feedback signal function transforms an input pattern before it is stored persistently in STM."
"Grossberg's work is mentioned in relation to STM, but the nature of their relationship is not explicitly stated."</data>
      <data key="d6">3a64c8c26895f111f00a349dd69bb505,653b7986c4757bd5d0a251369187efa6,7ba0dfde8cc54bb1dcf66b46fcdd88f8,fba20e559e6ecb61ebbf3a805e6d072c</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;GATED DIPOLE OPPONENT PROCESSING NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg has contributed to the understanding of the Gated Dipole Opponent Processing Network and its functions."</data>
      <data key="d6">be0954a6263de67c84da3141d95de445</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;GAUDIANO AND GROSSBERG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gaudiano and Grossberg are mentioned as collaborators with Grossberg in research papers."</data>
      <data key="d6">24771832864aa38abd6aebec04b13a10</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;GROSSBERG AND SEITZ&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg and Seitz are mentioned as collaborators with Grossberg in a research paper."</data>
      <data key="d6">24771832864aa38abd6aebec04b13a10</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;LTM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is mentioned as a researcher who has contributed to the understanding of LTM (Long-Term Memory) and its learning mechanisms."</data>
      <data key="d6">24771832864aa38abd6aebec04b13a10</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;OUTSTAR LEARNING&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Grossberg introduced Outstar Learning as a variant of Gated Steepest Descent Learning."
"Outstar Learning was introduced by Grossberg for spatial pattern learning."</data>
      <data key="d6">97ad8d6e6e3bf27ae6a7b457af9b312e,b286a9022774f24a400744b2a1b08bab</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;INSTAR LEARNING&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Grossberg also used Instar Learning as a variant of Gated Steepest Descent Learning."
"Instar Learning was used by Grossberg for learning bottom-up adaptive filters in Self-Organizing Map (SOM) models."</data>
      <data key="d6">97ad8d6e6e3bf27ae6a7b457af9b312e,b286a9022774f24a400744b2a1b08bab</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;ART&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg introduced ART, which uses Instars and Outstars for learning."</data>
      <data key="d6">43cf5e32e2df964318d03574e6cd6cdc</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;HECHT-NIELSEN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg's work on Instars and Outstars was referred to by Hecht-Nielsen as a counterpropagation network."</data>
      <data key="d6">43cf5e32e2df964318d03574e6cd6cdc</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;SHUNTING NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg has generalized the feedforward on-center off-surround shunting network equations, generating many useful properties."</data>
      <data key="d6">f290960776c5ec561653e90d2ac6751b</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;SHUNTING NETWORK EQUATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is a researcher who generalized the feedforward on-center off-surround shunting network equations."</data>
      <data key="d6">f2468cda326d1ca11c98f2fbde186400</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;(V^+)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is mentioned in relation to (V^+) in the context of symmetry-breaking and noise suppression."</data>
      <data key="d6">91f030f6c14c673e6d029c9bf1a66515</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;(V^-)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is mentioned in relation to (V^-) in the context of symmetry-breaking and noise suppression."</data>
      <data key="d6">91f030f6c14c673e6d029c9bf1a66515</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;RCF&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is mentioned in the context of developing a Recurrent Competitive Field, a type of recurrent neural network."</data>
      <data key="d6">3a64c8c26895f111f00a349dd69bb505</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;BRNN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg's work is mentioned in the context of a bidirectional Recurrent Neural Network, suggesting his contributions to the field."</data>
      <data key="d6">3a64c8c26895f111f00a349dd69bb505</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;BUBBLE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg's theorems proved the use of sigmoid signal functions to generate a self-normalizing bubble, or partial contrast-enhancement, above a quenching threshold."</data>
      <data key="d6">7ba0dfde8cc54bb1dcf66b46fcdd88f8</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;RECURRENT NONLINEAR DYNAMICAL SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg's theorems began the mathematical classification of recurrent nonlinear dynamical systems, which are applicable to various fields."</data>
      <data key="d6">7ba0dfde8cc54bb1dcf66b46fcdd88f8</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;COMPETITIVE SYSTEM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg developed a mathematical method to classify the dynamics of competitive systems."</data>
      <data key="d6">3334f6dcd53b71cf3ceb7648ead24d5a</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;DECISION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg's method was applied to study a general problem of competition, decision, and consensus."</data>
      <data key="d6">3334f6dcd53b71cf3ceb7648ead24d5a</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;VOTING PARADOX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg introduced the Voting Paradox in 1975 and studied it using a method of bRNNs."</data>
      <data key="d6">0af3b52f2586c4e957aee493160223ba</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;LIAPUNOV FUNCTIONAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg introduced the Liapunov Functional as a mathematical tool to analyze the behavior of systems."</data>
      <data key="d6">0af3b52f2586c4e957aee493160223ba</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;SOCIAL CHAOS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg's work focuses on the problem of Social Chaos, considering how complicated a system can be and still generate order."</data>
      <data key="d6">0af3b52f2586c4e957aee493160223ba</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;ALLIGOOD ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Alligood et al. is mentioned in the context of Grossberg's research on the trade-off between global consensus and local signals."</data>
      <data key="d6">597668e07c7554bd2d0cb29399285a39</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;SYSTEM (21)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg introduced a class of bRNNs, known as System (21), which generates globally-consistent decision-making."</data>
      <data key="d6">597668e07c7554bd2d0cb29399285a39</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;COHEN-GROSSBERG MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is a contributor to the Cohen-Grossberg Model, which focuses on symmetry in interaction coefficients."</data>
      <data key="d6">0dd75f3ca11854714bdbfc8a96ccf256</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;LAMINART FAMILY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is a researcher associated with the LAMINART Family model."</data>
      <data key="d6">6648b18760b8b182e1097ad15c4df685</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;LIST PARSE MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is a researcher associated with the LIST PARSE Model."</data>
      <data key="d6">6648b18760b8b182e1097ad15c4df685</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;CARTWORD MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is a researcher associated with the cARTWORD Model."</data>
      <data key="d6">6648b18760b8b182e1097ad15c4df685</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;TELOS MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is a co-author of the study that introduces the TELOS Model."</data>
      <data key="d6">75495c1fc835d41adf5afcb01e8e520a</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;LISTELOS MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is not explicitly mentioned in the context of the lisTELOS Model."</data>
      <data key="d6">75495c1fc835d41adf5afcb01e8e520a</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;SPATIAL PATTERN LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is mentioned in the context of Spatial Pattern Learning, indicating his involvement in research on this topic."</data>
      <data key="d6">5d6b6e0d1a9ace28e21dce2cb0ac78c0</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;SIGNAL TRANSMISSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is mentioned in the context of Signal Transmission, indicating his research on this topic."</data>
      <data key="d6">5d6b6e0d1a9ace28e21dce2cb0ac78c0</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;UNBIASED SPATIAL PATTERN LEARNING THEOREM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is a researcher who has contributed to the development of the Unbiased Spatial Pattern Learning Theorem."</data>
      <data key="d6">4959d1559344e462a6a7463fd3273659</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;COMPETITIVE LEARNING OR SELF-ORGANIZING MAP NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg has developed the Competitive Learning or Self-Organizing Map Network."</data>
      <data key="d6">644602009dec8474bb5cd4702b391d3e</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;ADAPTIVE RESONANCE THEORY&quot;">
      <data key="d4">4.0</data>
      <data key="d5">"Adaptive Resonance Theory was introduced by Grossberg."
"Grossberg introduced Adaptive Resonance Theory to propose how top-down learned expectations and attentional focusing could dynamically stabilize learning."
"Grossberg coined the term 'stability-plasticity dilemma' in relation to Adaptive Resonance Theory, indicating their association."
"Adaptive Resonance Theory is developed by Grossberg, who is also mentioned for his contributions to other areas of cognitive science."</data>
      <data key="d6">01e2a32da700813f593038a23a618e55,3d5b88f7f81ed9e14f07335bbef17020,554e8565591507441cecaa652cb926db,644602009dec8474bb5cd4702b391d3e</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;PASSIVE DECAY ASSOCIATIVE LAW&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg introduced the Passive Decay Associative Law as a learning rule for neural networks."</data>
      <data key="d6">01e2a32da700813f593038a23a618e55</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;ITEM-AND-ORDER MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg predicted that Item-and-Order models embody two constraints: the LTM Invariance Principle and the Normalization Rule."</data>
      <data key="d6">4364aa6091e1966365fa889b34f5cf90</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;LTM INVARIANCE PRINCIPLE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg predicted the LTM Invariance Principle to ensure stable learning and memory of list chunks."</data>
      <data key="d6">dcd38cdc6195b2bbf41d936af0bf1f5f</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;NORMALIZATION RULE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg predicted the Normalization Rule to support stable learning and memory of list chunks."</data>
      <data key="d6">dcd38cdc6195b2bbf41d936af0bf1f5f</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;WORKING MEMORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg has contributed to the analysis of working memories, including the proof that simple rules generate working memories that can support stable learning and long-term memory of list chunks."</data>
      <data key="d6">1976b19f768a8fdf37207b680c3b2b40</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;BRADSKI ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Bradski et al. cited Grossberg's work in their mathematical proof of Item-and-Order working memory patterns."</data>
      <data key="d6">c3257facbf1b0a5da49d6a115f66df87</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;AGAM ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg's predictions about working memory networks and list chunking networks are supported by the data reported by Agam et al."</data>
      <data key="d6">97a9ce754fa34aaf01d6cce57560b247</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;MILLER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Miller's work on immediate memory span is referenced in the context of Grossberg's predictions about working memory networks."</data>
      <data key="d6">97a9ce754fa34aaf01d6cce57560b247</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;MURDOCK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Murdock's work on recall patterns is referenced in the context of Grossberg's predictions about working memory networks."</data>
      <data key="d6">97a9ce754fa34aaf01d6cce57560b247</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;SERIAL VERBAL LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg's contributions to the understanding of Serial Verbal Learning include the distinction between associative and competitive mechanisms."</data>
      <data key="d6">b69b23b14e0feccb488ba5412db0824c</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;IMMEDIATE MEMORY SPAN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg has distinguished between the Immediate Memory Span and the Transient Memory Span."</data>
      <data key="d6">c580fa74e3c36285cfae7df56340a990</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;IMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg proved that the TMS is smaller than the IMS, which is estimated to have a capacity limit of four plus or minus one."</data>
      <data key="d6">392028b79561bd7471cb68e7c9258b1e</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;1978A&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"1978a is the year when Grossberg proved that the TMS is smaller than the IMS."</data>
      <data key="d6">392028b79561bd7471cb68e7c9258b1e</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;SHUNTING MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is mentioned as an author of the development of the Shunting Model."</data>
      <data key="d6">2e76149ff772441e6627913bc1df5000</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;COMMAND CELLS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg proposed the inclusion of command cells in circuits to allow for sensitivity to environmental feedback."</data>
      <data key="d6">7aeda101aa8aba76f319932f0bd568f7</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;CLAUS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Claus is mentioned in the context of Grossberg's work on global organization of brain mechanisms."</data>
      <data key="d6">eb6c9a7d24cc59ff93d554093a4360a4</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;SCHULTZ ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Schultz et al. is mentioned in the context of Grossberg's work on global organization of brain mechanisms."</data>
      <data key="d6">eb6c9a7d24cc59ff93d554093a4360a4</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;VON DER MALSBURG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg is mentioned in the context of his interaction with von der Malsburg on the introduction of Competitive Learning and Self-Organizing Maps."</data>
      <data key="d6">eb6c9a7d24cc59ff93d554093a4360a4</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;SELF-ORGANIZING AVALANCHE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg has made significant contributions to the concept of Self-Organizing Avalanche, including mathematical analyses and explanations of classical data properties."</data>
      <data key="d6">2ea6b3379a87077d75e5c45024f4f3e2</data>
    </edge>
    <edge source="&quot;GROSSBERG&quot;" target="&quot;MATHEMATICAL ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg has provided mathematical analyses to explain classical data properties such as the bowed serial position curve."</data>
      <data key="d6">2ea6b3379a87077d75e5c45024f4f3e2</data>
    </edge>
    <edge source="&quot;SHORT-TERM MEMORY&quot;" target="&quot;WORKING MEMORY&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Working Memory includes Short-Term Memory as a component that holds information for brief periods."
"Working Memory temporarily stores information for immediate use, similar to Short-Term Memory (STM)."</data>
      <data key="d6">3d5b88f7f81ed9e14f07335bbef17020,5c4e24fc9bd10d0bd59a84d56f960cf9</data>
    </edge>
    <edge source="&quot;FRANK ROSENBLATT&quot;" target="&quot;CLASSICAL PERCEPTRON MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Frank Rosenblatt developed the continuous-time STM equation used in the classical Perceptron model."</data>
      <data key="d6">25eb64dbb12dee61a753085741ee91d4</data>
    </edge>
    <edge source="&quot;FRANK CAIANIELLO&quot;" target="&quot;BINARY STM EQUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Frank Caianiello developed a binary STM equation influenced by activities at multiple times in the past."</data>
      <data key="d6">25eb64dbb12dee61a753085741ee91d4</data>
    </edge>
    <edge source="&quot;CAIANIELLO&quot;" target="&quot;STM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Caianiello's work introduced equations to change the weights in a learning model, which is mentioned in relation to STM."</data>
      <data key="d6">9e901f71d0d2339294da133518f2162f</data>
    </edge>
    <edge source="&quot;ROSENBLATT&quot;" target="&quot;LTM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Rosenblatt's work introduced equations to change the weights in a learning model, which are referred to as LTM traces."</data>
      <data key="d6">9e901f71d0d2339294da133518f2162f</data>
    </edge>
    <edge source="&quot;ROSENBLATT&quot;" target="&quot;LTM EQUATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Rosenblatt developed the LTM equations used for pattern classification."</data>
      <data key="d6">7c556574ea1f1f26ee3ad2a63d56b8e7</data>
    </edge>
    <edge source="&quot;WIDROW&quot;" target="&quot;ADELINE&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Widrow introduced the gradient descent Adeline adaptive pattern recognition machine."
"Widrow developed the gradient descent Adeline adaptive pattern recognition machine."</data>
      <data key="d6">7c556574ea1f1f26ee3ad2a63d56b8e7,9e901f71d0d2339294da133518f2162f</data>
    </edge>
    <edge source="&quot;ANDERSON&quot;" target="&quot;NEURAL PATTERN RECOGNITION&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Anderson described neural pattern recognition using a spatial cross-correlation function."
"Anderson initially described neural pattern recognition using a spatial cross-correlation function."</data>
      <data key="d6">7c556574ea1f1f26ee3ad2a63d56b8e7,9e901f71d0d2339294da133518f2162f</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;LTM&quot;">
      <data key="d4">7.0</data>
      <data key="d5">"STM and LTM are mentioned in the text in relation to uncoupled interactions in learning models."
"STM includes LTM as a type of memory system, which changes at a slower rate than STM."
"STM (Short-Term Memory) and LTM (Long-Term Memory) are mentioned in the context of their interaction and learning mechanisms."
"LTM traces learn to match the pattern of activities, or STM traces, of cells across the network."
"STM and LTM are different types of memory in the brain, with STM holding information temporarily for immediate use and LTM holding information for long-term storage and retrieval."
"STM and LTM are mentioned together in the text, indicating their relationship as different types of memory interactions."
"STM and LTM interact with each other during neuronal learning, allowing for the learning of spatial patterns."</data>
      <data key="d6">24771832864aa38abd6aebec04b13a10,53f5bc3f4c71310c593a23aef01d1633,653b7986c4757bd5d0a251369187efa6,97ad8d6e6e3bf27ae6a7b457af9b312e,9b30fc06ca06f49c2faa238da7eddc6f,9e901f71d0d2339294da133518f2162f,b881b9051ad24c6a16b468803fba51d3</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;MTM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"STM includes MTM as a type of memory system, which changes at a rate intermediate between STM and LTM."</data>
      <data key="d6">653b7986c4757bd5d0a251369187efa6</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;WILSON-COWAN MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Wilson-Cowan Model uses a sigmoid of sums that is multiplied by a shunting term, which is a characteristic of the STM equation."</data>
      <data key="d6">653b7986c4757bd5d0a251369187efa6</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;LEABRA MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"STM is a component used in the Leabra model, which temporarily stores and processes information."</data>
      <data key="d6">b40ff9b93414391d5e4b3c06dfe02bc9</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;THE BRAIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"STM is mentioned as a component of the brain that enables it to process and learn from temporal patterns of information."</data>
      <data key="d6">0d925896ce2bf9b73be90d8fa5ddb402</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;BRNN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"bRNN is mentioned in relation to STM, suggesting a connection or interaction between the two concepts."
"Short-Term Memory is a component mentioned in the context of a bidirectional Recurrent Neural Network."</data>
      <data key="d6">3a64c8c26895f111f00a349dd69bb505,91f030f6c14c673e6d029c9bf1a66515</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;BUBBLE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The bubble refers to a self-normalizing process that occurs during the storage of input patterns in STM."</data>
      <data key="d6">7ba0dfde8cc54bb1dcf66b46fcdd88f8</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;INPUTS I_I AND J_I&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"STM is described as a system that preserves patterns in input processes I_i and J_i under certain conditions."</data>
      <data key="d6">b4f6256f3430f1aa72ca8092809ebba1</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;PATTERN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"STM is capable of storing patterns, which are sequences or structures of input data."</data>
      <data key="d6">8da881a4f375e8a524fd0bf46ae2279e</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;SIGNAL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"STM requires a Signal Function that can suppress noise and store more than one feature or category, which is a challenge addressed in the text."</data>
      <data key="d6">35551dc55b5522082b778171ff6d1bf9</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;SIGMOID SIGNAL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"STM is used to store the partially contrast-enhanced patterns generated by the Sigmoid Signal Function."</data>
      <data key="d6">6a47ed5881928d48cdcb74e40867a711</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;NEURONAL LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"STM is a component involved in the process of neuronal learning, interacting with LTM to enable spatial pattern learning."</data>
      <data key="d6">b881b9051ad24c6a16b468803fba51d3</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;GENERALIZED ADDITIVE RNNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Generalized Additive RNNs includes interactions between STM and LTM, which are mentioned in the context of learning spatial patterns."</data>
      <data key="d6">5812b5d4bcdfbf80de28dca56a6559b3</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;WM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"WM is composed of Short-Term Memory, which stores information temporarily for immediate use."</data>
      <data key="d6">c580fa74e3c36285cfae7df56340a990</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;DIXON AND HORTON&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dixon and Horton are mentioned in the context of studying serial learning and the role of STM in this process."</data>
      <data key="d6">2e76149ff772441e6627913bc1df5000</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;HOVLAND&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hovland is mentioned as an author of a study on serial learning, which involves the role of STM."</data>
      <data key="d6">2e76149ff772441e6627913bc1df5000</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;HULL ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hull et al. are mentioned in the context of studying associative learning of temporal order information, which involves the role of STM."</data>
      <data key="d6">2e76149ff772441e6627913bc1df5000</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;JUNG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jung is mentioned as an author of a study on serial learning, which involves the role of STM."</data>
      <data key="d6">2e76149ff772441e6627913bc1df5000</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;MCGEOGH AND IRION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"McGeogh and Irion are mentioned as authors of a study on serial learning, which involves the role of STM."</data>
      <data key="d6">2e76149ff772441e6627913bc1df5000</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;OSGOOD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Osgood is mentioned as an author of a study on serial learning, which involves the role of STM."</data>
      <data key="d6">2e76149ff772441e6627913bc1df5000</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;UNDERWOOD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Underwood is mentioned as an author of a study on serial learning, which involves the role of STM."</data>
      <data key="d6">2e76149ff772441e6627913bc1df5000</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;ADDITIVE MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Additive Model was derived to explain associative learning of temporal order information, which involves the role of STM."</data>
      <data key="d6">2e76149ff772441e6627913bc1df5000</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;SHUNTING MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Shunting Model was derived to explain associative learning of temporal order information, which involves the role of STM."</data>
      <data key="d6">2e76149ff772441e6627913bc1df5000</data>
    </edge>
    <edge source="&quot;STM&quot;" target="&quot;AVALANCHE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Avalanche is described as a performance insensitive to stimulus sampling and encoding LTM in spatial pattern units, which involves the role of STM."</data>
      <data key="d6">2e76149ff772441e6627913bc1df5000</data>
    </edge>
    <edge source="&quot;LTM&quot;" target="&quot;LEABRA MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LTM is a component used in the Leabra model, which stores and retrieves information over an extended period of time."</data>
      <data key="d6">b40ff9b93414391d5e4b3c06dfe02bc9</data>
    </edge>
    <edge source="&quot;LTM&quot;" target="&quot;THE BRAIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LTM is mentioned as a component of the brain that enables it to learn and retain information over extended periods of time."</data>
      <data key="d6">0d925896ce2bf9b73be90d8fa5ddb402</data>
    </edge>
    <edge source="&quot;LTM&quot;" target="&quot;NEURONAL LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LTM is a component involved in the process of neuronal learning, interacting with STM to enable spatial pattern learning."</data>
      <data key="d6">b881b9051ad24c6a16b468803fba51d3</data>
    </edge>
    <edge source="&quot;LTM&quot;" target="&quot;GENERALIZED ADDITIVE RNNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Generalized Additive RNNs includes interactions between STM and LTM, which are mentioned in the context of learning spatial patterns."</data>
      <data key="d6">5812b5d4bcdfbf80de28dca56a6559b3</data>
    </edge>
    <edge source="&quot;LTM&quot;" target="&quot;WM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"WM also interacts with Long-Term Memory, which stores information for long-term retention and retrieval."</data>
      <data key="d6">c580fa74e3c36285cfae7df56340a990</data>
    </edge>
    <edge source="&quot;LTM&quot;" target="&quot;IMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LTM is mentioned as a factor that biases working memory toward more primacy dominance, which is relevant to the capacity limit of the IMS."</data>
      <data key="d6">392028b79561bd7471cb68e7c9258b1e</data>
    </edge>
    <edge source="&quot;PERCEPTRON&quot;" target="&quot;CLASSIFICATION TASKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Perceptron is a machine learning algorithm used for binary classification tasks, offering a simple yet effective solution for certain classification problems."</data>
      <data key="d6">dadca3c89b34dc48a60c53367ab55768</data>
    </edge>
    <edge source="&quot;PERCEPTRON&quot;" target="&quot;SCIKITLEARNNODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ScikitLearnNode is used to demonstrate the Perceptron model for classification tasks."</data>
      <data key="d6">35631fbf2ad11c53d75cb9b42e2c39b4</data>
    </edge>
    <edge source="&quot;PERCEPTRON&quot;" target="&quot;SCIKIT-LEARN&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"scikit-learn implements the Perceptron classifier, making it available for use in machine learning pipelines."
"Scikit-learn provides the Perceptron model for classification tasks."
"Perceptron is a machine learning algorithm provided by the Scikit-learn library for classification tasks."</data>
      <data key="d6">0036fb6f489e13c0db0f1c02bf3323be,d58662ee42c14a0787d839ebfd0a6e9b,f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </edge>
    <edge source="&quot;KOHONEN&quot;" target="&quot;NEURAL NETWORK RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Kohonen made a transition from linear algebra concepts to more biologically motivated studies in neural network research."</data>
      <data key="d6">7c556574ea1f1f26ee3ad2a63d56b8e7</data>
    </edge>
    <edge source="&quot;KOHONEN&quot;" target="&quot;INSTAR LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Instar Learning was also used by Kohonen in his applications of the Self-Organizing Map (SOM) model."</data>
      <data key="d6">b286a9022774f24a400744b2a1b08bab</data>
    </edge>
    <edge source="&quot;KOHONEN&quot;" target="&quot;SOM MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Kohonen used Instar Learning in his applications of the SOM model."</data>
      <data key="d6">43cf5e32e2df964318d03574e6cd6cdc</data>
    </edge>
    <edge source="&quot;HARTLINE&quot;" target="&quot;STEADY STATE HARTLINE-RATLIFF MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hartline's neurophysiological experiments led to the development of the steady state Hartline-Ratliff model."</data>
      <data key="d6">7c556574ea1f1f26ee3ad2a63d56b8e7</data>
    </edge>
    <edge source="&quot;HARTLINE-RATLIFF MODEL&quot;" target="&quot;H.K. HARTLINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"H.K. Hartline is a key figure in the development of the Hartline-Ratliff Model, which he co-authored with J.A. Ratliff."</data>
      <data key="d6">48763056731d01884b6cf37bf0e0d0db</data>
    </edge>
    <edge source="&quot;HARTLINE-RATLIFF MODEL&quot;" target="&quot;J.A. RATLIFF&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"J.A. Ratliff extended the steady-state Hartline-Ratliff model to a dynamical model, contributing to its development."</data>
      <data key="d6">48763056731d01884b6cf37bf0e0d0db</data>
    </edge>
    <edge source="&quot;HARTLINE-RATLIFF MODEL&quot;" target="&quot;LIMULUS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Neurophysiological experiments on the lateral eye of the Limulus inspired the development of the Hartline-Ratliff Model."</data>
      <data key="d6">48763056731d01884b6cf37bf0e0d0db</data>
    </edge>
    <edge source="&quot;J.A. RATLIFF&quot;" target="&quot;ADDITIVE MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Additive Model is described as a precursor of the dynamical model developed by J.A. Ratliff."</data>
      <data key="d6">48763056731d01884b6cf37bf0e0d0db</data>
    </edge>
    <edge source="&quot;ADDITIVE MODEL&quot;" target="&quot;EARLY APPLICATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Additive Model was used in Early Applications for computational analyses."</data>
      <data key="d6">9ed5e24bce2907e0ffa4acbe066dbfff</data>
    </edge>
    <edge source="&quot;ADDITIVE MODEL&quot;" target="&quot;COMPUTATIONAL ANALYSES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Additive Model is used in Computational Analyses for analysis."</data>
      <data key="d6">9ed5e24bce2907e0ffa4acbe066dbfff</data>
    </edge>
    <edge source="&quot;ADDITIVE MODEL&quot;" target="&quot;NEURAL NETWORKS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The Additive Model is a mathematical framework used in the field of Neural Networks for various computational analyses."
"The Additive Model is a concept used in the study and development of Neural Networks."</data>
      <data key="d6">47d1d12642cdab6e9a5d21c184f83c9c,df77d35da87a38cae0984a42b9a1d41c</data>
    </edge>
    <edge source="&quot;ADDITIVE MODEL&quot;" target="&quot;HOPFIELD&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"Hopfield is a physicist who independently developed a model similar to the Additive Model, which later became known as the Hopfield model."
"Hopfield stated a Liapunov function for the Additive Model, which is mentioned in the context of Cohen and Grossberg's generalization."
"The Additive Model has been erroneously called the Hopfield network, despite its earlier publications and the Liapunov function's earlier publication."</data>
      <data key="d6">98173c1c0fcd64ceb914e0dd6b366b30,d4afac3b7aed3d6e11ff5eaf34589c2d,df77d35da87a38cae0984a42b9a1d41c</data>
    </edge>
    <edge source="&quot;ADDITIVE MODEL&quot;" target="&quot;VISION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Additive Model has been applied in neural networks for computational analyses of vision."</data>
      <data key="d6">df77d35da87a38cae0984a42b9a1d41c</data>
    </edge>
    <edge source="&quot;ADDITIVE MODEL&quot;" target="&quot;LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Additive Model has been used for various learning tasks in neural networks, such as recognition and reinforcement learning."</data>
      <data key="d6">df77d35da87a38cae0984a42b9a1d41c</data>
    </edge>
    <edge source="&quot;ADDITIVE MODEL&quot;" target="&quot;SPEECH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Additive Model has been applied in neural networks for learning the temporal order of speech signals."</data>
      <data key="d6">df77d35da87a38cae0984a42b9a1d41c</data>
    </edge>
    <edge source="&quot;ADDITIVE MODEL&quot;" target="&quot;LANGUAGE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Additive Model has been used in neural networks for understanding and generating human language."</data>
      <data key="d6">df77d35da87a38cae0984a42b9a1d41c</data>
    </edge>
    <edge source="&quot;ADDITIVE MODEL&quot;" target="&quot;SENSORY-MOTOR CONTROL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Additive Model has been applied in neural networks for learning and coordinating sensory and motor activities."</data>
      <data key="d6">df77d35da87a38cae0984a42b9a1d41c</data>
    </edge>
    <edge source="&quot;ADDITIVE MODEL&quot;" target="&quot;DECISION-MAKING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Additive Model has been used in neural networks for modeling and understanding decision-making processes."</data>
      <data key="d6">df77d35da87a38cae0984a42b9a1d41c</data>
    </edge>
    <edge source="&quot;ADDITIVE MODEL&quot;" target="&quot;LIAPUNOV FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hopfield stated a Liapunov function for the Additive Model."</data>
      <data key="d6">be0954a6263de67c84da3141d95de445</data>
    </edge>
    <edge source="&quot;ADDITIVE MODEL&quot;" target="&quot;RCF&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Additive Model does not exhibit the self-normalization properties that arise from RCF shunting dynamics."</data>
      <data key="d6">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </edge>
    <edge source="&quot;ADDITIVE MODEL&quot;" target="&quot;COHEN-GROSSBERG MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Cohen-Grossberg Model includes the Additive Model as a type of network."</data>
      <data key="d6">0dd75f3ca11854714bdbfc8a96ccf256</data>
    </edge>
    <edge source="&quot;HUGH EVERETT&quot;" target="&quot;NEURAL NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hugh Everett extended a steady-state model to a dynamical model, which is a precursor to the Neural Networks studied by John Hopfield."</data>
      <data key="d6">caf44d8df044312829ae3fe56df0c440</data>
    </edge>
    <edge source="&quot;ANDREW HODGKIN&quot;" target="&quot;NEURAL NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Andrew Hodgkin and Alan Huxley's study of the squid giant axon in 1952 provides a foundation for the Neural Networks studied by John Hopfield."</data>
      <data key="d6">caf44d8df044312829ae3fe56df0c440</data>
    </edge>
    <edge source="&quot;ANDREW HODGKIN&quot;" target="&quot;SQUID GIANT AXON&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Andrew Hodgkin and Alan Huxley studied the squid giant axon in 1952."</data>
      <data key="d6">caf44d8df044312829ae3fe56df0c440</data>
    </edge>
    <edge source="&quot;ALAN HUXLEY&quot;" target="&quot;SQUID GIANT AXON&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Alan Huxley and Andrew Hodgkin studied the squid giant axon in 1952."</data>
      <data key="d6">caf44d8df044312829ae3fe56df0c440</data>
    </edge>
    <edge source="&quot;JOHN HOPFIELD&quot;" target="&quot;NEURAL NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"John Hopfield derived neural networks that form the foundation of most current biological neural network research in 1982."</data>
      <data key="d6">caf44d8df044312829ae3fe56df0c440</data>
    </edge>
    <edge source="&quot;JOHN HOPFIELD&quot;" target="&quot;RECURRENT NEURAL NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"John Hopfield's work on Hopfield networks, which are a type of recurrent neural network, is indirectly referenced in the text through the term 'infinite impulse response'."</data>
      <data key="d6">f5b970cf7201f4a918d8bd6a1267657c</data>
    </edge>
    <edge source="&quot;NEURAL NETWORKS&quot;" target="&quot;USHER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Usher is a researcher contributing to the development and study of Neural Networks."</data>
      <data key="d6">47d1d12642cdab6e9a5d21c184f83c9c</data>
    </edge>
    <edge source="&quot;NEURAL NETWORKS&quot;" target="&quot;MCCLELLAND&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"McClelland is a researcher contributing to the development and study of Neural Networks."</data>
      <data key="d6">47d1d12642cdab6e9a5d21c184f83c9c</data>
    </edge>
    <edge source="&quot;NEURAL NETWORKS&quot;" target="&quot;HOPFIELD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hopfield is a researcher contributing to the development of Neural Networks, specifically the Hopfield model."</data>
      <data key="d6">47d1d12642cdab6e9a5d21c184f83c9c</data>
    </edge>
    <edge source="&quot;NEURAL NETWORKS&quot;" target="&quot;STM EQUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The STM Equation is a concept used in the modeling of individual neurons within Neural Networks."</data>
      <data key="d6">47d1d12642cdab6e9a5d21c184f83c9c</data>
    </edge>
    <edge source="&quot;NEURAL NETWORKS&quot;" target="&quot;HYPERBOLIC TANGENT ACTIVATION FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Neural Networks utilize the Hyperbolic Tangent Activation Function in their nodes to process information."</data>
      <data key="d6">cb7823dcc9852e6a6f9e3607cb55134f</data>
    </edge>
    <edge source="&quot;NEURAL NETWORKS&quot;" target="&quot;GAUSSIAN DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Gaussian Distribution is used to model the distribution of reservoir activations in Neural Networks."</data>
      <data key="d6">cb7823dcc9852e6a6f9e3607cb55134f</data>
    </edge>
    <edge source="&quot;NEURAL NETWORKS&quot;" target="&quot;PAPER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Paper discusses the effectiveness of reservoir activation distributions in Neural Networks."</data>
      <data key="d6">cb7823dcc9852e6a6f9e3607cb55134f</data>
    </edge>
    <edge source="&quot;NEURAL NETWORKS&quot;" target="&quot;DAVID RUMELHART&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"David Rumelhart contributed to the development of neural networks in 1986."</data>
      <data key="d6">b31ca51b419f7270ee5f4910c90ea331</data>
    </edge>
    <edge source="&quot;NEURAL NETWORKS&quot;" target="&quot;SEQUENCE PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Neural Networks, such as Elman Networks, are capable of performing tasks such as sequence prediction."</data>
      <data key="d6">bf4dccb5096a917a6a71f0cc224e4d7c</data>
    </edge>
    <edge source="&quot;HOPFIELD&quot;" target="&quot;LIAPUNOV FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hopfield published a special case of the Additive Model and Liapunov function, asserting that trajectories approach equilibria."</data>
      <data key="d6">98173c1c0fcd64ceb914e0dd6b366b30</data>
    </edge>
    <edge source="&quot;STM TRACES&quot;" target="&quot;SOURCE CELLS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"STM Traces represent activities in a neural system, originating from Source Cells."</data>
      <data key="d6">18be9bfe53d3b9c1e15c1c8238674459</data>
    </edge>
    <edge source="&quot;STM TRACES&quot;" target="&quot;GENERALIZED ADDITIVE SYSTEM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"STM Traces are a component of the Generalized Additive System, representing the activities of the system."</data>
      <data key="d6">8bb0e63353e66a2c60a878028beff5f9</data>
    </edge>
    <edge source="&quot;SHUNTING MODEL&quot;" target="&quot;FEEDFORWARD ON-CENTER NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Feedforward On-Center Network is defined by cells that obey a simple version of the Shunting Model."</data>
      <data key="d6">68b4b33f0da5edc9dcb301a08821b352</data>
    </edge>
    <edge source="&quot;SHUNTING MODEL&quot;" target="&quot;COHEN-GROSSBERG MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Cohen-Grossberg Model includes the Shunting Model as a type of network."</data>
      <data key="d6">0dd75f3ca11854714bdbfc8a96ccf256</data>
    </edge>
    <edge source="&quot;COWAN&quot;" target="&quot;IMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cowan reviewed experimental data that supports the existence of a four plus or minus one WM capacity limit when LTM and grouping influences are minimized."</data>
      <data key="d6">392028b79561bd7471cb68e7c9258b1e</data>
    </edge>
    <edge source="&quot;COWAN&quot;" target="&quot;2001&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"2001 is the year when Cowan reviewed experimental data supporting the existence of a four plus or minus one WM capacity limit."</data>
      <data key="d6">392028b79561bd7471cb68e7c9258b1e</data>
    </edge>
    <edge source="&quot;MTM&quot;" target="&quot;LEABRA MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MTM is a component used in the Leabra model, which stores and retrieves information over a longer time frame than STM."</data>
      <data key="d6">b40ff9b93414391d5e4b3c06dfe02bc9</data>
    </edge>
    <edge source="&quot;COHEN AND GROSSBERG&quot;" target="&quot;ADDITIVE AND SHUNTING MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen and Grossberg derived a Liapunov function for a generalization of the Additive and Shunting Models."</data>
      <data key="d6">d4afac3b7aed3d6e11ff5eaf34589c2d</data>
    </edge>
    <edge source="&quot;COHEN AND GROSSBERG&quot;" target="&quot;MEDIUM-TERM MEMORY (MTM)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen and Grossberg's work involves the concept of Medium-term memory (MTM), which they describe as habituative transmitter gates."</data>
      <data key="d6">d4afac3b7aed3d6e11ff5eaf34589c2d</data>
    </edge>
    <edge source="&quot;COHEN AND GROSSBERG&quot;" target="&quot;COHEN-GROSSBERG SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen and Grossberg are the researchers who developed Cohen-Grossberg Systems."</data>
      <data key="d6">4b48be5db14c2e681ef8f4ee7de4b847</data>
    </edge>
    <edge source="&quot;COHEN AND GROSSBERG&quot;" target="&quot;LIAPUNOV METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen and Grossberg used Liapunov Methods as inspiration in their research."</data>
      <data key="d6">4b48be5db14c2e681ef8f4ee7de4b847</data>
    </edge>
    <edge source="&quot;COHEN AND GROSSBERG&quot;" target="&quot;MASKING FIELD MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen and Grossberg developed the Masking Field Model, which is a specific model within the broader context of their research."</data>
      <data key="d6">4b48be5db14c2e681ef8f4ee7de4b847</data>
    </edge>
    <edge source="&quot;COHEN AND GROSSBERG&quot;" target="&quot;GLOBAL EQUILIBRIUM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen and Grossberg attempted to prove Global Equilibrium by showing that all Cohen-Grossberg systems generate jump trees, and thus no jump cycles."</data>
      <data key="d6">4a78ff105fdd9a4b0d01ccf1e5816c74</data>
    </edge>
    <edge source="&quot;COHEN AND GROSSBERG&quot;" target="&quot;COHEN-GROSSBERG LIAPUNOV FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen and Grossberg developed the Cohen-Grossberg Liapunov Function to prove the existence of global equilibria."</data>
      <data key="d6">4a78ff105fdd9a4b0d01ccf1e5816c74</data>
    </edge>
    <edge source="&quot;COHEN AND GROSSBERG&quot;" target="&quot;LIAPUNOV FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen and Grossberg proposed a Liapunov function that includes the Additive Model and Shunting Model."</data>
      <data key="d6">98173c1c0fcd64ceb914e0dd6b366b30</data>
    </edge>
    <edge source="&quot;COHEN AND GROSSBERG&quot;" target="&quot;BURTON&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen and Grossberg's work has been referenced by Burton in their work."</data>
      <data key="d6">98173c1c0fcd64ceb914e0dd6b366b30</data>
    </edge>
    <edge source="&quot;COHEN AND GROSSBERG&quot;" target="&quot;BURWICK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen and Grossberg's work has been referenced by Burwick in their work."</data>
      <data key="d6">98173c1c0fcd64ceb914e0dd6b366b30</data>
    </edge>
    <edge source="&quot;COHEN AND GROSSBERG&quot;" target="&quot;GUO ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen and Grossberg's work has been referenced by Guo et al. in their work."</data>
      <data key="d6">98173c1c0fcd64ceb914e0dd6b366b30</data>
    </edge>
    <edge source="&quot;LIAPUNOV FUNCTION&quot;" target="&quot;COHEN-GROSSBERG MODEL&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"The Liapunov Function is a mathematical concept used to analyze the stability of the Cohen-Grossberg Model."
"The Cohen-Grossberg Model is analyzed using a Liapunov Function to prove the stability of the price in a market."
"The Cohen-Grossberg model uses a Liapunov function to prove global convergence."</data>
      <data key="d6">01e2a32da700813f593038a23a618e55,0dd75f3ca11854714bdbfc8a96ccf256,be0954a6263de67c84da3141d95de445</data>
    </edge>
    <edge source="&quot;LIAPUNOV FUNCTION&quot;" target="&quot;KOSKO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Kosko used the Liapunov function developed by Cohen and Grossberg to prove global convergence of the adapted system."</data>
      <data key="d6">01e2a32da700813f593038a23a618e55</data>
    </edge>
    <edge source="&quot;COHEN-GROSSBERG MODEL&quot;" target="&quot;COHEN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen is a contributor to the Cohen-Grossberg Model, which focuses on symmetry in interaction coefficients."</data>
      <data key="d6">0dd75f3ca11854714bdbfc8a96ccf256</data>
    </edge>
    <edge source="&quot;COHEN-GROSSBERG MODEL&quot;" target="&quot;THEOREM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Cohen-Grossberg Model is proven to have stable prices and balanced firm books using a Theorem."</data>
      <data key="d6">0dd75f3ca11854714bdbfc8a96ccf256</data>
    </edge>
    <edge source="&quot;COHEN-GROSSBERG MODEL&quot;" target="&quot;BRAIN-STATE-IN-A-BOX MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Cohen-Grossberg Model includes the Brain-State-in-a-Box Model as a type of network."</data>
      <data key="d6">0dd75f3ca11854714bdbfc8a96ccf256</data>
    </edge>
    <edge source="&quot;COHEN-GROSSBERG MODEL&quot;" target="&quot;KOSKO&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Kosko has adapted the Cohen-Grossberg Model."
"Kosko adapted the Cohen-Grossberg model to define a system that combines STM and LTM."</data>
      <data key="d6">01e2a32da700813f593038a23a618e55,644602009dec8474bb5cd4702b391d3e</data>
    </edge>
    <edge source="&quot;MEDIUM-TERM MEMORY&quot;" target="&quot;GATED DIPOLE OPPONENT PROCESSING NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Medium-term Memory traces enable reset events to occur in the Gated Dipole Opponent Processing Network."</data>
      <data key="d6">be0954a6263de67c84da3141d95de445</data>
    </edge>
    <edge source="&quot;ADAPTIVE RESONANCE THEORY&quot;" target="&quot;COMPETITIVE LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Adaptive Resonance Theory was introduced to stabilize learning in a Competitive Learning model."</data>
      <data key="d6">01e2a32da700813f593038a23a618e55</data>
    </edge>
    <edge source="&quot;ADAPTIVE RESONANCE THEORY&quot;" target="&quot;SELF-ORGANIZING MAP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Adaptive Resonance Theory was introduced to stabilize learning in a Self-Organizing Map model."</data>
      <data key="d6">01e2a32da700813f593038a23a618e55</data>
    </edge>
    <edge source="&quot;ADAPTIVE RESONANCE THEORY&quot;" target="&quot;BAM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"BAM was inspired by Adaptive Resonance Theory, indicating a connection between the two entities."</data>
      <data key="d6">554e8565591507441cecaa652cb926db</data>
    </edge>
    <edge source="&quot;ADAPTIVE RESONANCE THEORY&quot;" target="&quot;CARPENTER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Carpenter has discussed the problem of catastrophic forgetting in relation to Adaptive Resonance Theory, indicating their connection."</data>
      <data key="d6">554e8565591507441cecaa652cb926db</data>
    </edge>
    <edge source="&quot;ADAPTIVE RESONANCE THEORY&quot;" target="&quot;FRENCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"French has also discussed the problem of catastrophic forgetting in relation to Adaptive Resonance Theory, indicating their connection."</data>
      <data key="d6">554e8565591507441cecaa652cb926db</data>
    </edge>
    <edge source="&quot;ADAPTIVE RESONANCE THEORY&quot;" target="&quot;PAGE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Page has discussed the problem of catastrophic forgetting in relation to Adaptive Resonance Theory, indicating their connection."</data>
      <data key="d6">554e8565591507441cecaa652cb926db</data>
    </edge>
    <edge source="&quot;ADAPTIVE RESONANCE THEORY&quot;" target="&quot;DESIMONE&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Desimone has discussed the operation of attention via a form of self-normalizing 'biased competition' in relation to Adaptive Resonance Theory, indicating their connection."
"Desimone is mentioned in the text as having contributed to a concept that is relevant to Adaptive Resonance Theory."</data>
      <data key="d6">3d5b88f7f81ed9e14f07335bbef17020,554e8565591507441cecaa652cb926db</data>
    </edge>
    <edge source="&quot;VISUAL PERCEPTION&quot;" target="&quot;NORMALIZATION RULE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The normalization rule underlies many properties of limited capacity processing in the brain, notably those related to visual perception."</data>
      <data key="d6">c8c573c11d0f29d207b3b639a9466518</data>
    </edge>
    <edge source="&quot;VISUAL PERCEPTION&quot;" target="&quot;WEBER LAW&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Weber Law describes the relationship between the perceived intensity of a stimulus and its physical intensity in the context of visual perception."</data>
      <data key="d6">8202e13f45a323970b361921f923c605</data>
    </edge>
    <edge source="&quot;VISUAL PERCEPTION&quot;" target="&quot;BRIGHTNESS CONSTANCY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Brightness Constancy is a property of visual perception that ensures objects appear the same brightness regardless of their surrounding luminance."</data>
      <data key="d6">8202e13f45a323970b361921f923c605</data>
    </edge>
    <edge source="&quot;VISUAL PERCEPTION&quot;" target="&quot;BRIGHTNESS CONTRAST&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Brightness Contrast is a visual phenomenon that occurs when increasing the luminance of inputs to the off-surround makes the on-center look darker."</data>
      <data key="d6">8202e13f45a323970b361921f923c605</data>
    </edge>
    <edge source="&quot;GUTOWSKI&quot;" target="&quot;MTM TRACE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gutowski is mentioned in the context of the MTM Trace."</data>
      <data key="d6">c73b76e10166042ccaba5603ed67f380</data>
    </edge>
    <edge source="&quot;OGMEN AND GAGN&#201;&quot;" target="&quot;MTM TRACE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ogmen and Gagn&#233; are mentioned in the context of the MTM Trace."</data>
      <data key="d6">c73b76e10166042ccaba5603ed67f380</data>
    </edge>
    <edge source="&quot;ABBOTT ET AL.&quot;" target="&quot;MTM TRACE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Abbott et al. are mentioned in the context of the MTM Trace."</data>
      <data key="d6">c73b76e10166042ccaba5603ed67f380</data>
    </edge>
    <edge source="&quot;TSODYKS AND MARKRAM&quot;" target="&quot;MTM TRACE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Tsodyks and Markram are mentioned in the context of the MTM Trace."</data>
      <data key="d6">c73b76e10166042ccaba5603ed67f380</data>
    </edge>
    <edge source="&quot;GAUDIANO AND GROSSBERG&quot;" target="&quot;MASS ACTION INTERACTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gaudiano and Grossberg are mentioned in the context of the Mass Action Interaction."</data>
      <data key="d6">c73b76e10166042ccaba5603ed67f380</data>
    </edge>
    <edge source="&quot;GROSSBERG AND SEITZ&quot;" target="&quot;MASS ACTION INTERACTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg and Seitz are mentioned in the context of the Mass Action Interaction."</data>
      <data key="d6">c73b76e10166042ccaba5603ed67f380</data>
    </edge>
    <edge source="&quot;MTM TRACE&quot;" target="&quot;HABITUATIVE TRANSMITTER GATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The MTM Trace is described in relation to the Habituative Transmitter Gate."</data>
      <data key="d6">c73b76e10166042ccaba5603ed67f380</data>
    </edge>
    <edge source="&quot;MTM TRACE&quot;" target="&quot;MASS ACTION INTERACTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The MTM Trace is described in relation to the Mass Action Interaction."</data>
      <data key="d6">c73b76e10166042ccaba5603ed67f380</data>
    </edge>
    <edge source="&quot;OUTSTAR LEARNING&quot;" target="&quot;INSTAR LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Outstar Learning and Instar Learning are dual networks in the sense that they are the same, except for reversing which cells are sampling and which are sampled."</data>
      <data key="d6">b286a9022774f24a400744b2a1b08bab</data>
    </edge>
    <edge source="&quot;HEBBIAN TRACES&quot;" target="&quot;LONG-TERM MEMORY (LTM)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hebbian Traces are a type of connection strength in neural networks that are stored in Long-Term Memory (LTM)."</data>
      <data key="d6">b286a9022774f24a400744b2a1b08bab</data>
    </edge>
    <edge source="&quot;SELF-ORGANIZING MAP (SOM)&quot;" target="&quot;RCF&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-Organizing Map (SOM) has been developed by Kohonen, and it utilizes shunting dynamics in some versions, which is a property of RCF."</data>
      <data key="d6">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </edge>
    <edge source="&quot;O&#8217;REILLY&quot;" target="&quot;LEABRA MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"O&#8217;Reilly is a person associated with the development of the Leabra model."</data>
      <data key="d6">b40ff9b93414391d5e4b3c06dfe02bc9</data>
    </edge>
    <edge source="&quot;MUNAKATA&quot;" target="&quot;LEABRA MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Munakata is a person associated with the development of the Leabra model."</data>
      <data key="d6">b40ff9b93414391d5e4b3c06dfe02bc9</data>
    </edge>
    <edge source="&quot;O&#8217;REILLY AND MUNAKATA&quot;" target="&quot;THE BRAIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"O&#8217;Reilly and Munakata are mentioned in the context of the Leabra model, which is used to explain how the brain processes spatial patterns."</data>
      <data key="d6">0d925896ce2bf9b73be90d8fa5ddb402</data>
    </edge>
    <edge source="&quot;THE BRAIN&quot;" target="&quot;SPATIAL PATTERNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The brain is described as an organization that processes spatial patterns of information."</data>
      <data key="d6">0d925896ce2bf9b73be90d8fa5ddb402</data>
    </edge>
    <edge source="&quot;THE BRAIN&quot;" target="&quot;TEMPORAL PATTERNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The brain is described as an organization that processes temporal patterns of information."</data>
      <data key="d6">0d925896ce2bf9b73be90d8fa5ddb402</data>
    </edge>
    <edge source="&quot;NEURONS&quot;" target="&quot;BRAINS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Neurons are a component of brains that have evolved network designs to process variable input intensities."</data>
      <data key="d6">31080b985ce23ef751d488d0f7b9eff6</data>
    </edge>
    <edge source="&quot;NEURONS&quot;" target="&quot;NOISE-SATURATION DILEMMA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Neurons face the challenge of maintaining sensitivity to input patterns while dealing with variable input intensities, as described in the Noise-Saturation Dilemma."</data>
      <data key="d6">31080b985ce23ef751d488d0f7b9eff6</data>
    </edge>
    <edge source="&quot;NEURONS&quot;" target="&quot;MEMBRANE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Membrane, or shunting, is mentioned in the context of neuronal network interactions, which may influence their ability to process variable input intensities."</data>
      <data key="d6">31080b985ce23ef751d488d0f7b9eff6</data>
    </edge>
    <edge source="&quot;NEURONS&quot;" target="&quot;PAPER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Paper discusses the use of neurons in the context of the reservoir network."</data>
      <data key="d6">388cc054a99cc5cadff33147f95d6156</data>
    </edge>
    <edge source="&quot;NOISE-SATURATION DILEMMA&quot;" target="&quot;ON-CENTER OFF-SURROUND NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The On-center Off-Surround Network is a solution to the Noise-Saturation Dilemma, enabling neurons to retain sensitivity to the relative sizes of their inputs across the network."</data>
      <data key="d6">31080b985ce23ef751d488d0f7b9eff6</data>
    </edge>
    <edge source="&quot;CELL (V_I)&quot;" target="&quot;INPUT (I_I)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cell (v_i) maintains sensitivity to its input size (I_i), and increasing I_i increases the sensitivity of the cell."</data>
      <data key="d6">fe551b6e0c32ef0fb9ac0a07ff64d6ba</data>
    </edge>
    <edge source="&quot;CELL (V_I)&quot;" target="&quot;INPUT (I_K)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cell (v_i) competes with other inputs (I_k) to activate itself, and increasing any I_k decreases the sensitivity of the cell."</data>
      <data key="d6">fe551b6e0c32ef0fb9ac0a07ff64d6ba</data>
    </edge>
    <edge source="&quot;CELL (V_I)&quot;" target="&quot;KUFFLER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cell (v_i) is described as having an on-center off-surround anatomy, which was reported by Kuffler in the cat retina."</data>
      <data key="d6">fe551b6e0c32ef0fb9ac0a07ff64d6ba</data>
    </edge>
    <edge source="&quot;KUFFLER&quot;" target="&quot;ON-CENTER OFF-SURROUND ANATOMY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"On-Center Off-Surround Anatomy was reported by Kuffler in the cat retina in 1953."</data>
      <data key="d6">9d40ff29a29b7422b2b0c76b957c54f3</data>
    </edge>
    <edge source="&quot;ON-CENTER OFF-SURROUND ANATOMY&quot;" target="&quot;EXCITED SITES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"On-Center Off-Surround Anatomy activates cells by exciting their unexcited sites."</data>
      <data key="d6">9d40ff29a29b7422b2b0c76b957c54f3</data>
    </edge>
    <edge source="&quot;ON-CENTER OFF-SURROUND ANATOMY&quot;" target="&quot;UNEXCITED SITES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"On-Center Off-Surround Anatomy inhibits cells by inhibiting their excited sites."</data>
      <data key="d6">9d40ff29a29b7422b2b0c76b957c54f3</data>
    </edge>
    <edge source="&quot;EXCITED SITES&quot;" target="&quot;SPONTANEOUS DECAY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Excited Sites can spontaneously decay over time, allowing a cell to return to an equilibrium point."</data>
      <data key="d6">9d40ff29a29b7422b2b0c76b957c54f3</data>
    </edge>
    <edge source="&quot;FEEDFORWARD ON-CENTER NETWORK&quot;" target="&quot;EQUILIBRIUM VALUE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"When a fixed spatial pattern is presented and the total input is held constant, each cell in the Feedforward On-Center Network approaches an Equilibrium Value."</data>
      <data key="d6">68b4b33f0da5edc9dcb301a08821b352</data>
    </edge>
    <edge source="&quot;FEEDFORWARD ON-CENTER NETWORK&quot;" target="&quot;SATURATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"When the off-surround input is removed, all the cells in the Feedforward On-Center Network saturate at a specific value."</data>
      <data key="d6">68b4b33f0da5edc9dcb301a08821b352</data>
    </edge>
    <edge source="&quot;EQUATION (13)&quot;" target="&quot;OFF-SURROUND&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Equation (13) involves automatic gain control by the off-surround, which multiplies a variable and prevents saturation."</data>
      <data key="d6">04e132fa3a85e95e1e6164428852446e</data>
    </edge>
    <edge source="&quot;EQUATION (13)&quot;" target="&quot;MASS ACTION NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Equation (13) is characterized by mass action, or shunting, networks, where both the steady state and the rate of change of a variable depend upon input strength."</data>
      <data key="d6">04e132fa3a85e95e1e6164428852446e</data>
    </edge>
    <edge source="&quot;STEADY STATE&quot;" target="&quot;RATE OF CHANGE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both the steady state and the rate of change of a variable in a mass action network depend upon input strength."</data>
      <data key="d6">04e132fa3a85e95e1e6164428852446e</data>
    </edge>
    <edge source="&quot;ACTIVITIES (X_I)&quot;" target="&quot;INPUT STRENGTH (I)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The activities (x_i) depend on input strength (I), with both their steady state and rate of change being affected."</data>
      <data key="d6">c8c573c11d0f29d207b3b639a9466518</data>
    </edge>
    <edge source="&quot;INPUT STRENGTH (I)&quot;" target="&quot;TOTAL ACTIVITY (X)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The total activity (x) approaches a constant (B) as input strength (I) increases."</data>
      <data key="d6">c8c573c11d0f29d207b3b639a9466518</data>
    </edge>
    <edge source="&quot;TOTAL ACTIVITY (X)&quot;" target="&quot;NORMALIZATION RULE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The normalization rule ensures that the total activity (x) remains constant, even as individual activities (x_i) change."</data>
      <data key="d6">c8c573c11d0f29d207b3b639a9466518</data>
    </edge>
    <edge source="&quot;NORMALIZATION RULE&quot;" target="&quot;LTM INVARIANCE PRINCIPLE&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The LTM Invariance Principle and the Normalization Rule are two constraints predicted by Grossberg for stable learning and memory of list chunks."
"Both the LTM Invariance Principle and the Normalization Rule are predicted by Grossberg to support stable learning and memory of list chunks."</data>
      <data key="d6">4364aa6091e1966365fa889b34f5cf90,dcd38cdc6195b2bbf41d936af0bf1f5f</data>
    </edge>
    <edge source="&quot;NORMALIZATION RULE&quot;" target="&quot;RCFS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The Normalization Rule is likely realized by specialized RCFs, which are not explicitly defined in the text."
"RCFs are predicted to follow the Normalization Rule, which follows from the tendency of RCFs to normalize total network activity."</data>
      <data key="d6">4364aa6091e1966365fa889b34f5cf90,c38beddeac1d3cac8282ad59bc835788</data>
    </edge>
    <edge source="&quot;NORMALIZATION RULE&quot;" target="&quot;WORKING MEMORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Normalization Rule is a principle that applies to working memories, ensuring their limited capacity and activity redistribution when new items are stored."</data>
      <data key="d6">1976b19f768a8fdf37207b680c3b2b40</data>
    </edge>
    <edge source="&quot;SHIFT PROPERTY&quot;" target="&quot;SYSTEM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Shift Property causes the entire response curve of a system to shift without a loss of sensitivity."</data>
      <data key="d6">8202e13f45a323970b361921f923c605</data>
    </edge>
    <edge source="&quot;NORMALIZATION PROPERTY&quot;" target="&quot;LIMITED CAPACITY PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Normalization Property underlies many properties of limited capacity processing in the brain, such as visual perception and cognition."</data>
      <data key="d6">8202e13f45a323970b361921f923c605</data>
    </edge>
    <edge source="&quot;NORMALIZATION PROPERTY&quot;" target="&quot;WORKING MEMORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Normalization Property is a property that limits the capacity of working memory, which holds and manipulates information for immediate use in cognitive tasks."</data>
      <data key="d6">8202e13f45a323970b361921f923c605</data>
    </edge>
    <edge source="&quot;WORKING MEMORY&quot;" target="&quot;BADDELEY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Baddeley is a psychologist known for his contributions to the study of Working Memory."</data>
      <data key="d6">3d5b88f7f81ed9e14f07335bbef17020</data>
    </edge>
    <edge source="&quot;WORKING MEMORY&quot;" target="&quot;COGNITIVE SCIENTISTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cognitive Scientists are studying the processes of Working Memory to understand how it temporarily stores and manipulates information."</data>
      <data key="d6">5c4e24fc9bd10d0bd59a84d56f960cf9</data>
    </edge>
    <edge source="&quot;WORKING MEMORY&quot;" target="&quot;NEUROSCIENTISTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Neuroscientists are studying the brain and its functions, including the role of Working Memory in cognitive processes."</data>
      <data key="d6">5c4e24fc9bd10d0bd59a84d56f960cf9</data>
    </edge>
    <edge source="&quot;WORKING MEMORY&quot;" target="&quot;EVENT SEQUENCES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Working Memory is capable of temporarily storing sequences of events."</data>
      <data key="d6">5c4e24fc9bd10d0bd59a84d56f960cf9</data>
    </edge>
    <edge source="&quot;WORKING MEMORY&quot;" target="&quot;ATKINSON AND SHIFFRIN MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Atkinson and Shiffrin Model is a popular model of Working Memory that proposes binary activations of a series of items."</data>
      <data key="d6">5c4e24fc9bd10d0bd59a84d56f960cf9</data>
    </edge>
    <edge source="&quot;WORKING MEMORY&quot;" target="&quot;GROSSBERG'S ITEM-AND-ORDER WM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg's Item-and-Order WM is a model of Working Memory that represents items and their order as a temporally evolving spatial pattern of activity across working memory cells."</data>
      <data key="d6">5c4e24fc9bd10d0bd59a84d56f960cf9</data>
    </edge>
    <edge source="&quot;WORKING MEMORY&quot;" target="&quot;LONG-TERM MEMORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Working Memory and Long-Term Memory are related as cognitive systems that support stable learning and the retention of information over time."</data>
      <data key="d6">1976b19f768a8fdf37207b680c3b2b40</data>
    </edge>
    <edge source="&quot;WORKING MEMORY&quot;" target="&quot;RCFS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RCFs are mentioned as a type of network that can be embodied by specialized recurrent on-center off-surround shunting networks, which are related to the functioning of working memories."</data>
      <data key="d6">1976b19f768a8fdf37207b680c3b2b40</data>
    </edge>
    <edge source="&quot;SYSTEM&quot;" target="&quot;NVAR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NVAR is used to analyze and predict the dynamics of the System."</data>
      <data key="d6">59c163f6fc13814d6ae0ff1b04d22653</data>
    </edge>
    <edge source="&quot;SHUNTING NETWORK&quot;" target="&quot;NEUROPHYSIOLOGY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Shunting Network has the form of the Membrane Equation on which cellular neurophysiology is based, indicating a connection between the two fields."</data>
      <data key="d6">f290960776c5ec561653e90d2ac6751b</data>
    </edge>
    <edge source="&quot;WERBLIN&quot;" target="&quot;SHIFTING PROPERTY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Werblin has found a shift property similar to that described in the shunting network equations, indicating a connection between the two concepts."</data>
      <data key="d6">f290960776c5ec561653e90d2ac6751b</data>
    </edge>
    <edge source="&quot;MEMBRANE EQUATION&quot;" target="&quot;SHIFTING PROPERTY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Membrane Equation has a form similar to the shunting network equations with a shift property, indicating a connection between the two concepts."</data>
      <data key="d6">f290960776c5ec561653e90d2ac6751b</data>
    </edge>
    <edge source="&quot;MEMBRANE EQUATION&quot;" target="&quot;HODGKIN AND HUXLEY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hodgkin and Huxley are known for their work on the membrane equation in neurophysiology."</data>
      <data key="d6">f2468cda326d1ca11c98f2fbde186400</data>
    </edge>
    <edge source="&quot;MEMBRANE EQUATION&quot;" target="&quot;SHUNTING NETWORK EQUATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Shunting Equation has the form of the membrane equation, which is based on the work of Hodgkin and Huxley."</data>
      <data key="d6">f2468cda326d1ca11c98f2fbde186400</data>
    </edge>
    <edge source="&quot;MEMBRANE EQUATION&quot;" target="&quot;SODIUM CHANNEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sodium Channel contributes to the Membrane Equation as an excitatory saturation voltage."</data>
      <data key="d6">768cec2f00889d1e375cb4955c58ad60</data>
    </edge>
    <edge source="&quot;MEMBRANE EQUATION&quot;" target="&quot;POTASSIUM CHANNEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Potassium Channel contributes to the Membrane Equation as an inhibitory saturation voltage."</data>
      <data key="d6">768cec2f00889d1e375cb4955c58ad60</data>
    </edge>
    <edge source="&quot;MEMBRANE EQUATION&quot;" target="&quot;ION CHANNEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ion Channel is a general term for types of proteins that contribute to the Membrane Equation, such as Sodium Channel and Potassium Channel."</data>
      <data key="d6">768cec2f00889d1e375cb4955c58ad60</data>
    </edge>
    <edge source="&quot;HODGKIN AND HUXLEY&quot;" target="&quot;SIGNAL PROPAGATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hodgkin and Huxley proposed a model of signal propagation along axons and its effect on postsynaptic cells."</data>
      <data key="d6">a94f07c842345c77af089558b0786bfe</data>
    </edge>
    <edge source="&quot;SODIUM CHANNEL&quot;" target="&quot;POTASSIUM CHANNEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sodium Channel and Potassium Channel are both ion channels that contribute to the Membrane Equation, representing different types of ionic conductances."</data>
      <data key="d6">768cec2f00889d1e375cb4955c58ad60</data>
    </edge>
    <edge source="&quot;(20)&quot;" target="&quot;(V^+)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"(20) is mentioned in relation to (V^+) in the context of symmetry-breaking and noise suppression."</data>
      <data key="d6">91f030f6c14c673e6d029c9bf1a66515</data>
    </edge>
    <edge source="&quot;(20)&quot;" target="&quot;(V^-)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"(20) is mentioned in relation to (V^-) in the context of symmetry-breaking and noise suppression."</data>
      <data key="d6">91f030f6c14c673e6d029c9bf1a66515</data>
    </edge>
    <edge source="&quot;(20)&quot;" target="&quot;(V^P)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"(20) is mentioned in relation to (V^p) in the context of symmetry-breaking and noise suppression."</data>
      <data key="d6">91f030f6c14c673e6d029c9bf1a66515</data>
    </edge>
    <edge source="&quot;BRNN&quot;" target="&quot;RCF&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recurrent Competitive Field is a type of recurrent neural network mentioned in the context of a bidirectional Recurrent Neural Network."</data>
      <data key="d6">3a64c8c26895f111f00a349dd69bb505</data>
    </edge>
    <edge source="&quot;RCF&quot;" target="&quot;DOUGLAS ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Douglas et al. have applied shunting properties to simulate data about the properties of the cortical circuits that subserve visual perception, which is a property of RCF."</data>
      <data key="d6">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </edge>
    <edge source="&quot;RCF&quot;" target="&quot;GROSSBERG AND MINGOLLA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg and Mingolla have applied shunting properties in their research, which is a property of RCF."</data>
      <data key="d6">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </edge>
    <edge source="&quot;RCF&quot;" target="&quot;GROSSBERG AND TODOROVIC&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg and Todorovic have applied shunting properties in their research, which is a property of RCF."</data>
      <data key="d6">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </edge>
    <edge source="&quot;RCF&quot;" target="&quot;HEEGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Heeger has applied shunting properties in their research, which is a property of RCF."</data>
      <data key="d6">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </edge>
    <edge source="&quot;RCF&quot;" target="&quot;MCLAUGHLIN ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"McLaughlin et al. have applied shunting properties in their research, which is a property of RCF."</data>
      <data key="d6">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </edge>
    <edge source="&quot;RCF&quot;" target="&quot;COMPETITIVE LEARNING (CL)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Competitive Learning (CL) has been developed by Grossberg and others, and it utilizes shunting dynamics, which is a property of RCF."</data>
      <data key="d6">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </edge>
    <edge source="&quot;RCF&quot;" target="&quot;ADAPTIVE RESONANCE THEORY (ART)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Adaptive Resonance Theory (ART) has been developed by Grossberg, and it does not utilize shunting dynamics, which is not a property of RCF."</data>
      <data key="d6">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </edge>
    <edge source="&quot;RCF&quot;" target="&quot;PALMA ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Palma et al. have shown that an RCF with spiking neurons can replicate key properties of the Grossberg (1973) theorems for rate-based neurons, which is a property of RCF."</data>
      <data key="d6">2cbd29d6f0019f0c85bee43779ae8f4d</data>
    </edge>
    <edge source="&quot;FUNCTION F(W)&quot;" target="&quot;FUNCTION H(W)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Function f(w) is defined as the argument for Function h(w), indicating a relationship between the two mathematical functions."</data>
      <data key="d6">b4f6256f3430f1aa72ca8092809ebba1</data>
    </edge>
    <edge source="&quot;MATRIX A&quot;" target="&quot;MATRIX B&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The relationship between Matrix B and Matrix A is mentioned in the text, likely indicating their importance in the functioning of the STM system."</data>
      <data key="d6">b4f6256f3430f1aa72ca8092809ebba1</data>
    </edge>
    <edge source="&quot;NOISE&quot;" target="&quot;SIGNAL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Noise can be amplified by signal functions, potentially causing interference in data processing."</data>
      <data key="d6">8da881a4f375e8a524fd0bf46ae2279e</data>
    </edge>
    <edge source="&quot;SIGNAL FUNCTION&quot;" target="&quot;HILL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Signal functions can include Hill functions, which are mentioned as having a monotone decreasing property."</data>
      <data key="d6">8da881a4f375e8a524fd0bf46ae2279e</data>
    </edge>
    <edge source="&quot;SIGNAL FUNCTION&quot;" target="&quot;BIOLOGY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Signal Functions in biology are studied and must be bounded, as mentioned in the text."</data>
      <data key="d6">35551dc55b5522082b778171ff6d1bf9</data>
    </edge>
    <edge source="&quot;HILL FUNCTION&quot;" target="&quot;SLOWER-THAN-LINEAR SIGNAL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Hill Function of a Slower-than-Linear Signal Function is monotone decreasing."</data>
      <data key="d6">0ae1c3b9a183835b90295e9712b9656d</data>
    </edge>
    <edge source="&quot;HILL FUNCTION&quot;" target="&quot;FASTER-THAN-LINEAR SIGNAL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Hill Function of a Faster-than-Linear Signal Function is monotone increasing."</data>
      <data key="d6">0ae1c3b9a183835b90295e9712b9656d</data>
    </edge>
    <edge source="&quot;NETWORK&quot;" target="&quot;LINEAR SIGNAL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A Linear Signal Function amplifies noise in the Network and eliminates differences in inputs."</data>
      <data key="d6">0ae1c3b9a183835b90295e9712b9656d</data>
    </edge>
    <edge source="&quot;NETWORK&quot;" target="&quot;SLOWER-THAN-LINEAR SIGNAL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A Slower-than-Linear Signal Function also amplifies noise in the Network and eliminates differences in inputs."</data>
      <data key="d6">0ae1c3b9a183835b90295e9712b9656d</data>
    </edge>
    <edge source="&quot;NETWORK&quot;" target="&quot;FASTER-THAN-LINEAR SIGNAL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A Faster-than-Linear Signal Function suppresses noise in the Network and enhances differences in inputs."</data>
      <data key="d6">0ae1c3b9a183835b90295e9712b9656d</data>
    </edge>
    <edge source="&quot;NETWORK&quot;" target="&quot;EQUILIBRIUM POINTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Network's behavior is described in terms of its equilibrium points, which are the stable states of the system."</data>
      <data key="d6">d69baa85c856a1c0b5446a9c9fcd31b8</data>
    </edge>
    <edge source="&quot;NETWORK&quot;" target="&quot;SIGNAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Network's behavior is mentioned in relation to a signal, but the specific relationship is not explicitly described."</data>
      <data key="d6">d69baa85c856a1c0b5446a9c9fcd31b8</data>
    </edge>
    <edge source="&quot;NOISE SUPPRESSION&quot;" target="&quot;SIGMOID SIGNAL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Noise Suppression is achieved using a Sigmoid Signal Function, which suppresses noise and enhances activity patterns."</data>
      <data key="d6">6a47ed5881928d48cdcb74e40867a711</data>
    </edge>
    <edge source="&quot;SIGMOID SIGNAL FUNCTION&quot;" target="&quot;QUENCHING THRESHOLD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Quenching Threshold is a parameter of the Sigmoid Signal Function that determines when activity is quenched or contrast-enhanced."</data>
      <data key="d6">6a47ed5881928d48cdcb74e40867a711</data>
    </edge>
    <edge source="&quot;RCFS&quot;" target="&quot;LTM INVARIANCE PRINCIPLE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RCFs are predicted to obey the LTM Invariance Principle, suggesting that all working memories have a similar network design."</data>
      <data key="d6">c38beddeac1d3cac8282ad59bc835788</data>
    </edge>
    <edge source="&quot;MAY AND LEONARD MODEL&quot;" target="&quot;VOTING PARADOX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The May and Leonard Model is a mathematical model developed to study the voting paradox."</data>
      <data key="d6">3334f6dcd53b71cf3ceb7648ead24d5a</data>
    </edge>
    <edge source="&quot;MAY AND LEONARD MODEL&quot;" target="&quot;COMPETITIVE SYSTEM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The May and Leonard Model is an example of a competitive system."</data>
      <data key="d6">3334f6dcd53b71cf3ceb7648ead24d5a</data>
    </edge>
    <edge source="&quot;COMPETITIVE SYSTEM&quot;" target="&quot;DECISION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Competitive systems involve decision-making processes."</data>
      <data key="d6">3334f6dcd53b71cf3ceb7648ead24d5a</data>
    </edge>
    <edge source="&quot;SYSTEM (21)&quot;" target="&quot;ADAPTATION LEVEL SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"System (21) is a special case of Adaptation Level Systems."</data>
      <data key="d6">edd10f4a8bda41294ef582dc7f048ad5</data>
    </edge>
    <edge source="&quot;ADAPTATION LEVEL SYSTEMS&quot;" target="&quot;STATE-DEPENDENT AMPLIFICATION FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"State-dependent Amplification Function is used in the equations defining the behavior of Adaptation Level Systems."</data>
      <data key="d6">edd10f4a8bda41294ef582dc7f048ad5</data>
    </edge>
    <edge source="&quot;ADAPTATION LEVEL SYSTEMS&quot;" target="&quot;SELF-SIGNAL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Self-signal Function is used in the equations defining the behavior of Adaptation Level Systems."</data>
      <data key="d6">edd10f4a8bda41294ef582dc7f048ad5</data>
    </edge>
    <edge source="&quot;ADAPTATION LEVEL SYSTEMS&quot;" target="&quot;STATE-DEPENDENT ADAPTATION LEVEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"State-dependent Adaptation Level is used in the equations defining the behavior of Adaptation Level Systems."</data>
      <data key="d6">edd10f4a8bda41294ef582dc7f048ad5</data>
    </edge>
    <edge source="&quot;THEOREM&quot;" target="&quot;COMPETITIVE MARKET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Theorem is applied to a Competitive Market, proving its stability and the behavior of firms within it."</data>
      <data key="d6">0c9db6cd87deaca2e432c260d775349c</data>
    </edge>
    <edge source="&quot;COHEN-GROSSBERG SYSTEMS&quot;" target="&quot;GLOBAL EQUILIBRIUM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen-Grossberg Systems are the subject of research aimed at proving Global Equilibrium."</data>
      <data key="d6">4b48be5db14c2e681ef8f4ee7de4b847</data>
    </edge>
    <edge source="&quot;COHEN-GROSSBERG SYSTEMS&quot;" target="&quot;JUMP TREES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen-Grossberg Systems are hypothesized to generate jump trees, which are relevant to the proof of Global Equilibrium."</data>
      <data key="d6">4b48be5db14c2e681ef8f4ee7de4b847</data>
    </edge>
    <edge source="&quot;COHEN-GROSSBERG SYSTEMS&quot;" target="&quot;COHEN-GROSSBERG LIAPUNOV FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cohen-Grossberg Systems are mathematical models that use the Cohen-Grossberg Liapunov Function to prove the existence of global equilibria."</data>
      <data key="d6">4a78ff105fdd9a4b0d01ccf1e5816c74</data>
    </edge>
    <edge source="&quot;HOPFIELD NETWORK&quot;" target="&quot;JOHN J. HOPFIELD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"John J. Hopfield is a researcher who published the Hopfield Network model."</data>
      <data key="d6">643e65a5f4132289cfd1d5b954043642</data>
    </edge>
    <edge source="&quot;HOPFIELD NETWORK&quot;" target="&quot;COHEN-GROSSBERG-HOPFIELD MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Hopfield Network is often referred to as the Cohen-Grossberg-Hopfield Model."</data>
      <data key="d6">643e65a5f4132289cfd1d5b954043642</data>
    </edge>
    <edge source="&quot;HOPFIELD NETWORK&quot;" target="&quot;SYNCHRONIZED OSCILLATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Hopfield Network is described as a neural network that can undergo synchronized oscillations."</data>
      <data key="d6">643e65a5f4132289cfd1d5b954043642</data>
    </edge>
    <edge source="&quot;HOPFIELD NETWORK&quot;" target="&quot;ISING MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Hopfield Network was based on the work of Shun&#8217;ichi Amari, who made the Ising Model adaptive in 1972."</data>
      <data key="d6">f59839daadfb1f3832bb9f8d201a7126</data>
    </edge>
    <edge source="&quot;HOPFIELD NETWORK&quot;" target="&quot;SHUN&#8217;ICHI AMARI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Shun&#8217;ichi Amari made the Hopfield network adaptive in 1972."</data>
      <data key="d6">b31ca51b419f7270ee5f4910c90ea331</data>
    </edge>
    <edge source="&quot;HOPFIELD NETWORK&quot;" target="&quot;BIDIRECTIONAL ASSOCIATIVE MEMORY (BAM) NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Bidirectional Associative Memory (BAM) Network is a variant of Hopfield Network."</data>
      <data key="d6">a8c0edd2cdddb7d6d899284063b541f5</data>
    </edge>
    <edge source="&quot;COHEN-GROSSBERG-HOPFIELD MODEL&quot;" target="&quot;DAVID COHEN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"David Cohen is a contributor to the Cohen-Grossberg-Hopfield Model."</data>
      <data key="d6">643e65a5f4132289cfd1d5b954043642</data>
    </edge>
    <edge source="&quot;COHEN-GROSSBERG-HOPFIELD MODEL&quot;" target="&quot;MICHAEL I. GROSSBERG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Michael I. Grossberg is a contributor to the Cohen-Grossberg-Hopfield Model."</data>
      <data key="d6">643e65a5f4132289cfd1d5b954043642</data>
    </edge>
    <edge source="&quot;SYNCHRONIZED OSCILLATIONS&quot;" target="&quot;ORDER-PRESERVING LIMIT CYCLES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Order-Preserving Limit Cycles are a type of synchronized oscillation during attentive brain dynamics."</data>
      <data key="d6">53f5bc3f4c71310c593a23aef01d1633</data>
    </edge>
    <edge source="&quot;EXCITATORY FEEDBACK SIGNALS&quot;" target="&quot;INHIBITORY INTERNEURONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Excitatory Feedback Signals stimulate other populations, while Inhibitory Interneurons produce inhibitory signals that can slow down the activity of other neurons."</data>
      <data key="d6">53f5bc3f4c71310c593a23aef01d1633</data>
    </edge>
    <edge source="&quot;INHIBITORY INTERNEURONS&quot;" target="&quot;SHUNTING NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Shunting Networks use slow inhibitory interneurons to persistently oscillate."</data>
      <data key="d6">53f5bc3f4c71310c593a23aef01d1633</data>
    </edge>
    <edge source="&quot;HABITUATIVE GATES&quot;" target="&quot;PERSISTENT OSCILLATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Habituative Gates multiply recurrent signals in a neural network, allowing for persistent oscillations."</data>
      <data key="d6">53f5bc3f4c71310c593a23aef01d1633</data>
    </edge>
    <edge source="&quot;HABITUATIVE GATES&quot;" target="&quot;SLOW INHIBITORY INTERNEURONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Slow Inhibitory Interneurons multiply recurrent signals through Habituative Gates, as mentioned in the text."</data>
      <data key="d6">9b30fc06ca06f49c2faa238da7eddc6f</data>
    </edge>
    <edge source="&quot;BRNNS&quot;" target="&quot;HIGHLY DIFFERENTIATED ANATOMICAL STRUCTURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"bRNNs are embodied in highly differentiated anatomical structures in the brain."</data>
      <data key="d6">53f5bc3f4c71310c593a23aef01d1633</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS (RNNS)&quot;" target="&quot;BIOLOGICALLY REALISTIC NEURAL NETWORKS (BRNNS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recurrent Neural Networks (RNNs) are mentioned in the context of Biologically Realistic Neural Networks (bRNNs), indicating their relationship."</data>
      <data key="d6">9b30fc06ca06f49c2faa238da7eddc6f</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS (RNNS)&quot;" target="&quot;LONG SHORT-TERM MEMORY NETWORKS (LSTMS)&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"RNNs can be enhanced with controlled storage mechanisms, such as gated states or gated memory, to create LSTMs, which are better suited for handling long sequences of data."
"RNNs include LSTMs as a type that addresses the vanishing gradient problem, allowing them to learn and remember information from long sequences."</data>
      <data key="d6">04b89ad6396cb78ca75689473c47a247,24a607f45ad989d81411fed4f2941884</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS (RNNS)&quot;" target="&quot;GATED RECURRENT UNITS (GRUS)&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"RNNs can be modified to use a simpler structure, such as gated recurrent units, to control the flow of information, making them computationally more efficient."
"RNNs include GRUs as a type that uses gating mechanisms to control the flow of information, similar to LSTMs but with a simpler structure."</data>
      <data key="d6">04b89ad6396cb78ca75689473c47a247,24a607f45ad989d81411fed4f2941884</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS (RNNS)&quot;" target="&quot;TURING COMPLETENESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RNNs have the theoretical capability to simulate any Turing machine, given enough time and resources, which is known as Turing Completeness."</data>
      <data key="d6">04b89ad6396cb78ca75689473c47a247</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS (RNNS)&quot;" target="&quot;FEEDBACK NEURAL NETWORKS (FNNS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RNNs are compared to FNNs, which are a type of neural network that incorporates time delays or feedback loops, replacing standard storage mechanisms."</data>
      <data key="d6">24a607f45ad989d81411fed4f2941884</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS (RNNS)&quot;" target="&quot;HANDWRITING RECOGNITION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RNNs are particularly useful for Handwriting Recognition, an application where processing unsegmented, connected handwriting is a key challenge."</data>
      <data key="d6">24a607f45ad989d81411fed4f2941884</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS (RNNS)&quot;" target="&quot;SPEECH RECOGNITION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RNNs are useful for Speech Recognition, an application where recognizing and processing spoken language is a key task."</data>
      <data key="d6">24a607f45ad989d81411fed4f2941884</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS (RNNS)&quot;" target="&quot;SEQUENTIAL DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RNNs are designed to process sequential data, which is data arranged in sequences where the order matters and each data point is dependent on other data points in this sequence."</data>
      <data key="d6">24a607f45ad989d81411fed4f2941884</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS (RNNS)&quot;" target="&quot;SCIPY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RNNs are a type of technology used in scientific and technical computing, as they are implemented in various applications within the field, such as language translation, natural language processing (NLP), speech recognition, and image captioning."</data>
      <data key="d6">4246748fef7001ea0bd03ac702565b0d</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS (RNNS)&quot;" target="&quot;TEACHER FORCING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Teacher Forcing is commonly used in training Recurrent Neural Networks (RNNs) to improve their ability to learn sequential data."</data>
      <data key="d6">d0a69d653d08e58959dd8d0f2033e697</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORKS (RNNS)&quot;" target="&quot;ECHO STATE NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recurrent Neural Networks (RNNs) act as a random, nonlinear medium whose dynamic response is used as a signal base in echo state networks."</data>
      <data key="d6">a3368f9cab1f65643dba089af5a1f95e</data>
    </edge>
    <edge source="&quot;BIOLOGICALLY REALISTIC NEURAL NETWORKS (BRNNS)&quot;" target="&quot;CEREBRAL CORTEX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Cerebral Cortex is mentioned in the context of Biologically Realistic Neural Networks (bRNNs), indicating their relationship."</data>
      <data key="d6">9b30fc06ca06f49c2faa238da7eddc6f</data>
    </edge>
    <edge source="&quot;LAMINAR COMPUTING&quot;" target="&quot;LAMINART FAMILY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Laminar Computing is the computational paradigm used by the LAMINART family of models, as mentioned in the text."</data>
      <data key="d6">9b30fc06ca06f49c2faa238da7eddc6f</data>
    </edge>
    <edge source="&quot;LAMINART FAMILY&quot;" target="&quot;CAO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cao is a researcher associated with the LAMINART Family model."</data>
      <data key="d6">6648b18760b8b182e1097ad15c4df685</data>
    </edge>
    <edge source="&quot;LAMINART FAMILY&quot;" target="&quot;RAIZADA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Raizada is a researcher associated with the LAMINART Family model."</data>
      <data key="d6">6648b18760b8b182e1097ad15c4df685</data>
    </edge>
    <edge source="&quot;LIST PARSE MODEL&quot;" target="&quot;PEARSON&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Pearson is a researcher associated with the LIST PARSE Model."</data>
      <data key="d6">6648b18760b8b182e1097ad15c4df685</data>
    </edge>
    <edge source="&quot;CARTWORD MODEL&quot;" target="&quot;KAZEROUNIAN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Kazerounian is a researcher associated with the cARTWORD Model."</data>
      <data key="d6">6648b18760b8b182e1097ad15c4df685</data>
    </edge>
    <edge source="&quot;KAZEROUNIAN&quot;" target="&quot;TELOS MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Kazerounian is a co-author of the study that introduces the TELOS Model."</data>
      <data key="d6">75495c1fc835d41adf5afcb01e8e520a</data>
    </edge>
    <edge source="&quot;KAZEROUNIAN&quot;" target="&quot;LISTELOS MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Kazerounian is not explicitly mentioned in the context of the lisTELOS Model."</data>
      <data key="d6">75495c1fc835d41adf5afcb01e8e520a</data>
    </edge>
    <edge source="&quot;TELOS MODEL&quot;" target="&quot;BG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"BG is a part of the brain mentioned in the context of the TELOS Model.")&lt;|COMPLETE|&gt;The entities identified from the text are:1. ("entity"</data>
      <data key="d6">6648b18760b8b182e1097ad15c4df685</data>
    </edge>
    <edge source="&quot;TELOS MODEL&quot;" target="&quot;LISTELOS MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both models are related as they both deal with learning and choice of eye movements, but the lisTELOS Model involves sequences of saccadic eye movements and a different spatial working memory structure."</data>
      <data key="d6">75495c1fc835d41adf5afcb01e8e520a</data>
    </edge>
    <edge source="&quot;TELOS MODEL&quot;" target="&quot;PREFRONTAL CORTEX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Prefrontal Cortex is a key component of the TELOS Model."</data>
      <data key="d6">75495c1fc835d41adf5afcb01e8e520a</data>
    </edge>
    <edge source="&quot;TELOS MODEL&quot;" target="&quot;FRONTAL EYE FIELDS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Frontal Eye Fields are involved in the TELOS Model."</data>
      <data key="d6">75495c1fc835d41adf5afcb01e8e520a</data>
    </edge>
    <edge source="&quot;LISTELOS MODEL&quot;" target="&quot;PREFRONTAL CORTEX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Prefrontal Cortex is a key component of the lisTELOS Model, storing sequences of saccadic eye movement commands."</data>
      <data key="d6">75495c1fc835d41adf5afcb01e8e520a</data>
    </edge>
    <edge source="&quot;LISTELOS MODEL&quot;" target="&quot;FRONTAL EYE FIELDS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Frontal Eye Fields are involved in the lisTELOS Model."</data>
      <data key="d6">75495c1fc835d41adf5afcb01e8e520a</data>
    </edge>
    <edge source="&quot;POSTERIOR PARIETAL CORTEX (PPC)&quot;" target="&quot;FRONTAL EYE FIELDS (FEF)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"PPC and FEF interact to carry out specific operations."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;POSTERIOR PARIETAL CORTEX (PPC)&quot;" target="&quot;BASAL GANGLIA (BG)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"PPC and BG interact to carry out specific operations."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;POSTERIOR PARIETAL CORTEX (PPC)&quot;" target="&quot;SUPERIOR COLLICULUS (SC)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"PPC and SC interact to carry out specific operations."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;POSTERIOR PARIETAL CORTEX (PPC)&quot;" target="&quot;ARTSCAN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ARTSCAN model involves interactions between Posterior Parietal Cortex (PPC) and other brain regions in simulating view-invariant object learning and visual search."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;POSTERIOR PARIETAL CORTEX (PPC)&quot;" target="&quot;ARTSCENE SEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ARTSCENE Search model involves interactions between Posterior Parietal Cortex (PPC) and other brain regions in simulating object and spatial contextual cueing of visual search."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;SUPERIOR COLLICULUS (SC)&quot;" target="&quot;ARTSCAN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ARTSCAN model involves interactions between Superior Colliculus (SC) and other brain regions in simulating view-invariant object learning and visual search."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;SUPERIOR COLLICULUS (SC)&quot;" target="&quot;ARTSCENE SEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ARTSCENE Search model involves interactions between Superior Colliculus (SC) and other brain regions in simulating object and spatial contextual cueing of visual search."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;MOTIVATOR MODEL&quot;" target="&quot;INFEROTEMPORAL (IT) CORTEX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MOTIVATOR Model interacts with IT Cortex in cognitive-emotional interactions."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;MOTIVATOR MODEL&quot;" target="&quot;RHINAL (RHIN) CORTEX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MOTIVATOR Model interacts with RHIN Cortex in cognitive-emotional interactions."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;MOTIVATOR MODEL&quot;" target="&quot;LATERAL ORBITOFRONTAL CORTEX (ORBL)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MOTIVATOR Model interacts with ORBl in cognitive-emotional interactions."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;MOTIVATOR MODEL&quot;" target="&quot;MEDIAL ORBITOFRONTAL CORTEX (ORBM)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MOTIVATOR Model interacts with ORBm in cognitive-emotional interactions."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;MOTIVATOR MODEL&quot;" target="&quot;AMYGDALA (AMYGD)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MOTIVATOR Model interacts with AMYGD in cognitive-emotional interactions."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;MOTIVATOR MODEL&quot;" target="&quot;LATERAL HYPOTHALAMUS (LH)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MOTIVATOR Model interacts with LH in cognitive-emotional interactions."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;MOTIVATOR MODEL&quot;" target="&quot;BASAL GANGLIA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MOTIVATOR Model interacts with Basal Ganglia in cognitive-emotional interactions."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;BASAL GANGLIA&quot;" target="&quot;REWARD EXPECTATION FILTER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reward Expectation Filter modulates the reward value of stimuli in the Basal Ganglia, which is involved in movement, emotion, and motivation."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;BASAL GANGLIA&quot;" target="&quot;SONGBIRD SINGING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Basal ganglia modulate song performance in songbirds, enhancing their ability to produce complex and varied songs."</data>
      <data key="d6">44ddf121af4b66da2bfd6b2ac0637a23</data>
    </edge>
    <edge source="&quot;BASAL GANGLIA&quot;" target="&quot;FLEXIBLE PERFORMANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Basal Ganglia modulates song performance, indicating its role in flexible performance."</data>
      <data key="d6">7aeda101aa8aba76f319932f0bd568f7</data>
    </edge>
    <edge source="&quot;ARTSCAN MODEL&quot;" target="&quot;VISUAL CORTEX V1&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ARTSCAN Model interacts with Visual Cortex V1 during view-invariant object learning and visual search."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;ARTSCAN MODEL&quot;" target="&quot;VISUAL CORTEX V2&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ARTSCAN Model interacts with Visual Cortex V2 during view-invariant object learning and visual search."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;ARTSCAN MODEL&quot;" target="&quot;VISUAL CORTEX V3A&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ARTSCAN Model interacts with Visual Cortex V3A during view-invariant object learning and visual search."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;ARTSCAN MODEL&quot;" target="&quot;VISUAL CORTEX V4&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ARTSCAN Model interacts with Visual Cortex V4 during view-invariant object learning and visual search."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;ARTSCAN MODEL&quot;" target="&quot;PREFRONTAL CORTEX (PFC)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ARTSCAN Model interacts with Prefrontal Cortex (PFC) during view-invariant object learning and visual search."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;ARTSCAN MODEL&quot;" target="&quot;POSTERIOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ARTSCAN Model interacts with the Posterior region of the brain during view-invariant object learning and visual search."</data>
      <data key="d6">89a24f37cf198d043ccd6b6b795dc232</data>
    </edge>
    <edge source="&quot;PREFRONTAL CORTEX (PFC)&quot;" target="&quot;ARTSCAN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ARTSCAN model involves interactions between Prefrontal Cortex (PFC) and other brain regions in simulating view-invariant object learning and visual search."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;ARTSCAN&quot;" target="&quot;VISUAL CORTICES V1, V2, V3A, AND V4&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ARTSCAN model simulates view-invariant object learning and visual search during unconstrained saccadic eye movements, involving interactions between Visual Cortices V1, V2, V3A, and V4."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;ARTSCAN&quot;" target="&quot;LATERAL INTRAPARIETAL AREA (LIP)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ARTSCAN model involves interactions between Lateral Intraparietal Area (LIP) and other brain regions in simulating view-invariant object learning and visual search."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;ARTSCAN&quot;" target="&quot;POSTERIOR AND ANTERIOR INFEROTEMPORAL CORTEX (PIT, AIT)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ARTSCAN model involves interactions between Posterior and Anterior Inferotemporal Cortex (pIT, aIT) and other brain regions in simulating view-invariant object learning and visual search."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;ARTSCENE SEARCH&quot;" target="&quot;VISUAL CORTICES V1, V2, AND V4&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ARTSCENE Search model simulates object and spatial contextual cueing of visual search for desired objects in a scene, involving interactions between Visual Cortices V1, V2, and V4."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;ARTSCENE SEARCH&quot;" target="&quot;VENTRAL AND DORSOLATERAL PREFRONTAL CORTEX (VPFC, DLPFC)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ARTSCENE Search model involves interactions between Ventral and Dorsolateral Prefrontal Cortex (VPFC, DLPFC) and other brain regions in simulating object and spatial contextual cueing of visual search."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;ARTSCENE SEARCH&quot;" target="&quot;PERIRHINAL CORTEX (PRC)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ARTSCENE Search model involves interactions between Perirhinal Cortex (PRC) and other brain regions in simulating object and spatial contextual cueing of visual search."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;ARTSCENE SEARCH&quot;" target="&quot;PARAHIPPOCAMPAL CORTEX (PHC)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ARTSCENE Search model involves interactions between Parahippocampal Cortex (PHC) and other brain regions in simulating object and spatial contextual cueing of visual search."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;ARTSCENE SEARCH&quot;" target="&quot;POSTERIOR AND ANTERIOR INFEROTEMPORAL CORTEX (ITA, ITP)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ARTSCENE Search model involves interactions between Posterior and Anterior Inferotemporal Cortex (ITa, ITp) and other brain regions in simulating object and spatial contextual cueing of visual search."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;GRIDPLACEMAP&quot;" target="&quot;BRAIN REGIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The GridPlaceMap model simulates the formation of a grid cell representation of space, involving interactions between various brain regions."</data>
      <data key="d6">db27e91f327c97c16df11a500cdeab4d</data>
    </edge>
    <edge source="&quot;SPATIAL PATTERN LEARNING&quot;" target="&quot;GENERALIZED ADDITIVE RNNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Generalized Additive RNNs is a model capable of learning spatial patterns unbiasedly, through the interaction of STM and LTM."</data>
      <data key="d6">b881b9051ad24c6a16b468803fba51d3</data>
    </edge>
    <edge source="&quot;SPATIAL PATTERN LEARNING&quot;" target="&quot;MATHEMATICAL THEOREMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Mathematical Theorems provide a foundation for the capability of certain models to learn spatial patterns unbiasedly."</data>
      <data key="d6">b881b9051ad24c6a16b468803fba51d3</data>
    </edge>
    <edge source="&quot;GENERALIZED ADDITIVE RNNS&quot;" target="&quot;MATHEMATICAL THEOREMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The effectiveness of Generalized Additive RNNs is based on a foundation of Mathematical Theorems that demonstrate how STM and LTM laws can be joined to achieve unbiased learning."</data>
      <data key="d6">5812b5d4bcdfbf80de28dca56a6559b3</data>
    </edge>
    <edge source="&quot;SIGNAL VELOCITY&quot;" target="&quot;AXON LENGTH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Signal Velocity is not directly controlled by Axon Length, but they can be related through Axon Diameter."</data>
      <data key="d6">18be9bfe53d3b9c1e15c1c8238674459</data>
    </edge>
    <edge source="&quot;AXON LENGTH&quot;" target="&quot;AXON DIAMETER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Longer axons are often thicker, with Axon Diameter influencing Axon Length."</data>
      <data key="d6">18be9bfe53d3b9c1e15c1c8238674459</data>
    </edge>
    <edge source="&quot;LTM TRACES&quot;" target="&quot;GENERALIZED ADDITIVE SYSTEM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LTM Traces are a component of the Generalized Additive System, representing the adaptive weights of the system."</data>
      <data key="d6">8bb0e63353e66a2c60a878028beff5f9</data>
    </edge>
    <edge source="&quot;GENERALIZED ADDITIVE SYSTEM&quot;" target="&quot;SAMPLED CELLS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sampled Cells are a component of the Generalized Additive System, representing the cells that are being observed or sampled."</data>
      <data key="d6">8bb0e63353e66a2c60a878028beff5f9</data>
    </edge>
    <edge source="&quot;GENERALIZED ADDITIVE SYSTEM&quot;" target="&quot;SAMPLING CELLS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sampling Cells are a component of the Generalized Additive System, representing the cells that are actively sampling the system."</data>
      <data key="d6">8bb0e63353e66a2c60a878028beff5f9</data>
    </edge>
    <edge source="&quot;UNBIASED SPATIAL PATTERN LEARNING THEOREM&quot;" target="&quot;CONDITIONED STIMULI (CS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Unbiased Spatial Pattern Learning Theorem proves how unbiased learning may occur in response to correlated Conditioned Stimuli."</data>
      <data key="d6">4959d1559344e462a6a7463fd3273659</data>
    </edge>
    <edge source="&quot;UNBIASED SPATIAL PATTERN LEARNING THEOREM&quot;" target="&quot;UNCONDITIONED STIMULI (US)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Unbiased Spatial Pattern Learning Theorem proves how unbiased learning may occur in response to correlated Unconditioned Stimuli."</data>
      <data key="d6">4959d1559344e462a6a7463fd3273659</data>
    </edge>
    <edge source="&quot;UNBIASED SPATIAL PATTERN LEARNING THEOREM&quot;" target="&quot;SELF-ORGANIZING AVALANCHE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Self-Organizing Avalanche system learns spatial patterns based on the guarantee provided by the Unbiased Spatial Pattern Learning Theorem."</data>
      <data key="d6">6e8f1f4e6c7865b14f3b5665aa62e12e</data>
    </edge>
    <edge source="&quot;OUTSTAR LEARNING THEOREM&quot;" target="&quot;STANLEY GROSSBERG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Stanley Grossberg is the author of the Outstar Learning Theorem."</data>
      <data key="d6">be49e9beb2d7cc985fe9f6517fa0f4fe</data>
    </edge>
    <edge source="&quot;OUTSTAR LEARNING THEOREM&quot;" target="&quot;INSTAR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Outstar Learning Theorem is used in conjunction with the Instar in the Sparse Stable Category Learning Theorem."</data>
      <data key="d6">be49e9beb2d7cc985fe9f6517fa0f4fe</data>
    </edge>
    <edge source="&quot;STANLEY GROSSBERG&quot;" target="&quot;SPARSE STABLE CATEGORY LEARNING THEOREM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Stanley Grossberg is the author of the Sparse Stable Category Learning Theorem."</data>
      <data key="d6">be49e9beb2d7cc985fe9f6517fa0f4fe</data>
    </edge>
    <edge source="&quot;SPARSE STABLE CATEGORY LEARNING THEOREM&quot;" target="&quot;COMPETITIVE LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Sparse Stable Category Learning Theorem involves multiple Instars competing with each other via a RCF to form a Competitive Learning network."</data>
      <data key="d6">be49e9beb2d7cc985fe9f6517fa0f4fe</data>
    </edge>
    <edge source="&quot;SPARSE STABLE CATEGORY LEARNING THEOREM&quot;" target="&quot;SELF-ORGANIZING MAP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Sparse Stable Category Learning Theorem is also known as a Self-Organizing Map network."</data>
      <data key="d6">be49e9beb2d7cc985fe9f6517fa0f4fe</data>
    </edge>
    <edge source="&quot;COMPETITIVE LEARNING&quot;" target="&quot;SELF-ORGANIZING MAPS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Competitive Learning and Self-Organizing Maps are mentioned together as methods used to allow source cells to self-organize."</data>
      <data key="d6">eb6c9a7d24cc59ff93d554093a4360a4</data>
    </edge>
    <edge source="&quot;COMPETITIVE LEARNING&quot;" target="&quot;INSTAR-OUTSTAR MAPS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Instar-Outstar maps are mentioned as a type of learning circuit that includes learned Instars and Outstars, which are the result of Competitive Learning."</data>
      <data key="d6">eb6c9a7d24cc59ff93d554093a4360a4</data>
    </edge>
    <edge source="&quot;EVENT SEQUENCES&quot;" target="&quot;LIST CHUNKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Event sequences are grouped into unitized plans, or list chunks, through learning."</data>
      <data key="d6">5c4e24fc9bd10d0bd59a84d56f960cf9</data>
    </edge>
    <edge source="&quot;LIST CHUNKS&quot;" target="&quot;SERIAL LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"List Chunks are units that are introduced to explain sequence-sensitive contextual control in Serial Learning."</data>
      <data key="d6">cdd2935776d71ef9fd3a33979af0b9b5</data>
    </edge>
    <edge source="&quot;LTM INVARIANCE PRINCIPLE&quot;" target="&quot;GROSSBERG AND PEARSON&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grossberg and Pearson are the authors of a model that generalizes Item-and-Order working memories to include rank information, which is based on the LTM Invariance Principle."</data>
      <data key="d6">c38beddeac1d3cac8282ad59bc835788</data>
    </edge>
    <edge source="&quot;LTM INVARIANCE PRINCIPLE&quot;" target="&quot;VENTROLATERAL PREFRONTAL CORTEX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ventrolateral Prefrontal Cortex is a brain region where the deeper layers are predicted to realize verbal, spatial, and motor working memories, based on the LTM Invariance Principle."</data>
      <data key="d6">c38beddeac1d3cac8282ad59bc835788</data>
    </edge>
    <edge source="&quot;FRONTAL CORTEX&quot;" target="&quot;ITEM-AND-ORDER WORKING MEMORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Frontal Cortex is mentioned in the context of Item-and-Order Working Memory, suggesting its involvement in this cognitive function."</data>
      <data key="d6">296fa3812e2c81f685d8cdc403cb03dd</data>
    </edge>
    <edge source="&quot;FRONTAL CORTEX&quot;" target="&quot;SONGBIRD SINGING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Frontal cortex modulates song performance in songbirds, contributing to their flexible singing behavior."</data>
      <data key="d6">44ddf121af4b66da2bfd6b2ac0637a23</data>
    </edge>
    <edge source="&quot;FRONTAL CORTEX&quot;" target="&quot;FLEXIBLE PERFORMANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Frontal Cortex modulates song performance, indicating its role in flexible performance."</data>
      <data key="d6">7aeda101aa8aba76f319932f0bd568f7</data>
    </edge>
    <edge source="&quot;ITEM-AND-ORDER WORKING MEMORY&quot;" target="&quot;ITEM-ORDER-RANK WORKING MEMORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Item-Order-Rank Working Memory is a variant or extension of Item-and-Order Working Memory, sharing a similar conceptual framework."</data>
      <data key="d6">296fa3812e2c81f685d8cdc403cb03dd</data>
    </edge>
    <edge source="&quot;ITEM-AND-ORDER WORKING MEMORY&quot;" target="&quot;FREE RECALL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Free Recall data were a source of inspiration for the discovery of Item-and-Order Working Memory, as the patterns observed in free recall influenced the design of this cognitive model."</data>
      <data key="d6">296fa3812e2c81f685d8cdc403cb03dd</data>
    </edge>
    <edge source="&quot;BRADSKI ET AL.&quot;" target="&quot;STORE 1 MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Bradski et al. defined the STORE 1 model, which is a family of Item-and-Order RNNs with mathematically provable primacy, recency, and bowed gradient properties."</data>
      <data key="d6">392028b79561bd7471cb68e7c9258b1e</data>
    </edge>
    <edge source="&quot;BRADSKI ET AL.&quot;" target="&quot;1992&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"1992 is the year when Bradski et al. defined the STORE 1 model."</data>
      <data key="d6">392028b79561bd7471cb68e7c9258b1e</data>
    </edge>
    <edge source="&quot;BRADSKI ET AL.&quot;" target="&quot;1994&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"1994 is the year when Bradski et al. further developed the STORE model."</data>
      <data key="d6">392028b79561bd7471cb68e7c9258b1e</data>
    </edge>
    <edge source="&quot;BOARDMAN AND BULLOCK&quot;" target="&quot;ITEM-AND-ORDER WM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Boardman and Bullock have developed variants of the Item-and-Order working memory design."</data>
      <data key="d6">c3257facbf1b0a5da49d6a115f66df87</data>
    </edge>
    <edge source="&quot;HOUGHTON AND HARTLEY&quot;" target="&quot;ITEM-AND-ORDER WM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Houghton and Hartley have contributed to the Item-and-Order working memory design."</data>
      <data key="d6">c3257facbf1b0a5da49d6a115f66df87</data>
    </edge>
    <edge source="&quot;PAGE AND NORRIS&quot;" target="&quot;ITEM-AND-ORDER WM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Page and Norris have developed variants of the Item-and-Order working memory design."</data>
      <data key="d6">c3257facbf1b0a5da49d6a115f66df87</data>
    </edge>
    <edge source="&quot;RHODES ET AL.&quot;" target="&quot;ITEM-AND-ORDER WM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Rhodes et al. have developed variants of the Item-and-Order working memory design."</data>
      <data key="d6">c3257facbf1b0a5da49d6a115f66df87</data>
    </edge>
    <edge source="&quot;HOUGHTON&quot;" target="&quot;ITEM-AND-ORDER WM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Houghton has referred to Item-and-Order models as Competitive Queuing models."</data>
      <data key="d6">c3257facbf1b0a5da49d6a115f66df87</data>
    </edge>
    <edge source="&quot;FARRELL AND LEWANDOWSKY&quot;" target="&quot;ITEM-AND-ORDER WM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Farrell and Lewandowsky have provided experimental support for Item-and-Order working memory properties."</data>
      <data key="d6">c3257facbf1b0a5da49d6a115f66df87</data>
    </edge>
    <edge source="&quot;AVERBECK ET AL.&quot;" target="&quot;ITEM-AND-ORDER WM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Averbeck et al. have reported neurophysiological evidence supporting the primacy gradient in Item-and-Order working memory."</data>
      <data key="d6">c3257facbf1b0a5da49d6a115f66df87</data>
    </edge>
    <edge source="&quot;MILLER&quot;" target="&quot;IMMEDIATE MEMORY SPAN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Miller proposed the concept of the Immediate Memory Span, which limits the number of items that can be held in short-term memory."</data>
      <data key="d6">b69b23b14e0feccb488ba5412db0824c</data>
    </edge>
    <edge source="&quot;IMMEDIATE MEMORY SPAN&quot;" target="&quot;TRANSIENT MEMORY SPAN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Immediate Memory Span and Transient Memory Span are related concepts, with the latter suggesting a more dynamic and temporary holding capacity for items in memory."</data>
      <data key="d6">b69b23b14e0feccb488ba5412db0824c</data>
    </edge>
    <edge source="&quot;STORE 1 MODEL&quot;" target="&quot;SKI ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ski et al. are the authors of the STORE 1 Model, having introduced and developed it in their studies."</data>
      <data key="d6">9c685cea284029fa1f27ebaa280615a5</data>
    </edge>
    <edge source="&quot;STORE 1 MODEL&quot;" target="&quot;1992&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The STORE 1 Model was introduced by Ski et al. in 1992."</data>
      <data key="d6">9c685cea284029fa1f27ebaa280615a5</data>
    </edge>
    <edge source="&quot;STORE 1 MODEL&quot;" target="&quot;1994&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ski et al. further developed the STORE 1 Model in 1994."</data>
      <data key="d6">9c685cea284029fa1f27ebaa280615a5</data>
    </edge>
    <edge source="&quot;SERIAL LEARNING&quot;" target="&quot;SELF-ORGANIZING AVALANCHE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Self-Organizing Avalanche is a learning mechanism that can learn through time, a process known as serial learning."</data>
      <data key="d6">2ea6b3379a87077d75e5c45024f4f3e2</data>
    </edge>
    <edge source="&quot;SERIAL LEARNING&quot;" target="&quot;YOUNG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Young expresses skepticism about the usefulness of Serial Learning methods for studying verbal learning processes."</data>
      <data key="d6">cdd2935776d71ef9fd3a33979af0b9b5</data>
    </edge>
    <edge source="&quot;SERIAL LEARNING&quot;" target="&quot;UNDERWOOD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Underwood criticizes the applicability of Serial Learning methods in verbal learning research."</data>
      <data key="d6">cdd2935776d71ef9fd3a33979af0b9b5</data>
    </edge>
    <edge source="&quot;SERIAL LEARNING&quot;" target="&quot;VERBAL LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Serial Learning is a method that can influence Verbal Learning, as new verbal units are synthesized and the context of previous events can determine subsequent responses."</data>
      <data key="d6">cdd2935776d71ef9fd3a33979af0b9b5</data>
    </edge>
    <edge source="&quot;AVALANCHE&quot;" target="&quot;HVC-RA NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Avalanche circuit is a component of the HVC-RA Network that controls songbird singing."</data>
      <data key="d6">44ddf121af4b66da2bfd6b2ac0637a23</data>
    </edge>
    <edge source="&quot;AVALANCHE&quot;" target="&quot;COMMAND CELLS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Command cells activate the Avalanche circuit once a pulse is received, initiating a sequential activation of Outstars."
"The Avalanche circuit requires command cells for sensitivity to environmental feedback."</data>
      <data key="d6">44ddf121af4b66da2bfd6b2ac0637a23,7aeda101aa8aba76f319932f0bd568f7</data>
    </edge>
    <edge source="&quot;AVALANCHE&quot;" target="&quot;NONSPECIFIC AROUSAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The activation of the Avalanche circuit may lead to nonspecific arousal, increasing the alertness or readiness of the animal."</data>
      <data key="d6">44ddf121af4b66da2bfd6b2ac0637a23</data>
    </edge>
    <edge source="&quot;AVALANCHE&quot;" target="&quot;OUTSTARS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Outstars within the Avalanche circuit can fire only if activated by a command cell."</data>
      <data key="d6">7aeda101aa8aba76f319932f0bd568f7</data>
    </edge>
    <edge source="&quot;COMMAND CELLS&quot;" target="&quot;STEIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Stein has studied the role of command cells in controlling the rhythmic beating of crayfish swimmerets."</data>
      <data key="d6">7aeda101aa8aba76f319932f0bd568f7</data>
    </edge>
    <edge source="&quot;SELF-ORGANIZING AVALANCHE&quot;" target="&quot;DR. PAUL GROSSBERG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dr. Paul Grossberg is mentioned as a researcher who has contributed to the development of the Self-Organizing Avalanche system."</data>
      <data key="d6">6e8f1f4e6c7865b14f3b5665aa62e12e</data>
    </edge>
    <edge source="&quot;SELF-ORGANIZING AVALANCHE&quot;" target="&quot;CONTEXT-SENSITIVE SELF-ORGANIZING AVALANCHE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Context-Sensitive Self-Organizing Avalanche is an extension of the Self-Organizing Avalanche that can learn sequences of previous events and make decisions based on whole sequences."</data>
      <data key="d6">2ea6b3379a87077d75e5c45024f4f3e2</data>
    </edge>
    <edge source="&quot;MATHEMATICAL ANALYSIS&quot;" target="&quot;CLASSICAL DATA PROPERTIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Classical data properties are explained through mathematical analysis, such as the work provided by Grossberg."</data>
      <data key="d6">2ea6b3379a87077d75e5c45024f4f3e2</data>
    </edge>
    <edge source="&quot;MATHEMATICAL ANALYSIS&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES2N reservoir model is subjected to a Mathematical Analysis to understand its behavior."</data>
      <data key="d6">90c1a399dd410f75f1f4bb03fe1f5f33</data>
    </edge>
    <edge source="&quot;YOUNG&quot;" target="&quot;ESP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Young contributes to the algebraic conditions for additive-sigmoid neuron reservoirs, which are relevant to the ESP.")</data>
      <data key="d6">3b592e5ac113a5c031925f91a182baa6</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;WIKIPEDIA PAGE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Wikipedia page is a resource where more information about Echo State Networks can be found."</data>
      <data key="d6">83fafb2423a01afae7e522917d79ace9</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;RANDOM HIGH-DIMENSIONAL VECTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks use a reservoir to generate a Random High-Dimensional Vector, which is then used for efficient learning and decoding."</data>
      <data key="d6">82a734e7c7ada95b1c99783140dd7168</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;READOUT LAYER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks include a Readout Layer that is trained to decode the high-dimensional activation vectors from the reservoir and produce accurate predictions."</data>
      <data key="d6">0e6f0f7cd882a638ecb571ef36068868</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;GITHUB&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks are mentioned in the text in the context of resources and tutorials available on GitHub."</data>
      <data key="d6">0e6f0f7cd882a638ecb571ef36068868</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;INPUT-TO-READOUT CONNECTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input-to-readout connections are a feature used in Echo State Networks to enhance the model's ability to capture and utilize relevant input data."</data>
      <data key="d6">8965403859beb43a6ab7e5c8c916b857</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;HYPEROPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt is used in the paper to optimize the hyperparameters of Echo State Networks."</data>
      <data key="d6">46913f0d73ba0b8cecfdf42bde9862f4</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;HYPERPARAMETER OPTIMIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperparameter Optimization is the process of finding the best hyperparameters for Echo State Networks to improve their performance."</data>
      <data key="d6">46913f0d73ba0b8cecfdf42bde9862f4</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;HYPERPARAMETERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The text discusses the importance of understanding and optimizing hyperparameters in the context of Echo State Networks."</data>
      <data key="d6">73e81fd6509a2ba400a8435793ade3c5</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;MACKEY-GLASS TIMES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The text uses the Mackey-Glass Times data set for testing and benchmarking Echo State Networks."</data>
      <data key="d6">73e81fd6509a2ba400a8435793ade3c5</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;RNN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks are designed to train RNNs, as mentioned in the text."</data>
      <data key="d6">f0b3b2a88425b0563005400ea246528b</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;SCHMIDHUBER ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Schmidhuber et al. have used margin-maximization criteria in the context of Echo State Networks, as mentioned in the text."</data>
      <data key="d6">f0b3b2a88425b0563005400ea246528b</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;SIGMOID UNIT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks use Sigmoid Units in their basic discrete-time structure, as mentioned in the text."</data>
      <data key="d6">f0b3b2a88425b0563005400ea246528b</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;JAEGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jaeger has contributed to the theoretical research on Echo State Networks."</data>
      <data key="d6">5a9eaff8c67e594f49fae0318a502c6a</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;WHITE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"White has contributed to the theoretical research on Echo State Networks."</data>
      <data key="d6">5a9eaff8c67e594f49fae0318a502c6a</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;SOMPOLINSKY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sompolinsky has contributed to the theoretical research on Echo State Networks."</data>
      <data key="d6">5a9eaff8c67e594f49fae0318a502c6a</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;HERMANS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hermans has contributed to the theoretical research on Echo State Networks."</data>
      <data key="d6">5a9eaff8c67e594f49fae0318a502c6a</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;SCHRAUWEN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Schrauwen has contributed to the theoretical research on Echo State Networks."</data>
      <data key="d6">5a9eaff8c67e594f49fae0318a502c6a</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;SCHMIDHUBER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Schmidhuber has contributed to the theoretical research on Echo State Networks and their limitations."</data>
      <data key="d6">5a9eaff8c67e594f49fae0318a502c6a</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;MAASS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Maass has contributed to the theoretical research on Echo State Networks and their ability to realize unbounded memory spans."
"Maass is a researcher who made significant contributions to the development of Echo State Networks and their applications in Machine Learning."</data>
      <data key="d6">5a9eaff8c67e594f49fae0318a502c6a,6de297d888d10db4c987b5eafc6398b2</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;NATSCHLAEGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Natschlaeger has contributed to the theoretical research on Echo State Networks."</data>
      <data key="d6">5a9eaff8c67e594f49fae0318a502c6a</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;MARKRAM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Markram has contributed to the theoretical research on Echo State Networks."</data>
      <data key="d6">5a9eaff8c67e594f49fae0318a502c6a</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;BOUNDED MEMORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks are limited by their bounded memory capacity."</data>
      <data key="d6">5a9eaff8c67e594f49fae0318a502c6a</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;UNBOUNDED MEMORY SPANS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks can realize unbounded memory spans through the use of output units with feedback to the reservoir."</data>
      <data key="d6">5a9eaff8c67e594f49fae0318a502c6a</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;STATISTICAL LEARNING THEORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Statistical Learning Theory provides the foundation for optimizing various biases in Echo State Networks, including manual experimentation."</data>
      <data key="d6">2a2a93486d6198ce228e77e120dc3c0c</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;REAL-TIME RECURRENT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Real-time Recurrent Learning is a traditional RNN algorithm that Echo State Networks have outperformed."</data>
      <data key="d6">2a2a93486d6198ce228e77e120dc3c0c</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;BACKPROPAGATION THROUGH TIME&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Backpropagation Through Time is a traditional RNN algorithm that Echo State Networks have outperformed."</data>
      <data key="d6">2a2a93486d6198ce228e77e120dc3c0c</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;EXTENDED KALMAN FILTERING BASED METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Extended Kalman Filtering Based Methods is a traditional RNN algorithm that Echo State Networks have outperformed."</data>
      <data key="d6">2a2a93486d6198ce228e77e120dc3c0c</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;ATIYA-PARLOS ALGORITHM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Atiya-Parlos Algorithm is a traditional RNN algorithm that Echo State Networks have outperformed."</data>
      <data key="d6">2a2a93486d6198ce228e77e120dc3c0c</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;2010&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks started to gain relevance and popularity around the year 2010."</data>
      <data key="d6">257d4cf08ffc32b99856b6e31fa4221e</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;OPTICAL MICROCHIPS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks have been used in conjunction with Optical Microchips."</data>
      <data key="d6">257d4cf08ffc32b99856b6e31fa4221e</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;MECHANICAL NANO-OSCILLATORS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks have been used in conjunction with Mechanical Nano-oscillators."</data>
      <data key="d6">257d4cf08ffc32b99856b6e31fa4221e</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;MEMRISTOR-BASED NEUROMORPHIC MICROCHIPS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks have been used in conjunction with Memristor-based Neuromorphic Microchips."</data>
      <data key="d6">257d4cf08ffc32b99856b6e31fa4221e</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;CARBON-NANOTUBE / POLYMER MIXTURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks have been used in conjunction with Carbon-nanotube / Polymer Mixtures."</data>
      <data key="d6">257d4cf08ffc32b99856b6e31fa4221e</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;ARTIFICIAL SOFT LIMBS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks have been used in conjunction with Artificial Soft Limbs."</data>
      <data key="d6">257d4cf08ffc32b99856b6e31fa4221e</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;SIGNAL PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks have been found to be relevant and popular in the field of Signal Processing."</data>
      <data key="d6">257d4cf08ffc32b99856b6e31fa4221e</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;BIOSIGNAL PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks have been used in the application of Biosignal Processing, as mentioned by Kudithipudi et al. (2015)."</data>
      <data key="d6">257d4cf08ffc32b99856b6e31fa4221e</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;REMOTE SENSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks have been used in the application of Remote Sensing, as mentioned by Antonelo (2017)."</data>
      <data key="d6">257d4cf08ffc32b99856b6e31fa4221e</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;ROBOT MOTOR CONTROL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks have been used in the application of Robot Motor Control, as mentioned by Polydoros et al. (2015)."</data>
      <data key="d6">257d4cf08ffc32b99856b6e31fa4221e</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;EDGE OF STABILITY ECHO STATE NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Edge of Stability Echo State Network is a model that is based on Echo State Networks."</data>
      <data key="d6">578045eb341c5e05d5a912f634854499</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;ECHO STATE PROPERTY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks work under the Echo State Property principle."</data>
      <data key="d6">578045eb341c5e05d5a912f634854499</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;BAM NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"BAM Network and Echo State Networks are both types of neural networks, with BAM Network having two layers and Echo State Networks having a sparsely connected random hidden layer."</data>
      <data key="d6">423cdb622c47fa8cec25f22eb9f9f01f</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;LIQUID STATE MACHINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Liquid State Machine is a variant of Echo State Networks for spiking neurons."</data>
      <data key="d6">423cdb622c47fa8cec25f22eb9f9f01f</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;FIT METHOD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Fit Method is used to train the echo state network model by optimizing the parameters of the readout layer."</data>
      <data key="d6">ed28ba3543e07641536ff1eb5e0749dd</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;RUN METHOD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Run Method is used to generate predictions or forecasts using the trained echo state network model."</data>
      <data key="d6">ed28ba3543e07641536ff1eb5e0749dd</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;WARMUP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Warmup is a technique used in echo state networks to initialize the reservoir with a sequence of input data before making predictions or forecasts."</data>
      <data key="d6">ed28ba3543e07641536ff1eb5e0749dd</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;WOLFGANG MAASS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks are related to Liquid State Machines, which were independently developed by Wolfgang Maass."</data>
      <data key="d6">158f53cd85edbb4f2e4c77b78c5e7acc</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;DIFFERENTIAL EQUATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks can include physical models defined by differential equations."</data>
      <data key="d6">10112a11d47463e2aad7352c52922d61</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;RANDOM, NONLINEAR MEDIUM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks use a fixed RNN as a random, nonlinear medium to process input signals."</data>
      <data key="d6">10112a11d47463e2aad7352c52922d61</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;TIME SERIES PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks have been shown to perform well on time series prediction tasks."</data>
      <data key="d6">10112a11d47463e2aad7352c52922d61</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;AUTODIFFERENTIATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Autodifferentiation is a technique used in deep learning libraries that has made Echo State Networks less error-prone and faster to train."</data>
      <data key="d6">10112a11d47463e2aad7352c52922d61</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;LSTM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LSTM is a type of recurrent neural network architecture that has been developed to address the unique selling point of Echo State Networks, which has been lost with the advent of autodifferentiation libraries."</data>
      <data key="d6">10112a11d47463e2aad7352c52922d61</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS&quot;" target="&quot;GRU&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GRU is a type of recurrent neural network architecture that is similar to LSTM and has also been developed to address the unique selling point of Echo State Networks."</data>
      <data key="d6">10112a11d47463e2aad7352c52922d61</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORK&quot;" target="&quot;RESERVOIR NODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A Recurrent Neural Network contains reservoir nodes, which are responsible for updating the reservoir's state and transforming the input data into high-dimensional representations."</data>
      <data key="d6">e1bf3df1ff001613df1451d6d8bf3ee4</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORK&quot;" target="&quot;ARTIFICIAL NEURAL NETWORK&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"A Recurrent Neural Network is a type of Artificial Neural Network that processes sequences of inputs using internal state and allows outputs from some nodes to affect future inputs to the same nodes."
"Recurrent Neural Networks are a type of Artificial Neural Network, characterized by the direction of the flow of information between their layers."</data>
      <data key="d6">e1bf3df1ff001613df1451d6d8bf3ee4,f5b970cf7201f4a918d8bd6a1267657c</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORK&quot;" target="&quot;HANDWRITING RECOGNITION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recurrent Neural Networks are suitable for tasks such as handwriting recognition due to their ability to process arbitrary sequences of inputs."</data>
      <data key="d6">f5b970cf7201f4a918d8bd6a1267657c</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORK&quot;" target="&quot;SPEECH RECOGNITION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recurrent Neural Networks are suitable for tasks such as speech recognition due to their ability to process temporal dynamic behavior."</data>
      <data key="d6">f5b970cf7201f4a918d8bd6a1267657c</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORK&quot;" target="&quot;TIME SERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A time series is mentioned as input to the Recurrent Neural Network, which processes all entries of the time series through the layers of the neural network."</data>
      <data key="d6">5445391448d4ac43471e2bce5eb41a70</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORK&quot;" target="&quot;MODERN LIBRARIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Modern Libraries are mentioned as providers of runtime-optimized implementations for recurrent neural networks."</data>
      <data key="d6">5445391448d4ac43471e2bce5eb41a70</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORK&quot;" target="&quot;ECHO STATE NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Network uses a Recurrent Neural Network."</data>
      <data key="d6">dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </edge>
    <edge source="&quot;TIME SERIES FORECASTING&quot;" target="&quot;TIME SERIES ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Analysis is a fundamental method used in Time Series Forecasting to predict future values based on previously observed values."</data>
      <data key="d6">dc76db79c20c315f30e0297619904b6f</data>
    </edge>
    <edge source="&quot;TIME SERIES FORECASTING&quot;" target="&quot;TIME REVERSIBILITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Reversibility is a characteristic of time series models that influences Time Series Forecasting, expressing values for a given period as deriving from past values, rather than future values."</data>
      <data key="d6">dc76db79c20c315f30e0297619904b6f</data>
    </edge>
    <edge source="&quot;TIME SERIES FORECASTING&quot;" target="&quot;STOCHASTIC MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Stochastic Models are used in Time Series Forecasting to account for the relationship between observations close together in time."</data>
      <data key="d6">e94f386a2ed7de2156b4864797cc199e</data>
    </edge>
    <edge source="&quot;TIME SERIES FORECASTING&quot;" target="&quot;REAL-VALUED CONTINUOUS DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Real-Valued Continuous Data is a type of data that can be used in Time Series Forecasting."</data>
      <data key="d6">e94f386a2ed7de2156b4864797cc199e</data>
    </edge>
    <edge source="&quot;TIME SERIES FORECASTING&quot;" target="&quot;DISCRETE NUMERIC DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Discrete Numeric Data is a type of data that can be used in Time Series Forecasting."</data>
      <data key="d6">e94f386a2ed7de2156b4864797cc199e</data>
    </edge>
    <edge source="&quot;TIME SERIES FORECASTING&quot;" target="&quot;DISCRETE SYMBOLIC DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Discrete Symbolic Data is a type of data that can be used in Time Series Forecasting."</data>
      <data key="d6">e94f386a2ed7de2156b4864797cc199e</data>
    </edge>
    <edge source="&quot;TIME SERIES FORECASTING&quot;" target="&quot;FEEDBACK CONNECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feedback connections enable the network to remember and utilize past information for current processing, which is beneficial for time series forecasting tasks."</data>
      <data key="d6">57a27a1504a5ef7d330172c0ac1085c9</data>
    </edge>
    <edge source="&quot;TIME SERIES FORECASTING&quot;" target="&quot;'&lt;&lt;' OPERATOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time series forecasting uses the '&lt;&lt;' operator to incorporate previous predictions into current processing."</data>
      <data key="d6">c4b54c2da2dda7e660de7bd6de6f13b4</data>
    </edge>
    <edge source="&quot;TIME SERIES FORECASTING&quot;" target="&quot;CLOSED LOOP GENERATIVE MODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Closed Loop Generative Mode is a technique used in Time Series Forecasting to generate subsequent predictions based on the output of the model."</data>
      <data key="d6">05ba4f2e1a9472bd286417154cb0c0d4</data>
    </edge>
    <edge source="&quot;TIME SERIES FORECASTING&quot;" target="&quot;NUMPY.VSTACK()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"numpy.vstack() is used in Time Series Forecasting to combine arrays for plotting a single timeseries."</data>
      <data key="d6">05ba4f2e1a9472bd286417154cb0c0d4</data>
    </edge>
    <edge source="&quot;TIME SERIES FORECASTING&quot;" target="&quot;TIME SERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Forecasting is the process of predicting future values in a Time Series."</data>
      <data key="d6">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </edge>
    <edge source="&quot;TIME SERIES FORECASTING&quot;" target="&quot;ESN MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network (ESN) model is used for Time Series Forecasting, which is the primary task performed by the ESN Model."</data>
      <data key="d6">52d001cd1786e3d9f36e0c57538bc21e</data>
    </edge>
    <edge source="&quot;INRIA&quot;" target="&quot;XAVIER HINAUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Xavier Hinaut is a contact person for the library and works at INRIA."</data>
      <data key="d6">20b16c2e1cb8813ade96fea5f9591631</data>
    </edge>
    <edge source="&quot;INRIA&quot;" target="&quot;BORDEAUX&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Inria is located at Bordeaux, France."
"Inria is located in Bordeaux, France."</data>
      <data key="d6">296bb6eb4ef7d170f7224efcccbbbaf7,2a197220a94bac0b44fc0b07712e45ba</data>
    </edge>
    <edge source="&quot;RESERVOIRPY DOCUMENTATION&quot;" target="&quot;ESNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirPy documentation provides information about creating Echo State Networks."</data>
      <data key="d6">6be085e79e86abc5b1a7eaff6bda1ec5</data>
    </edge>
    <edge source="&quot;RESERVOIRPY DOCUMENTATION&quot;" target="&quot;NP.PI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ReservoirPy documentation is mentioned in the context of np.pi, suggesting that it may contain information about np.pi."</data>
      <data key="d6">475ca77684df5045266ddf079f2e37f1</data>
    </edge>
    <edge source="&quot;WIKIPEDIA PAGE&quot;" target="&quot;ESNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Wikipedia page provides information about Echo State Networks."</data>
      <data key="d6">6be085e79e86abc5b1a7eaff6bda1ec5</data>
    </edge>
    <edge source="&quot;ESNS&quot;" target="&quot;MAASS, JOSHI &amp; SONTAG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Maass, Joshi &amp; Sontag are the authors of a research paper that contributes to the theoretical properties of ESNs."</data>
      <data key="d6">e805d3f438bd9c485639f1c69f917ae5</data>
    </edge>
    <edge source="&quot;ESNS&quot;" target="&quot;PASCANU &amp; JAEGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Pascanu &amp; Jaeger are the authors of a research paper that introduces an ESN-based model of working memory."</data>
      <data key="d6">e805d3f438bd9c485639f1c69f917ae5</data>
    </edge>
    <edge source="&quot;ESNS&quot;" target="&quot;MAASS, NATSCHLAEGER &amp; MARKRAM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Maass, Natschlaeger &amp; Markram are the authors of a research paper on Liquid State Machines, which is a theoretical framework related to ESNs."</data>
      <data key="d6">e805d3f438bd9c485639f1c69f917ae5</data>
    </edge>
    <edge source="&quot;ESNS&quot;" target="&quot;NONLINEAR MODELING TASKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESNs are a type of neural network used in practical nonlinear modeling tasks."</data>
      <data key="d6">e805d3f438bd9c485639f1c69f917ae5</data>
    </edge>
    <edge source="&quot;ESNS&quot;" target="&quot;RNNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESNs are introduced as an alternative to RNNs due to their fast and simple training algorithms, outperforming other methods in benchmark tasks."</data>
      <data key="d6">eafe89ad19a57846f953a1dfcf8571f8</data>
    </edge>
    <edge source="&quot;ESNS&quot;" target="&quot;DEEP LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESNs are still a viable alternative when the modeled system is not too complex and fast, cheap, and adaptive training is desired, especially in applications like biosignal processing and remote sensing."</data>
      <data key="d6">eafe89ad19a57846f953a1dfcf8571f8</data>
    </edge>
    <edge source="&quot;ESNS&quot;" target="&quot;RC MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESNs are a popular flavor of RC, which consists of a RNN of recurrently connected neurons that enables to project input data into a high-dimensional non-linear space, which is then decoded by a single layer of neurons with trained connections."</data>
      <data key="d6">295606b4bc5d12929a913a3c79f93734</data>
    </edge>
    <edge source="&quot;ESNS&quot;" target="&quot;INPUT-TO-READOUT CONNECTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input-to-readout connections are a feature of ESNs that allow for more complex data processing."</data>
      <data key="d6">f1fc6fbc8158d3da070d55544041a2ca</data>
    </edge>
    <edge source="&quot;ESNS&quot;" target="&quot;MODEL CREATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Model creation is the process of building a neural network model, including defining nodes, connections, and parameters, which is used in ESNs."</data>
      <data key="d6">f1fc6fbc8158d3da070d55544041a2ca</data>
    </edge>
    <edge source="&quot;ESNS&quot;" target="&quot;FEEDBACK CONNECTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESNs utilize feedback connections to allow nodes to access the state of other nodes with a time delay."</data>
      <data key="d6">7f2d69f9a9baca70ffd25a6865189206</data>
    </edge>
    <edge source="&quot;ESNS&quot;" target="&quot;WOLFGANG MAASS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Wolfgang Maass independently developed Echo State Networks."</data>
      <data key="d6">b32958d42199d47252887dc7be40ab5a</data>
    </edge>
    <edge source="&quot;FEATURE&quot;" target="&quot;LABEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Features are attributes used to predict labels."</data>
      <data key="d6">6be085e79e86abc5b1a7eaff6bda1ec5</data>
    </edge>
    <edge source="&quot;HOUSE PRICE MODEL&quot;" target="&quot;LABELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The House Price Model is designed to predict the Labels, which are the actual prices of the houses."</data>
      <data key="d6">8c15845717f6b8610fe30ac08cc78b4e</data>
    </edge>
    <edge source="&quot;HOUSE PRICE MODEL&quot;" target="&quot;RECURRENT NEURAL NETWORK (RNN)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The House Price Model may utilize a Recurrent Neural Network (RNN) to process and learn patterns from the data, improving its ability to make accurate predictions."</data>
      <data key="d6">8c15845717f6b8610fe30ac08cc78b4e</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORK (RNN)&quot;" target="&quot;ECHO STATE NETWORK (ESN)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network (ESN) belongs to the Recurrent Neural Network (RNN) family."</data>
      <data key="d6">a4b801e70cf2ba3a3101d34899450087</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORK (RNN)&quot;" target="&quot;BACKPROPAGATION THROUGH TIME&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Backpropagation Through Time is a learning algorithm for Recurrent Neural Networks."</data>
      <data key="d6">a4b801e70cf2ba3a3101d34899450087</data>
    </edge>
    <edge source="&quot;RECURRENT NEURAL NETWORK (RNN)&quot;" target="&quot;REAL-TIME RECURRENT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Real-Time Recurrent Learning is a learning algorithm for Recurrent Neural Networks."</data>
      <data key="d6">a4b801e70cf2ba3a3101d34899450087</data>
    </edge>
    <edge source="&quot;CONVOLUTIONAL NEURAL NETWORKS (CNNS)&quot;" target="&quot;FEEDBACK NEURAL NETWORKS (FNNS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"CNNs can be designed to incorporate time delays or feedback loops, replacing standard storage, to create FNNs, which are suitable for handling sequences and time-dependent data."</data>
      <data key="d6">04b89ad6396cb78ca75689473c47a247</data>
    </edge>
    <edge source="&quot;SPEECH RECOGNITION&quot;" target="&quot;FEEDBACK CONNECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feedback connections allow the network to access and utilize previous phoneme activations, which is useful for improving the accuracy of speech recognition tasks."</data>
      <data key="d6">57a27a1504a5ef7d330172c0ac1085c9</data>
    </edge>
    <edge source="&quot;SPEECH RECOGNITION&quot;" target="&quot;'&lt;&lt;' OPERATOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Speech recognition uses the '&lt;&lt;' operator to incorporate previous phoneme activations into current processing."</data>
      <data key="d6">c4b54c2da2dda7e660de7bd6de6f13b4</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;DATA POINTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series is composed of a sequence of Data Points collected at successive equally spaced points in time."</data>
      <data key="d6">8a015d76b241ce06eb72867bbb712edd</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;TIME SERIES ANALYSIS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Time Series Analysis is the process of analyzing Time Series data to extract meaningful statistics and characteristics."
"Time Series Analysis is the process of analyzing Time Series data."</data>
      <data key="d6">70c3a879c0f6e6b76a13d02d67bce1a8,8a015d76b241ce06eb72867bbb712edd</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;NUMPY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NumPy is mentioned in the context of formatting data for Time Series analysis."</data>
      <data key="d6">14bccd672d2f8dd2cd7300581c8844fb</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;MATHEMATICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series is a concept that originates from the field of Mathematics."</data>
      <data key="d6">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;STATISTICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series is commonly used in the field of Statistics."</data>
      <data key="d6">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;SIGNAL PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series is a type of data that is often analyzed in Signal Processing."</data>
      <data key="d6">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;PATTERN RECOGNITION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series is a type of data that is often analyzed in Pattern Recognition."</data>
      <data key="d6">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;ECONOMETRICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series is commonly used in the field of Econometrics."</data>
      <data key="d6">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;MATHEMATICAL FINANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series is a type of data that is often analyzed in Mathematical Finance."</data>
      <data key="d6">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;WEATHER FORECASTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series is commonly used in Weather Forecasting."</data>
      <data key="d6">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;EARTHQUAKE PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series is a type of data that is often analyzed in Earthquake Prediction."</data>
      <data key="d6">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;REGRESSION ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Regression Analysis is a statistical method that can be used to analyze relationships between different Time Series."</data>
      <data key="d6">70c3a879c0f6e6b76a13d02d67bce1a8</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES2N model is used to forecast the Time Series data."</data>
      <data key="d6">854761a5b5b5b90af10bc6b6c76cc355</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;ESN&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"ESN is used for predicting and forecasting Time Series data."
"ESN is used to work with time series data, which is a sequence of data points collected at regular time intervals."
"The ESN model is used for analyzing and making predictions about Time Series data."</data>
      <data key="d6">0ae9f3cf96547c05eff54812cb72ac31,29aad23ce67e778ac31d4fb287fd20c7,76963fa19a9caab847e50167f71c86a2</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;NVAR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NVAR is used for time series prediction, learning an internal representation of the local dynamics of the attractor."</data>
      <data key="d6">2035514bf3ab5b7f12ae1321972551f1</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;LORENZ MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Lorenz Model is used to study the behavior of the Time Series data, representing a complex system with chaotic properties."</data>
      <data key="d6">c838b1b4744bc0f400abf85f791950cf</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;NVAR MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The NVAR Model is used to analyze and predict the Time Series data."</data>
      <data key="d6">c838b1b4744bc0f400abf85f791950cf</data>
    </edge>
    <edge source="&quot;TIME SERIES&quot;" target="&quot;ESN MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN Model is used to predict future values in a Time Series, as demonstrated in the provided text."</data>
      <data key="d6">973d44d321c7ceee7add295c60b085d2</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;MATHEMATICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Mathematics is the foundation for Time Series Analysis, providing the theoretical framework and methods for analyzing Time Series data."</data>
      <data key="d6">8a015d76b241ce06eb72867bbb712edd</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;REGRESSION ANALYSIS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Time Series Analysis and Regression Analysis are often used together to analyze and test relationships between different time series."
"Regression Analysis is not typically called Time Series Analysis, as Time Series Analysis specifically refers to relationships between different points in time within a single series."</data>
      <data key="d6">8e69dad9d25c6b8f037f22592687e195,dc76db79c20c315f30e0297619904b6f</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;CROSS-SECTIONAL STUDIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Analysis and Cross-Sectional Studies are distinct methods, with time series data having a natural temporal ordering."</data>
      <data key="d6">dc76db79c20c315f30e0297619904b6f</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;SPATIAL DATA ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Analysis and Spatial Data Analysis are different methods, with time series data typically not relating to geographical locations."</data>
      <data key="d6">dc76db79c20c315f30e0297619904b6f</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;STOCHASTIC MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Stochastic Models are used in Time Series Analysis to reflect the fact that observations close together in time will be more closely related than observations further apart."</data>
      <data key="d6">dc76db79c20c315f30e0297619904b6f</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;STOCHASTIC MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Stochastic Model is often used in Time Series Analysis to account for uncertainty in the data."</data>
      <data key="d6">8e69dad9d25c6b8f037f22592687e195</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;TIME SERIES DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Analysis is applied to Time Series Data, which have a natural temporal ordering."</data>
      <data key="d6">8e69dad9d25c6b8f037f22592687e195</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;FREQUENCY-DOMAIN METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Analysis includes techniques such as Spectral Analysis and Wavelet Analysis, which are classified as Frequency-domain Methods."</data>
      <data key="d6">c7d17582a93a296eaaf9b9fca737ba51</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;TIME-DOMAIN METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Analysis includes techniques such as Auto-correlation and Cross-correlation Analysis, which are classified as Time-domain Methods."</data>
      <data key="d6">c7d17582a93a296eaaf9b9fca737ba51</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;PARAMETRIC METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Analysis includes approaches that assume the underlying structure of the stochastic process, such as Parametric Methods."</data>
      <data key="d6">c7d17582a93a296eaaf9b9fca737ba51</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;NON-PARAMETRIC METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Analysis includes approaches that do not assume the underlying structure of the stochastic process, such as Non-parametric Methods."</data>
      <data key="d6">c7d17582a93a296eaaf9b9fca737ba51</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;STATISTICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Analysis is commonly used in the field of statistics."</data>
      <data key="d6">a2b394d556da06b8c14dd2f5e106343b</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;ECONOMETRICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Analysis is used in econometrics for forecasting."</data>
      <data key="d6">a2b394d556da06b8c14dd2f5e106343b</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;QUANTITATIVE FINANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Analysis is used in quantitative finance for forecasting."</data>
      <data key="d6">a2b394d556da06b8c14dd2f5e106343b</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;TUBERCULOSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Analysis is used to analyze data on Tuberculosis incidence."</data>
      <data key="d6">4b75a8a7637b05307e62f309c682d43b</data>
    </edge>
    <edge source="&quot;TIME SERIES ANALYSIS&quot;" target="&quot;CORPORATE DATA ANALYSTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Corporate Data Analysts face challenges in using exploratory time series analysis techniques."</data>
      <data key="d6">4b75a8a7637b05307e62f309c682d43b</data>
    </edge>
    <edge source="&quot;REGRESSION ANALYSIS&quot;" target="&quot;CURVE FITTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Regression Analysis is a statistical technique used in Curve Fitting to infer relationships among two or more variables."</data>
      <data key="d6">630c86e110e2dabbe068f446b619cef3</data>
    </edge>
    <edge source="&quot;REGRESSION ANALYSIS&quot;" target="&quot;FUNCTION APPROXIMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Regression Analysis is a technique used in Function Approximation to approximate a function when only a set of points is provided."</data>
      <data key="d6">9ec0dac4c72bcc2c78c7df43b9969fe7</data>
    </edge>
    <edge source="&quot;CROSS-SECTIONAL STUDIES&quot;" target="&quot;TEMPORAL ORDERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Temporal Ordering is a characteristic of time series data that distinguishes it from cross-sectional studies, which do not have a natural ordering of observations."</data>
      <data key="d6">dc76db79c20c315f30e0297619904b6f</data>
    </edge>
    <edge source="&quot;NUMPY&quot;" target="&quot;API&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NumPy is an open-source organization that provides a library for working with arrays, which can be accessed through an API."</data>
      <data key="d6">14bccd672d2f8dd2cd7300581c8844fb</data>
    </edge>
    <edge source="&quot;NUMPY&quot;" target="&quot;SCIPY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"SciPy builds on NumPy and provides a large collection of algorithms and functions for scientific and technical computing."</data>
      <data key="d6">4246748fef7001ea0bd03ac702565b0d</data>
    </edge>
    <edge source="&quot;NUMPY&quot;" target="&quot;ESN&quot;">
      <data key="d4">4.0</data>
      <data key="d5">"Numpy is used to initialize parameters in ESN, such as bias vectors and other arrays or matrices."
"Numpy is used to create arrays and perform mathematical operations in the context of Echo State Networks."
"ESN uses Numpy for numerical computing tasks, such as handling arrays and matrices."
"Numpy is mentioned in the context of ESNs for numerical computations."</data>
      <data key="d6">1cbfde86d1258f2b267135412e50a590,38b3e8ea0ec280360770513327b0d9d3,593080a95ef7640b3925b07cad1bedd4,cdc64af0dde941250d89b191d0666c9b</data>
    </edge>
    <edge source="&quot;NUMPY&quot;" target="&quot;NORMAL DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Numpy provides a function to generate matrices from a Normal Distribution."</data>
      <data key="d6">d5e39e29b61f6ea0ffe0c868ba7a4252</data>
    </edge>
    <edge source="&quot;NUMPY&quot;" target="&quot;UNIFORM DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Numpy provides a function to generate matrices from a Uniform Distribution."</data>
      <data key="d6">d5e39e29b61f6ea0ffe0c868ba7a4252</data>
    </edge>
    <edge source="&quot;NUMPY&quot;" target="&quot;BERNOULLI DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Numpy is not explicitly mentioned in the context of generating matrices from a Bernoulli Distribution."</data>
      <data key="d6">d5e39e29b61f6ea0ffe0c868ba7a4252</data>
    </edge>
    <edge source="&quot;NUMPY&quot;" target="&quot;ESN MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Numpy is used for numerical computations in the ESN Model, such as array manipulations."</data>
      <data key="d6">8294eed5fc10df1c118f9afa266910e4</data>
    </edge>
    <edge source="&quot;NUMPY&quot;" target="&quot;SINE WAVE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Numpy is used to generate a Sine Wave for the example."</data>
      <data key="d6">71f966d00b6d0eceb580d00b9cb86b1e</data>
    </edge>
    <edge source="&quot;NUMPY&quot;" target="&quot;MACKEY-GLASS TIMESERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Numpy is used to perform numerical computations on the Mackey-Glass Timeseries dataset."</data>
      <data key="d6">4073cafddb73621f26061385c5570659</data>
    </edge>
    <edge source="&quot;API&quot;" target="&quot;VERBOSITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Verbosity is mentioned in the context of using an API, referring to the level of detail provided in the output."</data>
      <data key="d6">14bccd672d2f8dd2cd7300581c8844fb</data>
    </edge>
    <edge source="&quot;VERBOSITY&quot;" target="&quot;SOFTWARE APPLICATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Verbosity refers to the level of detail provided in the output of Software Applications."</data>
      <data key="d6">f838f4cbb7060f4409ba2d174a396fb1</data>
    </edge>
    <edge source="&quot;SEED&quot;" target="&quot;SOFTWARE APPLICATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A seed is used to initialize a pseudorandom number generator in Software Applications, ensuring deterministic and repeatable results."</data>
      <data key="d6">f838f4cbb7060f4409ba2d174a396fb1</data>
    </edge>
    <edge source="&quot;SEED&quot;" target="&quot;HP_SPACE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"hp_space is used to explore the seed parameter."</data>
      <data key="d6">80033e741d8e10abdcfe20dd17192152</data>
    </edge>
    <edge source="&quot;SEED&quot;" target="&quot;HYPEROPT-MULTISCROLL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"hyperopt-multiscroll is associated with the parameter seed, which ensures reproducibility."</data>
      <data key="d6">adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </edge>
    <edge source="&quot;SEED&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Seed is a parameter used to initialize the random number generator in the ESN model, ensuring reproducibility."</data>
      <data key="d6">72e6eee633bcb5b1458c4cee3975cee1</data>
    </edge>
    <edge source="&quot;SEED&quot;" target="&quot;HYPEROPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Seed is a parameter mentioned in the Hyperopt configuration."</data>
      <data key="d6">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </edge>
    <edge source="&quot;SOFTWARE APPLICATIONS&quot;" target="&quot;APIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Software Applications use APIs to communicate with each other, enabling integration and interaction."</data>
      <data key="d6">f838f4cbb7060f4409ba2d174a396fb1</data>
    </edge>
    <edge source="&quot;SCIPY&quot;" target="&quot;ESN&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"Scipy is used to work with sparse matrices in ESN, allowing for efficient computation and memory usage."
"ESN uses Scipy for scientific computing tasks, such as handling sparse matrices."
"Scipy is mentioned in the context of ESNs for working with sparse matrices."</data>
      <data key="d6">1cbfde86d1258f2b267135412e50a590,38b3e8ea0ec280360770513327b0d9d3,cdc64af0dde941250d89b191d0666c9b</data>
    </edge>
    <edge source="&quot;SCIPY&quot;" target="&quot;NARMA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Scipy is used to generate the NARMA Timeseries data."</data>
      <data key="d6">8e2e87aa712be195790cf15483428d7f</data>
    </edge>
    <edge source="&quot;SCIPY&quot;" target="&quot;BERNOULLI DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Scipy is not explicitly mentioned in the context of generating matrices from a Bernoulli Distribution."</data>
      <data key="d6">d5e39e29b61f6ea0ffe0c868ba7a4252</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;BACKPROPAGATION OF ERROR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RNNs utilize the method of Backpropagation of Error to train their models and estimate gradients."</data>
      <data key="d6">001240f9b2caf047ee61a89e03f7b309</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;GRADIENT DESCENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RNNs face problems with gradient descent-based training, such as bifurcations and slow convergence."</data>
      <data key="d6">eafe89ad19a57846f953a1dfcf8571f8</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;DEEP LEARNING FRAMEWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RNNs are often replicated using popular Deep Learning frameworks."</data>
      <data key="d6">418f92b0dd08e03a20637ffec8193bfc</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;LSTMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LSTMs are a type of technology that competes with state-of-the-art RNN methods like RNNs."</data>
      <data key="d6">418f92b0dd08e03a20637ffec8193bfc</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;WAN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Wan is a contributor to the development of RNNs."</data>
      <data key="d6">7ede01f521333d9e39fc34a245103242</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;BEAUFAYS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Beaufays is a contributor to the development of RNNs."</data>
      <data key="d6">7ede01f521333d9e39fc34a245103242</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;CAMPOLUCCI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Campolucci is a contributor to the development of RNNs."</data>
      <data key="d6">7ede01f521333d9e39fc34a245103242</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;UNCINI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Uncini is a contributor to the development of RNNs."</data>
      <data key="d6">7ede01f521333d9e39fc34a245103242</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;PIAZZA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Piazza is a contributor to the development of RNNs."</data>
      <data key="d6">7ede01f521333d9e39fc34a245103242</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;GENETIC ALGORITHMS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Genetic Algorithms are a global optimization method mentioned for training RNNs."
"Genetic Algorithms are used for training RNNs, evolving multiple neural networks to minimize the mean-squared error."</data>
      <data key="d6">797480b3d8c00dbb7f02fccb2ab8256a,7ede01f521333d9e39fc34a245103242</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;SIMULATED ANNEALING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Simulated Annealing is a global optimization technique that may be used to seek a good set of weights for RNNs."</data>
      <data key="d6">797480b3d8c00dbb7f02fccb2ab8256a</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;PARTICLE SWARM OPTIMIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Particle Swarm Optimization is a global optimization technique that may be used for training RNNs, seeking a good set of weights."</data>
      <data key="d6">797480b3d8c00dbb7f02fccb2ab8256a</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;DYNAMICAL SYSTEMS THEORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dynamical Systems Theory may be used for analyzing the behavior of RNNs, which can appear chaotic."</data>
      <data key="d6">797480b3d8c00dbb7f02fccb2ab8256a</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;RECURSIVE NEURAL NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RNNs are a specific type of Recursive Neural Network that operate on the linear progression of time."</data>
      <data key="d6">797480b3d8c00dbb7f02fccb2ab8256a</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;SCHILLER AND STEIL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Schiller and Steil demonstrated the dominance of output weight changes in conventional training approaches for RNNs."</data>
      <data key="d6">b32958d42199d47252887dc7be40ab5a</data>
    </edge>
    <edge source="&quot;RNNS&quot;" target="&quot;L. SCHOMAKER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"L. Schomaker described how a desired target output could be obtained from an RNN by learning to combine signals from a randomly configured ensemble of spiking neural oscillators."</data>
      <data key="d6">b32958d42199d47252887dc7be40ab5a</data>
    </edge>
    <edge source="&quot;BACKPROPAGATION OF ERROR&quot;" target="&quot;NEURAL NETWORK MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Backpropagation of Error is a method used to train Neural Network Models."</data>
      <data key="d6">8fdd0220497c9b9d8c2ece14be6a8f25</data>
    </edge>
    <edge source="&quot;BACKPROPAGATION OF ERROR&quot;" target="&quot;GRADIENT DESCENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gradient Descent is often used in conjunction with Backpropagation of Error to update the network parameters."</data>
      <data key="d6">8fdd0220497c9b9d8c2ece14be6a8f25</data>
    </edge>
    <edge source="&quot;BACKPROPAGATION OF ERROR&quot;" target="&quot;STOCHASTIC GRADIENT DESCENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Stochastic Gradient Descent is a variant of gradient descent that updates the parameters using random subsets of data, often used in conjunction with Backpropagation of Error."</data>
      <data key="d6">8fdd0220497c9b9d8c2ece14be6a8f25</data>
    </edge>
    <edge source="&quot;GRADIENT DESCENT&quot;" target="&quot;BACKPROPAGATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Backpropagation is commonly used with Gradient Descent to update the network parameters."</data>
      <data key="d6">f60e4bd6b9e356b88d3a008130e8ac4b</data>
    </edge>
    <edge source="&quot;GRADIENT DESCENT&quot;" target="&quot;DEEP LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep Learning has solved the problems faced by gradient descent-based training of RNNs, making them less unique compared to ESNs."</data>
      <data key="d6">eafe89ad19a57846f953a1dfcf8571f8</data>
    </edge>
    <edge source="&quot;GRADIENT DESCENT&quot;" target="&quot;RNN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gradient Descent is used for optimizing parameters in RNNs, facing challenges such as vanishing gradients."</data>
      <data key="d6">1aec5b03f663d1614b2ecbf97981a5c2</data>
    </edge>
    <edge source="&quot;GRADIENT DESCENT&quot;" target="&quot;BPTT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"BPTT is used in the context of optimizing parameters in RNNs, where it faces challenges related to Gradient Descent."</data>
      <data key="d6">1aec5b03f663d1614b2ecbf97981a5c2</data>
    </edge>
    <edge source="&quot;GRADIENT DESCENT&quot;" target="&quot;RTRL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RTRL is used in the context of optimizing parameters in RNNs, where it faces challenges related to Gradient Descent."</data>
      <data key="d6">1aec5b03f663d1614b2ecbf97981a5c2</data>
    </edge>
    <edge source="&quot;GRADIENT DESCENT&quot;" target="&quot;LSTM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LSTM is used in the context of optimizing parameters in RNNs, addressing the vanishing gradients problem related to Gradient Descent."</data>
      <data key="d6">1aec5b03f663d1614b2ecbf97981a5c2</data>
    </edge>
    <edge source="&quot;GRADIENT DESCENT&quot;" target="&quot;INDRNN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IndRNN is used in the context of optimizing parameters in RNNs, addressing the vanishing gradients problem related to Gradient Descent."</data>
      <data key="d6">1aec5b03f663d1614b2ecbf97981a5c2</data>
    </edge>
    <edge source="&quot;STOCHASTIC GRADIENT DESCENT&quot;" target="&quot;BACKPROPAGATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Backpropagation can also be used with Stochastic Gradient Descent, which updates the parameters using random subsets of data."</data>
      <data key="d6">f60e4bd6b9e356b88d3a008130e8ac4b</data>
    </edge>
    <edge source="&quot;LINEAR REGRESSION&quot;" target="&quot;LEAST SQUARES METHOD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Linear Regression often uses the Least Squares Method to find the best-fit line for a set of paired data."</data>
      <data key="d6">f60e4bd6b9e356b88d3a008130e8ac4b</data>
    </edge>
    <edge source="&quot;LINEAR REGRESSION&quot;" target="&quot;ORGANIZATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Linear Regression is beneficial for organizations, helping them transform raw data into actionable information and make better decisions."</data>
      <data key="d6">573ef2ebe6a637a429cdc073a8508ca4</data>
    </edge>
    <edge source="&quot;LINEAR REGRESSION&quot;" target="&quot;IBM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IBM is referenced as a source of detailed information about Linear Regression, providing insights into its application and usage."</data>
      <data key="d6">573ef2ebe6a637a429cdc073a8508ca4</data>
    </edge>
    <edge source="&quot;LINEAR REGRESSION&quot;" target="&quot;INDEPENDENT VARIABLE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Linear Regression is used to predict the relationship between two variables, with the independent variable being used to explain changes in the dependent variable."</data>
      <data key="d6">573ef2ebe6a637a429cdc073a8508ca4</data>
    </edge>
    <edge source="&quot;LINEAR REGRESSION&quot;" target="&quot;DEPENDENT VARIABLE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Linear Regression is used to predict the relationship between two variables, with the dependent variable being the variable being predicted or explained."</data>
      <data key="d6">573ef2ebe6a637a429cdc073a8508ca4</data>
    </edge>
    <edge source="&quot;LINEAR REGRESSION&quot;" target="&quot;LASSO REGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LASSO Regression builds on Linear Regression, addressing its shortcomings and providing an advantage in variable selection."</data>
      <data key="d6">861c28cb739722ddeb0babb7e1427409</data>
    </edge>
    <edge source="&quot;LINEAR REGRESSION&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Linear Regression is used for training connections in the Echo State Network."</data>
      <data key="d6">f730c6800099724052a2d061f3cd8c2e</data>
    </edge>
    <edge source="&quot;LINEAR REGRESSION&quot;" target="&quot;DESIRED OUTPUT WEIGHTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Desired Output Weights are computed using linear regression, which involves finding the relationship between two variables."</data>
      <data key="d6">6a4432cd530b28770e2b903fe242a0d1</data>
    </edge>
    <edge source="&quot;LINEAR REGRESSION&quot;" target="&quot;RIDGE READOUT&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Linear Regression is used as the method for solving tasks in the Ridge Readout."
"Linear Regression is used by the Ridge Readout to solve a task."</data>
      <data key="d6">5d3baa9818a4e01fe1196c43378a2cea,d25cd385546ec6a033287e75d65a551a</data>
    </edge>
    <edge source="&quot;LEAST SQUARES METHOD&quot;" target="&quot;LINEAR REGRESSION CALCULATORS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Linear Regression Calculators use the 'Least Squares' Method to find the best-fit line for a set of paired data."</data>
      <data key="d6">573ef2ebe6a637a429cdc073a8508ca4</data>
    </edge>
    <edge source="&quot;IBM&quot;" target="&quot;OVERFITTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IBM provides information about overfitting on their website, making it a relevant source for understanding the concept."</data>
      <data key="d6">173a2da2c7ea80f95b04db8422ced004</data>
    </edge>
    <edge source="&quot;OVERFITTING&quot;" target="&quot;FITTING PROCESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Fitting Process, if not properly regularized, can lead to overfitting, where the model learns the training data too well, including its noise and irrelevant details, resulting in poor generalization on new data."</data>
      <data key="d6">87757855658e1d198ec49a3290760dd5</data>
    </edge>
    <edge source="&quot;OVERFITTING&quot;" target="&quot;RIDGE READOUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Overfitting is a problem that Ridge Readout helps mitigate by using a regularization term to prevent the model from fitting the noise in the training data."</data>
      <data key="d6">b09c66bc81fd63720bef0cfa941ee65b</data>
    </edge>
    <edge source="&quot;OVERFITTING&quot;" target="&quot;RIDGE PARAMETER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Ridge Parameter is used to prevent overfitting by adding a penalty to the magnitude of the weights during training."</data>
      <data key="d6">173a2da2c7ea80f95b04db8422ced004</data>
    </edge>
    <edge source="&quot;FEEDBACK CONNECTIONS&quot;" target="&quot;ECHO STATE NETWORK (ESN)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feedback Connections are structural links established between nodes in an Echo State Network (ESN) that allow recurrent information flow within the network, utilizing past activations to influence current processing."</data>
      <data key="d6">3ecffab3c205dece73b47f9a7004fc89</data>
    </edge>
    <edge source="&quot;FEEDBACK CONNECTIONS&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feedback connections are connections from the readout to the input in ESNs, which can be used for generating signals or long term forecasting."</data>
      <data key="d6">ead6383a44acd8ebd17907b85a910455</data>
    </edge>
    <edge source="&quot;FEEDBACK CONNECTIONS&quot;" target="&quot;GENERATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feedback connections can be used for generating signals."</data>
      <data key="d6">ead6383a44acd8ebd17907b85a910455</data>
    </edge>
    <edge source="&quot;FEEDBACK CONNECTIONS&quot;" target="&quot;LONG TERM FORECASTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feedback connections can be used for long term forecasting."</data>
      <data key="d6">ead6383a44acd8ebd17907b85a910455</data>
    </edge>
    <edge source="&quot;READOUT LAYER&quot;" target="&quot;RIDGE NODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Readout Layer in Echo State Networks is created using a Ridge Node, which is a form of regularized linear regression that helps prevent overfitting and ensures the model generalizes well to new data."</data>
      <data key="d6">0e6f0f7cd882a638ecb571ef36068868</data>
    </edge>
    <edge source="&quot;READOUT LAYER&quot;" target="&quot;INPUT DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input Data is directly fed to the Readout Layer, bypassing the Reservoir, allowing the model to utilize raw input features."</data>
      <data key="d6">4da651284dbab3f68dc3cae41e6e0311</data>
    </edge>
    <edge source="&quot;READOUT LAYER&quot;" target="&quot;SPECIAL CONCAT NODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Special Concat Node combines inputs from different sources and feeds the combined vector to the Readout Layer, enhancing its ability to process complex data."</data>
      <data key="d6">a35f6cae32a3d24b18ee17ec0471a9d4</data>
    </edge>
    <edge source="&quot;RIDGE NODE&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge nodes are used in the ESN and are capable of performing regularized linear regression on the reservoir&#8217;s activations."</data>
      <data key="d6">9b360c6a33aafa6827417de5bd4faa82</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK (ESN)&quot;" target="&quot;TIME CONSTANT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Changing the Time Constant in an Echo State Network (ESN) affects how quickly the neurons in the reservoir update their states in response to inputs."</data>
      <data key="d6">77c3759b4ed32509aaf1403c6fa8030f</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK (ESN)&quot;" target="&quot;LEAKING RATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Leaking Rate parameter in an Echo State Network (ESN) controls the rate at which neurons forget previous states."</data>
      <data key="d6">77c3759b4ed32509aaf1403c6fa8030f</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK (ESN)&quot;" target="&quot;SPECTRAL RADIUS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Spectral Radius of the reservoir matrix in an Echo State Network (ESN) determines the stability and memory capacity of the reservoir."</data>
      <data key="d6">77c3759b4ed32509aaf1403c6fa8030f</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK (ESN)&quot;" target="&quot;CHAINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chaining is used to sequentially connect nodes in an Echo State Network (ESN), structuring the network and enabling proper data transformation and learning."</data>
      <data key="d6">b641b2be224e677674f7d3523e87ccde</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK (ESN)&quot;" target="&quot;FEEDBACK CONNECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Network (ESN) utilizes feedback connections to incorporate past activations into current processing, enhancing its dynamic capabilities and memory abilities."</data>
      <data key="d6">57a27a1504a5ef7d330172c0ac1085c9</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK (ESN)&quot;" target="&quot;TEACHER VECTORS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Teacher Vectors are used as feedback during the training phase of an Echo State Network (ESN) to improve learning and stabilize the training process."</data>
      <data key="d6">3ecffab3c205dece73b47f9a7004fc89</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK (ESN)&quot;" target="&quot;MODEL.FIT()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Model.fit() method is used to train an Echo State Network (ESN) by optimizing its parameters to minimize the error between predicted and actual target values."</data>
      <data key="d6">3ecffab3c205dece73b47f9a7004fc89</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK (ESN)&quot;" target="&quot;FORCE FEEDBACK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Force Feedback is a training technique used in an Echo State Network (ESN) that involves using teacher vectors (actual target values) as feedback to stabilize the training process and improve learning."</data>
      <data key="d6">3ecffab3c205dece73b47f9a7004fc89</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK (ESN)&quot;" target="&quot;RECENTLY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Network (ESN) is mentioned in the context of recent optimizations for increased network stability and relevance to real-world applications."</data>
      <data key="d6">a8c0edd2cdddb7d6d899284063b541f5</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK (ESN)&quot;" target="&quot;TESTING DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network (ESN) model is evaluated using Testing Data."</data>
      <data key="d6">1365a36c76afc697ac626fd0f784804a</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK (ESN)&quot;" target="&quot;GAUSSIAN PROCESS MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network (ESN) is used to obtain a Gaussian Process Model with an ESN-driven kernel function."</data>
      <data key="d6">a4b801e70cf2ba3a3101d34899450087</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK (ESN)&quot;" target="&quot;AUTHOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author discusses the Echo State Network (ESN) and its applications."</data>
      <data key="d6">a4b801e70cf2ba3a3101d34899450087</data>
    </edge>
    <edge source="&quot;LEAKING RATE&quot;" target="&quot;SPECTRAL RADIUS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Spectral Radius and Leaking Rate are parameters mentioned in the text, which are log-uniformly distributed within specified ranges."</data>
      <data key="d6">0a9b132ecb1c4b63fdbb0e144295362e</data>
    </edge>
    <edge source="&quot;LEAKING RATE&quot;" target="&quot;HYPEROPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt is exploring the Leaking Rate parameter."</data>
      <data key="d6">65ba78d1f678e080bd930319c54234ef</data>
    </edge>
    <edge source="&quot;SPECTRAL RADIUS&quot;" target="&quot;RESERVOIR MATRIX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Spectral Radius is a property of the Reservoir Matrix in an Echo State Network (ESN)."</data>
      <data key="d6">01f8dd8235ba0d4cf0837b5ea958ec95</data>
    </edge>
    <edge source="&quot;SPECTRAL RADIUS&quot;" target="&quot;RESERVOIR DYNAMICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The behavior of the reservoir neurons, or Reservoir Dynamics, can be influenced by the Spectral Radius."</data>
      <data key="d6">01f8dd8235ba0d4cf0837b5ea958ec95</data>
    </edge>
    <edge source="&quot;SPECTRAL RADIUS&quot;" target="&quot;HYPER-PARAMETER EXPLORATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Spectral Radius is a hyper-parameter in Reservoir Computing that can be explored during the hyper-parameter exploration process to optimize the stability and dynamics of the reservoir layer."</data>
      <data key="d6">716940af834825642e01a3cb59a7e006</data>
    </edge>
    <edge source="&quot;SPECTRAL RADIUS&quot;" target="&quot;STABLE DYNAMICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A spectral radius close to 1 is associated with stable dynamics, indicating predictable and consistent reservoir behavior."</data>
      <data key="d6">1f30b86a46d4819603edc730df816c49</data>
    </edge>
    <edge source="&quot;SPECTRAL RADIUS&quot;" target="&quot;CHAOTIC DYNAMICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A spectral radius further from 1 is associated with chaotic dynamics, indicating unpredictable and sensitive reservoir behavior."</data>
      <data key="d6">1f30b86a46d4819603edc730df816c49</data>
    </edge>
    <edge source="&quot;SPECTRAL RADIUS&quot;" target="&quot;ECHO STATE PROPERTY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Property is theoretically associated with a spectral radius close to 1, allowing the reservoir states to be less affected by their initial conditions."</data>
      <data key="d6">1f30b86a46d4819603edc730df816c49</data>
    </edge>
    <edge source="&quot;SPECTRAL RADIUS&quot;" target="&quot;ADDITIVE-SIGMOID NEURON RESERVOIRS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Spectral Radius is mentioned in the context of Additive-Sigmoid Neuron Reservoirs and their relationship with the Echo State Property."</data>
      <data key="d6">a9f53979e9dbe6b936ff3374c73006dd</data>
    </edge>
    <edge source="&quot;SPECTRAL RADIUS&quot;" target="&quot;ESP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESP is erroneously identified with a Spectral Radius below 1, but the relationship is more complex and depends on input amplitude.")</data>
      <data key="d6">3b592e5ac113a5c031925f91a182baa6</data>
    </edge>
    <edge source="&quot;SPECTRAL RADIUS&quot;" target="&quot;RANDOM SPARSE MATRICES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Spectral Radius is a property of a matrix mentioned in the context of creating random sparse matrices."</data>
      <data key="d6">ff860bc63e3d697a6183c0b850689048</data>
    </edge>
    <edge source="&quot;SPECTRAL RADIUS&quot;" target="&quot;HYPEROPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt is exploring the Spectral Radius parameter."</data>
      <data key="d6">65ba78d1f678e080bd930319c54234ef</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS (ESNS)&quot;" target="&quot;CANONICAL METHOD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Canonical Method is a common approach used for creating and training Echo State Networks."</data>
      <data key="d6">fa7c410cf411eb68eb517e23427ec1c8</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS (ESNS)&quot;" target="&quot;NON-CANONICAL METHOD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Non-Canonical Method is an alternative approach used for creating and training Echo State Networks."</data>
      <data key="d6">fa7c410cf411eb68eb517e23427ec1c8</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS (ESNS)&quot;" target="&quot;FORECASTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks are used for time series prediction and analysis, which includes the process of forecasting future data points."</data>
      <data key="d6">fa7c410cf411eb68eb517e23427ec1c8</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS (ESNS)&quot;" target="&quot;WARMUP PARAMETER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Warmup Parameter is used in the training process of Echo State Networks to discard initial transient states."</data>
      <data key="d6">fa7c410cf411eb68eb517e23427ec1c8</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS (ESNS)&quot;" target="&quot;FORCED FEEDBACK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks (ESNs) use Forced Feedback as a technique to improve training efficiency."</data>
      <data key="d6">0d922ae20673124fc4588949e3863ed0</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS (ESNS)&quot;" target="&quot;TEACHER FORCING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Teacher Forcing is also used in training Echo State Networks (ESNs) to help the model learn the mapping between inputs and targets."</data>
      <data key="d6">d0a69d653d08e58959dd8d0f2033e697</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS (ESNS)&quot;" target="&quot;GENERATION&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Echo State Networks (ESNs) are used for generating timeseries data by predicting the next data point and using this prediction as input."
"ESNs are used for generating future values of a timeseries based on past data."</data>
      <data key="d6">0c3779349544e78c4d650ccf76623127,4b78fdc153f982e64291112395c316c7</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS (ESNS)&quot;" target="&quot;LONG-TERM FORECASTING&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Echo State Networks (ESNs) are also used for long-term forecasting, predicting future values of a timeseries over an extended period."
"ESNs are used for long-term forecasting, predicting many steps ahead in a timeseries."</data>
      <data key="d6">0c3779349544e78c4d650ccf76623127,4b78fdc153f982e64291112395c316c7</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS (ESNS)&quot;" target="&quot;SHIFT_FB=TRUE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The shift_fb=True parameter is used in Echo State Networks (ESNs) to ensure the correct temporal alignment between input and feedback timeseries."</data>
      <data key="d6">0c3779349544e78c4d650ccf76623127</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS (ESNS)&quot;" target="&quot;WITH_FEEDBACK() CONTEXT MANAGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The with_feedback() context manager is used in Echo State Networks (ESNs) to temporarily change the feedback received by the reservoir for experimentation or manipulation."</data>
      <data key="d6">0c3779349544e78c4d650ccf76623127</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORKS (ESNS)&quot;" target="&quot;ROBOT FALLING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Robot Falling use case demonstrates the use of Echo State Networks (ESNs) to analyze data from a robot falling."</data>
      <data key="d6">af2db1cc5ab6b16acae2c93d3facb668</data>
    </edge>
    <edge source="&quot;RESERVOIR DYNAMICS&quot;" target="&quot;CHAOTICITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chaoticity is a measure of the complexity and unpredictability in the Reservoir Dynamics, which can be altered by changing the Spectral Radius."</data>
      <data key="d6">01f8dd8235ba0d4cf0837b5ea958ec95</data>
    </edge>
    <edge source="&quot;NP.PI&quot;" target="&quot;NP.LINSPACE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.pi is used as an endpoint in the np.linspace function to generate evenly spaced numbers over the interval [0, 6*np.pi]."</data>
      <data key="d6">475ca77684df5045266ddf079f2e37f1</data>
    </edge>
    <edge source="&quot;NP.SIN&quot;" target="&quot;NP.LINSPACE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.sin is applied to the output of np.linspace to compute the sine of all elements in the array."</data>
      <data key="d6">475ca77684df5045266ddf079f2e37f1</data>
    </edge>
    <edge source="&quot;NP.SIN&quot;" target="&quot;NP.RESHAPE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.reshape is used to reshape the output of np.sin into a column vector with 100 rows and 1 column."</data>
      <data key="d6">475ca77684df5045266ddf079f2e37f1</data>
    </edge>
    <edge source="&quot;NP.LINSPACE&quot;" target="&quot;SINGLE TIMESTEP OF DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The concept of a single timestep of data is mentioned in the context of np.linspace, suggesting that it may be relevant to the usage of np.linspace in time series analysis."</data>
      <data key="d6">475ca77684df5045266ddf079f2e37f1</data>
    </edge>
    <edge source="&quot;TIMESTEP&quot;" target="&quot;TIMESERIES&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"A Timestep is a single point in time within a Timeseries, while a Timeseries is a collection of multiple Timesteps."
"Timesteps are individual data points that collectively form a Timeseries, capturing the evolution of data over time."</data>
      <data key="d6">c41b9b19460dc63e06639ea4bbbd1515,fa082948fa919150e9c06c6f5c1b53b0</data>
    </edge>
    <edge source="&quot;TIMESTEP&quot;" target="&quot;WARMUP PARAMETER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Warmup Parameter is used to discard initial Timesteps, allowing the Reservoir's internal state to stabilize and accurately reflect the input data."</data>
      <data key="d6">fa082948fa919150e9c06c6f5c1b53b0</data>
    </edge>
    <edge source="&quot;INPUT DATA&quot;" target="&quot;ESN&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"The ESN uses the Input Data to learn and capture the temporal dynamics of the timeseries."
"Input Data is used to train or make predictions using an Echo State Network (ESN) in this context."
"The ESN model is trained on input data, in this case, a sine wave."</data>
      <data key="d6">693e4d1e43289f46866236c10207a17e,7b294b788fe5ee385d08c4aabe2ca71d,b338d2dcc1fe6ccf42407444c02cad7c</data>
    </edge>
    <edge source="&quot;INPUT DATA&quot;" target="&quot;INPUT SCALING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Input Scaling parameter is used to control the magnitude of the input data in the Echo State Network (ESN) model."</data>
      <data key="d6">1365a36c76afc697ac626fd0f784804a</data>
    </edge>
    <edge source="&quot;INPUT DATA&quot;" target="&quot;INPUT CONNECTIVITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Input Connectivity parameter is used to control the connection between the input data and the reservoir in the Echo State Network (ESN) model."</data>
      <data key="d6">1365a36c76afc697ac626fd0f784804a</data>
    </edge>
    <edge source="&quot;NULL&quot;" target="&quot;PROGRAMMING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Null is a special value used to represent the absence of a value or object in programming languages."</data>
      <data key="d6">56cde5dc9d350498c1544cd57733ca8f</data>
    </edge>
    <edge source="&quot;NP.EMPTY&quot;" target="&quot;STATES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.empty is used to create the 'states' array, which is then filled with data representing the activations of neurons in a reservoir computing system."</data>
      <data key="d6">53e61c078c8f43b7a9b0efb347f394a6</data>
    </edge>
    <edge source="&quot;RESERVOIR.OUTPUT_DIM&quot;" target="&quot;STATES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"reservoir.output_dim specifies the number of output dimensions of the reservoir, defining the second dimension of the 'states' array."</data>
      <data key="d6">53e61c078c8f43b7a9b0efb347f394a6</data>
    </edge>
    <edge source="&quot;STATES&quot;" target="&quot;STATES[:, :20]&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"states[:, :20] is a slice notation used to access and visualize the activations of the first 20 neurons across all timesteps in the timeseries."</data>
      <data key="d6">53e61c078c8f43b7a9b0efb347f394a6</data>
    </edge>
    <edge source="&quot;STATES&quot;" target="&quot;FEATURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Features are used as input for machine learning models, while states are variables that store the internal state of a system or model, representing the current condition or configuration of the system."</data>
      <data key="d6">53e61c078c8f43b7a9b0efb347f394a6</data>
    </edge>
    <edge source="&quot;STATES&quot;" target="&quot;INPUTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"States describe the current situation in a dynamic system, which can be influenced by inputs from a dataset."</data>
      <data key="d6">54b1174770e13d4a2bc0916db477cc56</data>
    </edge>
    <edge source="&quot;FOR-LOOP&quot;" target="&quot;FEATURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A for-loop can be used to iterate over features in a dataset, such as processing each pixel in an image."</data>
      <data key="d6">54b1174770e13d4a2bc0916db477cc56</data>
    </edge>
    <edge source="&quot;FEATURES&quot;" target="&quot;INPUTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Features describe aspects of an input, which are examples from a dataset passed to a model."</data>
      <data key="d6">54b1174770e13d4a2bc0916db477cc56</data>
    </edge>
    <edge source="&quot;FEATURES&quot;" target="&quot;SUPERVISED LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Supervised Learning models use labeled data and features to learn patterns and make predictions."</data>
      <data key="d6">e72d27e122bf954a854a23a367c9c609</data>
    </edge>
    <edge source="&quot;FEATURES&quot;" target="&quot;UNSUPERVISED LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Unsupervised Learning models use features to find patterns and group data without the need for labeled data."</data>
      <data key="d6">e72d27e122bf954a854a23a367c9c609</data>
    </edge>
    <edge source="&quot;FEATURES&quot;" target="&quot;IMAGE CLASSIFICATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Image Classification models use visual features of images, such as pixels, to categorize images into different classes."</data>
      <data key="d6">e72d27e122bf954a854a23a367c9c609</data>
    </edge>
    <edge source="&quot;FEATURES&quot;" target="&quot;MACHINE TRANSLATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Machine Translation models use linguistic features of sentences or words to translate text from one language to another."</data>
      <data key="d6">e72d27e122bf954a854a23a367c9c609</data>
    </edge>
    <edge source="&quot;FEATURES&quot;" target="&quot;REINFORCEMENT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reinforcement Learning models use features of states to make decisions and optimize behavior in an environment."</data>
      <data key="d6">e72d27e122bf954a854a23a367c9c609</data>
    </edge>
    <edge source="&quot;FEATURES&quot;" target="&quot;MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Model learns linear coefficients for the Features, which are displayed in a plot."</data>
      <data key="d6">2b1ebc74c60c857b93e8f426a9cd7605</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;TRAINING TASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Model learns through the Training Task, which specifies the objective and guides its parameter adjustment."</data>
      <data key="d6">c41b9b19460dc63e06639ea4bbbd1515</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;READOUT NODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Readout Node is a component of the Model that is trained as a standalone node to perform a specific task."</data>
      <data key="d6">c41b9b19460dc63e06639ea4bbbd1515</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;RESERVOIR.NODES.ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"reservoir.nodes.ESN is not compatible with the standard node interface required for integration into a Model."</data>
      <data key="d6">df811c27ddc46d5b90c5863a52666a4b</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;NP.R_&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.r_ is used to concatenate arrays in the context of updating the model."</data>
      <data key="d6">c9e71660f79df626c288b3a58eab0f2f</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;NP.ARANGE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.arange is used to generate evenly spaced values for visualizing the model's output."</data>
      <data key="d6">c9e71660f79df626c288b3a58eab0f2f</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;NP.RAVEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.ravel is used to flatten multi-dimensional arrays for visualizing the model's output."</data>
      <data key="d6">c9e71660f79df626c288b3a58eab0f2f</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;BIAS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Bias is a constant value added to the input of a neuron in the model to improve predictions."</data>
      <data key="d6">c9e71660f79df626c288b3a58eab0f2f</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;X_TRAIN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The machine learning model is trained using the x_train dataset."
"X_train is used for training the Model."</data>
      <data key="d6">75e530c1a04e30b373dc7cc68e3ad819,a0feae89e52a4291db0a512a3a102d8e</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;LOSS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The loss metric is calculated based on the performance of the machine learning model."</data>
      <data key="d6">75e530c1a04e30b373dc7cc68e3ad819</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;R2&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The R-squared (R2) metric is calculated based on the performance of the machine learning model."</data>
      <data key="d6">75e530c1a04e30b373dc7cc68e3ad819</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;TIMESERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Model is a mathematical representation of a Timeseries, used to understand or predict the system."</data>
      <data key="d6">3edbc4fd903a173282dd592f5e8437d1</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;PEARSON CORRELATION COEFFICIENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Model uses the Pearson Correlation Coefficient to evaluate the linear relationship between two variables."</data>
      <data key="d6">3edbc4fd903a173282dd592f5e8437d1</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;DATASET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Model is applied to a Dataset, which is a collection of data used for analysis or modeling."</data>
      <data key="d6">3edbc4fd903a173282dd592f5e8437d1</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;NODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Nodes are connected together inside directed acyclic graphs to form a Model, which allows for complex operations to be represented."</data>
      <data key="d6">dc46bcef51e88747b544f7efb111203a</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;FEEDBACK CONNECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Some nodes in a Model may be connected through Feedback Connections, which delay the signal, allowing for more complex operations to be represented."</data>
      <data key="d6">dc46bcef51e88747b544f7efb111203a</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN node in ReservoirPy cannot be integrated into a Model, which is a structure used for creating models."</data>
      <data key="d6">3bee7b78d0ab9582cc9bffe9e305df2e</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;NVAR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The NVAR provides data to the Model, which is trained to perform one-step-ahead prediction."</data>
      <data key="d6">4f0156c4eb24a5c168fff8417c6f046f</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;LINEAR COEFFICIENTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Model learns Linear Coefficients, which are displayed in a plot."</data>
      <data key="d6">4f0156c4eb24a5c168fff8417c6f046f</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;WOUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Model learns the weights or coefficients (Wout) for the Features, which are displayed in a plot."</data>
      <data key="d6">2b1ebc74c60c857b93e8f426a9cd7605</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;U&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The model function takes the variable u as input."</data>
      <data key="d6">7b8e1f350eefb392053be12f35fe7daf</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;RES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The model function is used to update the variable u, which is then stored in the variable res."</data>
      <data key="d6">7b8e1f350eefb392053be12f35fe7daf</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;ATTRACTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Model is used to generate the behavior of the Attractor, which describes the long-term behavior of the system."</data>
      <data key="d6">29ce72a8f609c311ebb852cc96aee54d</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;X&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The variable X is used in the mathematical representation of the Model."</data>
      <data key="d6">29ce72a8f609c311ebb852cc96aee54d</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;DX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The derivative of X, dX, is used in the mathematical representation of the Model."</data>
      <data key="d6">29ce72a8f609c311ebb852cc96aee54d</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;TRAIN STEPS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Train Steps are used to train the Model, optimizing its coefficients."</data>
      <data key="d6">29ce72a8f609c311ebb852cc96aee54d</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;WARM STEPS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Warm Steps are used to initialize the Model before training, allowing it to converge to a stable state."</data>
      <data key="d6">29ce72a8f609c311ebb852cc96aee54d</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;TEST STEPS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Test Steps are used to evaluate the Model's performance after training, ensuring its accuracy and reliability."</data>
      <data key="d6">29ce72a8f609c311ebb852cc96aee54d</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;RESEARCHER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Researcher uses the Model to infer values based on the input data."</data>
      <data key="d6">60639eb7c0f26a58e503c93e29c050b3</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;LORENZ ATTRACTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Model is used in the analysis of the Lorenz Attractor, a mathematical concept."</data>
      <data key="d6">60639eb7c0f26a58e503c93e29c050b3</data>
    </edge>
    <edge source="&quot;MODEL&quot;" target="&quot;TIME SERIES PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Model is used for Time Series Prediction, aiming to predict future values based on past data."</data>
      <data key="d6">46dcc47b4358d3895c1eeb1182c6f997</data>
    </edge>
    <edge source="&quot;SUPERVISED LEARNING&quot;" target="&quot;UNSUPERVISED LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Supervised Learning and Unsupervised Learning are two different approaches in machine learning, with Supervised Learning using labeled data and Unsupervised Learning focusing on finding patterns in unlabeled data."</data>
      <data key="d6">58330f62da357197950f63388e4ceaff</data>
    </edge>
    <edge source="&quot;SUPERVISED LEARNING&quot;" target="&quot;ESN METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN methods are mentioned in the context of Supervised Learning."</data>
      <data key="d6">dbca0570761b1698d32f0c0bfb593b1a</data>
    </edge>
    <edge source="&quot;UNSUPERVISED LEARNING&quot;" target="&quot;NEURAL HISTORY COMPRESSOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Neural History Compressor is an unsupervised learning model that learns patterns from unlabeled data."</data>
      <data key="d6">7b6ff30ef255db2d2c68326d78cf0115</data>
    </edge>
    <edge source="&quot;CONTEXT MANAGER&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Context Manager is a Python feature used to temporarily modify the feedback received by a node in an Echo State Network (ESN)."</data>
      <data key="d6">b338d2dcc1fe6ccf42407444c02cad7c</data>
    </edge>
    <edge source="&quot;ARTIFICIAL NEURAL NETWORK&quot;" target="&quot;RIDGE READOUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Artificial Neural Networks can use Ridge Readout as an offline readout, which needs to be fitted with data before usage."</data>
      <data key="d6">7e6b5dcab1703bfec57161a4d5543848</data>
    </edge>
    <edge source="&quot;ARTIFICIAL NEURAL NETWORK&quot;" target="&quot;ONLINE READOUTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Artificial Neural Networks can also use Online Readouts, which can update their weights continuously as new data arrives."</data>
      <data key="d6">7e6b5dcab1703bfec57161a4d5543848</data>
    </edge>
    <edge source="&quot;ARTIFICIAL NEURAL NETWORK&quot;" target="&quot;WIKIPEDIA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The provided information about Artificial Neural Networks and their components is sourced from the Wikipedia page on Neural Networks."</data>
      <data key="d6">7e6b5dcab1703bfec57161a4d5543848</data>
    </edge>
    <edge source="&quot;RESERVOIR NODE&quot;" target="&quot;DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Data is processed by the Reservoir Node in the Echo State Network (ESN) model."</data>
      <data key="d6">32f8cfb6373e6cb4d6daa32e52aa74fc</data>
    </edge>
    <edge source="&quot;RESERVOIR NODE&quot;" target="&quot;READOUT NODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The output of the Reservoir Node is used as input to the Readout Node in the Echo State Network (ESN) model."</data>
      <data key="d6">32f8cfb6373e6cb4d6daa32e52aa74fc</data>
    </edge>
    <edge source="&quot;RESERVOIR NODE&quot;" target="&quot;FEEDBACK CONNECTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feedback Connection allows the state of one Reservoir Node to influence another Reservoir Node with a one-timestep delay in the Echo State Network (ESN) model."</data>
      <data key="d6">32f8cfb6373e6cb4d6daa32e52aa74fc</data>
    </edge>
    <edge source="&quot;RIDGE READOUT&quot;" target="&quot;FITTING PROCESS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Fitting Process is used to train a Ridge Readout, allowing it to learn the connections from the reservoir to the readout neurons based on the provided data."</data>
      <data key="d6">87757855658e1d198ec49a3290760dd5</data>
    </edge>
    <edge source="&quot;RIDGE READOUT&quot;" target="&quot;RIDGE PARAMETER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Ridge Parameter is a hyperparameter used in Ridge Readout to control the strength of the regularization term, with a value of 1e-7 used to prevent overfitting."</data>
      <data key="d6">b09c66bc81fd63720bef0cfa941ee65b</data>
    </edge>
    <edge source="&quot;RIDGE READOUT&quot;" target="&quot;RIDGE REGULARIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge Regularization is used in the Ridge Readout to avoid overfitting during training."</data>
      <data key="d6">5d3baa9818a4e01fe1196c43378a2cea</data>
    </edge>
    <edge source="&quot;RIDGE READOUT&quot;" target="&quot;TRAINING TASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Ridge Readout is trained to solve a specific Training Task, where it creates a mapping from input to target timeseries."</data>
      <data key="d6">5d3baa9818a4e01fe1196c43378a2cea</data>
    </edge>
    <edge source="&quot;RIDGE READOUT&quot;" target="&quot;SINE WAVE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Ridge Readout is trained to predict the next value in the Sine Wave sequence."</data>
      <data key="d6">f2d5625f36aa4cb036089ce89ec607eb</data>
    </edge>
    <edge source="&quot;RIDGE READOUT&quot;" target="&quot;TRAINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Ridge Readout undergoes the Training process to learn the Sine Wave sequence."</data>
      <data key="d6">f2d5625f36aa4cb036089ce89ec607eb</data>
    </edge>
    <edge source="&quot;TEACHER VECTORS&quot;" target="&quot;TRAINING TASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Teacher Vectors are the target outputs used during the training of a machine learning model to achieve a specific Training Task."</data>
      <data key="d6">173a2da2c7ea80f95b04db8422ced004</data>
    </edge>
    <edge source="&quot;TEACHER VECTORS&quot;" target="&quot;FORCED FEEDBACK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Teacher Vectors are used as feedback in the Forced Feedback technique for Echo State Networks (ESNs)."</data>
      <data key="d6">0d922ae20673124fc4588949e3863ed0</data>
    </edge>
    <edge source="&quot;TIMESERIES&quot;" target="&quot;ESN&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"The ESN is being trained to make one-step-ahead forecasts of a Timeseries data set."
"The Echo State Network is trained on a Timeseries to make one-step-ahead forecasts."
"Echo State Networks (ESNs) are used for prediction and generation tasks on Timeseries data."</data>
      <data key="d6">7f70879016c133fe58e4838172a69613,f730c6800099724052a2d061f3cd8c2e,f7f7dbc1e69b3b0e801bc5ba9c0cabca</data>
    </edge>
    <edge source="&quot;TIMESERIES&quot;" target="&quot;NARMA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NARMA is a system used for generating Timeseries data."</data>
      <data key="d6">8e2e87aa712be195790cf15483428d7f</data>
    </edge>
    <edge source="&quot;TIMESERIES&quot;" target="&quot;MACKEY-GLASS EQUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Mackey-Glass Equation is used to generate a Timeseries, which is then visualized and analyzed."</data>
      <data key="d6">518f1e492b92054cf2f5c5289444da02</data>
    </edge>
    <edge source="&quot;TIMESERIES&quot;" target="&quot;TUTORIAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The tutorial uses timeseries as sequential data for training the Echo State Network (ESN) created using ReservoirPy."</data>
      <data key="d6">74c073137c970e32982756d008532cb8</data>
    </edge>
    <edge source="&quot;TIMESERIES&quot;" target="&quot;GENERATIVE MODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Generative Mode is used to generate and compare Timeseries data."</data>
      <data key="d6">70db98fabc82fc96ecf8cc2c023b586b</data>
    </edge>
    <edge source="&quot;READOUT NODE&quot;" target="&quot;CONCATENATE NODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Concatenate Node combines multiple inputs and passes the result to the Readout Node in the Echo State Network (ESN) model."</data>
      <data key="d6">32f8cfb6373e6cb4d6daa32e52aa74fc</data>
    </edge>
    <edge source="&quot;FORECASTING&quot;" target="&quot;TIME SERIES DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Forecasting is used to predict future values in Time Series Data, such as predicting the next values of a sine wave."</data>
      <data key="d6">cc12e22afbf2ef530100516df59d24f2</data>
    </edge>
    <edge source="&quot;FORECASTING&quot;" target="&quot;MANUAL MANAGEMENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Manual Management may not leverage the seamless data flow and integrated training provided by the canonical method, potentially leading to less efficient training and suboptimal performance in forecasting."</data>
      <data key="d6">cc12e22afbf2ef530100516df59d24f2</data>
    </edge>
    <edge source="&quot;FORECASTING&quot;" target="&quot;NP.ARANGE() FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.arange() function generates an array of values used for forecasting future time steps."</data>
      <data key="d6">4ac00cf37a752d89d55a749c01c6f6fd</data>
    </edge>
    <edge source="&quot;FORECASTING&quot;" target="&quot;DOUBLE-SCROLL ATTRACTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Double-Scroll Attractor is used as a complex system of differential equations in the example, and the goal is to forecast its future values 10 steps ahead."</data>
      <data key="d6">2d8ea1123f365fb047b024022ba4fdc4</data>
    </edge>
    <edge source="&quot;FORECASTING&quot;" target="&quot;ONLINE TIME SERIES APPROXIMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Online Time Series Approximation is a problem that can contribute to the process of forecasting by providing an approximate representation of data that supports time series queries."</data>
      <data key="d6">9261efcc24379d9c0b2d35a2fde8275d</data>
    </edge>
    <edge source="&quot;FORECASTING&quot;" target="&quot;STATISTICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Statistics provides a means of transferring knowledge about a sample to other related populations, which is not necessarily the same as forecasting over time."</data>
      <data key="d6">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </edge>
    <edge source="&quot;FORECASTING&quot;" target="&quot;MACKEY-GLASS TIME SERIES&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Forecasting is the process of predicting future values of the Mackey-Glass Time Series dataset."
"The Mackey-Glass Time Series dataset is used for forecasting, as demonstrated in the provided code using the ReservoirPy library to convert it into a forecasting format."</data>
      <data key="d6">4a7ca13b3f869961817e2aa723e67d24,b2beacacc8c190393e4583a69518378c</data>
    </edge>
    <edge source="&quot;MATRIX&quot;" target="&quot;WEIGHT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Weights are individual numerical values in a Matrix that define the strength of connections between nodes in a neural network."</data>
      <data key="d6">cc12e22afbf2ef530100516df59d24f2</data>
    </edge>
    <edge source="&quot;PARALLELIZATION&quot;" target="&quot;NEURAL NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Parallelization is used to improve performance and efficiency in training neural networks by dividing the task into smaller sub-tasks that can be executed simultaneously on multiple processors or cores."</data>
      <data key="d6">cc12e22afbf2ef530100516df59d24f2</data>
    </edge>
    <edge source="&quot;PARALLELIZATION&quot;" target="&quot;ESN&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"ESN training can be parallelized to speed up computation and reduce overall training time."
"Parallelization is mentioned in the context of training and running Echo State Networks to speed up computation and improve efficiency."
"The ESN node in ReservoirPy uses parallelization techniques to improve efficiency in processing multiple sequences or timesteps simultaneously."</data>
      <data key="d6">1cbfde86d1258f2b267135412e50a590,3bee7b78d0ab9582cc9bffe9e305df2e,593080a95ef7640b3925b07cad1bedd4</data>
    </edge>
    <edge source="&quot;PARALLELIZATION&quot;" target="&quot;MULTIPROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multiprocessing is a method of parallel computation that is mentioned in the context of parallelization."</data>
      <data key="d6">593080a95ef7640b3925b07cad1bedd4</data>
    </edge>
    <edge source="&quot;NEURAL NETWORK&quot;" target="&quot;DEEP ARCHITECTURE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep Architecture refers to a neural network with multiple hidden layers, allowing it to learn and represent more complex patterns and features in the data."</data>
      <data key="d6">a16039f06e545c915f8e7668c39c3e5c</data>
    </edge>
    <edge source="&quot;NEURAL NETWORK&quot;" target="&quot;MACHINE LEARNING&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Machine Learning is a field that focuses on developing neural networks and other statistical models to enable computers to learn patterns and make predictions based on data."
"Neural Network is a type of machine learning model used for various tasks."</data>
      <data key="d6">8e2e87aa712be195790cf15483428d7f,a16039f06e545c915f8e7668c39c3e5c</data>
    </edge>
    <edge source="&quot;NEURAL NETWORK&quot;" target="&quot;INTRINSIC PLASTICITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Intrinsic Plasticity is a method used to adapt the parameters of a Neural Network during training."</data>
      <data key="d6">8e2e87aa712be195790cf15483428d7f</data>
    </edge>
    <edge source="&quot;NEURAL NETWORK&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN is a type of neural network used for training, which is a machine learning model."</data>
      <data key="d6">894d59d781535ca85389c4226715c007</data>
    </edge>
    <edge source="&quot;TIME SERIES DATA&quot;" target="&quot;PANEL DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Series Data is a type of Panel Data, which is a more general class of multidimensional data set."</data>
      <data key="d6">c087c124713c7ade4223617d95928cbf</data>
    </edge>
    <edge source="&quot;TIME SERIES DATA&quot;" target="&quot;AUTOREGRESSIVE (AR) MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Autoregressive (AR) Models are used to model variations in Time Series Data."</data>
      <data key="d6">a000a3fbf1f8fad62e4c25b495858c79</data>
    </edge>
    <edge source="&quot;TIME SERIES DATA&quot;" target="&quot;INTEGRATED (I) MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Integrated (I) Models are used to model variations in the level of Time Series Data."</data>
      <data key="d6">a000a3fbf1f8fad62e4c25b495858c79</data>
    </edge>
    <edge source="&quot;TIME SERIES DATA&quot;" target="&quot;MOVING-AVERAGE (MA) MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Moving-Average (MA) Models are used to model variations in Time Series Data."</data>
      <data key="d6">a000a3fbf1f8fad62e4c25b495858c79</data>
    </edge>
    <edge source="&quot;TIME SERIES DATA&quot;" target="&quot;SINE WAVE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Sine Wave is an example of Time Series Data, a sequence of data points collected at regular time intervals."</data>
      <data key="d6">2bcc39da2ecef3011cc3da428fca5dd5</data>
    </edge>
    <edge source="&quot;MACHINE LEARNING&quot;" target="&quot;IPRESERVOIR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The IPReservoir model is a machine learning model that falls under the field of Machine Learning."</data>
      <data key="d6">eadceb9674dd1ce90473d99e0b58e141</data>
    </edge>
    <edge source="&quot;DEEP ARCHITECTURES&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep Architectures involve combining nodes in various ways to create more complex structures than a simple reservoir and readout, as demonstrated in the ESN model."</data>
      <data key="d6">f0c8d4d322d73f46464e3e9f6914f2ee</data>
    </edge>
    <edge source="&quot;INPUT-TO-READOUT CONNECTIONS&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"More advanced ESNs may include direct connections from input to readout."</data>
      <data key="d6">ead6383a44acd8ebd17907b85a910455</data>
    </edge>
    <edge source="&quot;INPUT-TO-READOUT CONNECTIONS&quot;" target="&quot;RESERVOIR COMPUTING MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing Models utilize Input-to-readout Connections."</data>
      <data key="d6">71f966d00b6d0eceb580d00b9cb86b1e</data>
    </edge>
    <edge source="&quot;INPUT NODES&quot;" target="&quot;RESERVOIR NODES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input Nodes pass their processed data to Reservoir Nodes, allowing the network to learn and capture patterns."</data>
      <data key="d6">b641b2be224e677674f7d3523e87ccde</data>
    </edge>
    <edge source="&quot;RESERVOIR NODES&quot;" target="&quot;READOUT NODES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Nodes pass their processed data to Readout Nodes, which produce the final output based on the input data."</data>
      <data key="d6">b641b2be224e677674f7d3523e87ccde</data>
    </edge>
    <edge source="&quot;READOUT NODES&quot;" target="&quot;MANY-TO-ONE CONNECTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Many-to-one Connections allow multiple inputs to be aggregated and processed by a single Readout Node, enhancing the model's ability to process complex data."</data>
      <data key="d6">b641b2be224e677674f7d3523e87ccde</data>
    </edge>
    <edge source="&quot;READOUT NODES&quot;" target="&quot;ONE-TO-MANY CONNECTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"One-to-many Connections allow the same data to be processed in different ways by multiple Readout Nodes, enabling diverse output representations."</data>
      <data key="d6">b641b2be224e677674f7d3523e87ccde</data>
    </edge>
    <edge source="&quot;ONE-TO-MANY CONNECTIONS&quot;" target="&quot;ITERABLES OF NODES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Iterables of Nodes can be used to create One-to-Many Connections, enabling the same input data to be processed in different ways by different nodes."</data>
      <data key="d6">a35f6cae32a3d24b18ee17ec0471a9d4</data>
    </edge>
    <edge source="&quot;NODES&quot;" target="&quot;CONCAT NODE&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"A Concat Node aggregates multiple input vectors into a single concatenated vector, allowing subsequent nodes that can only process a single input to handle the combined data.""Nodes can be connected to a Concat Node to handle multiple inputs, while most nodes are designed to process a single input vector."</data>
      <data key="d6">e9f7bc2274e59b0767e1172a848ddca9</data>
    </edge>
    <edge source="&quot;ESN MODEL&quot;" target="&quot;TRAINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN Model is trained during the Training event, initializing nodes and training the Ridge readout."</data>
      <data key="d6">8ade7819a5f8d1ec26e9bdbd059142e6</data>
    </edge>
    <edge source="&quot;ESN MODEL&quot;" target="&quot;RUNNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN Model is used to make predictions during the Running event on unseen data."</data>
      <data key="d6">8ade7819a5f8d1ec26e9bdbd059142e6</data>
    </edge>
    <edge source="&quot;ESN MODEL&quot;" target="&quot;AUTHOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author demonstrates expertise in using the ESN Model to analyze and predict Time Series data."</data>
      <data key="d6">973d44d321c7ceee7add295c60b085d2</data>
    </edge>
    <edge source="&quot;ESN MODEL&quot;" target="&quot;MATPLOTLIB&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Matplotlib is used to create visualizations of the ESN Model's output, such as the sine wave plot."</data>
      <data key="d6">8294eed5fc10df1c118f9afa266910e4</data>
    </edge>
    <edge source="&quot;ESN MODEL&quot;" target="&quot;JAEGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jaeger is a developer of the Echo State Network (ESN) model, which is used in the ESN Model."</data>
      <data key="d6">52d001cd1786e3d9f36e0c57538bc21e</data>
    </edge>
    <edge source="&quot;ESN MODEL&quot;" target="&quot;HAAS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Haas is a developer of the Echo State Network (ESN) model, which is used in the ESN Model."</data>
      <data key="d6">52d001cd1786e3d9f36e0c57538bc21e</data>
    </edge>
    <edge source="&quot;DATA&quot;" target="&quot;AUTHOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author uses data as input and output for the reservoir computing model."</data>
      <data key="d6">069ae9388dfd52fec9c184c7168f64dd</data>
    </edge>
    <edge source="&quot;FEEDBACK CONNECTION&quot;" target="&quot;CONTROL SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feedback connections enable the network to use past control signals to adjust future control actions, which is beneficial for control systems tasks."</data>
      <data key="d6">57a27a1504a5ef7d330172c0ac1085c9</data>
    </edge>
    <edge source="&quot;CONTROL SYSTEMS&quot;" target="&quot;'&lt;&lt;' OPERATOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Control systems uses the '&lt;&lt;' operator to incorporate past control signals into current processing."</data>
      <data key="d6">c4b54c2da2dda7e660de7bd6de6f13b4</data>
    </edge>
    <edge source="&quot;'&gt;&gt;' OPERATOR&quot;" target="&quot;'&lt;&lt;' OPERATOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The '&gt;&gt;' operator is used to chain connections between nodes in sequence, while the '&lt;&lt;' operator creates a feedback connection with a one-timestep delay."</data>
      <data key="d6">c4b54c2da2dda7e660de7bd6de6f13b4</data>
    </edge>
    <edge source="&quot;'&lt;&lt;' OPERATOR&quot;" target="&quot;'&lt;&lt;=' OPERATOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both the '&lt;&lt;' and '&lt;&lt;=' operators establish a feedback connection, but the '&lt;&lt;=' operator does so without creating a copy of the receiver node."</data>
      <data key="d6">c4b54c2da2dda7e660de7bd6de6f13b4</data>
    </edge>
    <edge source="&quot;FORCED FEEDBACK&quot;" target="&quot;MACHINE LEARNING MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The machine learning model may use forced feedback to maintain functionality when direct feedback connections are not available."</data>
      <data key="d6">9f1e5883a5f969a6d913beed5a5abd4f</data>
    </edge>
    <edge source="&quot;FORCED FEEDBACK&quot;" target="&quot;SHIFT FEEDBACK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Shift feedback is used to align past outputs with future inputs in the forced feedback timeseries."</data>
      <data key="d6">9f1e5883a5f969a6d913beed5a5abd4f</data>
    </edge>
    <edge source="&quot;MODEL.FIT()&quot;" target="&quot;ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Model.fit() is a method used to train an Echo State Network (ESN) on input data."</data>
      <data key="d6">b338d2dcc1fe6ccf42407444c02cad7c</data>
    </edge>
    <edge source="&quot;TEACHER FORCING&quot;" target="&quot;SEQUENCE MODELING&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Teacher Forcing is a training technique used in sequence modeling where the model is provided with the correct input at each time step during training, allowing it to learn the sequence more effectively."
"Teacher Forcing is a technique used in Sequence Modeling to help the network learn the correct sequence of outputs."</data>
      <data key="d6">3ecffab3c205dece73b47f9a7004fc89,d0a69d653d08e58959dd8d0f2033e697</data>
    </edge>
    <edge source="&quot;TEACHER FORCING&quot;" target="&quot;CONVERGENCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Convergence is a result of the training process in Teacher Forcing, indicating that the model has effectively learned the mapping between inputs and targets."</data>
      <data key="d6">d0a69d653d08e58959dd8d0f2033e697</data>
    </edge>
    <edge source="&quot;CONVERGENCE&quot;" target="&quot;MACHINE LEARNING MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The machine learning model reaches convergence, indicating it has learned patterns in the training data."</data>
      <data key="d6">9f1e5883a5f969a6d913beed5a5abd4f</data>
    </edge>
    <edge source="&quot;MACHINE LEARNING MODEL&quot;" target="&quot;HYPERPARAMETER OPTIMIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperparameter Optimization is used to improve the performance of a Machine Learning Model by finding the best hyperparameters for the model."</data>
      <data key="d6">bd4cf5e35045463b7f0d8da82debc122</data>
    </edge>
    <edge source="&quot;MACHINE LEARNING MODEL&quot;" target="&quot;ALPHAS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ALPHAS is a parameter used in the machine learning model, influencing its development and performance."</data>
      <data key="d6">96366d7c23d50de6294c54c3444eac86</data>
    </edge>
    <edge source="&quot;MACHINE LEARNING MODEL&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ES2N is a machine learning model, which is the subject of the text, with its components and parameters being described."</data>
      <data key="d6">96366d7c23d50de6294c54c3444eac86</data>
    </edge>
    <edge source="&quot;MACHINE LEARNING MODEL&quot;" target="&quot;UNITS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"UNITS is a parameter used in the machine learning model, influencing its architecture and functionality."</data>
      <data key="d6">96366d7c23d50de6294c54c3444eac86</data>
    </edge>
    <edge source="&quot;MACHINE LEARNING MODEL&quot;" target="&quot;SPECTRAL_RADIUS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"spectral_radius is a parameter used in the machine learning model, influencing its architecture and performance."</data>
      <data key="d6">96366d7c23d50de6294c54c3444eac86</data>
    </edge>
    <edge source="&quot;MACHINE LEARNING MODEL&quot;" target="&quot;ALPHA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"alpha is a parameter used in the machine learning model, influencing its architecture and performance."</data>
      <data key="d6">96366d7c23d50de6294c54c3444eac86</data>
    </edge>
    <edge source="&quot;MACHINE LEARNING MODEL&quot;" target="&quot;INPUT_SCALING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"input_scaling is a parameter used in the machine learning model, influencing its architecture and performance."</data>
      <data key="d6">96366d7c23d50de6294c54c3444eac86</data>
    </edge>
    <edge source="&quot;MACHINE LEARNING MODEL&quot;" target="&quot;PROXIMITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"proximity is a parameter used in the ES2N machine learning model, influencing its architecture and performance."</data>
      <data key="d6">96366d7c23d50de6294c54c3444eac86</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;CUSTOM WEIGHT MATRIX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A Custom Weight Matrix is used in the ESN to adjust the input-reservoir connections, potentially improving the model's performance."</data>
      <data key="d6">693e4d1e43289f46866236c10207a17e</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;ESN OFFLINE TRAINING WITH RIDGE REGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN is trained using ridge regression for offline tasks."</data>
      <data key="d6">18e4624a6da9e8e6d9b9b2ed260bf9b2</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;LINSPACE()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"linspace() is used to generate input and target sequences for the ESN, which it then trains and runs."</data>
      <data key="d6">df811c27ddc46d5b90c5863a52666a4b</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;WORKERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The workers parameter specifies the number of parallel processes to use for training and running the ESN."</data>
      <data key="d6">3695f5d218cdda0a91ae6a2f9b296837</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;BACKEND&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The backend parameter specifies the method of execution for the ESN."</data>
      <data key="d6">3695f5d218cdda0a91ae6a2f9b296837</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;COMPLEX MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Complex Models are advanced versions of the ESN model that can handle more complex tasks."</data>
      <data key="d6">3695f5d218cdda0a91ae6a2f9b296837</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;MODEL-0&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Model-0 is a specific model or iteration of an Echo State Network."</data>
      <data key="d6">9078b0f36522f21a9e8e1aadac48ed9c</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;X_TEST1&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network (ESN) is processing the 'X_test1' input data."</data>
      <data key="d6">9078b0f36522f21a9e8e1aadac48ed9c</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;OPTIMIZATION ALGORITHMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN is a model function used in the optimization process, often combined with the loss function to find local minima."</data>
      <data key="d6">4f7b43545046f0e6f9b6fb3816da1d79</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;RMSE&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"RMSE is used to evaluate the performance of the Echo State Network."
"The Root Mean Squared Error (RMSE) loss function is used to evaluate the quality of parameters in the Echo State Network (ESN) model."
"Root Mean Squared Error is used to evaluate the performance of the Echo State Network."</data>
      <data key="d6">251a50c2ae8ceea4fd7da1127cc5f461,bf4eaad93f89884d02cdad6a50f145a6,f730c6800099724052a2d061f3cd8c2e</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;R^2 SCORE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"R^2 Score is used to evaluate the performance of the Echo State Network."</data>
      <data key="d6">f730c6800099724052a2d061f3cd8c2e</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;MACKEY-GLASS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network (ESN) is used as an example for timeseries prediction using the Mackey-Glass chaotic system."</data>
      <data key="d6">0b6c69085074b2cf23267eb149068b9f</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;ICANN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A guide on exploring hyper-parameters for Echo State Networks was presented at ICANN, which involves the use of Echo State Networks."</data>
      <data key="d6">088d2280349d652200861994c09d7dd5</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;STATE COLLECTION MATRIX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"During the training of the ESN, extended system states are filed row-wise into a State Collection Matrix."</data>
      <data key="d6">f18a060e6d2bb1da70432cbc71378770</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;SYSTEM EQUATIONS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The ESN is described by System Equations, which include mathematical expressions for the reservoir and input states, and the output activation function."
"System Equations are used in the ESN model to describe its behavior."</data>
      <data key="d6">6a4432cd530b28770e2b903fe242a0d1,f18a060e6d2bb1da70432cbc71378770</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;OUTPUT FEEDBACK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN model may include output feedback, where correct outputs are written into the output units during the generation of system states."</data>
      <data key="d6">6a4432cd530b28770e2b903fe242a0d1</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;DESIRED OUTPUTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN model aims to produce desired outputs, which are the target values it aims to achieve."</data>
      <data key="d6">6a4432cd530b28770e2b903fe242a0d1</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;MANJUNATH AND JAEGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Manjunath and Jaeger are the authors of a study that discusses the properties of Echo State Networks, including memory capacity."</data>
      <data key="d6">d4563a00dc04ebf7bcf01e5062fde46f</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;INPUT SIGNAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network processes the Input Signal to produce an Output Signal, and its memory capacity is quantified based on the correlation between delayed input signals and trained output signals."</data>
      <data key="d6">d4563a00dc04ebf7bcf01e5062fde46f</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;OUTPUT SIGNAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network produces an Output Signal as a result of processing the Input Signal, and its memory capacity is evaluated by training the network to predict and memorize delayed input signals."</data>
      <data key="d6">d4563a00dc04ebf7bcf01e5062fde46f</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;MEMORY CAPACITY&quot;">
      <data key="d4">4.0</data>
      <data key="d5">"The Echo State Network's memory capacity is a measure of its ability to retain and recall information from past input signals, which is quantified in the text."
"ESN models are used to measure Memory Capacity."
"The Memory Capacity of the ESN model is being measured and compared to other models."
"ESN is analyzed for its Memory Capacity, which is compared to ES^2N."</data>
      <data key="d6">24b347e60cb01aea26f46f3067f5a0f0,c53825a1ab5e01a794a428988435a7a7,d4563a00dc04ebf7bcf01e5062fde46f,f16792dcee6dab8ea8f8c8c6793bbc3d</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;TIME SERIES PREDICTION&quot;">
      <data key="d4">7.0</data>
      <data key="d5">"ESN is a model used for Time Series Prediction, as it can be trained to forecast future values based on past observations."
"The ESN model is used for Time Series Prediction tasks."
"ESN is used for the task of Time Series Prediction, which involves forecasting future values based on past data."
"ESN is primarily used for Time Series Prediction, which involves forecasting future values based on past observations."
"ESN is used for time series prediction, which involves forecasting future values based on past observations."
"ESN is a type of network used for time series prediction."
"ESN is used for the task of Time Series Prediction in the provided code."</data>
      <data key="d6">36e4df75a46fb977f9516f2d2f1f9bc2,5f841065cf74ef7bbc28efb775d5585e,6a7bea5f60347ea864c06adc327829dc,7b9936d57ece8ba985947a7aca12e2c7,cc1fb6ca5695434ad0279c2606e928af,eb7a223eeb120e3fcc45a96a6018707d,f0c8d4d322d73f46464e3e9f6914f2ee</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ES2N is a variant of the ESN model that incorporates a proximity parameter to improve performance in Time Series Prediction."</data>
      <data key="d6">5f841065cf74ef7bbc28efb775d5585e</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;LEAKY ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Leaky ESN is a modification of the ESN model that introduces a leaking rate parameter to improve stability in Time Series Prediction."</data>
      <data key="d6">5f841065cf74ef7bbc28efb775d5585e</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;LINEARESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LinearESN is a variant of the ESN model that uses a linear activation function in the reservoir layer, which can improve performance in Time Series Prediction."</data>
      <data key="d6">5f841065cf74ef7bbc28efb775d5585e</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;ORTHOESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"OrthoESN is a variant of the ESN model that uses an orthogonal initialization method for the reservoir weights, which can improve performance in Time Series Prediction."</data>
      <data key="d6">5f841065cf74ef7bbc28efb775d5585e</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;LEARNING RATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN model's performance is being investigated with respect to different Learning Rates."</data>
      <data key="d6">24b347e60cb01aea26f46f3067f5a0f0</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;RESEARCHER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Researcher uses ESN in their analysis."</data>
      <data key="d6">f16792dcee6dab8ea8f8c8c6793bbc3d</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;HYPER-PARAMETERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyper-parameters are used in the analysis of ESN."</data>
      <data key="d6">f16792dcee6dab8ea8f8c8c6793bbc3d</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;POWERNORM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN uses PowerNorm for data visualization, enhancing the presentation of results."</data>
      <data key="d6">6425e3620184116a3ee92d5690e4f891</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;NRMSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN is optimized using the NRMSE metric, indicating its role in model performance evaluation."</data>
      <data key="d6">6425e3620184116a3ee92d5690e4f891</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;NON-LINEARITY STRENGTH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Non-linearity strength is a parameter used to adjust the complexity of the ESN system."</data>
      <data key="d6">6425e3620184116a3ee92d5690e4f891</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;DELAY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Delay is a parameter used to introduce a time lag in the ESN system."</data>
      <data key="d6">6425e3620184116a3ee92d5690e4f891</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;NP.TANH&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The ESN model uses the hyperbolic tangent activation function (np.tanh) as a hyperparameter."
"The ESN uses the np.tanh function to apply the hyperbolic tangent activation function, as demonstrated in the provided code."</data>
      <data key="d6">6a7bea5f60347ea864c06adc327829dc,d2cc4243ed9b1bb887527f7cc1153033</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;ES^2N&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ESN and ES^2N are compared in their performance on the Multiple Superimposed Oscillator Task."
"ES^2N is a variant of ESN that incorporates additional features for improved time series prediction, as demonstrated in the provided code."</data>
      <data key="d6">4fb25ea25c60216b307931b5edacc5cb,d2cc4243ed9b1bb887527f7cc1153033</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;MULTIPLE SUPERIMPOSED OSCILLATOR TASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN is used to analyze and predict the Multiple Superimposed Oscillator Task."</data>
      <data key="d6">4fb25ea25c60216b307931b5edacc5cb</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;NOISY_TANH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"noisy_tanh is used as an activation function in the ESN machine learning model."</data>
      <data key="d6">1298c65a923053e1de35aacddc13832c</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;X_TRAIN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"x_train is used for training the ESN machine learning model."
"The Echo State Network (ESN) system is trained using the training input data (X_train)."</data>
      <data key="d6">1298c65a923053e1de35aacddc13832c,80c9f51870e239404ed671ef0374f191</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;Y_PRED_ESN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"y_pred_esn is the predicted output data from the ESN machine learning model."
"ESN is the model used to generate the predictions stored in y_pred_esn."</data>
      <data key="d6">1298c65a923053e1de35aacddc13832c,12d680622df43439e6de83058b734953</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;MATPLOTLIB&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Matplotlib is used to create visualizations of the time series prediction results obtained using the ESN, as demonstrated in the provided code."</data>
      <data key="d6">d2cc4243ed9b1bb887527f7cc1153033</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;X_TRAIN1&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ESN is trained using the X_train1 dataset."
"ESN is trained using a subset of the input data X_train1."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7,993a69efae014a8f8d6ec0c235104d46</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;Y_TRAIN1&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ESN is trained using the y_train1 dataset as the target output."
"ESN is trained using a subset of the target data y_train1."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7,993a69efae014a8f8d6ec0c235104d46</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;TRAINING&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"ESN is trained on a dataset during the Training process."
"ESN is trained using the input data to optimize its parameters and improve its ability to make accurate predictions."
"ESN is used for training, which is the process of learning patterns and making predictions using a neural network model."</data>
      <data key="d6">36e4df75a46fb977f9516f2d2f1f9bc2,894d59d781535ca85389c4226715c007,cc1fb6ca5695434ad0279c2606e928af</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;TEST&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN is tested on new data to evaluate its performance."</data>
      <data key="d6">cc1fb6ca5695434ad0279c2606e928af</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;AUTHOR&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"The author uses the ESN model to perform time series prediction and forecasting."
"The author demonstrates expertise in using the ESN model for time series prediction and generative tasks."
"The author creates an ESN model using the ReservoirPy library."</data>
      <data key="d6">069ae9388dfd52fec9c184c7168f64dd,29aad23ce67e778ac31d4fb287fd20c7,76963fa19a9caab847e50167f71c86a2</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;ONE-TIMESTEP-AHEAD FORECASTING TASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN is trained on a one-timestep-ahead forecasting task to predict the next time step in a time series."</data>
      <data key="d6">0ae9f3cf96547c05eff54812cb72ac31</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;CLOSED LOOP GENERATIVE MODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN is run on its own predictions in closed loop generative mode to generate new data."</data>
      <data key="d6">0ae9f3cf96547c05eff54812cb72ac31</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;ONE-TIMESTEP-AHEAD FORECAST&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ESN is used to perform one-timestep-ahead forecasting on the input data."
"The ESN model is used to perform the task of One-timestep-ahead Forecast."</data>
      <data key="d6">29aad23ce67e778ac31d4fb287fd20c7,593306edfb8d4c7ef4b99d24fa009970</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;GENERATIVE MODE&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"ESN is used to generate new data points in the time series using the generative mode."
"ESN is used to generate new time series data in Generative Mode."
"The ESN model is also used to generate new data points in Generative Mode."</data>
      <data key="d6">29aad23ce67e778ac31d4fb287fd20c7,424bf7c7b82dc966139c25f7c9ccffb7,593306edfb8d4c7ef4b99d24fa009970</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;NP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np is a library used in the code snippet to perform numerical computations for the ESN model."</data>
      <data key="d6">593306edfb8d4c7ef4b99d24fa009970</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;PLT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"plt is a library used in the code snippet to create visualizations for the ESN model."</data>
      <data key="d6">593306edfb8d4c7ef4b99d24fa009970</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;X&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"X is the input data used in the ESN model for time series prediction and generation."
"ESN takes input data X for training and prediction."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7,593306edfb8d4c7ef4b99d24fa009970</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;X_T&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"X_t is the true values of the input data used in the ESN model for evaluation."</data>
      <data key="d6">593306edfb8d4c7ef4b99d24fa009970</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;X_GEN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"X_gen is the generated values of the input data produced by the ESN model."</data>
      <data key="d6">593306edfb8d4c7ef4b99d24fa009970</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;FORCE ALGORITHM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN uses the FORCE Algorithm for online learning, allowing the update of readout parameters at every timestep of input series."</data>
      <data key="d6">424bf7c7b82dc966139c25f7c9ccffb7</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;SPECIAL NODE E&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN includes a special node E, which plays a significant role in the training process."</data>
      <data key="d6">894d59d781535ca85389c4226715c007</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;VOCAB&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network (ESN) system uses a vocabulary (vocab) to map the predicted output to the corresponding target value."</data>
      <data key="d6">80c9f51870e239404ed671ef0374f191</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;DIRECT CONNECTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"More advanced ESNs may include direct connections from input to readout."</data>
      <data key="d6">ead6383a44acd8ebd17907b85a910455</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;CUSTOM WEIGHT MATRICES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Custom weight matrices can be used to create more complex ESNs."</data>
      <data key="d6">ead6383a44acd8ebd17907b85a910455</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;'DEEP' ARCHITECTURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"'Deep' architectures can be created by stacking multiple ESNs together."</data>
      <data key="d6">ead6383a44acd8ebd17907b85a910455</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;CONCAT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network model uses the Concat node to handle multiple inputs to the readout node."</data>
      <data key="d6">cf15a09e77b695a117e1cca05461aea2</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;MODEL.RUN()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Model.run() is a method used to make predictions using a trained Echo State Network (ESN) on new input data."</data>
      <data key="d6">b338d2dcc1fe6ccf42407444c02cad7c</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;FEEDBACK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feedback is the process of using the output of an Echo State Network (ESN) as input to influence its behavior."</data>
      <data key="d6">b338d2dcc1fe6ccf42407444c02cad7c</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;FEEDBACK TIMESERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feedback Timeseries is a sequence of data used to influence the behavior of an Echo State Network (ESN) during the prediction phase."</data>
      <data key="d6">b338d2dcc1fe6ccf42407444c02cad7c</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;TEACHER VALUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Teacher Values are target values used during the training phase of an Echo State Network (ESN) to guide its learning."</data>
      <data key="d6">b338d2dcc1fe6ccf42407444c02cad7c</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;INITIALIZER FUNCTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN uses Initializer Functions to initialize parameters, such as weight matrices and readouts."</data>
      <data key="d6">38b3e8ea0ec280360770513327b0d9d3</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;WEIGHT MATRICES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN uses Weight Matrices to store connections between nodes, influencing the model's behavior."</data>
      <data key="d6">38b3e8ea0ec280360770513327b0d9d3</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;READOUTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN uses Readouts to output predictions based on the internal state of the model."</data>
      <data key="d6">38b3e8ea0ec280360770513327b0d9d3</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;RESERVOIRS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN uses Reservoirs to store and process information, influencing the model's ability to learn and forecast."</data>
      <data key="d6">38b3e8ea0ec280360770513327b0d9d3</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;JOBLIB&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Joblib can be used to parallelize the computation of node states over independent sequences of inputs in ESN training/running, exploiting multiprocessing."</data>
      <data key="d6">fe90abb0dde126fafbf44782aeb6738c</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;DATA PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN node in ReservoirPy is used for data processing tasks, such as processing input sequences to produce target sequences."</data>
      <data key="d6">3bee7b78d0ab9582cc9bffe9e305df2e</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;SEQUENTIAL BACKEND&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN uses the Sequential Backend for processing data in a sequential manner."</data>
      <data key="d6">f0c8d4d322d73f46464e3e9f6914f2ee</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;MULTIPROCESSING BACKEND&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN uses the Multiprocessing Backend for processing data in parallel using multiple cores."</data>
      <data key="d6">f0c8d4d322d73f46464e3e9f6914f2ee</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;PREDICTIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN model makes predictions, which attempt to forecast the next value in a timeseries."</data>
      <data key="d6">7b294b788fe5ee385d08c4aabe2ca71d</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;Y&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN predicts target data y based on the input data."</data>
      <data key="d6">09198e939639c229c2c97555f65b12a7</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;CHAPTER 2&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN (Echo State Network) is mentioned in Chapter 2 as a type of recurrent neural network used in the context of the generative mode."</data>
      <data key="d6">e396354e3a9be76616392af11f56e671</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;X_TRAIN3&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"X_train3 is used for training the esn model."</data>
      <data key="d6">0c5a253fb2bcebe8674581a5dc12fd96</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;Y_TRAIN3&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"y_train3 is used for training the esn model."</data>
      <data key="d6">0c5a253fb2bcebe8674581a5dc12fd96</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;X_TEST3&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"X_test3 is used for testing the esn model."</data>
      <data key="d6">0c5a253fb2bcebe8674581a5dc12fd96</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;Y_TEST3&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"y_test3 is used for testing the esn model."</data>
      <data key="d6">0c5a253fb2bcebe8674581a5dc12fd96</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;CHAPTER 3&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The esn model is discussed in Chapter 3."</data>
      <data key="d6">0c5a253fb2bcebe8674581a5dc12fd96</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;LBR.FEATURE.DELTA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN uses the lbr.feature.delta function for feature extraction."</data>
      <data key="d6">71366a4c7e791080872ba783d3787bd7</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;INPUTS SCALING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Inputs Scaling is a parameter that scales the input data to improve the performance of the ESN model."</data>
      <data key="d6">72e6eee633bcb5b1458c4cee3975cee1</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;REGULARIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Regularization is a technique used to prevent overfitting in the ESN model by adding a penalty term to the loss function."</data>
      <data key="d6">72e6eee633bcb5b1458c4cee3975cee1</data>
    </edge>
    <edge source="&quot;ESN&quot;" target="&quot;MACKEY-GLASS EQUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ESN is used for analyzing and predicting time series data generated by the Mackey-Glass Equation."</data>
      <data key="d6">1db5e6cd356c6066227de5e273de1abe</data>
    </edge>
    <edge source="&quot;NP.RANDOM.NORMAL&quot;" target="&quot;RESERVOIRPY.MAT_GEN.RANDOM_SPARSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.random.normal is used to generate random numbers for the weight matrix in reservoirpy.mat_gen.random_sparse."</data>
      <data key="d6">4a8a4a7eeebd68a535cf84cfaecebaba</data>
    </edge>
    <edge source="&quot;RESERVOIRPY.MAT_GEN&quot;" target="&quot;RANDOM_SPARSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The reservoirpy.mat_gen submodule provides the random_sparse function for initializing sparse matrices."</data>
      <data key="d6">8f2f2cfd667a304a288723de779c9bee</data>
    </edge>
    <edge source="&quot;PLT.HIST&quot;" target="&quot;NP.RAVEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"plt.hist uses np.ravel to flatten a multi-dimensional array into a one-dimensional array for creating a histogram."</data>
      <data key="d6">4a8a4a7eeebd68a535cf84cfaecebaba</data>
    </edge>
    <edge source="&quot;RANDOM_SPARSE&quot;" target="&quot;UNIFORM DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The random_sparse function can use a uniform distribution to generate non-zero elements of the matrix."</data>
      <data key="d6">8f2f2cfd667a304a288723de779c9bee</data>
    </edge>
    <edge source="&quot;UNIFORM DISTRIBUTION&quot;" target="&quot;MATRIX CREATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A uniform distribution is used to generate non-zero elements of a matrix during its creation, ensuring values are evenly spread within a specified range."</data>
      <data key="d6">8559ec6650745de27a3f41815fbfde09</data>
    </edge>
    <edge source="&quot;DELAYED MATRIX CREATION&quot;" target="&quot;MATRIX CREATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Delayed matrix creation differs from matrix creation in that it initializes the parameters only when needed, such as at the first run."</data>
      <data key="d6">8559ec6650745de27a3f41815fbfde09</data>
    </edge>
    <edge source="&quot;GAUSSIAN DISTRIBUTION&quot;" target="&quot;BERNOULLI RANDOM VARIABLE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A Gaussian distribution and a Bernoulli random variable are both probability distributions, but they have different characteristics and applications."</data>
      <data key="d6">8559ec6650745de27a3f41815fbfde09</data>
    </edge>
    <edge source="&quot;GAUSSIAN DISTRIBUTION&quot;" target="&quot;HYPERBOLIC TANGENT ACTIVATION FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Hyperbolic Tangent Activation Function is recommended to be used with a Gaussian Distribution of parameters &#956;=0 and &#963; for optimal results."</data>
      <data key="d6">cb7823dcc9852e6a6f9e3607cb55134f</data>
    </edge>
    <edge source="&quot;GAUSSIAN DISTRIBUTION&quot;" target="&quot;PAPER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Paper recommends a Gaussian Distribution as the most effective reservoir activation distribution for neurons equipped with a hyperbolic tangent activation function."</data>
      <data key="d6">388cc054a99cc5cadff33147f95d6156</data>
    </edge>
    <edge source="&quot;BERNOULLI RANDOM VARIABLE&quot;" target="&quot;NORMAL DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Normal Distribution is not directly mentioned in relation to Bernoulli Random Variable."</data>
      <data key="d6">1cbfde86d1258f2b267135412e50a590</data>
    </edge>
    <edge source="&quot;MULTIPROCESSING&quot;" target="&quot;COMPUTING NODE STATES OVER INDEPENDENT SEQUENCES OF INPUTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multiprocessing is used to process independent sequences of inputs in parallel, enabling faster computation."</data>
      <data key="d6">18e4624a6da9e8e6d9b9b2ed260bf9b2</data>
    </edge>
    <edge source="&quot;JOBLIB&quot;" target="&quot;COMPUTING NODE STATES OVER INDEPENDENT SEQUENCES OF INPUTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Joblib is mentioned as a tool that can be used to parallelize the computation of node states over independent sequences of inputs."</data>
      <data key="d6">18e4624a6da9e8e6d9b9b2ed260bf9b2</data>
    </edge>
    <edge source="&quot;JOBLIB&quot;" target="&quot;COMPUTING NODE STATES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Joblib is mentioned in the context of computing node states over independent sequences of inputs, indicating its potential use in this process."</data>
      <data key="d6">085c9d7a2af51b93826fc393600682d8</data>
    </edge>
    <edge source="&quot;JOBLIB&quot;" target="&quot;DELAYED()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Joblib provides the delayed() function, which is used to create a lazy evaluation of the function it wraps."</data>
      <data key="d6">f3b5b178557c4991ab5b81d869a4752e</data>
    </edge>
    <edge source="&quot;JOBLIB&quot;" target="&quot;PARALLEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Joblib provides the parallel function, which is used to run tasks concurrently, allowing for faster data loading and processing."</data>
      <data key="d6">f3b5b178557c4991ab5b81d869a4752e</data>
    </edge>
    <edge source="&quot;JOBLIB&quot;" target="&quot;DATA SCIENTIST&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The data scientist uses the joblib library for parallel and efficient computation."</data>
      <data key="d6">9fdaabd6c7e893a275a3848c10007477</data>
    </edge>
    <edge source="&quot;SEQUENCES&quot;" target="&quot;DIFFERENT LENGTHS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sequences are mentioned as having the potential for different lengths, which is a challenge that is acknowledged and addressed with suggested techniques."</data>
      <data key="d6">085c9d7a2af51b93826fc393600682d8</data>
    </edge>
    <edge source="&quot;SEQUENCES&quot;" target="&quot;DIMENSIONALITY REDUCTION TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dimensionality Reduction Techniques are used to match the dimensionality of sequences to a common size."</data>
      <data key="d6">6f3d0ba82cf3b10b19b64f73ace8c695</data>
    </edge>
    <edge source="&quot;SEQUENCES&quot;" target="&quot;FEATURE ENGINEERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Feature Engineering is used to add or remove features from sequences to match the required dimensionality."</data>
      <data key="d6">6f3d0ba82cf3b10b19b64f73ace8c695</data>
    </edge>
    <edge source="&quot;SEQUENCES&quot;" target="&quot;INTERPOLATION OR EXTRAPOLATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Interpolation or Extrapolation is used to adjust the length of sequences to a common size."</data>
      <data key="d6">6f3d0ba82cf3b10b19b64f73ace8c695</data>
    </edge>
    <edge source="&quot;SEQUENCES&quot;" target="&quot;TRANSFORMATION TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Transformation Techniques are used to transform input sequences to a uniform dimensionality before feeding them into the model."</data>
      <data key="d6">6f3d0ba82cf3b10b19b64f73ace8c695</data>
    </edge>
    <edge source="&quot;SEQUENCES&quot;" target="&quot;NP.LINSPACE()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.linspace() is used to generate an array of evenly spaced values that are used to create sine wave sequences."</data>
      <data key="d6">6f3d0ba82cf3b10b19b64f73ace8c695</data>
    </edge>
    <edge source="&quot;SEQUENCES&quot;" target="&quot;NP.SIN()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.sin() is used to compute the sine of input values to create sine wave sequences."</data>
      <data key="d6">6f3d0ba82cf3b10b19b64f73ace8c695</data>
    </edge>
    <edge source="&quot;PADDING&quot;" target="&quot;TRUNCATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Padding and Truncation are techniques used to standardize sequence lengths for efficient processing in machine learning models."</data>
      <data key="d6">33a235bffff79a56e3ff5e5a9e86a3de</data>
    </edge>
    <edge source="&quot;DYNAMIC BATCHING&quot;" target="&quot;SEQUENCE MASKING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dynamic Batching and Sequence Masking are techniques used to minimize padding and ignore padded values during processing and computation."</data>
      <data key="d6">33a235bffff79a56e3ff5e5a9e86a3de</data>
    </edge>
    <edge source="&quot;DIMENSIONALITY REDUCTION&quot;" target="&quot;FEATURE ENGINEERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dimensionality Reduction and Feature Engineering are techniques used to match the required dimensionality of sequences."</data>
      <data key="d6">33a235bffff79a56e3ff5e5a9e86a3de</data>
    </edge>
    <edge source="&quot;INTERPOLATION&quot;" target="&quot;EXTRAPOLATION&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Interpolation and Extrapolation are techniques used to adjust the length of sequences to a common length."
"Extrapolation is similar to Interpolation, but it is used to estimate values beyond the range of known data points."</data>
      <data key="d6">33a235bffff79a56e3ff5e5a9e86a3de,472b44b36407c9a89cf5c51459188263</data>
    </edge>
    <edge source="&quot;INTERPOLATION&quot;" target="&quot;CURVE FITTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Interpolation is a method used in Curve Fitting to construct an exact fit to the data."</data>
      <data key="d6">630c86e110e2dabbe068f446b619cef3</data>
    </edge>
    <edge source="&quot;INTERPOLATION&quot;" target="&quot;ECONOMIC TIME SERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Interpolation is a method used in the construction of Economic Time Series to estimate unknown quantities."</data>
      <data key="d6">bde7c826c746ece93a512a0cf167fa3e</data>
    </edge>
    <edge source="&quot;INTERPOLATION&quot;" target="&quot;POLYNOMIAL INTERPOLATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Polynomial Interpolation is a type of Interpolation that fits piecewise polynomial functions into time intervals."</data>
      <data key="d6">472b44b36407c9a89cf5c51459188263</data>
    </edge>
    <edge source="&quot;INTERPOLATION&quot;" target="&quot;SPLINE INTERPOLATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Spline Interpolation is a type of Interpolation that uses piecewise continuous functions composed of many polynomials."</data>
      <data key="d6">472b44b36407c9a89cf5c51459188263</data>
    </edge>
    <edge source="&quot;INTERPOLATION&quot;" target="&quot;FUNCTION APPROXIMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Interpolation is a technique used in Function Approximation to approximate a function when only a set of points is provided."</data>
      <data key="d6">9ec0dac4c72bcc2c78c7df43b9969fe7</data>
    </edge>
    <edge source="&quot;EXTRAPOLATION&quot;" target="&quot;CURVE FITTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Extrapolation is the use of a fitted curve beyond the range of the observed data in Curve Fitting."</data>
      <data key="d6">630c86e110e2dabbe068f446b619cef3</data>
    </edge>
    <edge source="&quot;EXTRAPOLATION&quot;" target="&quot;ECONOMIC TIME SERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Extrapolation is a method used to predict values of a function in the context of Economic Time Series, subject to uncertainty."</data>
      <data key="d6">bde7c826c746ece93a512a0cf167fa3e</data>
    </edge>
    <edge source="&quot;EXTRAPOLATION&quot;" target="&quot;FUNCTION APPROXIMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Extrapolation is a technique used in Function Approximation to approximate a function when only a set of points is provided."</data>
      <data key="d6">9ec0dac4c72bcc2c78c7df43b9969fe7</data>
    </edge>
    <edge source="&quot;CPU CORES&quot;" target="&quot;WORKERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The workers parameter specifies the number of parallel processes used for training and running the ESN, which can be managed to optimize the use of CPU cores."</data>
      <data key="d6">df811c27ddc46d5b90c5863a52666a4b</data>
    </edge>
    <edge source="&quot;COMPLEX MODELS&quot;" target="&quot;HIERARCHICAL ESNS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Hierarchical ESNs are a type of complex model that utilize multiple layers of ESNs to handle hierarchical structures and tasks."
"Hierarchical ESNs are a type of Complex Model, utilizing a sequential hierarchy of nodes for data processing."</data>
      <data key="d6">22499cd4a0b7216dad5b05eb109fcb73,3695f5d218cdda0a91ae6a2f9b296837</data>
    </edge>
    <edge source="&quot;COMPLEX MODELS&quot;" target="&quot;DEEP ESNS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Deep ESNs are a type of complex model that utilize multiple layers of ESNs to learn deep representations of data."
"Deep ESNs are a type of Complex Model, involving multiple layers of reservoirs connected in series and parallel pathways."</data>
      <data key="d6">22499cd4a0b7216dad5b05eb109fcb73,3695f5d218cdda0a91ae6a2f9b296837</data>
    </edge>
    <edge source="&quot;COMPLEX MODELS&quot;" target="&quot;MULTI-INPUTS ESNS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Multi-inputs ESNs are a type of complex model that can handle multiple input streams simultaneously, allowing for more complex data processing."
"Multi-inputs ESNs are a type of Complex Model, capable of handling multiple input sources for data integration."</data>
      <data key="d6">22499cd4a0b7216dad5b05eb109fcb73,3695f5d218cdda0a91ae6a2f9b296837</data>
    </edge>
    <edge source="&quot;COMPLEX MODELS&quot;" target="&quot;DICTIONARY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dictionaries can be used in the training of Complex Models to specify target outputs for each readout node."</data>
      <data key="d6">22499cd4a0b7216dad5b05eb109fcb73</data>
    </edge>
    <edge source="&quot;DICTIONARY&quot;" target="&quot;SCIKITLEARNNODE&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The concept of a dictionary is mentioned in the context of using ScikitLearnNode, but there is no direct relationship between the two."
"A dictionary is used to specify the parameters of a model when using a ScikitLearnNode."</data>
      <data key="d6">84cacfea14ea9ff46a34150e77a0767a,861c28cb739722ddeb0babb7e1427409</data>
    </edge>
    <edge source="&quot;DEEP ESN&quot;" target="&quot;MULTI-INPUTS ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep ESNs and Multi-inputs ESNs are both types of models, and they can be combined to create more complex and versatile models that can handle multiple inputs and have multiple layers of reservoirs."</data>
      <data key="d6">c7c2383410ac00bad82831596a2d27a6</data>
    </edge>
    <edge source="&quot;DEEP ESN&quot;" target="&quot;MACKEY-GLASS EQUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Mackey-Glass equation is a concept used to describe the temporal behavior of different physiological signals, which can be applied in the context of Deep ESNs for modeling and analysis."</data>
      <data key="d6">c7c2383410ac00bad82831596a2d27a6</data>
    </edge>
    <edge source="&quot;DEEP ESN&quot;" target="&quot;TIME SERIES PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep ESN models are commonly used for the task of Time Series Prediction."</data>
      <data key="d6">2336a57d055095c6ffa9d156ddee0096</data>
    </edge>
    <edge source="&quot;MULTI-INPUTS ESN&quot;" target="&quot;MACKEY-GLASS EQUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Mackey-Glass equation is a concept used to describe the temporal behavior of different physiological signals, which can be applied in the context of Multi-inputs ESNs for modeling and analysis."</data>
      <data key="d6">c7c2383410ac00bad82831596a2d27a6</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS EQUATION&quot;" target="&quot;CHAOTIC SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Mackey-Glass Equation is used to study Chaotic Systems and describe their behavior."</data>
      <data key="d6">9f13e40ee24c7913a62781d708e3b47e</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS EQUATION&quot;" target="&quot;TIME DELAY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time Delay is a parameter in the Mackey-Glass Equation that controls the chaotic behavior of the system."</data>
      <data key="d6">9f13e40ee24c7913a62781d708e3b47e</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS EQUATION&quot;" target="&quot;DATA RESCALING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Data Rescaling is a preprocessing step used in the context of the Mackey-Glass Equation to standardize data."</data>
      <data key="d6">9f13e40ee24c7913a62781d708e3b47e</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS EQUATION&quot;" target="&quot;PHASE DIAGRAM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Mackey-Glass Equation is used to generate a Phase Diagram, which shows the relationship between two variables over time."</data>
      <data key="d6">518f1e492b92054cf2f5c5289444da02</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS EQUATION&quot;" target="&quot;TAU&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Mackey-Glass Equation includes a time delay parameter, Tau, which affects the behavior of the system."</data>
      <data key="d6">518f1e492b92054cf2f5c5289444da02</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS EQUATION&quot;" target="&quot;DATA PREPROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Data Preprocessing is applied to the Mackey-Glass Equation data to prepare it for analysis."</data>
      <data key="d6">c5c29ba06a5cc70a086c2c2c8858e5aa</data>
    </edge>
    <edge source="&quot;CHAOTIC SYSTEMS&quot;" target="&quot;LORENZ CHAOTIC ATTRACTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Lorenz Chaotic Attractor is a mathematical model used to describe chaotic systems."</data>
      <data key="d6">af2db1cc5ab6b16acae2c93d3facb668</data>
    </edge>
    <edge source="&quot;CHAOTIC SYSTEMS&quot;" target="&quot;H&#201;NON MAP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"H&#233;non Map is a mathematical model used to describe chaotic systems."</data>
      <data key="d6">af2db1cc5ab6b16acae2c93d3facb668</data>
    </edge>
    <edge source="&quot;CHAOTIC SYSTEMS&quot;" target="&quot;LOGISTIC MAP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Logistic Map is a mathematical model used to describe chaotic systems."</data>
      <data key="d6">af2db1cc5ab6b16acae2c93d3facb668</data>
    </edge>
    <edge source="&quot;CHAOTIC SYSTEMS&quot;" target="&quot;DOUBLE SCROLL ATTRACTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Double Scroll Attractor is a mathematical model used to describe chaotic systems."</data>
      <data key="d6">af2db1cc5ab6b16acae2c93d3facb668</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS TIMESERIES&quot;" target="&quot;STANDARDIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Standardization is applied to Mackey-Glass timeseries for preprocessing, improving performance and stability."</data>
      <data key="d6">4ac00cf37a752d89d55a749c01c6f6fd</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS TIMESERIES&quot;" target="&quot;MATPLOTLIB.PYPLOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"matplotlib.pyplot is used to visualize the Mackey-Glass timeseries."</data>
      <data key="d6">94fd1ebf256db17e4ac2255b89caa473</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS TIMESERIES&quot;" target="&quot;MACKEY-GLASS EQUATIONS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Mackey-Glass Timeseries is generated by the Mackey-Glass Equations."
"The Mackey-Glass Equations are used to generate Mackey-Glass Timeseries data."</data>
      <data key="d6">238049de5f28dca3e857a46a8b1bed03,2f4c992d69812866e6fce6dbb52d8612</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS TIMESERIES&quot;" target="&quot;TAU&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The parameter Tau in the Mackey-Glass Equations influences the chaotic behavior of the generated Mackey-Glass Timeseries."</data>
      <data key="d6">238049de5f28dca3e857a46a8b1bed03</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS TIMESERIES&quot;" target="&quot;MATPLOTLIB&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Matplotlib is used to create visualizations of the Mackey-Glass Timeseries dataset."</data>
      <data key="d6">4073cafddb73621f26061385c5570659</data>
    </edge>
    <edge source="&quot;MAGMA COLORMAP&quot;" target="&quot;NP.ARANGE() FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Magma colormap is applied to the plot generated by np.arange() function to visualize the data."</data>
      <data key="d6">4ac00cf37a752d89d55a749c01c6f6fd</data>
    </edge>
    <edge source="&quot;NP.ARANGE(0, 500)&quot;" target="&quot;X_TRAIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.arange(0, 500) is used to select a subset of elements from X_train."</data>
      <data key="d6">7d747b0c740e8b5b60726dcf7dcadef5</data>
    </edge>
    <edge source="&quot;X_TRAIN&quot;" target="&quot;PLT.PLOT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"X_train is used as input data for the plt.plot function to create a plot."</data>
      <data key="d6">7d747b0c740e8b5b60726dcf7dcadef5</data>
    </edge>
    <edge source="&quot;X_TRAIN&quot;" target="&quot;JAPANESE_VOWELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The japanese_vowels function is used to obtain data for machine learning tasks, providing input data for X_train."</data>
      <data key="d6">b3361508c3e49b5bb3089f10e31d2c81</data>
    </edge>
    <edge source="&quot;X_TRAIN&quot;" target="&quot;U&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"x_train is a subset of the input data array u."</data>
      <data key="d6">f3e58b69b1a93175e3094a2ba65c0429</data>
    </edge>
    <edge source="&quot;X_TRAIN&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"x_train is used for training the ES2N machine learning model."</data>
      <data key="d6">1298c65a923053e1de35aacddc13832c</data>
    </edge>
    <edge source="&quot;X_TRAIN&quot;" target="&quot;ESN_MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The esn_model is trained using the X_train data input."</data>
      <data key="d6">751b176a8d6149a853e597c65a6fe0cf</data>
    </edge>
    <edge source="&quot;X_TRAIN&quot;" target="&quot;NP.VSTACK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.vstack is used to vertically stack arrays to create the input data for training the ESN."</data>
      <data key="d6">71366a4c7e791080872ba783d3787bd7</data>
    </edge>
    <edge source="&quot;X_TRAIN&quot;" target="&quot;DATASET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"X_train is a variable used to store the training input data in the dataset."</data>
      <data key="d6">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </edge>
    <edge source="&quot;X_TRAIN&quot;" target="&quot;SCIKITLEARNNODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ScikitLearnNode component is trained using the training input data."</data>
      <data key="d6">dc3bd3697a140b64d70e0e3ac6db6c7e</data>
    </edge>
    <edge source="&quot;X_TRAIN&quot;" target="&quot;RESERVOIR MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir Model is trained using the X_train dataset."</data>
      <data key="d6">0970cd32ce54f6ee1180ab237fdcefe1</data>
    </edge>
    <edge source="&quot;TO_FORECASTING&quot;" target="&quot;X&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"X is used as input data for the to_forecasting function to prepare data for forecasting."</data>
      <data key="d6">7d747b0c740e8b5b60726dcf7dcadef5</data>
    </edge>
    <edge source="&quot;X&quot;" target="&quot;Y&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"X and Y are variables used together in the time series forecasting process."</data>
      <data key="d6">f70c7d3d89baaabbeaad57b58e379e08</data>
    </edge>
    <edge source="&quot;X&quot;" target="&quot;NVAR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The function nvar is called with the variable X as input."</data>
      <data key="d6">7b8e1f350eefb392053be12f35fe7daf</data>
    </edge>
    <edge source="&quot;X&quot;" target="&quot;CHAPTER 2&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The variable X is mentioned in Chapter 2 as input data used in the context of the generative mode."</data>
      <data key="d6">e396354e3a9be76616392af11f56e671</data>
    </edge>
    <edge source="&quot;READOUT.WOUT&quot;" target="&quot;MODEL TRAINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"readout.Wout is checked to see if it's equal to 0.0, indicating whether the model has been trained."</data>
      <data key="d6">7d747b0c740e8b5b60726dcf7dcadef5</data>
    </edge>
    <edge source="&quot;OFFLINE TRAINING&quot;" target="&quot;ONLINE TRAINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Offline Training and Online Training are different methods for training a model, with Offline Training using a complete dataset and Online Training updating the model incrementally."</data>
      <data key="d6">7d747b0c740e8b5b60726dcf7dcadef5</data>
    </edge>
    <edge source="&quot;OFFLINE TRAINING&quot;" target="&quot;DATASET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Offline Training involves training the model on a complete dataset in one go."</data>
      <data key="d6">c9e71660f79df626c288b3a58eab0f2f</data>
    </edge>
    <edge source="&quot;ONLINE TRAINING&quot;" target="&quot;DATASET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Online Training updates the model incrementally as new data arrives."</data>
      <data key="d6">c9e71660f79df626c288b3a58eab0f2f</data>
    </edge>
    <edge source="&quot;DATASET&quot;" target="&quot;OBJECTIVE FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Objective Function processes the Dataset, which is split into training and testing sets."</data>
      <data key="d6">d4684af3c445d312afe4d838abc45502</data>
    </edge>
    <edge source="&quot;DATASET&quot;" target="&quot;TRAINING SERIES&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"training series is a subset of the dataset used for training an ESN.")
"The dataset is split into a training series for training an Echo State Network (ESN)."</data>
      <data key="d6">1db191f05801d40d5a346febd10d3352,80033e741d8e10abdcfe20dd17192152</data>
    </edge>
    <edge source="&quot;DATASET&quot;" target="&quot;TESTING SERIES&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"testing series is a subset of the dataset used for evaluating the performance of the trained ESN.")
"The dataset is split into a testing series for evaluating the performance of an Echo State Network (ESN) after training."</data>
      <data key="d6">1db191f05801d40d5a346febd10d3352,80033e741d8e10abdcfe20dd17192152</data>
    </edge>
    <edge source="&quot;DATASET&quot;" target="&quot;RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research function is used to perform hyperparameter optimization on the dataset."</data>
      <data key="d6">1db191f05801d40d5a346febd10d3352</data>
    </edge>
    <edge source="&quot;DATASET&quot;" target="&quot;OBJECTIVE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The objective function is used to evaluate the performance of the machine learning model on the dataset."</data>
      <data key="d6">f70c7d3d89baaabbeaad57b58e379e08</data>
    </edge>
    <edge source="&quot;DATASET&quot;" target="&quot;ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Dataset is used in the analysis, although its specific role is not explicitly stated."</data>
      <data key="d6">f16792dcee6dab8ea8f8c8c6793bbc3d</data>
    </edge>
    <edge source="&quot;DATASET&quot;" target="&quot;TRAIN_LEN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Train_len is a variable used to set the length of the training data in the dataset."</data>
      <data key="d6">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </edge>
    <edge source="&quot;DATASET&quot;" target="&quot;FORECAST&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Forecast is a variable used to set the forecasting horizon in the dataset."</data>
      <data key="d6">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </edge>
    <edge source="&quot;NP.R_&quot;" target="&quot;BIAS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.r_ is used to concatenate the 'bias' array with another array."</data>
      <data key="d6">9078b0f36522f21a9e8e1aadac48ed9c</data>
    </edge>
    <edge source="&quot;NP.R_&quot;" target="&quot;WOUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.r_ is used to concatenate the 'Wout' array with another array."</data>
      <data key="d6">9078b0f36522f21a9e8e1aadac48ed9c</data>
    </edge>
    <edge source="&quot;BIAS&quot;" target="&quot;ESN MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Bias refers to the assumptions or preferences that can influence the results of ESN models, and it is a term used in machine learning to describe this effect."</data>
      <data key="d6">e805d3f438bd9c485639f1c69f917ae5</data>
    </edge>
    <edge source="&quot;ABSOLUTE DEVIATION&quot;" target="&quot;ESN PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Absolute deviation is a measure of the difference between the True value and the ESN prediction."</data>
      <data key="d6">ee83abbbbc707d8131952b2b01ebc268</data>
    </edge>
    <edge source="&quot;MODEL-0&quot;" target="&quot;X_TEST1&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Model-0 is processing the data points from the X_test1 input."</data>
      <data key="d6">c122738fd421d3d662f759af5a0a23f3</data>
    </edge>
    <edge source="&quot;MODEL-0&quot;" target="&quot;Y_PRED1&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Model-0 is generating the predicted output data set y_pred1."</data>
      <data key="d6">c122738fd421d3d662f759af5a0a23f3</data>
    </edge>
    <edge source="&quot;MODEL-0&quot;" target="&quot;ACCURACY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Accuracy is a metric used to evaluate the performance of Model-0."</data>
      <data key="d6">c122738fd421d3d662f759af5a0a23f3</data>
    </edge>
    <edge source="&quot;MODEL-0&quot;" target="&quot;PRECISION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Precision is a metric used to evaluate the performance of Model-0."</data>
      <data key="d6">c122738fd421d3d662f759af5a0a23f3</data>
    </edge>
    <edge source="&quot;MODEL-0&quot;" target="&quot;RECALL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recall is a metric used to evaluate the performance of Model-0."</data>
      <data key="d6">c122738fd421d3d662f759af5a0a23f3</data>
    </edge>
    <edge source="&quot;MODEL-0&quot;" target="&quot;MEAN SQUARED ERROR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Mean Squared Error is a metric used to evaluate the performance of Model-0."</data>
      <data key="d6">c122738fd421d3d662f759af5a0a23f3</data>
    </edge>
    <edge source="&quot;MODEL-0&quot;" target="&quot;R^2 OR RSQUARE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"R^2 or rsquare is a metric used to evaluate the performance of Model-0."</data>
      <data key="d6">c122738fd421d3d662f759af5a0a23f3</data>
    </edge>
    <edge source="&quot;MODEL-0&quot;" target="&quot;NRMSE OR NRMSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NRMSE or nrmse is a metric used to evaluate the performance of Model-0."</data>
      <data key="d6">c122738fd421d3d662f759af5a0a23f3</data>
    </edge>
    <edge source="&quot;Y_TEST1&quot;" target="&quot;Y_PRED1&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The actual output data set y_test1 is compared with the predicted output data set y_pred1 to calculate metrics."</data>
      <data key="d6">c122738fd421d3d662f759af5a0a23f3</data>
    </edge>
    <edge source="&quot;RSQUARE&quot;" target="&quot;NRMSE&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"rsquare and NRMSE are both measures used to evaluate the accuracy of predictions, with rsquare indicating the proportion of variance explained and NRMSE providing a normalized measure of prediction error."
"nrmse and rsquare are metrics used to evaluate the performance of a machine learning model, mentioned together in the provided code."</data>
      <data key="d6">0753d4e507badadd900c522ee03ad28d,d8022c6a3caf781300e2abc1dfd2ed44</data>
    </edge>
    <edge source="&quot;RSQUARE&quot;" target="&quot;ONE-TIMESTEP-AHEAD FORECASTING TASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The one-timestep-ahead forecasting task is a situation where the rsquare measure is used to evaluate the model's ability to predict the next value in a time series."</data>
      <data key="d6">d8022c6a3caf781300e2abc1dfd2ed44</data>
    </edge>
    <edge source="&quot;NRMSE&quot;" target="&quot;ONE-TIMESTEP-AHEAD FORECASTING TASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The one-timestep-ahead forecasting task is a situation where the NRMSE measure is used to evaluate the model's accuracy in predicting the next value in a time series."</data>
      <data key="d6">d8022c6a3caf781300e2abc1dfd2ed44</data>
    </edge>
    <edge source="&quot;NRMSE&quot;" target="&quot;ES&#178;N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ES&#178;N is optimized using the NRMSE metric."</data>
      <data key="d6">7cb18067f7d75fd3cd20998c669a1741</data>
    </edge>
    <edge source="&quot;NRMSE&quot;" target="&quot;R-SQUARED&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both NRMSE and R-squared are metrics used to evaluate the performance of a model in the provided code."</data>
      <data key="d6">82ff270b1bbdfe0ee11e603de1e326c7</data>
    </edge>
    <edge source="&quot;ONE-TIMESTEP-AHEAD FORECASTING TASK&quot;" target="&quot;CLOSED LOOP GENERATIVE MODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A closed loop generative mode system could potentially involve a one-timestep-ahead forecasting task, as the output of the system is fed back into the input for continuous generation and feedback."</data>
      <data key="d6">d8022c6a3caf781300e2abc1dfd2ed44</data>
    </edge>
    <edge source="&quot;CLOSED LOOP GENERATIVE MODE&quot;" target="&quot;NUMPY.ZEROS()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"numpy.zeros() is used in Closed Loop Generative Mode to create an initial array of zeros for the prediction process."</data>
      <data key="d6">05ba4f2e1a9472bd286417154cb0c0d4</data>
    </edge>
    <edge source="&quot;NP.ZEROS()&quot;" target="&quot;ONLINE LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.zeros() is used to initialize an array with zeros, which is then updated incrementally with each new sample of data in the context of Online Learning."</data>
      <data key="d6">d55aca098e1ba2aea26a4bf33cc2d4a2</data>
    </edge>
    <edge source="&quot;ONLINE LEARNING&quot;" target="&quot;FORCE ALGORITHM&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The FORCE Algorithm is a type of Online Learning algorithm, as it updates the parameters of the model incrementally with each new sample of data."
"FORCE Algorithm is used for Online Learning, allowing for the continuous updating of the readout parameters at every timestep of input series."</data>
      <data key="d6">1b9bc5f1bd54d2b0c90359b6ed022bb6,d55aca098e1ba2aea26a4bf33cc2d4a2</data>
    </edge>
    <edge source="&quot;FORCE ALGORITHM&quot;" target="&quot;ZIP()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The zip() function is not explicitly mentioned in the context of the FORCE Algorithm, but it could potentially be used to pair input data with corresponding target values during the learning process."</data>
      <data key="d6">d55aca098e1ba2aea26a4bf33cc2d4a2</data>
    </edge>
    <edge source="&quot;FORCE ALGORITHM&quot;" target="&quot;SUSSILLO AND ABOTT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sussillo and Abott are the authors of the FORCE Algorithm, which is used in the ESN model for online learning."</data>
      <data key="d6">424bf7c7b82dc966139c25f7c9ccffb7</data>
    </edge>
    <edge source="&quot;FORCE ALGORITHM&quot;" target="&quot;SUSSILLO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sussillo is a contributor to the development of the FORCE Algorithm."</data>
      <data key="d6">1b9bc5f1bd54d2b0c90359b6ed022bb6</data>
    </edge>
    <edge source="&quot;FORCE ALGORITHM&quot;" target="&quot;ABOTT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Abott is a contributor to the development of the FORCE Algorithm."</data>
      <data key="d6">1b9bc5f1bd54d2b0c90359b6ed022bb6</data>
    </edge>
    <edge source="&quot;FIRST ORDER REDUCED AND CONTROLLED ERROR (FORCE) ALGORITHM&quot;" target="&quot;ENUMERATE() FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The FORCE algorithm is used in a line of code that includes the enumerate() function."</data>
      <data key="d6">4d8b9e762d08c8cdf5189130be11021e</data>
    </edge>
    <edge source="&quot;FIRST ORDER REDUCED AND CONTROLLED ERROR (FORCE) ALGORITHM&quot;" target="&quot;ZIP() FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The FORCE algorithm is used in a line of code that includes the zip() function."</data>
      <data key="d6">4d8b9e762d08c8cdf5189130be11021e</data>
    </edge>
    <edge source="&quot;ENUMERATE() FUNCTION&quot;" target="&quot;ZIP() FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The enumerate() function is used in a line of code that includes the zip() function."</data>
      <data key="d6">4d8b9e762d08c8cdf5189130be11021e</data>
    </edge>
    <edge source="&quot;LORENZ CHAOTIC ATTRACTOR&quot;" target="&quot;LORENZ SYSTEM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Lorenz chaotic attractors are solutions to the Lorenz system of differential equations."</data>
      <data key="d6">4d114857b77ff15b495bb6456c9ad30c</data>
    </edge>
    <edge source="&quot;LORENZ CHAOTIC ATTRACTOR&quot;" target="&quot;EDWARD LORENZ&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Edward Lorenz discovered Lorenz chaotic attractors while studying atmospheric convection."</data>
      <data key="d6">4d114857b77ff15b495bb6456c9ad30c</data>
    </edge>
    <edge source="&quot;LORENZ CHAOTIC ATTRACTOR&quot;" target="&quot;H&#201;NON MAP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The H&#233;non map generates complex patterns and exhibits chaotic behavior, similar to the Lorenz chaotic attractor."</data>
      <data key="d6">4d114857b77ff15b495bb6456c9ad30c</data>
    </edge>
    <edge source="&quot;LORENZ SYSTEM&quot;" target="&quot;LORENZ ATTRACTOR&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The Lorenz attractor is a visual representation of the chaotic behavior in the Lorenz system."
"The Lorenz System generates the Lorenz Attractor, a strange attractor with chaotic solutions."</data>
      <data key="d6">715720b663e85c5e16cbf8b1ef4ec208,d4080e34001a0ebe22f20efdb204240b</data>
    </edge>
    <edge source="&quot;LORENZ SYSTEM&quot;" target="&quot;LORENZ SYSTEM WIKIPEDIA PAGE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Lorenz system Wikipedia page provides information about the Lorenz system and its properties."</data>
      <data key="d6">d4080e34001a0ebe22f20efdb204240b</data>
    </edge>
    <edge source="&quot;LORENZ SYSTEM&quot;" target="&quot;RESEARCHER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The researcher is working on the Lorenz System to predict the z coordinate."</data>
      <data key="d6">715720b663e85c5e16cbf8b1ef4ec208</data>
    </edge>
    <edge source="&quot;H&#201;NON MAP&quot;" target="&quot;H&#201;NON STRANGE ATTRACTOR&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The H&#233;non strange attractor is a fractal structure that results from the chaotic behavior of the H&#233;non map."
"The H&#233;non strange attractor is the attractor of the H&#233;non map, a mathematical model that exhibits chaotic behavior."</data>
      <data key="d6">9ada201f787cd4e88cd18dae60de346d,d4080e34001a0ebe22f20efdb204240b</data>
    </edge>
    <edge source="&quot;H&#201;NON MAP&quot;" target="&quot;H&#201;NON MAP WIKIPEDIA PAGE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The H&#233;non map Wikipedia page provides information about the H&#233;non map and its properties."</data>
      <data key="d6">d4080e34001a0ebe22f20efdb204240b</data>
    </edge>
    <edge source="&quot;H&#201;NON MAP&quot;" target="&quot;LORENZ MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The H&#233;non map models the Poincar&#233; section of the Lorenz model, illustrating complex dynamics."</data>
      <data key="d6">9ada201f787cd4e88cd18dae60de346d</data>
    </edge>
    <edge source="&quot;LORENZ ATTRACTOR&quot;" target="&quot;NVAR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NVAR is used for forecasting the Lorenz Attractor."</data>
      <data key="d6">41ea479401d7ff7c83c9d38c91d76cd9</data>
    </edge>
    <edge source="&quot;LORENZ ATTRACTOR&quot;" target="&quot;RESERVOIR COMPUTING MACHINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing Machine is used for processing time-series data associated with the Lorenz Attractor."</data>
      <data key="d6">41ea479401d7ff7c83c9d38c91d76cd9</data>
    </edge>
    <edge source="&quot;LORENZ ATTRACTOR&quot;" target="&quot;NEXT GENERATION RESERVOIR COMPUTING MACHINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Next Generation Reservoir Computing Machine is used for forecasting the Lorenz Attractor using linear weights and nonlinear functionals."</data>
      <data key="d6">41ea479401d7ff7c83c9d38c91d76cd9</data>
    </edge>
    <edge source="&quot;LORENZ ATTRACTOR&quot;" target="&quot;GAUTHIER ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Lorenz Attractor is studied by the group Gauthier et al., as mentioned in the figure and legend."</data>
      <data key="d6">74dd26ac71a37c92f3eda8552701ca33</data>
    </edge>
    <edge source="&quot;LORENZ ATTRACTOR&quot;" target="&quot;LORENZ (1963)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Lorenz Attractor was introduced by Edward Lorenz in 1963."</data>
      <data key="d6">74dd26ac71a37c92f3eda8552701ca33</data>
    </edge>
    <edge source="&quot;LORENZ ATTRACTOR&quot;" target="&quot;CHAOTIC SYSTEM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Lorenz Attractor is a mathematical model that represents a chaotic system, exhibiting sensitive dependence on initial conditions."</data>
      <data key="d6">ac2eb4232eaa7c1adbf00d4a0be3d799</data>
    </edge>
    <edge source="&quot;LORENZ ATTRACTOR&quot;" target="&quot;RECONSTRUCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Lorenz Attractor is the subject of the Reconstruction process, which aims to predict the z-coordinate based on the x-coordinate and y-coordinate."</data>
      <data key="d6">ac3456a3574f6939fcb6d5242c202810</data>
    </edge>
    <edge source="&quot;LOGISTIC MAP&quot;" target="&quot;CHAOTIC BEHAVIOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The logistic map models population dynamics and shows chaotic behavior for certain values of a parameter."</data>
      <data key="d6">9ada201f787cd4e88cd18dae60de346d</data>
    </edge>
    <edge source="&quot;LORENZ MODEL&quot;" target="&quot;TRAINER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Trainer sets up and trains the Lorenz Model to study the behavior of a complex system with chaotic properties."</data>
      <data key="d6">c838b1b4744bc0f400abf85f791950cf</data>
    </edge>
    <edge source="&quot;DOUBLE SCROLL ATTRACTOR&quot;" target="&quot;CHUA'S CIRCUIT&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Double scroll attractors, also known as Chua's attractors, are strange attractors observed in a chaotic electronic circuit called Chua's circuit."
"Double scroll attractors are observed in Chua's circuit, which exhibits chaotic behavior."</data>
      <data key="d6">538ff8c18495002c85cbc9020b0146f9,9ada201f787cd4e88cd18dae60de346d</data>
    </edge>
    <edge source="&quot;DOUBLE SCROLL ATTRACTOR&quot;" target="&quot;NVAR&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The Double Scroll Attractor is forecasted using the NVAR method."
"NVAR is a model used to analyze the dynamics of the Double Scroll Attractor."</data>
      <data key="d6">684b1edf65b327cc06ceb69ca1279d74,770691846086629ac7d541f51760552c</data>
    </edge>
    <edge source="&quot;DOUBLE SCROLL ATTRACTOR&quot;" target="&quot;MATHEMATICAL MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Double Scroll Attractor is a mathematical model defined by a set of differential equations."</data>
      <data key="d6">770691846086629ac7d541f51760552c</data>
    </edge>
    <edge source="&quot;DOUBLE SCROLL ATTRACTOR&quot;" target="&quot;AUTHOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author is analyzing the dynamics of the Double Scroll Attractor."</data>
      <data key="d6">684b1edf65b327cc06ceb69ca1279d74</data>
    </edge>
    <edge source="&quot;SCIKIT-LEARN&quot;" target="&quot;MODULENOTFOUNDERROR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ModuleNotFoundError occurs when scikit-learn, a machine learning library, is not installed on the system."</data>
      <data key="d6">538ff8c18495002c85cbc9020b0146f9</data>
    </edge>
    <edge source="&quot;SCIKIT-LEARN&quot;" target="&quot;INSTANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Scikit-learn models handle data points as instances, which are used for training and prediction."</data>
      <data key="d6">b130d3d59f0d3a2bb4feac9fdb85ed5b</data>
    </edge>
    <edge source="&quot;SCIKIT-LEARN&quot;" target="&quot;OUTPUT FEATURE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Scikit-learn models are designed to predict output features, which are the target variables that the model aims to forecast."</data>
      <data key="d6">b130d3d59f0d3a2bb4feac9fdb85ed5b</data>
    </edge>
    <edge source="&quot;SCIKIT-LEARN&quot;" target="&quot;MULTIPLE OUTPUT FEATURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Scikit-learn models can handle multiple output features by creating multiple instances of the model under the hood, each dispatched to predict a specific output feature."</data>
      <data key="d6">b130d3d59f0d3a2bb4feac9fdb85ed5b</data>
    </edge>
    <edge source="&quot;SCIKIT-LEARN&quot;" target="&quot;REGRESSION METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Scikit-learn provides various regression methods, which are statistical techniques used to model the relationship between a dependent variable and one or more independent variables."</data>
      <data key="d6">b130d3d59f0d3a2bb4feac9fdb85ed5b</data>
    </edge>
    <edge source="&quot;SCIKIT-LEARN&quot;" target="&quot;RIDGECLASSIFIER&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"scikit-learn implements the RidgeClassifier, making it available for use in machine learning pipelines."
"Scikit-learn provides the RidgeClassifier model for classification tasks."
"RidgeClassifier is a machine learning algorithm provided by the Scikit-learn library for classification tasks."</data>
      <data key="d6">0036fb6f489e13c0db0f1c02bf3323be,d58662ee42c14a0787d839ebfd0a6e9b,f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </edge>
    <edge source="&quot;SCIKIT-LEARN&quot;" target="&quot;LOGISTICREGRESSION&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"scikit-learn implements the LogisticRegression classifier, making it available for use in machine learning pipelines."
"Scikit-learn provides the LogisticRegression model for classification tasks."
"LogisticRegression is a machine learning algorithm provided by the Scikit-learn library for classification tasks."</data>
      <data key="d6">0036fb6f489e13c0db0f1c02bf3323be,d58662ee42c14a0787d839ebfd0a6e9b,f5358a50d00a1cac02dd4ad8fcb167ee</data>
    </edge>
    <edge source="&quot;SCIKIT-LEARN&quot;" target="&quot;ML TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"scikit-learn is a library that offers a simple API to apply ML techniques."</data>
      <data key="d6">295606b4bc5d12929a913a3c79f93734</data>
    </edge>
    <edge source="&quot;SCIKIT-LEARN&quot;" target="&quot;COMPLEX NEURAL NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"scikit-learn lacks flexibility and is not meant to create complex neural networks operating on timeseries, with feedback loops and online learning rules."</data>
      <data key="d6">295606b4bc5d12929a913a3c79f93734</data>
    </edge>
    <edge source="&quot;SCIKIT-LEARN&quot;" target="&quot;DV BUONOMANO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"scikit-learn is mentioned in the context of a scientific paper by DV Buonomano and M. M. Merzenich."</data>
      <data key="d6">ce7b58ffc7f43f36bc78154597d01903</data>
    </edge>
    <edge source="&quot;GLOB.GLOB()&quot;" target="&quot;FILE PATHS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"glob.glob() retrieves and returns a list of file paths that match a specified pattern."</data>
      <data key="d6">538ff8c18495002c85cbc9020b0146f9</data>
    </edge>
    <edge source="&quot;GLOB.GLOB()&quot;" target="&quot;R4-DATA/EXPERIMENTS/&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"glob.glob() retrieves all file paths within the r4-data/experiments/ directory."</data>
      <data key="d6">fb40afaf160923869aba1456b3a1ddca</data>
    </edge>
    <edge source="&quot;PARALLEL()&quot;" target="&quot;DELAYED()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Parallel() is used to enable parallel processing, and delayed() is used to create a lazy evaluation of the function it wraps, allowing for parallel execution of tasks."</data>
      <data key="d6">fb40afaf160923869aba1456b3a1ddca</data>
    </edge>
    <edge source="&quot;DELAYED()&quot;" target="&quot;PD.READ_CSV()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"delayed() is used to create a lazy evaluation of pd.read_csv(), enabling parallel reading of multiple CSV files."</data>
      <data key="d6">fb40afaf160923869aba1456b3a1ddca</data>
    </edge>
    <edge source="&quot;DELAYED()&quot;" target="&quot;PD.READ_CSV&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The pd.read_csv function is wrapped by the delayed() function, allowing for concurrent execution of data loading tasks."</data>
      <data key="d6">f3b5b178557c4991ab5b81d869a4752e</data>
    </edge>
    <edge source="&quot;NP.ROLL&quot;" target="&quot;ARRAY ELEMENTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The np.roll function is used to shift the elements of an array by a specified number of positions."</data>
      <data key="d6">f3b5b178557c4991ab5b81d869a4752e</data>
    </edge>
    <edge source="&quot;NP.ROLL&quot;" target="&quot;CIRCULAR SHIFT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.roll is used to perform a circular shift of the elements of an array."</data>
      <data key="d6">584889d2db258e32e7f673d3c0a0e603</data>
    </edge>
    <edge source="&quot;RMSE&quot;" target="&quot;MODEL PERFORMANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The rmse function is used to calculate the Root Mean Square Error, a common metric for evaluating the performance of a model."</data>
      <data key="d6">f3b5b178557c4991ab5b81d869a4752e</data>
    </edge>
    <edge source="&quot;RMSE&quot;" target="&quot;AVERAGED RMSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RMSE is used to calculate Averaged RMSE, which provides an overall measure of prediction accuracy."</data>
      <data key="d6">584889d2db258e32e7f673d3c0a0e603</data>
    </edge>
    <edge source="&quot;RMSE&quot;" target="&quot;HYPEROPT&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"Hyperopt uses RMSE as a loss function to evaluate the quality of the parameters chosen in the optimization process.""RMSE is used as a loss function within the optimization process conducted by Hyperopt."
"Hyperopt uses the Root Mean Squared Error (RMSE) loss function to evaluate the quality of parameters in optimization algorithms."</data>
      <data key="d6">251a50c2ae8ceea4fd7da1127cc5f461,7c7818502732457fb71aacdd9a90ee36</data>
    </edge>
    <edge source="&quot;ROOT MEAN SQUARED ERROR (RMSE)&quot;" target="&quot;OPTIMIZATION ALGORITHMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Root Mean Squared Error (RMSE) is a loss function used in the optimization process to evaluate the quality of parameters."</data>
      <data key="d6">4f7b43545046f0e6f9b6fb3816da1d79</data>
    </edge>
    <edge source="&quot;AVERAGED RMSE&quot;" target="&quot;AVERAGED RMSE (WITH THRESHOLD)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Averaged RMSE (with threshold) is a variant of Averaged RMSE that considers only predictions within a specific range."</data>
      <data key="d6">584889d2db258e32e7f673d3c0a0e603</data>
    </edge>
    <edge source="&quot;GLOB.GLOB&quot;" target="&quot;FILE RETRIEVAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"glob.glob is used to retrieve files with a specific extension, allowing for their subsequent processing."</data>
      <data key="d6">584889d2db258e32e7f673d3c0a0e603</data>
    </edge>
    <edge source="&quot;SORTED&quot;" target="&quot;ASCENDING ORDER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"sorted is used to sort the elements of a list in ascending order."</data>
      <data key="d6">584889d2db258e32e7f673d3c0a0e603</data>
    </edge>
    <edge source="&quot;LBR.FEATURE.MFCC&quot;" target="&quot;SPEECH AND AUDIO PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"lbr.feature.mfcc is used to compute the Mel-frequency cepstral coefficients (MFCCs) of a given audio signal, which are commonly used for speech and audio processing tasks."</data>
      <data key="d6">584889d2db258e32e7f673d3c0a0e603</data>
    </edge>
    <edge source="&quot;SORTED() FUNCTION&quot;" target="&quot;GLOB.GLOB() FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The sorted() function is used in conjunction with the glob.glob() function to sort the list of audio file paths in ascending order."</data>
      <data key="d6">7bea9e814256104a2d7ede466ddc3364</data>
    </edge>
    <edge source="&quot;MFCCS&quot;" target="&quot;LIBROSA LIBRARY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MFCCs are features extracted from audio signals using the librosa library, which provides functions for this purpose."</data>
      <data key="d6">7bea9e814256104a2d7ede466ddc3364</data>
    </edge>
    <edge source="&quot;MFCCS&quot;" target="&quot;LIBROSA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"librosa provides functions for extracting Mel-Frequency Cepstral Coefficients (MFCCs) from audio signals."</data>
      <data key="d6">e3828ad4e78d575fabb543e0eab86160</data>
    </edge>
    <edge source="&quot;MFCCS&quot;" target="&quot;DELTA COEFFICIENTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Delta Coefficients are derived from the Mel-Frequency Cepstral Coefficients (MFCCs) to enhance the feature set for tasks like audio and speech processing."</data>
      <data key="d6">e3828ad4e78d575fabb543e0eab86160</data>
    </edge>
    <edge source="&quot;DELTA&quot;" target="&quot;LIBRISPEECH DATASET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Delta is used to extract features from the audio files in the Librispeech Dataset."</data>
      <data key="d6">adfade0d7bc85c6420e61ecd1ce7095c</data>
    </edge>
    <edge source="&quot;LIBROSA&quot;" target="&quot;DELTA COEFFICIENTS&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"librosa provides a function to compute Delta Coefficients, which are derived from the MFCCs to capture their temporal dynamics."
"librosa provides functions for calculating delta coefficients of the MFCCs."
"librosa is used to calculate delta coefficients, which measure the rate of change of a signal."</data>
      <data key="d6">7c8a0a6b9506a584f1c98495097d48ee,aea362ee35c2a3a01b76020d0b892cbd,e3828ad4e78d575fabb543e0eab86160</data>
    </edge>
    <edge source="&quot;LIBROSA&quot;" target="&quot;MEL-FREQUENCY CEPSTRAL COEFFICIENTS (MFCCS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"librosa provides functions for calculating MFCCs."</data>
      <data key="d6">7c8a0a6b9506a584f1c98495097d48ee</data>
    </edge>
    <edge source="&quot;LIBROSA&quot;" target="&quot;SECOND-ORDER DIFFERENCES (OR DELTA-DELTAS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"librosa provides functions for calculating second-order differences (or delta-deltas) of the MFCCs."</data>
      <data key="d6">7c8a0a6b9506a584f1c98495097d48ee</data>
    </edge>
    <edge source="&quot;LIBROSA&quot;" target="&quot;CHAPTER 5&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Librosa is used in Chapter 5 for audio processing and analysis."</data>
      <data key="d6">1e8ee805d22cd143d2372d300997d253</data>
    </edge>
    <edge source="&quot;MEL-FREQUENCY CEPSTRAL COEFFICIENTS (MFCCS)&quot;" target="&quot;DELTA COEFFICIENTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Delta coefficients are calculated from the MFCCs to capture their temporal dynamics."</data>
      <data key="d6">7c8a0a6b9506a584f1c98495097d48ee</data>
    </edge>
    <edge source="&quot;MEL-FREQUENCY CEPSTRAL COEFFICIENTS (MFCCS)&quot;" target="&quot;SECOND-ORDER DIFFERENCES (OR DELTA-DELTAS)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Second-order differences (or delta-deltas) of the MFCCs are calculated from the MFCCs to capture additional temporal dynamics information."</data>
      <data key="d6">7c8a0a6b9506a584f1c98495097d48ee</data>
    </edge>
    <edge source="&quot;DELTA COEFFICIENTS&quot;" target="&quot;EDGE EFFECTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Edge effects can affect the accuracy of delta coefficients, which measure the rate of change of a signal."</data>
      <data key="d6">aea362ee35c2a3a01b76020d0b892cbd</data>
    </edge>
    <edge source="&quot;DATAFRAME&quot;" target="&quot;NAMED TUPLES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"DataFrame.itertuples() returns each row as a named tuple, allowing for attribute-based access to the data."</data>
      <data key="d6">aea362ee35c2a3a01b76020d0b892cbd</data>
    </edge>
    <edge source="&quot;ONE-HOT ENCODING&quot;" target="&quot;CATEGORICAL DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"One-hot encoding is used to convert categorical data variables into a format that can be used by machine learning algorithms."</data>
      <data key="d6">aea362ee35c2a3a01b76020d0b892cbd</data>
    </edge>
    <edge source="&quot;CATEGORICAL DATA&quot;" target="&quot;ONEHOTENCODER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The OneHotEncoder function is used to transform categorical data into a binary matrix representation."</data>
      <data key="d6">d5477dbd84525291f2e017ae618de222</data>
    </edge>
    <edge source="&quot;ONEHOTENCODER&quot;" target="&quot;SPARSE_OUTPUT&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The OneHotEncoder function has a parameter called sparse_output that determines whether the output should be a sparse matrix or a dense array."
"sparse_output is a parameter used in the OneHotEncoder function to specify the format of the resulting one-hot encoded matrix."</data>
      <data key="d6">84a64dd2c683e779d55aaccea16b1032,d5477dbd84525291f2e017ae618de222</data>
    </edge>
    <edge source="&quot;ONEHOTENCODER&quot;" target="&quot;MACHINE LEARNING MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"One-hot encoding is a preprocessing step that can improve the performance of machine learning models by converting categorical data into a format that is more suitable for the models."</data>
      <data key="d6">d5477dbd84525291f2e017ae618de222</data>
    </edge>
    <edge source="&quot;ONEHOTENCODER&quot;" target="&quot;FLATTEN()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"flatten() is used to convert the multi-dimensional array resulting from the OneHotEncoder function into a one-dimensional array."</data>
      <data key="d6">84a64dd2c683e779d55aaccea16b1032</data>
    </edge>
    <edge source="&quot;ONEHOTENCODER&quot;" target="&quot;HYPEROPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt is used for optimizing hyperparameters, which may include the parameters used in the OneHotEncoder function."</data>
      <data key="d6">84a64dd2c683e779d55aaccea16b1032</data>
    </edge>
    <edge source="&quot;ARGMAX()&quot;" target="&quot;HYPEROPT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"argmax() is used in the context of optimizing hyperparameters with the Hyperopt library."</data>
      <data key="d6">84a64dd2c683e779d55aaccea16b1032</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;NP.ARGMAX()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.argmax() is used in the context of Hyperopt to return the indices of the maximum values, which may be relevant in the process of optimizing hyperparameters."</data>
      <data key="d6">3b4d50c051c177770830f7c0a6b3dd69</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;PARAMETERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt is used to optimize Parameters."</data>
      <data key="d6">6a6d88a8f9731e1ed05b786e0a9ba6dc</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;XAVIER HINAUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Xavier Hinaut is an author of the paper that introduces the use of Hyperopt for Echo State Networks hyperparameter optimization."</data>
      <data key="d6">46913f0d73ba0b8cecfdf42bde9862f4</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;NICOLAS TROUVAIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Nicolas Trouvain is an author of the paper that introduces the use of Hyperopt for Echo State Networks hyperparameter optimization."</data>
      <data key="d6">46913f0d73ba0b8cecfdf42bde9862f4</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;PYTHON NOTEBOOKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt is a Python library for hyperparameter optimization that may be used in the Python Notebooks for reservoir computing."</data>
      <data key="d6">0b6c69085074b2cf23267eb149068b9f</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;TROUVAIN ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Trouvain et al. are authors of a tutorial that uses Hyperopt for hyperparameter optimization."</data>
      <data key="d6">280cbdf53022bbaed48ccb34ebe142bc</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;R&#178;&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt also computes the R&#178; metric to measure the proportion of the variance in the dependent variable that is predictable from the independent variable(s)."</data>
      <data key="d6">251a50c2ae8ceea4fd7da1127cc5f461</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;AUTHOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author discusses the use of Hyperopt to approximate a function and find a minimum."</data>
      <data key="d6">11749b7d0fdadf05ea29da6025618407</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;OBJECTIVE FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt is used to optimize the objective function, which evaluates the performance of a machine learning model."</data>
      <data key="d6">0753d4e507badadd900c522ee03ad28d</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;HYPEROPT_CONFIG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt uses the hyperopt_config file to define the parameters for hyperparameter optimization."</data>
      <data key="d6">75e530c1a04e30b373dc7cc68e3ad819</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;RANDOM SEARCH ALGORITHM&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Hyperopt is mentioned as using the Random Search Algorithm for hyperparameter optimization."
"Random Search Algorithm is mentioned as a method used by Hyperopt to optimize parameters."</data>
      <data key="d6">0a9b132ecb1c4b63fdbb0e144295362e,25743a99f36f3e56551ffafbba8d15c4</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;GRID SEARCH&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Hyperopt is mentioned as not using the Grid Search method for hyperparameter optimization."
"Grid Search is mentioned as a method that could add a bias during optimization when used with Hyperopt."</data>
      <data key="d6">0a9b132ecb1c4b63fdbb0e144295362e,25743a99f36f3e56551ffafbba8d15c4</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;RANDOM SEED&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Hyperopt is mentioned as using a Random Seed for the ESN initialization."
"Hyperopt is using a fixed Random Seed parameter for ESN initialization."</data>
      <data key="d6">0a9b132ecb1c4b63fdbb0e144295362e,65ba78d1f678e080bd930319c54234ef</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;PROCEED&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt is presented at the Proceed event."</data>
      <data key="d6">a3a74dc4754a8c8b0730f808285893e2</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;JAMES BERGSTRA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"James Bergstra is an author of Hyperopt, a python library for optimizing the hyperparameters of machine learning algorithms."</data>
      <data key="d6">82de30f43839f4985de20a981b524af1</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;DAN YAMINS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dan Yamins is an author of Hyperopt, a python library for optimizing the hyperparameters of machine learning algorithms."</data>
      <data key="d6">82de30f43839f4985de20a981b524af1</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;DAVID D COX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"David D Cox is an author of Hyperopt, a python library for optimizing the hyperparameters of machine learning algorithms."</data>
      <data key="d6">82de30f43839f4985de20a981b524af1</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;RANDOM SEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt uses the Random Search method to explore different sets of parameters for optimization."</data>
      <data key="d6">82ff270b1bbdfe0ee11e603de1e326c7</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;RANDOM METHOD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt uses the Random Method to choose different sets of parameters."</data>
      <data key="d6">65ba78d1f678e080bd930319c54234ef</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;INPUT SCALING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt is using a fixed Input Scaling parameter."</data>
      <data key="d6">65ba78d1f678e080bd930319c54234ef</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;RIDGE REGULARIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperopt is using a fixed Ridge Regularization parameter."</data>
      <data key="d6">65ba78d1f678e080bd930319c54234ef</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;CHOICE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Choice is a parameter mentioned in the Hyperopt configuration."</data>
      <data key="d6">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </edge>
    <edge source="&quot;HYPEROPT&quot;" target="&quot;LOGUNIFORM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Loguniform is a parameter mentioned in the Hyperopt configuration."</data>
      <data key="d6">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </edge>
    <edge source="&quot;UNITS&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES2N system operates on a specific number of units."</data>
      <data key="d6">9dd8e12c7acbe10cf34817ff14780d24</data>
    </edge>
    <edge source="&quot;SPECTRAL_RADIUS&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ES2N is mentioned in the context of the spectral_radius concept."</data>
      <data key="d6">97f5d2e9d34b3b50f8e922fc4bb7f824</data>
    </edge>
    <edge source="&quot;CORRELATION&quot;" target="&quot;NP.CORRCOEF()&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The concept of Correlation is explained using the np.corrcoef() function to calculate the correlation coefficient matrix."</data>
      <data key="d6">a8df60a94e25d863b436f47f4f8e6a6d</data>
    </edge>
    <edge source="&quot;CORRELATION&quot;" target="&quot;INPUT SCALING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input Scaling is mentioned in the context of correlation, indicating its impact on the relationship between variables."</data>
      <data key="d6">b957e1bf5bf175c7630222ca742c7933</data>
    </edge>
    <edge source="&quot;INPUT_SCALING&quot;" target="&quot;PERSON CORRELATION COEFFICIENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"INPUT_SCALING affects the calculation of the Person Correlation Coefficient, influencing the correlation between states and inputs."</data>
      <data key="d6">76f47f241e255f9f36646409d2ec30f1</data>
    </edge>
    <edge source="&quot;INPUT_SCALING&quot;" target="&quot;HP_SPACE&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"hp_space is the configuration parameter that explores the input_scaling parameter, which represents the input scaling."
"hp_space is used to explore the input_scaling parameter."
"hp_space is associated with the parameter input_scaling, which specifies the input scaling."</data>
      <data key="d6">5cea9edfd65fcfa25a081554300b28cc,80033e741d8e10abdcfe20dd17192152,adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </edge>
    <edge source="&quot;INPUT_SCALING&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ES2N is mentioned in the context of the input_scaling concept."</data>
      <data key="d6">97f5d2e9d34b3b50f8e922fc4bb7f824</data>
    </edge>
    <edge source="&quot;RC_CONNECTIVITY&quot;" target="&quot;DOUBLE-SCROLL ATTRACTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RC_CONNECTIVITY determines the density of the reservoir's internal matrix, which may have an impact on the observation of Double-Scroll Attractors."</data>
      <data key="d6">76f47f241e255f9f36646409d2ec30f1</data>
    </edge>
    <edge source="&quot;INPUT_CONNECTIVITY&quot;" target="&quot;REGULARIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both INPUT_CONNECTIVITY and REGULARIZATION are hyperparameters that influence the behavior of the reservoir, potentially affecting the regularization process."</data>
      <data key="d6">76f47f241e255f9f36646409d2ec30f1</data>
    </edge>
    <edge source="&quot;REGULARIZATION&quot;" target="&quot;ESN MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Regularization is a technique used to smooth the output of the ESN Models and reduce error on the Validation Set."</data>
      <data key="d6">5972cf7d440b1c3fdc0f05fca305f18d</data>
    </edge>
    <edge source="&quot;DOUBLE-SCROLL ATTRACTOR&quot;" target="&quot;RK23&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RK23 generates a Double-scroll Attractor, which is the subject of optimization and experimentation."</data>
      <data key="d6">7c7818502732457fb71aacdd9a90ee36</data>
    </edge>
    <edge source="&quot;OPTIMIZATION ALGORITHMS&quot;" target="&quot;R-SQUARED (R^2)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"R-squared (R^2) is an additional metric used in the optimization process to assess the goodness of fit of a model."</data>
      <data key="d6">4f7b43545046f0e6f9b6fb3816da1d79</data>
    </edge>
    <edge source="&quot;HYPERPARAMETER OPTIMIZATION&quot;" target="&quot;CONFIG FILE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Hyperparameter Optimization process uses the settings and hyperparameters defined in the Config File."</data>
      <data key="d6">d4684af3c445d312afe4d838abc45502</data>
    </edge>
    <edge source="&quot;HYPERPARAMETER OPTIMIZATION&quot;" target="&quot;CONFIGURATION FILE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Configuration File is used in the Hyperparameter Optimization process to define the settings and hyperparameters for the optimization process."</data>
      <data key="d6">bd4cf5e35045463b7f0d8da82debc122</data>
    </edge>
    <edge source="&quot;HYPERPARAMETER OPTIMIZATION&quot;" target="&quot;RANDOM SEARCH ALGORITHM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Random Search Algorithm is a technique used in the Hyperparameter Optimization process to randomly choose parameters within a specified range."</data>
      <data key="d6">bd4cf5e35045463b7f0d8da82debc122</data>
    </edge>
    <edge source="&quot;HYPERPARAMETER OPTIMIZATION&quot;" target="&quot;GRID SEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grid Search is a technique used in the Hyperparameter Optimization process to systematically search through a manually specified subset of the hyperparameter space."</data>
      <data key="d6">bd4cf5e35045463b7f0d8da82debc122</data>
    </edge>
    <edge source="&quot;OBJECTIVE FUNCTION&quot;" target="&quot;OPTIMIZATION ALGORITHM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Optimization Algorithm is used to optimize the Objective Function, which measures the performance of the forecasting task."</data>
      <data key="d6">91704ce63f9ba41247fdc452a7a62ba6</data>
    </edge>
    <edge source="&quot;OBJECTIVE FUNCTION&quot;" target="&quot;EXPERIMENTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Experimentation is the process of designing and conducting experiments to test the Objective Function, which in this case is the Root Mean Squared Error (RMSE) loss function."</data>
      <data key="d6">251a50c2ae8ceea4fd7da1127cc5f461</data>
    </edge>
    <edge source="&quot;OBJECTIVE FUNCTION&quot;" target="&quot;AUTHOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author discusses the definition and structure of the Objective Function."</data>
      <data key="d6">11749b7d0fdadf05ea29da6025618407</data>
    </edge>
    <edge source="&quot;GRID SEARCH&quot;" target="&quot;RANDOM SEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Grid search and random search are methods used for hyperparameter tuning in machine learning, with random search being considered more efficient than grid search."</data>
      <data key="d6">b3361508c3e49b5bb3089f10e31d2c81</data>
    </edge>
    <edge source="&quot;PARAMETERS&quot;" target="&quot;LOSS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Loss is a measure of error that depends on the Parameters of a model."</data>
      <data key="d6">6a6d88a8f9731e1ed05b786e0a9ba6dc</data>
    </edge>
    <edge source="&quot;PARAMETERS&quot;" target="&quot;EXPERIMENTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Experimentation involves testing and optimizing Parameters."</data>
      <data key="d6">6a6d88a8f9731e1ed05b786e0a9ba6dc</data>
    </edge>
    <edge source="&quot;PARAMETERS&quot;" target="&quot;PAPER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Paper provides information about various parameters used in the reservoir network."</data>
      <data key="d6">388cc054a99cc5cadff33147f95d6156</data>
    </edge>
    <edge source="&quot;LOSS&quot;" target="&quot;HYPER-PARAMETER EXPLORATION&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Loss is a measure used during hyper-parameter exploration to evaluate the performance of a machine learning model with different hyper-parameters."
"During Hyper-parameter Exploration, the Loss is minimized to optimize the model's performance."</data>
      <data key="d6">136d135c710f6cf78a4c536d43276fe1,716940af834825642e01a3cb59a7e006</data>
    </edge>
    <edge source="&quot;HINAUT, X.&quot;" target="&quot;TROUVAIN, N.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hinaut, X. and Trouvain, N. are co-authors of a guide on hyperparameter exploration."</data>
      <data key="d6">6a6d88a8f9731e1ed05b786e0a9ba6dc</data>
    </edge>
    <edge source="&quot;HINAUT, X.&quot;" target="&quot;ICANN 2021&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hinaut, X. presented a guide on hyperparameter exploration at ICANN 2021."</data>
      <data key="d6">6a6d88a8f9731e1ed05b786e0a9ba6dc</data>
    </edge>
    <edge source="&quot;HINAUT, X.&quot;" target="&quot;ICANN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hinaut, X. is an author of a guide presented at ICANN on exploring hyper-parameters for Echo State Networks."</data>
      <data key="d6">088d2280349d652200861994c09d7dd5</data>
    </edge>
    <edge source="&quot;TROUVAIN, N.&quot;" target="&quot;ICANN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Trouvain, N. is an author of a guide presented at ICANN on exploring hyper-parameters for Echo State Networks."</data>
      <data key="d6">088d2280349d652200861994c09d7dd5</data>
    </edge>
    <edge source="&quot;ICANN 2021&quot;" target="&quot;TROUVAIN &amp; HINAUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Trouvain &amp; Hinaut presented a paper at ICANN 2021 on Canary Song Decoder, Transduction, and Implicit Segmentation with ESNs and LTSMs."</data>
      <data key="d6">20b16c2e1cb8813ade96fea5f9591631</data>
    </edge>
    <edge source="&quot;ICANN 2021&quot;" target="&quot;XAVIER HINAUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Xavier Hinaut is an author of a guide presented at ICANN 2021."</data>
      <data key="d6">25743a99f36f3e56551ffafbba8d15c4</data>
    </edge>
    <edge source="&quot;ICANN 2021&quot;" target="&quot;NICOLAS TROUVAIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Nicolas Trouvain is an author of a guide presented at ICANN 2021."</data>
      <data key="d6">25743a99f36f3e56551ffafbba8d15c4</data>
    </edge>
    <edge source="&quot;XAVIER HINAUT&quot;" target="&quot;CANARY SONG DECODER RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Xavier Hinaut is an author of a research paper on canary song decoder: transduction and implicit segmentation with esns and ltsms."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;HP_SPACE&quot;" target="&quot;N&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"hp_space is the configuration parameter that explores the N parameter, which represents the number of neurons."
"hp_space is associated with the parameter N, which specifies the number of neurons."</data>
      <data key="d6">5cea9edfd65fcfa25a081554300b28cc,adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </edge>
    <edge source="&quot;HP_SPACE&quot;" target="&quot;SR&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"hp_space is the configuration parameter that explores the sr parameter, which represents the spectral radius."
"hp_space is associated with the parameter sr, which specifies the spectral radius."</data>
      <data key="d6">5cea9edfd65fcfa25a081554300b28cc,adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </edge>
    <edge source="&quot;HP_SPACE&quot;" target="&quot;LR&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"hp_space is the configuration parameter that explores the lr parameter, which represents the leaking rate."
"hp_space is associated with the parameter lr, which specifies the leaking rate."</data>
      <data key="d6">5cea9edfd65fcfa25a081554300b28cc,adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </edge>
    <edge source="&quot;HP_SPACE&quot;" target="&quot;HYPEROPT-MULTISCROLL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"hyperopt-multiscroll is associated with the parameter hp_space, which specifies the ranges of parameters explored."</data>
      <data key="d6">adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </edge>
    <edge source="&quot;SR&quot;" target="&quot;HYPER-PARAMETER EXPLORATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Support Vector Regression (SR) is one of the models used in Hyper-parameter Exploration."</data>
      <data key="d6">136d135c710f6cf78a4c536d43276fe1</data>
    </edge>
    <edge source="&quot;SR&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"SR is a parameter used in the configuration of the ES2N system."</data>
      <data key="d6">9dd8e12c7acbe10cf34817ff14780d24</data>
    </edge>
    <edge source="&quot;LR&quot;" target="&quot;HYPER-PARAMETER EXPLORATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Logistic Regression (LR) is one of the models used in Hyper-parameter Exploration."</data>
      <data key="d6">136d135c710f6cf78a4c536d43276fe1</data>
    </edge>
    <edge source="&quot;JSAN.DUMP()&quot;" target="&quot;HYPEROPT_CONFIG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"jsan.dump() is used to serialize the hyperopt_config object and write it to a file."</data>
      <data key="d6">80033e741d8e10abdcfe20dd17192152</data>
    </edge>
    <edge source="&quot;HYPEROPT_CONFIG&quot;" target="&quot;JSON.DUMP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"json.dump is used to serialize the hyperopt_config object and write it to a file."</data>
      <data key="d6">1db191f05801d40d5a346febd10d3352</data>
    </edge>
    <edge source="&quot;OBJECTIVE&quot;" target="&quot;RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The research function is used to optimize the objective function during hyperparameter optimization."</data>
      <data key="d6">1db191f05801d40d5a346febd10d3352</data>
    </edge>
    <edge source="&quot;OBJECTIVE&quot;" target="&quot;BEST&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Objective is the objective function used in the hyperparameter optimization process to find the best hyperparameters."</data>
      <data key="d6">0982b8d1eb1e636b19fa2e9d9361e566</data>
    </edge>
    <edge source="&quot;FORCE LEARNING&quot;" target="&quot;TRAINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"FORCE learning is a method used for online training of an Echo State Network (ESN)."</data>
      <data key="d6">1db191f05801d40d5a346febd10d3352</data>
    </edge>
    <edge source="&quot;BACKPROPAGATION-DECORRELATION&quot;" target="&quot;TRAINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Backpropagation-Decorrelation is a method used for training an Echo State Network (ESN) to minimize output error while preserving echo state properties."</data>
      <data key="d6">1db191f05801d40d5a346febd10d3352</data>
    </edge>
    <edge source="&quot;RESERVOIR ADAPTATION&quot;" target="&quot;TRAINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Adaptation is a method used for training an Echo State Network (ESN) by modifying internal reservoir parameters based on performance metrics."</data>
      <data key="d6">1db191f05801d40d5a346febd10d3352</data>
    </edge>
    <edge source="&quot;TRAINING&quot;" target="&quot;DEEP ECHO STATE NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep Echo State Networks are trained through the process of delivering targets to each readout."</data>
      <data key="d6">59b469bdd618b3f36b3547f4f2b8a862</data>
    </edge>
    <edge source="&quot;HYPER-PARAMETER EXPLORATION&quot;" target="&quot;R^2 SCORE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"R^2 Score is a statistical measure used during hyper-parameter exploration to assess the proportion of the variance explained by a machine learning model."</data>
      <data key="d6">716940af834825642e01a3cb59a7e006</data>
    </edge>
    <edge source="&quot;HYPER-PARAMETER EXPLORATION&quot;" target="&quot;LEARNING RATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Learning Rate is a hyper-parameter that can be explored during the hyper-parameter exploration process to optimize the learning rate of a machine learning model."</data>
      <data key="d6">716940af834825642e01a3cb59a7e006</data>
    </edge>
    <edge source="&quot;HYPER-PARAMETER EXPLORATION&quot;" target="&quot;RIDGE REGULARIZATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ridge Regularization is a technique that can be explored during the hyper-parameter exploration process to prevent overfitting in machine learning models by adding a penalty term to the loss function."</data>
      <data key="d6">716940af834825642e01a3cb59a7e006</data>
    </edge>
    <edge source="&quot;HYPER-PARAMETER EXPLORATION&quot;" target="&quot;R&#178; SCORE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"During Hyper-parameter Exploration, the R&#178; Score is maximized to improve the model's explanatory power."</data>
      <data key="d6">136d135c710f6cf78a4c536d43276fe1</data>
    </edge>
    <edge source="&quot;REGRESSION TASK&quot;" target="&quot;PASSIVEAGGRESSIVEREGRESSOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The PassiveAggressiveRegressor model is primarily used for regression tasks."</data>
      <data key="d6">35631fbf2ad11c53d75cb9b42e2c39b4</data>
    </edge>
    <edge source="&quot;REGRESSION&quot;" target="&quot;CLASSIFICATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Regression and classification are two different tasks with distinct objectives. Regression models predict continuous variables, while classification models assign data points to predefined categories."</data>
      <data key="d6">1c462a6eef00aac37dc1ab33a689b930</data>
    </edge>
    <edge source="&quot;REGRESSION&quot;" target="&quot;STATISTICAL LEARNING THEORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Regression is a statistical technique that is viewed as a supervised learning problem within the framework of Statistical Learning Theory."</data>
      <data key="d6">9261efcc24379d9c0b2d35a2fde8275d</data>
    </edge>
    <edge source="&quot;CLASSIFICATION&quot;" target="&quot;SCIKITLEARNNODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Classification is a method that can benefit from the integration of Scikit-Learn models through the use of ScikitLearnNode in ReservoirPy."</data>
      <data key="d6">c05906c1f12c4edfc32a04aa9935067e</data>
    </edge>
    <edge source="&quot;CLASSIFICATION&quot;" target="&quot;STATISTICAL LEARNING THEORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Classification is a statistical problem that is viewed as a supervised learning problem within the framework of Statistical Learning Theory."</data>
      <data key="d6">9261efcc24379d9c0b2d35a2fde8275d</data>
    </edge>
    <edge source="&quot;CLASSIFICATION&quot;" target="&quot;HAND MOVEMENTS IN SIGN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hand Movements in Sign is a technique used for identifying words based on a series of hand movements, which can be viewed as a classification problem."</data>
      <data key="d6">9261efcc24379d9c0b2d35a2fde8275d</data>
    </edge>
    <edge source="&quot;CLASSIFICATION&quot;" target="&quot;SIGNAL ESTIMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Classification and Signal Estimation are related concepts, as both involve the analysis and interpretation of data patterns, although in different contexts."</data>
      <data key="d6">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </edge>
    <edge source="&quot;REGRESSION MODELS&quot;" target="&quot;LOGISTIC REGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Regression Models include Logistic Regression, which is used to predict probabilities and can be part of a classifier."</data>
      <data key="d6">d1617d5101da7c02e732e53860c49383</data>
    </edge>
    <edge source="&quot;SVM&quot;" target="&quot;CLASSIFICATION ALGORITHMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"SVM is a true classification algorithm that does not provide probabilities."</data>
      <data key="d6">d1617d5101da7c02e732e53860c49383</data>
    </edge>
    <edge source="&quot;LINEAR PREDICTION COEFFICIENT (LPC)&quot;" target="&quot;LINEAR PREDICTIVE CODING (LPC)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Linear Prediction Coefficient (LPC) is a result of the Linear Predictive Coding (LPC) method, which is used for signal representation and analysis."</data>
      <data key="d6">d1617d5101da7c02e732e53860c49383</data>
    </edge>
    <edge source="&quot;LINEAR PREDICTIVE CODING (LPC)&quot;" target="&quot;CEPSTRAL DOMAIN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Linear Predictive Coding (LPC) is a signal processing technique used in the cepstral domain, but the specific relationship is not explicitly stated."
"The cepstral domain is a representation of a signal that separates the source and filter characteristics, which can be beneficial for tasks such as pitch detection. LPC is a method used to represent the spectral envelope of a signal, making it relevant to the cepstral domain."</data>
      <data key="d6">c1ba6d7a4f4bd16c4fd25baf07c9747c,d1617d5101da7c02e732e53860c49383</data>
    </edge>
    <edge source="&quot;LINEAR PREDICTIVE CODING (LPC)&quot;" target="&quot;LINEAR PREDICTION COEFFICIENTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LPC is a process that results in Linear Prediction Coefficients, which are used for signal representation and analysis."</data>
      <data key="d6">c1ba6d7a4f4bd16c4fd25baf07c9747c</data>
    </edge>
    <edge source="&quot;SCIKITLEARNNODE&quot;" target="&quot;LASSO REGRESSION&quot;">
      <data key="d4">3.0</data>
      <data key="d5">"ScikitLearnNode allows the integration of Lasso Regression models into ReservoirPy workflows, enabling the combination of reservoir computing with Scikit-Learn&#8217;s machine learning algorithms."
"ScikitLearnNode can be used to implement Lasso Regression as a regular offline readout node within the ReservoirPy library."
"ScikitLearnNode is used to integrate the Lasso Regression model into the reservoir computing framework."</data>
      <data key="d6">84cacfea14ea9ff46a34150e77a0767a,8c66981c9d2009113219bbf2681f664c,b11a9f7777c0232bfa7323ae82ad139b</data>
    </edge>
    <edge source="&quot;SCIKITLEARNNODE&quot;" target="&quot;LINEAR_MODEL.LASSO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ScikitLearnNode is used to implement the linear_model.Lasso machine learning model."</data>
      <data key="d6">dddc79e4cd04d2d07e35930dd8458168</data>
    </edge>
    <edge source="&quot;SCIKITLEARNNODE&quot;" target="&quot;SCIKITLEARN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ScikitLearnNode is a component that allows the use of models from the ScikitLearn library within the ReservoirPy framework."</data>
      <data key="d6">8c66981c9d2009113219bbf2681f664c</data>
    </edge>
    <edge source="&quot;SCIKITLEARNNODE&quot;" target="&quot;LINEAR_MODEL.PASSIVEAGGRESSIVEREGRESSOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ScikitLearnNode is used to interface with the linear_model.PassiveAggressiveRegressor machine learning model."</data>
      <data key="d6">ee83abbbbc707d8131952b2b01ebc268</data>
    </edge>
    <edge source="&quot;SCIKITLEARNNODE&quot;" target="&quot;PASSIVEAGGRESSIVEREGRESSOR&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ScikitLearnNode is used to handle the PassiveAggressiveRegressor model for regression tasks."
"ScikitLearnNode is used to initialize a node with the PassiveAggressiveRegressor model."</data>
      <data key="d6">2cba60e2f36479613bb0243a19f3a3b4,35631fbf2ad11c53d75cb9b42e2c39b4</data>
    </edge>
    <edge source="&quot;SCIKITLEARNNODE&quot;" target="&quot;RIDGECLASSIFIER&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ScikitLearnNode is used to demonstrate the RidgeClassifier model for classification tasks."
"ScikitLearnNode is used to initialize a node with the RidgeClassifier model."</data>
      <data key="d6">2cba60e2f36479613bb0243a19f3a3b4,35631fbf2ad11c53d75cb9b42e2c39b4</data>
    </edge>
    <edge source="&quot;SCIKITLEARNNODE&quot;" target="&quot;LOGISTICREGRESSION&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ScikitLearnNode is used to demonstrate the LogisticRegression model for classification tasks."
"ScikitLearnNode is used to initialize a node with the LogisticRegression model."</data>
      <data key="d6">2cba60e2f36479613bb0243a19f3a3b4,35631fbf2ad11c53d75cb9b42e2c39b4</data>
    </edge>
    <edge source="&quot;SCIKITLEARNNODE&quot;" target="&quot;LINEAR MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ScikitLearnNode uses the Linear Model component to create a machine learning model."</data>
      <data key="d6">dc3bd3697a140b64d70e0e3ac6db6c7e</data>
    </edge>
    <edge source="&quot;SCIKITLEARNNODE&quot;" target="&quot;LASSO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ScikitLearnNode uses the Lasso type of linear model to create a machine learning model."</data>
      <data key="d6">dc3bd3697a140b64d70e0e3ac6db6c7e</data>
    </edge>
    <edge source="&quot;SCIKITLEARNNODE&quot;" target="&quot;JAPANESE_VOWELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The japanese_vowels dataset is used in conjunction with ScikitLearnNode for classification tasks."</data>
      <data key="d6">2cba60e2f36479613bb0243a19f3a3b4</data>
    </edge>
    <edge source="&quot;LASSO REGRESSION&quot;" target="&quot;SCIKITLEARN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ScikitLearn provides the Lasso Regression model used in the code."</data>
      <data key="d6">b11a9f7777c0232bfa7323ae82ad139b</data>
    </edge>
    <edge source="&quot;LASSO REGRESSION&quot;" target="&quot;MACKEY-GLASS TASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Lasso Regression model is used in the Mackey-Glass task for time series prediction."</data>
      <data key="d6">b11a9f7777c0232bfa7323ae82ad139b</data>
    </edge>
    <edge source="&quot;REGRESSION METHODS&quot;" target="&quot;OUTLIER DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Outlier Data can significantly shift the decision boundary in regression tasks, making them less suitable for classification tasks."</data>
      <data key="d6">dadca3c89b34dc48a60c53367ab55768</data>
    </edge>
    <edge source="&quot;CLASSIFICATION TASKS&quot;" target="&quot;RIDGECLASSIFIER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RidgeClassifier is a machine learning algorithm specifically designed for classification tasks, offering a solution to the limitations of regression methods."</data>
      <data key="d6">dadca3c89b34dc48a60c53367ab55768</data>
    </edge>
    <edge source="&quot;CLASSIFICATION TASKS&quot;" target="&quot;LOGISTICREGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LogisticRegression is a machine learning algorithm specifically designed for classification tasks, providing a probabilistic approach to categorizing data."</data>
      <data key="d6">dadca3c89b34dc48a60c53367ab55768</data>
    </edge>
    <edge source="&quot;CLASSIFICATION TASKS&quot;" target="&quot;ARGMAX() FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The argmax() function is used to find the indices of the maximum values along an axis in a given array, which is a common operation in classification tasks."</data>
      <data key="d6">dadca3c89b34dc48a60c53367ab55768</data>
    </edge>
    <edge source="&quot;JAPANESE_VOWELS() FUNCTION&quot;" target="&quot;REPEAT_TARGETS PARAMETER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The repeat_targets parameter in the japanese_vowels() function ensures that one label is obtained per timestep, not one label per utterance, which is important for data processing and analysis."</data>
      <data key="d6">dadca3c89b34dc48a60c53367ab55768</data>
    </edge>
    <edge source="&quot;NP.ARGMAX&quot;" target="&quot;KEEPDIMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The keepdims parameter is used in the np.argmax function to maintain the dimensions of the original array after performing the argmax operation."</data>
      <data key="d6">b3361508c3e49b5bb3089f10e31d2c81</data>
    </edge>
    <edge source="&quot;RANDOM SEARCH&quot;" target="&quot;HYPERPARAMETERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Random Search is a method for hyperparameter exploration that samples more efficiently and does not waste evaluations on dimensions that do not significantly impact performance."</data>
      <data key="d6">4a9f33fa18891b67267b7615d61caaac</data>
    </edge>
    <edge source="&quot;ECHO STATE PROPERTY (ESP)&quot;" target="&quot;SPECTRAL RADIUS (SR)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Property (ESP) states that the spectral radius should be less than 1, but in practice, the optimal spectral radius can be greater than 1, making it a rough guide rather than a strict rule."</data>
      <data key="d6">4a9f33fa18891b67267b7615d61caaac</data>
    </edge>
    <edge source="&quot;HYPERPARAMETERS&quot;" target="&quot;AUTHOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author presents a basic example of optimizing hyperparameters using Hyperopt and ReservoirPy.hyper tools."</data>
      <data key="d6">73e81fd6509a2ba400a8435793ade3c5</data>
    </edge>
    <edge source="&quot;HYPERPARAMETERS&quot;" target="&quot;OPTIMIZATION TOOLS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Optimization Tools are used to find the best values for hyperparameters, such as the Leaking Rate, to improve the performance of the model."</data>
      <data key="d6">2d8ea1123f365fb047b024022ba4fdc4</data>
    </edge>
    <edge source="&quot;HYPERPARAMETER EXPLORATION&quot;" target="&quot;MACKEY-GLASS TIME SERIES PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperparameter Exploration is used to optimize the performance of Mackey-Glass Time Series Prediction."</data>
      <data key="d6">1315547792fcb5d618913a4c7ac03511</data>
    </edge>
    <edge source="&quot;HYPERPARAMETER EXPLORATION&quot;" target="&quot;LORENZ TIME SERIES PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperparameter Exploration is used to optimize the performance of Lorenz Time Series Prediction."</data>
      <data key="d6">1315547792fcb5d618913a4c7ac03511</data>
    </edge>
    <edge source="&quot;HYPERPARAMETER EXPLORATION&quot;" target="&quot;HYPERPARAMETER INTERDEPENDENCY PLOTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperparameter Interdependency Plots are used as a tool in the Hyperparameter Exploration process to evaluate the interdependencies between hyperparameters."</data>
      <data key="d6">1315547792fcb5d618913a4c7ac03511</data>
    </edge>
    <edge source="&quot;HYPERPARAMETER INTERDEPENDENCY PLOTS&quot;" target="&quot;HYPERPARAMETER SEARCH METHOD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyperparameter Interdependency Plots are used to evaluate the Hyperparameter Search Method and manage the interdependencies between hyperparameters."</data>
      <data key="d6">4ea4de00090795130d7ae12a57a729ec</data>
    </edge>
    <edge source="&quot;HYPERPARAMETER SEARCH METHOD&quot;" target="&quot;MACKEY-GLASS TIME SERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Hyperparameter Search Method is used to optimize the prediction of Mackey-Glass Time Series."</data>
      <data key="d6">4ea4de00090795130d7ae12a57a729ec</data>
    </edge>
    <edge source="&quot;HYPERPARAMETER SEARCH METHOD&quot;" target="&quot;LORENZ TIME SERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Hyperparameter Search Method is used to optimize the prediction of Lorenz Time Series."</data>
      <data key="d6">4ea4de00090795130d7ae12a57a729ec</data>
    </edge>
    <edge source="&quot;HYPERPARAMETER SEARCH METHOD&quot;" target="&quot;TEST DOCUMENT RECEPTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Test Document Reception is a test used to evaluate the effectiveness of the Hyperparameter Search Method."</data>
      <data key="d6">4ea4de00090795130d7ae12a57a729ec</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS TIME SERIES&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ES2N is evaluated using the Mackey-Glass Time Series dataset for forecasting."</data>
      <data key="d6">4a7ca13b3f869961817e2aa723e67d24</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS TIME SERIES&quot;" target="&quot;TASK 1: 10 TIMESTEPS AHEAD FORECAST&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Mackey-Glass Time Series is the data set used in Task 1 to predict 10 timesteps ahead."</data>
      <data key="d6">fac681bdc38ae5829173c747ee6240fa</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS TIME SERIES&quot;" target="&quot;DATA PREPROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Mackey-Glass Time Series dataset is used for data preprocessing, such as converting it into a forecasting format."</data>
      <data key="d6">b2beacacc8c190393e4583a69518378c</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS TASK&quot;" target="&quot;THE MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The model is tested on the Mackey-Glass task to evaluate its prediction capabilities."</data>
      <data key="d6">b3c8de6f33c2ebb84f0d2797933d0cad</data>
    </edge>
    <edge source="&quot;THE MODEL&quot;" target="&quot;THE AUTHOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author creates the model and evaluates its performance on the Mackey-Glass task."</data>
      <data key="d6">b3c8de6f33c2ebb84f0d2797933d0cad</data>
    </edge>
    <edge source="&quot;CASTING(MG, FORECAST=10, TEST_SIZE=0.2)&quot;" target="&quot;ESN PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The casting(mg, forecast=10, test_size=0.2) process involves generating ESN predictions."</data>
      <data key="d6">ee83abbbbc707d8131952b2b01ebc268</data>
    </edge>
    <edge source="&quot;ESN PREDICTION&quot;" target="&quot;TRUE VALUE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN predictions are compared to the True value in data analysis or modeling."</data>
      <data key="d6">ee83abbbbc707d8131952b2b01ebc268</data>
    </edge>
    <edge source="&quot;ESN PREDICTION&quot;" target="&quot;RSQUARE(Y_TEST, Y_PRED)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"rsquare(y_test, y_pred) is used to evaluate the goodness of fit of the ESN prediction model."</data>
      <data key="d6">ee83abbbbc707d8131952b2b01ebc268</data>
    </edge>
    <edge source="&quot;ESN PREDICTION&quot;" target="&quot;NRMSE(Y_TEST, Y_PRED)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"nrmse(y_test, y_pred) is used to evaluate the accuracy of the ESN prediction model."</data>
      <data key="d6">ee83abbbbc707d8131952b2b01ebc268</data>
    </edge>
    <edge source="&quot;PASSIVEAGGRESSIVEREGRESSOR&quot;" target="&quot;SCIKIT-LEARN NODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Scikit-learn Node is a wrapper for using the PassiveAggressiveRegressor model from scikit-learn."</data>
      <data key="d6">52d001cd1786e3d9f36e0c57538bc21e</data>
    </edge>
    <edge source="&quot;PYTHON&quot;" target="&quot;MATPLOTLIB&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Matplotlib is a library used in Python for data visualization, such as creating heatmaps to visualize the performance of machine learning models."</data>
      <data key="d6">ef9bf350e25daa8f123b0b5c4d60de5f</data>
    </edge>
    <edge source="&quot;TROUVAIN ET AL.&quot;" target="&quot;TROUVAIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Trouvain is a member of the author group Trouvain et al., mentioned in the tutorial on exploring hyperparameters."</data>
      <data key="d6">280cbdf53022bbaed48ccb34ebe142bc</data>
    </edge>
    <edge source="&quot;HINAUT ET AL.&quot;" target="&quot;HINAUT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hinaut is a member of the author group Hinaut et al., mentioned in the paper providing advice and a method for exploring hyperparameters."</data>
      <data key="d6">280cbdf53022bbaed48ccb34ebe142bc</data>
    </edge>
    <edge source="&quot;PAGLIARINI ET AL.&quot;" target="&quot;ICDL 2021&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Pagliarini et al. presented a paper at ICDL 2021 on Canary Vocal Sensorimotor Model with RNN Decoder and Low-dimensional GAN Generator."</data>
      <data key="d6">20b16c2e1cb8813ade96fea5f9591631</data>
    </edge>
    <edge source="&quot;PAGLIARINI ET AL.&quot;" target="&quot;HAL PREPRINT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Pagliarini et al. published a paper as a HAL preprint on What does the Canary Say? Low-Dimensional GAN Applied to Birdsong."</data>
      <data key="d6">20b16c2e1cb8813ade96fea5f9591631</data>
    </edge>
    <edge source="&quot;MNEMOSYNE GROUP&quot;" target="&quot;BORDEAUX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Mnemosyne group is located at Bordeaux, France."</data>
      <data key="d6">2a197220a94bac0b44fc0b07712e45ba</data>
    </edge>
    <edge source="&quot;NATHAN TROUVAIN&quot;" target="&quot;CANARY SONG DECODER RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Nathan Trouvain is an author of a research paper on canary song decoder: transduction and implicit segmentation with esns and ltsms."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;AUTHOR&quot;" target="&quot;LOSS FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author mentions the use of the Loss Function in the context of the Objective Function."</data>
      <data key="d6">11749b7d0fdadf05ea29da6025618407</data>
    </edge>
    <edge source="&quot;AUTHOR&quot;" target="&quot;R&#178;&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author mentions the computation of the R&#178; metric in addition to the Loss Function."</data>
      <data key="d6">11749b7d0fdadf05ea29da6025618407</data>
    </edge>
    <edge source="&quot;AUTHOR&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Author developed the ES2N model for time series forecasting."</data>
      <data key="d6">854761a5b5b5b90af10bc6b6c76cc355</data>
    </edge>
    <edge source="&quot;AUTHOR&quot;" target="&quot;NVAR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author is using the NVAR model to analyze the Double Scroll Attractor."</data>
      <data key="d6">684b1edf65b327cc06ceb69ca1279d74</data>
    </edge>
    <edge source="&quot;AUTHOR&quot;" target="&quot;TIME SERIES PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author is using Echo State Networks for time series prediction."</data>
      <data key="d6">41fa16855df7da666dc6fc38d2f8ee53</data>
    </edge>
    <edge source="&quot;AUTHOR&quot;" target="&quot;ROBOT PERFORMANCE EVALUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author discusses the process of evaluating Robot Performance in the text."</data>
      <data key="d6">e7d249cdab85dc69b631d43ac6b62915</data>
    </edge>
    <edge source="&quot;AUTHOR&quot;" target="&quot;CANARY SONG DECODING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The author mentions the use case of Canary Song Decoding in the text."</data>
      <data key="d6">e7d249cdab85dc69b631d43ac6b62915</data>
    </edge>
    <edge source="&quot;ECHO STATE PROPERTY&quot;" target="&quot;ADDITIVE-SIGMOID NEURON RESERVOIRS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Property is a property that applies to Additive-Sigmoid Neuron Reservoirs."</data>
      <data key="d6">a9f53979e9dbe6b936ff3374c73006dd</data>
    </edge>
    <edge source="&quot;ECHO STATE PROPERTY&quot;" target="&quot;EDGE OF STABILITY ECHO STATE NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Edge of Stability Echo State Network is based on the Echo State Property principle."</data>
      <data key="d6">578045eb341c5e05d5a912f634854499</data>
    </edge>
    <edge source="&quot;INPUT SCALING&quot;" target="&quot;REGULARIZATION PARAMETER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input Scaling and Regularization Parameter are parameters mentioned in the text, which are fixed."</data>
      <data key="d6">0a9b132ecb1c4b63fdbb0e144295362e</data>
    </edge>
    <edge source="&quot;INPUT SCALING&quot;" target="&quot;ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input Scaling is a parameter used in the configuration of the ES2N system."</data>
      <data key="d6">9dd8e12c7acbe10cf34817ff14780d24</data>
    </edge>
    <edge source="&quot;HYPEROPT-MULTISCROLL&quot;" target="&quot;HP_MAX_EVALS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"hyperopt-multiscroll is associated with the parameter hp_max_evals, which specifies the number of different sets of parameters to try."</data>
      <data key="d6">adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </edge>
    <edge source="&quot;HYPEROPT-MULTISCROLL&quot;" target="&quot;HP_METHOD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"hyperopt-multiscroll is associated with the parameter hp_method, which specifies the method used to choose sets of parameters."</data>
      <data key="d6">adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </edge>
    <edge source="&quot;HYPEROPT-MULTISCROLL&quot;" target="&quot;INSTANCES_PER_TRIAL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"hyperopt-multiscroll is associated with the parameter instances_per_trial, which specifies how many random ESN will be tried with each set of parameters."</data>
      <data key="d6">adfc38e9dc5e6fd0fe67ce83dfa1f154</data>
    </edge>
    <edge source="&quot;Y&quot;" target="&quot;U&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Y is generated using the input data array u and the arrays TAUS and NUS."</data>
      <data key="d6">f3e58b69b1a93175e3094a2ba65c0429</data>
    </edge>
    <edge source="&quot;Y&quot;" target="&quot;MAX_TAU&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MAX_TAU is a parameter used in the generation of the NumPy array Y."</data>
      <data key="d6">abd3ca2db22004a5de548dc22010c4d4</data>
    </edge>
    <edge source="&quot;Y&quot;" target="&quot;NUS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NUS is a parameter used in the generation of the NumPy array Y."</data>
      <data key="d6">abd3ca2db22004a5de548dc22010c4d4</data>
    </edge>
    <edge source="&quot;Y&quot;" target="&quot;LEAKYESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The NumPy array Y is used as input data for the leakyESN model."</data>
      <data key="d6">abd3ca2db22004a5de548dc22010c4d4</data>
    </edge>
    <edge source="&quot;Y&quot;" target="&quot;CHAPTER 2&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The variable Y is mentioned in Chapter 2 as output or target data used in the context of the generative mode."</data>
      <data key="d6">e396354e3a9be76616392af11f56e671</data>
    </edge>
    <edge source="&quot;TRAIN_LEN&quot;" target="&quot;FORECAST&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"train_len and forecast are parameters used together to determine the lengths of the training and forecasted data."</data>
      <data key="d6">f70c7d3d89baaabbeaad57b58e379e08</data>
    </edge>
    <edge source="&quot;IMPROVING RESERVOIRS&quot;" target="&quot;BENJAMIN SCHRAUWEN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Benjamin Schrauwen is a contributor to the study on improving reservoirs."</data>
      <data key="d6">414aba25cd4916c4a9e916beef385e81</data>
    </edge>
    <edge source="&quot;IMPROVING RESERVOIRS&quot;" target="&quot;MARION WARDERMANN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Marion Wardermann is a contributor to the study on improving reservoirs."</data>
      <data key="d6">414aba25cd4916c4a9e916beef385e81</data>
    </edge>
    <edge source="&quot;IMPROVING RESERVOIRS&quot;" target="&quot;DAVID VERSTRAETEN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"David Verstraeten is a contributor to the study on improving reservoirs."</data>
      <data key="d6">414aba25cd4916c4a9e916beef385e81</data>
    </edge>
    <edge source="&quot;IMPROVING RESERVOIRS&quot;" target="&quot;JOCHEN J. STEIL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jochen J. Steil is a contributor to the study on improving reservoirs."</data>
      <data key="d6">414aba25cd4916c4a9e916beef385e81</data>
    </edge>
    <edge source="&quot;IMPROVING RESERVOIRS&quot;" target="&quot;DIRK STROOBANDT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dirk Stroobandt is a contributor to the study on improving reservoirs."</data>
      <data key="d6">414aba25cd4916c4a9e916beef385e81</data>
    </edge>
    <edge source="&quot;IMPROVING RESERVOIRS&quot;" target="&quot;NEUROCOMPUTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The study on improving reservoirs was published in Neurocomputing."</data>
      <data key="d6">414aba25cd4916c4a9e916beef385e81</data>
    </edge>
    <edge source="&quot;IMPROVING RESERVOIRS&quot;" target="&quot;TRIESCH, J.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Triesch, J.'s study on a gradient rule for the plasticity of a neuron's intrinsic excitability is related to the study on improving reservoirs."</data>
      <data key="d6">414aba25cd4916c4a9e916beef385e81</data>
    </edge>
    <edge source="&quot;BENJAMIN SCHRAUWEN&quot;" target="&quot;OGER RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Benjamin Schrauwen is an author of a research paper on oger: modular learning architectures for large-scale sequential processing."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;DAVID VERSTRAETEN&quot;" target="&quot;OGER RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"David Verstraeten is an author of a research paper on oger: modular learning architectures for large-scale sequential processing."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;NARMA&quot;" target="&quot;INTRINSIC PLASTICITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Intrinsic Plasticity is a technique used for NARMA timeseries prediction."</data>
      <data key="d6">53489f27f4a6a2b135fe6ea6fac9b479</data>
    </edge>
    <edge source="&quot;IPRESERVOIR&quot;" target="&quot;AUTHORS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Authors have extended the ideas of intrinsic plasticity to the IPReservoir node."</data>
      <data key="d6">325bd631a690a34736918180b01f6917</data>
    </edge>
    <edge source="&quot;IPRESERVOIR&quot;" target="&quot;IMPROVING RESERVOIRS USING INTRINSIC PLASTICITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IPReservoir is implemented to improve reservoirs using the method described in the paper Improving Reservoirs Using Intrinsic Plasticity."</data>
      <data key="d6">701ffa843b8d26f96c23dae69e683b58</data>
    </edge>
    <edge source="&quot;IPRESERVOIR&quot;" target="&quot;SCHRAUWEN ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IPReservoir implements the method described in the paper Improving Reservoirs Using Intrinsic Plasticity by Schrauwen et al."</data>
      <data key="d6">701ffa843b8d26f96c23dae69e683b58</data>
    </edge>
    <edge source="&quot;IPRESERVOIR&quot;" target="&quot;INTRINSIC PLASTICITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IPReservoir is a type of reservoir used in the technique of Intrinsic Plasticity."</data>
      <data key="d6">53489f27f4a6a2b135fe6ea6fac9b479</data>
    </edge>
    <edge source="&quot;IPRESERVOIR&quot;" target="&quot;TIME SERIES PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The IPReservoir model is primarily used for Time Series Prediction tasks."</data>
      <data key="d6">eadceb9674dd1ce90473d99e0b58e141</data>
    </edge>
    <edge source="&quot;IP RULE&quot;" target="&quot;INTRINSIC PLASTICITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The IP Rule is a method that implements the concept of Intrinsic Plasticity, as described by Triesch."</data>
      <data key="d6">83ded1f13bd74b00694092be69a83870</data>
    </edge>
    <edge source="&quot;IP RULE&quot;" target="&quot;SCHRAUWEN ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Schrauwen et al. describe the implementation of the IP Rule in their paper."</data>
      <data key="d6">83ded1f13bd74b00694092be69a83870</data>
    </edge>
    <edge source="&quot;TRIESCH&quot;" target="&quot;INTRINSIC PLASTICITY&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Triesch is the author of the paper where the concept of Intrinsic Plasticity was first described."
"Triesch first described a gradient rule for the plasticity of a neuron&#8217;s intrinsic excitability, which is a concept related to Intrinsic Plasticity."</data>
      <data key="d6">701ffa843b8d26f96c23dae69e683b58,83ded1f13bd74b00694092be69a83870</data>
    </edge>
    <edge source="&quot;INTRINSIC PLASTICITY&quot;" target="&quot;SIGMOID ACTIVATION FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sigmoid Activation Function is used in the context of Intrinsic Plasticity."</data>
      <data key="d6">53489f27f4a6a2b135fe6ea6fac9b479</data>
    </edge>
    <edge source="&quot;INTRINSIC PLASTICITY&quot;" target="&quot;EXPONENTIAL DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Exponential Distribution is used as the activation distribution for neurons equipped with a sigmoid activation function in Intrinsic Plasticity."</data>
      <data key="d6">53489f27f4a6a2b135fe6ea6fac9b479</data>
    </edge>
    <edge source="&quot;CONNECTIVITY&quot;" target="&quot;RANDOM SPARSE MATRICES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Connectivity is a property of a matrix mentioned in the context of creating random sparse matrices."</data>
      <data key="d6">ff860bc63e3d697a6183c0b850689048</data>
    </edge>
    <edge source="&quot;HYPERBOLIC TANGENT ACTIVATION FUNCTION&quot;" target="&quot;PAPER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Paper discusses the use of the Hyperbolic Tangent Activation Function in the context of the reservoir network."</data>
      <data key="d6">388cc054a99cc5cadff33147f95d6156</data>
    </edge>
    <edge source="&quot;PAPER&quot;" target="&quot;RESERVOIR NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Paper discusses the use of a Reservoir Network in the context of the reservoir activation distribution."</data>
      <data key="d6">388cc054a99cc5cadff33147f95d6156</data>
    </edge>
    <edge source="&quot;PAPER&quot;" target="&quot;RESERVOIR ACTIVATION DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Paper discusses the importance of the reservoir activation distribution in the context of the reservoir network."</data>
      <data key="d6">388cc054a99cc5cadff33147f95d6156</data>
    </edge>
    <edge source="&quot;TIME SERIES PREDICTION&quot;" target="&quot;LINEARSCR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LinearSCR is a model used for Time Series Prediction, as it can be trained to forecast future values based on past observations."</data>
      <data key="d6">5f841065cf74ef7bbc28efb775d5585e</data>
    </edge>
    <edge source="&quot;TIME SERIES PREDICTION&quot;" target="&quot;ES^2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES^2N model is used for Time Series Prediction tasks."</data>
      <data key="d6">6a7bea5f60347ea864c06adc327829dc</data>
    </edge>
    <edge source="&quot;TIME SERIES PREDICTION&quot;" target="&quot;ECHO STATE NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Networks are commonly used for the task of Time Series Prediction."</data>
      <data key="d6">29f9b2e5fa311519b18e7aef31c68d0a</data>
    </edge>
    <edge source="&quot;TIME SERIES PREDICTION&quot;" target="&quot;TRAINING THE ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Training the ESN is a step in the process of Time Series Prediction, allowing the model to learn patterns and make predictions."</data>
      <data key="d6">eb7a223eeb120e3fcc45a96a6018707d</data>
    </edge>
    <edge source="&quot;TIME SERIES PREDICTION&quot;" target="&quot;RESERVOIR COMPUTING MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reservoir Computing Models are commonly used for the task of Time Series Prediction, which involves forecasting future values based on past observations."</data>
      <data key="d6">0113164912437e96423379cb9c039f56</data>
    </edge>
    <edge source="&quot;GLOBAL ACTIVATION&quot;" target="&quot;RESERVOIR ACTIVATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Global activation refers to the distribution of reservoir activations on a global scale."</data>
      <data key="d6">7f3ec3328c157265cc61021bd62b0ed7</data>
    </edge>
    <edge source="&quot;GLOBAL ACTIVATION&quot;" target="&quot;TARGET DISTRIBUTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The relationship is described through the comparison of the global activation distribution with the target distribution."</data>
      <data key="d6">7f3ec3328c157265cc61021bd62b0ed7</data>
    </edge>
    <edge source="&quot;TARGET DISTRIBUTION&quot;" target="&quot;RESERVOIR ACTIVATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Target distribution refers to the desired probability density of reservoir activations."</data>
      <data key="d6">7f3ec3328c157265cc61021bd62b0ed7</data>
    </edge>
    <edge source="&quot;PARAMETRIC METHODS&quot;" target="&quot;NON-PARAMETRIC METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Parametric Methods and Non-Parametric Methods are different approaches to time series analysis, with Parametric Methods assuming a certain structure and Non-Parametric Methods not making such assumptions."</data>
      <data key="d6">c087c124713c7ade4223617d95928cbf</data>
    </edge>
    <edge source="&quot;LINEAR METHODS&quot;" target="&quot;NON-LINEAR METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Linear Methods and Non-Linear Methods are different types of time series analysis methods, with Linear Methods using linear models and Non-Linear Methods using non-linear models."</data>
      <data key="d6">c087c124713c7ade4223617d95928cbf</data>
    </edge>
    <edge source="&quot;UNIVARIATE METHODS&quot;" target="&quot;MULTIVARIATE METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Univariate Methods and Multivariate Methods are different types of time series analysis methods, with Univariate Methods focusing on a single variable and Multivariate Methods focusing on multiple variables."</data>
      <data key="d6">c087c124713c7ade4223617d95928cbf</data>
    </edge>
    <edge source="&quot;PANEL DATA&quot;" target="&quot;CROSS-SECTIONAL DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Cross-Sectional Data and Panel Data are different types of data sets, with Cross-Sectional Data determined by a non-time identifier and Panel Data including both time series and cross-sectional data."</data>
      <data key="d6">c087c124713c7ade4223617d95928cbf</data>
    </edge>
    <edge source="&quot;UNITED STATES&quot;" target="&quot;TUBERCULOSIS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Tuberculosis incidence rates are analyzed in the context of the United States."
"Tuberculosis incidence data is mentioned in the context of the United States."</data>
      <data key="d6">4b75a8a7637b05307e62f309c682d43b,a2b394d556da06b8c14dd2f5e106343b</data>
    </edge>
    <edge source="&quot;SERIES ANALYSIS&quot;" target="&quot;HEAT MAP MATRICES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Heat Map Matrices are a visual tool used in Series Analysis to represent time series data."</data>
      <data key="d6">630c86e110e2dabbe068f446b619cef3</data>
    </edge>
    <edge source="&quot;SERIES ANALYSIS&quot;" target="&quot;CURVE FITTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Curve Fitting is a technique used in Series Analysis to discover patterns in data over time."</data>
      <data key="d6">630c86e110e2dabbe068f446b619cef3</data>
    </edge>
    <edge source="&quot;CURVE FITTING&quot;" target="&quot;SMOOTHING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Smoothing is a method used in Curve Fitting to construct a 'smooth' function that approximately fits the data."</data>
      <data key="d6">630c86e110e2dabbe068f446b619cef3</data>
    </edge>
    <edge source="&quot;CURVE FITTING&quot;" target="&quot;PROCESSES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Curve Fitting is used to analyze and study processes that are expected to generally grow in magnitude."</data>
      <data key="d6">630c86e110e2dabbe068f446b619cef3</data>
    </edge>
    <edge source="&quot;CURVE FITTING&quot;" target="&quot;FUNCTION APPROXIMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Curve Fitting is a technique used in Function Approximation to approximate a function when only a set of points is provided."</data>
      <data key="d6">9ec0dac4c72bcc2c78c7df43b9969fe7</data>
    </edge>
    <edge source="&quot;FUNCTION APPROXIMATION&quot;" target="&quot;POLYNOMIAL REGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Polynomial Regression is a method used in Function Approximation to model an entire data set with a single polynomial."</data>
      <data key="d6">472b44b36407c9a89cf5c51459188263</data>
    </edge>
    <edge source="&quot;FUNCTION APPROXIMATION&quot;" target="&quot;SPLINE INTERPOLATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Spline Interpolation is a method used in Function Approximation to model a data set with piecewise continuous functions composed of many polynomials."</data>
      <data key="d6">472b44b36407c9a89cf5c51459188263</data>
    </edge>
    <edge source="&quot;FUNCTION APPROXIMATION&quot;" target="&quot;APPROXIMATION THEORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Function Approximation is a branch of Approximation Theory that focuses on selecting a function from a well-defined class to closely match a target function."</data>
      <data key="d6">9ec0dac4c72bcc2c78c7df43b9969fe7</data>
    </edge>
    <edge source="&quot;FUNCTION APPROXIMATION&quot;" target="&quot;CLASSIFICATION PROBLEM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Classification Problem is a problem that can arise in Function Approximation when the target function has a finite codomain, requiring data point categorization."</data>
      <data key="d6">9ec0dac4c72bcc2c78c7df43b9969fe7</data>
    </edge>
    <edge source="&quot;FUNCTION APPROXIMATION&quot;" target="&quot;ONLINE TIME SERIES APPROXIMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Online Time Series Approximation is a problem that can be approached using techniques of Function Approximation to summarize and represent time series data."</data>
      <data key="d6">9ec0dac4c72bcc2c78c7df43b9969fe7</data>
    </edge>
    <edge source="&quot;WORLD WAR II&quot;" target="&quot;SIGNAL ESTIMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"World War II significantly accelerated the development of mathematical techniques and technologies for signal processing and estimation."</data>
      <data key="d6">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </edge>
    <edge source="&quot;NORBERT WIENER&quot;" target="&quot;SIGNAL ESTIMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Norbert Wiener made significant contributions to signal processing and filtering during World War II, significantly accelerating the development of these techniques."</data>
      <data key="d6">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </edge>
    <edge source="&quot;RUDOLF E. K&#193;LM&#193;N&quot;" target="&quot;SIGNAL ESTIMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Rudolf E. K&#225;lm&#225;n developed the Kalman filter, a mathematical tool for signal estimation and prediction, which was accelerated during World War II."</data>
      <data key="d6">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </edge>
    <edge source="&quot;DENNIS GABOR&quot;" target="&quot;SIGNAL ESTIMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dennis Gabor contributed to the development of signal processing and spectral density estimation during World War II, accelerating these techniques."</data>
      <data key="d6">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </edge>
    <edge source="&quot;SEGMENTATION&quot;" target="&quot;TIME-SERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Segmentation is the process of splitting a time-series into a sequence of segments, each with its own characteristic properties."</data>
      <data key="d6">423d3b5ec1acc9a4cb448a15d3b6b595</data>
    </edge>
    <edge source="&quot;DIGITAL SIGNAL PROCESSING&quot;" target="&quot;TIME-SERIES SEGMENTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Digital Signal Processing is a field that includes the technique of Time-Series Segmentation."</data>
      <data key="d6">5d6a266e9d567f013be17768c04cbc09</data>
    </edge>
    <edge source="&quot;DIGITAL SIGNAL PROCESSING&quot;" target="&quot;TIME-SERIES CLUSTERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Digital Signal Processing is a field that includes the technique of Time-Series Clustering."</data>
      <data key="d6">5d6a266e9d567f013be17768c04cbc09</data>
    </edge>
    <edge source="&quot;TIME-SERIES CLUSTERING&quot;" target="&quot;SUBSEQUENCE TIME-SERIES CLUSTERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time-Series Clustering includes the specific approach of Subsequence Time-Series Clustering."</data>
      <data key="d6">5d6a266e9d567f013be17768c04cbc09</data>
    </edge>
    <edge source="&quot;AUTOREGRESSIVE (AR) MODELS&quot;" target="&quot;AUTOREGRESSIVE MOVING-AVERAGE (ARMA) MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Autoregressive Moving-Average (ARMA) Models combine Autoregressive and Moving-Average concepts."</data>
      <data key="d6">a000a3fbf1f8fad62e4c25b495858c79</data>
    </edge>
    <edge source="&quot;AUTOREGRESSIVE (AR) MODELS&quot;" target="&quot;AUTOREGRESSIVE FRACTIONALLY INTEGRATED MOVING-AVERAGE (ARFIMA) MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Autoregressive Fractionally Integrated Moving-Average (ARFIMA) Models generalize Autoregressive, Integrated, and Moving-Average concepts."</data>
      <data key="d6">a000a3fbf1f8fad62e4c25b495858c79</data>
    </edge>
    <edge source="&quot;INTEGRATED (I) MODELS&quot;" target="&quot;AUTOREGRESSIVE INTEGRATED MOVING-AVERAGE (ARIMA) MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Autoregressive Integrated Moving-Average (ARIMA) Models combine Autoregressive and Integrated concepts with Moving-Average."</data>
      <data key="d6">a000a3fbf1f8fad62e4c25b495858c79</data>
    </edge>
    <edge source="&quot;INTEGRATED (I) MODELS&quot;" target="&quot;AUTOREGRESSIVE FRACTIONALLY INTEGRATED MOVING-AVERAGE (ARFIMA) MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Autoregressive Fractionally Integrated Moving-Average (ARFIMA) Models generalize Autoregressive, Integrated, and Moving-Average concepts."</data>
      <data key="d6">a000a3fbf1f8fad62e4c25b495858c79</data>
    </edge>
    <edge source="&quot;MOVING-AVERAGE (MA) MODELS&quot;" target="&quot;AUTOREGRESSIVE MOVING-AVERAGE (ARMA) MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Autoregressive Moving-Average (ARMA) Models combine Autoregressive and Moving-Average concepts."</data>
      <data key="d6">a000a3fbf1f8fad62e4c25b495858c79</data>
    </edge>
    <edge source="&quot;MOVING-AVERAGE (MA) MODELS&quot;" target="&quot;AUTOREGRESSIVE FRACTIONALLY INTEGRATED MOVING-AVERAGE (ARFIMA) MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Autoregressive Fractionally Integrated Moving-Average (ARFIMA) Models generalize Autoregressive, Integrated, and Moving-Average concepts."</data>
      <data key="d6">a000a3fbf1f8fad62e4c25b495858c79</data>
    </edge>
    <edge source="&quot;VECTOR-VALUED DATA&quot;" target="&quot;MULTIVARIATE TIME-SERIES MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multivariate Time-Series Models are extensions of univariate models to deal with Vector-Valued Data."</data>
      <data key="d6">a000a3fbf1f8fad62e4c25b495858c79</data>
    </edge>
    <edge source="&quot;MULTIVARIATE TIME-SERIES MODELS&quot;" target="&quot;ARIMA MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multivariate Time-Series Models are extensions of ARIMA Models that can handle vector-valued data."</data>
      <data key="d6">e5e350429a2ca41c6bfd4aa812e4c391</data>
    </edge>
    <edge source="&quot;MULTIVARIATE TIME-SERIES MODELS&quot;" target="&quot;VAR MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"VAR Models are a type of Multivariate Time-Series Model that stands for Vector Autoregression."</data>
      <data key="d6">e5e350429a2ca41c6bfd4aa812e4c391</data>
    </edge>
    <edge source="&quot;ARIMA MODELS&quot;" target="&quot;ARFIMA MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ARFIMA Models are an extension of ARIMA Models, generalizing them to include fractionally integrated components."</data>
      <data key="d6">e5e350429a2ca41c6bfd4aa812e4c391</data>
    </edge>
    <edge source="&quot;ARIMA MODELS&quot;" target="&quot;EXOGENOUS TIME-SERIES MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Exogenous Time-Series Models are extensions of ARIMA Models that account for the influence of a 'forcing' time-series."</data>
      <data key="d6">e5e350429a2ca41c6bfd4aa812e4c391</data>
    </edge>
    <edge source="&quot;KANTZ AND SCHREIBER&quot;" target="&quot;NONLINEAR TIME SERIES ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Nonlinear Time Series Analysis is mentioned in the context of references by Kantz and Schreiber."</data>
      <data key="d6">e5e350429a2ca41c6bfd4aa812e4c391</data>
    </edge>
    <edge source="&quot;ABARBANEL&quot;" target="&quot;NONLINEAR TIME SERIES ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Nonlinear Time Series Analysis is mentioned in the context of references by Abarbanel."</data>
      <data key="d6">e5e350429a2ca41c6bfd4aa812e4c391</data>
    </edge>
    <edge source="&quot;NON-LINEAR TIME SERIES MODELS&quot;" target="&quot;AUTOREGRESSIVE CONDITIONAL HETEROSKEDASTICITY (ARCH)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Non-linear Time Series Models include Autoregressive Conditional Heteroskedasticity (ARCH) as a subcategory for modeling changes in variability over time."</data>
      <data key="d6">7b54e70c4e190dec8a2da85292b3e4af</data>
    </edge>
    <edge source="&quot;AUTOREGRESSIVE CONDITIONAL HETEROSKEDASTICITY (ARCH)&quot;" target="&quot;GARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GARCH is a specific model within Autoregressive Conditional Heteroskedasticity (ARCH) that predicts variability based on recent past values of the observed series."</data>
      <data key="d6">7b54e70c4e190dec8a2da85292b3e4af</data>
    </edge>
    <edge source="&quot;AUTOREGRESSIVE CONDITIONAL HETEROSKEDASTICITY (ARCH)&quot;" target="&quot;TARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"TARCH is a variant of GARCH that incorporates a threshold to model asymmetric volatility patterns."</data>
      <data key="d6">7b54e70c4e190dec8a2da85292b3e4af</data>
    </edge>
    <edge source="&quot;AUTOREGRESSIVE CONDITIONAL HETEROSKEDASTICITY (ARCH)&quot;" target="&quot;EGARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"EGARCH is a modification of GARCH that allows for the modeling of long-term memory effects in time series data."</data>
      <data key="d6">7b54e70c4e190dec8a2da85292b3e4af</data>
    </edge>
    <edge source="&quot;AUTOREGRESSIVE CONDITIONAL HETEROSKEDASTICITY (ARCH)&quot;" target="&quot;FIGARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"FIGARCH is an extension of GARCH that incorporates fractional integration to handle non-stationary data."</data>
      <data key="d6">7b54e70c4e190dec8a2da85292b3e4af</data>
    </edge>
    <edge source="&quot;AUTOREGRESSIVE CONDITIONAL HETEROSKEDASTICITY (ARCH)&quot;" target="&quot;CGARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"CGARCH is a generalization of GARCH that allows for the modeling of conditional correlations and volatilities in multivariate time series data."</data>
      <data key="d6">7b54e70c4e190dec8a2da85292b3e4af</data>
    </edge>
    <edge source="&quot;WAVELET TRANSFORM BASED METHODS&quot;" target="&quot;LOCALLY STATIONARY WAVELETS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Locally Stationary Wavelets are a subcategory of Wavelet Transform Based Methods that allow for the modeling of non-stationary data by adapting the wavelet basis to local characteristics."</data>
      <data key="d6">7b54e70c4e190dec8a2da85292b3e4af</data>
    </edge>
    <edge source="&quot;WAVELET TRANSFORM BASED METHODS&quot;" target="&quot;WAVELET DECOMPOSED NEURAL NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Wavelet Decomposed Neural Networks are a combination of Wavelet Transform Based Methods and Neural Networks that decompose time series data at multiple scales and use neural networks for modeling and prediction."</data>
      <data key="d6">7b54e70c4e190dec8a2da85292b3e4af</data>
    </edge>
    <edge source="&quot;MARKOV SWITCHING MULTIFRACTAL (MSMF)&quot;" target="&quot;VOLATILITY MODELING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Markov Switching Multifractal (MSMF) techniques are a category of models used to model volatility in financial time series data."</data>
      <data key="d6">7b54e70c4e190dec8a2da85292b3e4af</data>
    </edge>
    <edge source="&quot;WAVELET TRANSFORM&quot;" target="&quot;MULTISCALE TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Wavelet Transform is a type of Multiscale Technique used for time-frequency analysis."</data>
      <data key="d6">f5ca4e75341eb9da478381a489ae6058</data>
    </edge>
    <edge source="&quot;HIDDEN MARKOV MODEL&quot;" target="&quot;SKTIME&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hidden Markov Model is a time series model collected in the Sktime Python package."</data>
      <data key="d6">f5ca4e75341eb9da478381a489ae6058</data>
    </edge>
    <edge source="&quot;ERGODICITY&quot;" target="&quot;STATIONARITY&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Ergodicity implies Stationarity, but the converse is not necessarily the case."
"Ergodicity implies stationarity, but the converse is not necessarily the case."</data>
      <data key="d6">cd814464801a2b4d72fc06b10de5e865,f5ca4e75341eb9da478381a489ae6058</data>
    </edge>
    <edge source="&quot;TIME-SERIES ANALYSIS&quot;" target="&quot;TOOLS FOR TIME-SERIES ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time-series Analysis makes use of various tools for investigating time-series data."</data>
      <data key="d6">cd814464801a2b4d72fc06b10de5e865</data>
    </edge>
    <edge source="&quot;TIME-SERIES ANALYSIS&quot;" target="&quot;TIME-FREQUENCY ANALYSIS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time-series Analysis can be applied where the series are seasonally stationary or non-stationary, and situations where the amplitudes of frequency components change with time can be dealt with in time-frequency analysis."</data>
      <data key="d6">cd814464801a2b4d72fc06b10de5e865</data>
    </edge>
    <edge source="&quot;TIME-SERIES ANALYSIS&quot;" target="&quot;TIME-SERIES METRICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Time-series Metrics are used for time series classification or regression analysis in the field of time-series analysis."</data>
      <data key="d6">cd814464801a2b4d72fc06b10de5e865</data>
    </edge>
    <edge source="&quot;TIME-SERIES ANALYSIS&quot;" target="&quot;VISUALIZATION TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Visualization Techniques are used to present and compare time series data in the field of time-series analysis."</data>
      <data key="d6">cd814464801a2b4d72fc06b10de5e865</data>
    </edge>
    <edge source="&quot;TIME-SERIES METRICS&quot;" target="&quot;MEASURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Measures is a category that includes Time-series Metrics, which are used for time series classification or regression analysis."</data>
      <data key="d6">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </edge>
    <edge source="&quot;VISUALIZATION&quot;" target="&quot;PYTHON CODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Python Code is used to create visualizations to represent data and results."</data>
      <data key="d6">d15f6d075c072f0335b5332f11c00299</data>
    </edge>
    <edge source="&quot;OVERLAPPING CHARTS&quot;" target="&quot;BRAIDED GRAPHS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Braided Graphs is a type of Overlapping Chart used for visualizing time series data."</data>
      <data key="d6">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </edge>
    <edge source="&quot;OVERLAPPING CHARTS&quot;" target="&quot;LINE CHARTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Line Charts is a type of Overlapping Chart used for visualizing time series data."</data>
      <data key="d6">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </edge>
    <edge source="&quot;OVERLAPPING CHARTS&quot;" target="&quot;SLOPE GRAPHS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Slope Graphs is a type of Overlapping Chart used for visualizing time series data."</data>
      <data key="d6">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </edge>
    <edge source="&quot;OVERLAPPING CHARTS&quot;" target="&quot;GAPCHART&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GapChart is a type of Overlapping Chart used for visualizing time series data."</data>
      <data key="d6">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </edge>
    <edge source="&quot;SEPARATED CHARTS&quot;" target="&quot;HORIZON GRAPHS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Horizon Graphs is a type of Separated Chart used for visualizing time series data."</data>
      <data key="d6">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </edge>
    <edge source="&quot;SEPARATED CHARTS&quot;" target="&quot;REDUCED LINE CHART&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Reduced Line Chart, also known as Small Multiples, is a type of Separated Chart used for visualizing time series data."</data>
      <data key="d6">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </edge>
    <edge source="&quot;SEPARATED CHARTS&quot;" target="&quot;SILHOUETTE GRAPH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Silhouette Graph is a type of Separated Chart used for visualizing time series data."</data>
      <data key="d6">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </edge>
    <edge source="&quot;SEPARATED CHARTS&quot;" target="&quot;CIRCULAR SILHOUETTE GRAPH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Circular Silhouette Graph is a type of Separated Chart used for visualizing time series data, which is a variation of the Silhouette Graph."</data>
      <data key="d6">a9c0a1c4d1c08c2c79806dc970186e79</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;HERBERT JAEGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Herbert Jaeger is a key figure in the development of Echo State Networks."</data>
      <data key="d6">804bd76fa6f4950ef9a5cf8f0025fc1c</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;LIQUID STATE MACHINES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Liquid State Machines and Echo State Networks share similarities in their architecture and development."</data>
      <data key="d6">804bd76fa6f4950ef9a5cf8f0025fc1c</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;SIGMOID FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network uses a sigmoid function, such as the logistic sigmoid or the tanh function, to map input values in the reservoir state update equation."</data>
      <data key="d6">cc7c60d8e36838743d509a97c9ac3a4b</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;RESERVOIR WEIGHT MATRIX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network uses the reservoir weight matrix to update the reservoir state in the state update equation."</data>
      <data key="d6">cc7c60d8e36838743d509a97c9ac3a4b</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;INPUT WEIGHT MATRIX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network uses the input weight matrix to map input signals to the reservoir state in the state update equation."</data>
      <data key="d6">cc7c60d8e36838743d509a97c9ac3a4b</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;OUTPUT FEEDBACK MATRIX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network uses the output feedback matrix to provide feedback from the output signal to the reservoir state in the state update equation."</data>
      <data key="d6">cc7c60d8e36838743d509a97c9ac3a4b</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;JAEGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jaeger is mentioned in the text as a contributor to the development of Echo State Networks."</data>
      <data key="d6">2dca9849c50a439b0637cf370afde7dd</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;LUKO&#352;EVI&#268;IUS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Luko&#353;evi&#269;ius is mentioned in the text as a writer about practical techniques for optimizing Echo State Networks."</data>
      <data key="d6">2dca9849c50a439b0637cf370afde7dd</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;REAL-TIME RECURRENT LEARNING ALGORITHM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Echo State Network is mentioned in the text in the context of the Real-time Recurrent Learning Algorithm."</data>
      <data key="d6">2dca9849c50a439b0637cf370afde7dd</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;RESERVOIR COMPUTER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Network is a type of Reservoir Computer."</data>
      <data key="d6">dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;SPARSELY CONNECTED HIDDEN LAYER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Network has a Sparsely Connected Hidden Layer."</data>
      <data key="d6">dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;OUTPUT NEURONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Network has Output Neurons that can produce or reproduce specific temporal patterns."</data>
      <data key="d6">dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;SYNAPSES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Network uses Synapses to transmit signals between neurons."</data>
      <data key="d6">dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;GAUSSIAN PROCESS MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Network is demonstrated to be outperformed by a Gaussian Process Model with ESN-driven kernel function."</data>
      <data key="d6">dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </edge>
    <edge source="&quot;ECHO STATE NETWORK&quot;" target="&quot;AURESERVOIR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Echo State Network is publicly available in the form of Aureservoir, an efficient C++ library."</data>
      <data key="d6">dcd6355fc1ed8a61a1b70c50ce60fd36</data>
    </edge>
    <edge source="&quot;HERBERT JAEGER&quot;" target="&quot;JACOBS UNIVERSITY BREMEN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Herbert Jaeger is affiliated with Jacobs University Bremen."</data>
      <data key="d6">804bd76fa6f4950ef9a5cf8f0025fc1c</data>
    </edge>
    <edge source="&quot;HERBERT JAEGER&quot;" target="&quot;CORR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Herbert Jaeger is an author of a research paper published in CoRR."</data>
      <data key="d6">c8b7bd13cf99920ecce56cb563910cb3</data>
    </edge>
    <edge source="&quot;HERBERT JAEGER&quot;" target="&quot;CORR, ABS/1403.3369, 2014&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Herbert Jaeger published a paper on controlling recurrent neural networks by conceptors."</data>
      <data key="d6">546551fd625e354e9afe1245e060bed3</data>
    </edge>
    <edge source="&quot;JACOBS UNIVERSITY BREMEN&quot;" target="&quot;BREMEN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jacobs University Bremen is located in Bremen."</data>
      <data key="d6">804bd76fa6f4950ef9a5cf8f0025fc1c</data>
    </edge>
    <edge source="&quot;BREMEN&quot;" target="&quot;GERMANY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Bremen is a city located in Germany."</data>
      <data key="d6">804bd76fa6f4950ef9a5cf8f0025fc1c</data>
    </edge>
    <edge source="&quot;LIQUID STATE MACHINES&quot;" target="&quot;WOLFGANG MAASS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Wolfgang Maass is a key figure in the development of Liquid State Machines."
"Wolfgang Maass independently developed Liquid State Machines."</data>
      <data key="d6">804bd76fa6f4950ef9a5cf8f0025fc1c,b32958d42199d47252887dc7be40ab5a</data>
    </edge>
    <edge source="&quot;WOLFGANG MAASS&quot;" target="&quot;GREGOR M. HOERZER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gregor M. Hoerzer and Wolfgang Maass co-authored a scientific paper on a specific topic, but the relationship description is not explicitly stated in the text."</data>
      <data key="d6">ce7b58ffc7f43f36bc78154597d01903</data>
    </edge>
    <edge source="&quot;WOLFGANG MAASS&quot;" target="&quot;NEURAL COMPUTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Wolfgang Maass is an author of a research paper mentioned in the text, but no specific publication is mentioned."</data>
      <data key="d6">c8b7bd13cf99920ecce56cb563910cb3</data>
    </edge>
    <edge source="&quot;PETER F. DOMINEY&quot;" target="&quot;SEQUENCE PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Peter F. Dominey analyzed a process related to the modeling of sequence processing in the mammalian brain."</data>
      <data key="d6">b32958d42199d47252887dc7be40ab5a</data>
    </edge>
    <edge source="&quot;L. SCHOMAKER&quot;" target="&quot;RESERVOIR COMPUTING IDEA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"L. Schomaker contributed to the development of the reservoir computing idea."</data>
      <data key="d6">a621b44739e0cb4379645a4a58f16697</data>
    </edge>
    <edge source="&quot;RNN&quot;" target="&quot;TRAINING INPUT-OUTPUT SEQUENCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The RNN is trained using the Training Input-Output Sequence to behave as a tunable frequency generator."</data>
      <data key="d6">a621b44739e0cb4379645a4a58f16697</data>
    </edge>
    <edge source="&quot;RNN&quot;" target="&quot;LINEAR CHAIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RNNs have a structure that corresponds to a linear chain."</data>
      <data key="d6">7b6ff30ef255db2d2c68326d78cf0115</data>
    </edge>
    <edge source="&quot;RNN&quot;" target="&quot;SECOND-ORDER RNNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recurrent Neural Networks (RNN) are used in the development of Second-order RNNs."</data>
      <data key="d6">4470a7f7ad60a2866b31907a2a3ca96e</data>
    </edge>
    <edge source="&quot;RNN&quot;" target="&quot;LONG SHORT-TERM MEMORY (LSTM)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recurrent Neural Networks (RNN) are used in the development of Long short-term memory (LSTM)."</data>
      <data key="d6">4470a7f7ad60a2866b31907a2a3ca96e</data>
    </edge>
    <edge source="&quot;RNN&quot;" target="&quot;BPTT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"BPTT is a method for training RNNs that uses backpropagation through time."</data>
      <data key="d6">1aec5b03f663d1614b2ecbf97981a5c2</data>
    </edge>
    <edge source="&quot;RNN&quot;" target="&quot;RTRL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"RTRL is a method for training RNNs that uses real-time recursive learning."</data>
      <data key="d6">1aec5b03f663d1614b2ecbf97981a5c2</data>
    </edge>
    <edge source="&quot;RNN&quot;" target="&quot;LSTM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LSTM is a variant of RNN that addresses the vanishing gradients problem."</data>
      <data key="d6">1aec5b03f663d1614b2ecbf97981a5c2</data>
    </edge>
    <edge source="&quot;RNN&quot;" target="&quot;INDRNN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IndRNN is a variant of RNN that reduces the context of a neuron to its own past state."</data>
      <data key="d6">1aec5b03f663d1614b2ecbf97981a5c2</data>
    </edge>
    <edge source="&quot;SIGMOID FUNCTION&quot;" target="&quot;LOGISTIC SIGMOID&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The logistic sigmoid is a type of sigmoid function that maps input values to a range between 0 and 1."</data>
      <data key="d6">cc7c60d8e36838743d509a97c9ac3a4b</data>
    </edge>
    <edge source="&quot;SIGMOID FUNCTION&quot;" target="&quot;TANH FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The tanh function is a type of sigmoid function that maps input values to a range between -1 and 1."</data>
      <data key="d6">cc7c60d8e36838743d509a97c9ac3a4b</data>
    </edge>
    <edge source="&quot;DESIRED OUTPUTS&quot;" target="&quot;DESIRED OUTPUT WEIGHTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Desired Output Weights are the linear regression weights of the desired outputs on the harvested extended states."</data>
      <data key="d6">6a4432cd530b28770e2b903fe242a0d1</data>
    </edge>
    <edge source="&quot;DESIRED OUTPUT WEIGHTS&quot;" target="&quot;PSEUDOINVERSE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Desired Output Weights can be computed using the pseudoinverse, which is a mathematical operation."</data>
      <data key="d6">6a4432cd530b28770e2b903fe242a0d1</data>
    </edge>
    <edge source="&quot;PSEUDOINVERSE&quot;" target="&quot;OUTPUT WEIGHTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Pseudoinverse is used in the computation of Output Weights."</data>
      <data key="d6">a9f53979e9dbe6b936ff3374c73006dd</data>
    </edge>
    <edge source="&quot;LINEAR SIGNAL PROCESSING&quot;" target="&quot;OUTPUT WEIGHTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Linear Signal Processing is mentioned as a method for computing Output Weights."</data>
      <data key="d6">a9f53979e9dbe6b936ff3374c73006dd</data>
    </edge>
    <edge source="&quot;JAEGER 2003&quot;" target="&quot;OUTPUT WEIGHTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jaeger 2003 is a reference mentioned as a source for online adaptive methods used to compute Output Weights."</data>
      <data key="d6">a9f53979e9dbe6b936ff3374c73006dd</data>
    </edge>
    <edge source="&quot;ESP&quot;" target="&quot;JAEGER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jaeger contributes to the understanding of Echo State Properties, including providing abstract characterizations and conditions for additive-sigmoid neuron reservoirs.")</data>
      <data key="d6">3b592e5ac113a5c031925f91a182baa6</data>
    </edge>
    <edge source="&quot;ESP&quot;" target="&quot;BUEHNER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Buehner contributes to the algebraic conditions for additive-sigmoid neuron reservoirs, which are relevant to the ESP.")</data>
      <data key="d6">3b592e5ac113a5c031925f91a182baa6</data>
    </edge>
    <edge source="&quot;ESP&quot;" target="&quot;YILDIZ&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Yildiz contributes to the algebraic conditions for a specific subclass of reservoirs, which are relevant to the ESP.")</data>
      <data key="d6">3b592e5ac113a5c031925f91a182baa6</data>
    </edge>
    <edge source="&quot;ESP&quot;" target="&quot;MANJUNATH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Manjunath explores the relationship between input signal characteristics and the ESP, providing a fundamental 0-1-law.")</data>
      <data key="d6">3b592e5ac113a5c031925f91a182baa6</data>
    </edge>
    <edge source="&quot;JAEGER&quot;" target="&quot;LEAKY INTEGRATOR NEURONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jaeger et al. spell out the algebraic conditions for Leaky Integrator Neurons, which are a type of neuron mentioned in the text.")</data>
      <data key="d6">3b592e5ac113a5c031925f91a182baa6</data>
    </edge>
    <edge source="&quot;JAEGER&quot;" target="&quot;STATE NOISE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jaeger is a researcher who has contributed to the development of the concept of state noise in the context of ESN Models."</data>
      <data key="d6">5972cf7d440b1c3fdc0f05fca305f18d</data>
    </edge>
    <edge source="&quot;JAEGER&quot;" target="&quot;RC NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jaeger is a person who firstly formulated the concept of RC networks."</data>
      <data key="d6">418f92b0dd08e03a20637ffec8193bfc</data>
    </edge>
    <edge source="&quot;JAEGER&quot;" target="&quot;PYTORCH-ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"PyTorch-ESN is developed by Stefano Lugli, who proposed a mechanism for controlling the dynamics of reservoirs."</data>
      <data key="d6">a1adb5de4156f0a4a448caf79056e886</data>
    </edge>
    <edge source="&quot;MEMORY CAPACITY&quot;" target="&quot;ES2N&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"ES2N models are used to measure Memory Capacity."
"The Memory Capacity of the ES2N model is being measured and compared to other models."</data>
      <data key="d6">24b347e60cb01aea26f46f3067f5a0f0,c53825a1ab5e01a794a428988435a7a7</data>
    </edge>
    <edge source="&quot;MEMORY CAPACITY&quot;" target="&quot;LINEAR ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Linear ESN models are used to measure Memory Capacity."</data>
      <data key="d6">c53825a1ab5e01a794a428988435a7a7</data>
    </edge>
    <edge source="&quot;MEMORY CAPACITY&quot;" target="&quot;ORTHOGONAL ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Orthogonal ESN models are used to measure Memory Capacity."</data>
      <data key="d6">c53825a1ab5e01a794a428988435a7a7</data>
    </edge>
    <edge source="&quot;MEMORY CAPACITY&quot;" target="&quot;ES^2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ES^2N is analyzed for its Memory Capacity, which is compared to ESN."</data>
      <data key="d6">f16792dcee6dab8ea8f8c8c6793bbc3d</data>
    </edge>
    <edge source="&quot;SCHMIDHUBER&quot;" target="&quot;LONG SHORT TERM MEMORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Schmidhuber is a researcher who made significant contributions to the development of Long Short Term Memory (LSTM) cells for training Recurrent Neural Networks."</data>
      <data key="d6">6de297d888d10db4c987b5eafc6398b2</data>
    </edge>
    <edge source="&quot;SCHMIDHUBER&quot;" target="&quot;BACKPROPAGATION THROUGH TIME&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Schmidhuber developed methods for training recurrent neural networks, including Backpropagation Through Time."</data>
      <data key="d6">0bb54b1de8d2297293defe94addb8076</data>
    </edge>
    <edge source="&quot;SCHMIDHUBER&quot;" target="&quot;REAL-TIME RECURRENT LEARNING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Schmidhuber contributed to the development of Real-Time Recurrent Learning."</data>
      <data key="d6">6bbaf3df0fa2fac979f6d6a64abb2e91</data>
    </edge>
    <edge source="&quot;TEST ERROR&quot;" target="&quot;VALIDATION SET&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Test Error is a performance metric that is monitored using a Validation Set during model training."</data>
      <data key="d6">e805d3f438bd9c485639f1c69f917ae5</data>
    </edge>
    <edge source="&quot;VALIDATION SET&quot;" target="&quot;ESN MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The performance of the ESN Models is evaluated using the Validation Set, which is a portion of the Original Training Data withheld for this purpose."</data>
      <data key="d6">5972cf7d440b1c3fdc0f05fca305f18d</data>
    </edge>
    <edge source="&quot;ESN MODELS&quot;" target="&quot;STATE NOISE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"State Noise is a method used to stabilize solutions in ESN Models with output feedback, potentially improving performance and reducing error on the Validation Set."</data>
      <data key="d6">5972cf7d440b1c3fdc0f05fca305f18d</data>
    </edge>
    <edge source="&quot;REAL-TIME RECURRENT LEARNING&quot;" target="&quot;HOCHREITER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hochreiter contributed to the development of Real-Time Recurrent Learning."</data>
      <data key="d6">6bbaf3df0fa2fac979f6d6a64abb2e91</data>
    </edge>
    <edge source="&quot;REAL-TIME RECURRENT LEARNING&quot;" target="&quot;PEARLMUTTER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Pearlmutter contributed to the development of Real-Time Recurrent Learning."</data>
      <data key="d6">6bbaf3df0fa2fac979f6d6a64abb2e91</data>
    </edge>
    <edge source="&quot;BACKPROPAGATION THROUGH TIME&quot;" target="&quot;WERBOS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Werbos developed methods for training recurrent neural networks, including Backpropagation Through Time."
"Werbos contributed to the development of Backpropagation through Time."</data>
      <data key="d6">0bb54b1de8d2297293defe94addb8076,6bbaf3df0fa2fac979f6d6a64abb2e91</data>
    </edge>
    <edge source="&quot;BACKPROPAGATION THROUGH TIME&quot;" target="&quot;WILLIAMS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Williams developed methods for training recurrent neural networks, including Backpropagation Through Time."
"Williams contributed to the development of Backpropagation through Time."</data>
      <data key="d6">0bb54b1de8d2297293defe94addb8076,6bbaf3df0fa2fac979f6d6a64abb2e91</data>
    </edge>
    <edge source="&quot;BACKPROPAGATION THROUGH TIME&quot;" target="&quot;ROBINSON&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Robinson developed methods for training recurrent neural networks, including Backpropagation Through Time."
"Robinson contributed to the development of Backpropagation through Time."</data>
      <data key="d6">0bb54b1de8d2297293defe94addb8076,6bbaf3df0fa2fac979f6d6a64abb2e91</data>
    </edge>
    <edge source="&quot;BACKPROPAGATION THROUGH TIME&quot;" target="&quot;HOCHREITER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hochreiter developed methods for training recurrent neural networks, including Backpropagation Through Time."</data>
      <data key="d6">0bb54b1de8d2297293defe94addb8076</data>
    </edge>
    <edge source="&quot;BACKPROPAGATION THROUGH TIME&quot;" target="&quot;PEARLMUTTER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Pearlmutter developed methods for training recurrent neural networks, including Backpropagation Through Time."</data>
      <data key="d6">0bb54b1de8d2297293defe94addb8076</data>
    </edge>
    <edge source="&quot;OPTICAL MICROCHIPS&quot;" target="&quot;NONLINEAR RESERVOIR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Optical Microchips are used as a nonlinear reservoir."</data>
      <data key="d6">7767d42e08c8eab856e8e3025c692309</data>
    </edge>
    <edge source="&quot;ARTIFICIAL SOFT LIMBS&quot;" target="&quot;NONLINEAR RESERVOIR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Artificial Soft Limbs are used as a nonlinear reservoir."</data>
      <data key="d6">7767d42e08c8eab856e8e3025c692309</data>
    </edge>
    <edge source="&quot;B&#220;RGER ET AL.&quot;" target="&quot;ESN METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"B&#252;rger et al. is mentioned in the context of using ESN methods."</data>
      <data key="d6">dbca0570761b1698d32f0c0bfb593b1a</data>
    </edge>
    <edge source="&quot;DALE ET AL.&quot;" target="&quot;ESN METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dale et al. is mentioned in the context of using ESN methods."</data>
      <data key="d6">dbca0570761b1698d32f0c0bfb593b1a</data>
    </edge>
    <edge source="&quot;NAKAJIMA, HAUSER AND PFEIFER&quot;" target="&quot;ESN METHODS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Nakajima, Hauser and Pfeifer is mentioned in the context of using ESN methods."</data>
      <data key="d6">dbca0570761b1698d32f0c0bfb593b1a</data>
    </edge>
    <edge source="&quot;ESN METHODS&quot;" target="&quot;LIQUID STATE MACHINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN methods are mentioned in the same context as Liquid State Machine."</data>
      <data key="d6">dbca0570761b1698d32f0c0bfb593b1a</data>
    </edge>
    <edge source="&quot;CENI&quot;" target="&quot;EDGE OF STABILITY ECHO STATE NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Ceni is an author who contributed to the development of the Edge of Stability Echo State Network."</data>
      <data key="d6">578045eb341c5e05d5a912f634854499</data>
    </edge>
    <edge source="&quot;GALLICCHIO&quot;" target="&quot;EDGE OF STABILITY ECHO STATE NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gallicchio is an author who contributed to the development of the Edge of Stability Echo State Network."</data>
      <data key="d6">578045eb341c5e05d5a912f634854499</data>
    </edge>
    <edge source="&quot;ES2N&quot;" target="&quot;JACOBIAN&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The ES2N system is used to calculate the Jacobian, which represents the derivative of a vector-valued function."
"The ES2N system is used to calculate the Jacobian, which is a matrix of first-order partial derivatives of a vector-valued function."</data>
      <data key="d6">9dd8e12c7acbe10cf34817ff14780d24,b49cfd8b041b59aa70d9ef614256eab2</data>
    </edge>
    <edge source="&quot;ES2N&quot;" target="&quot;PROXIMITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Proximity is a parameter used in the configuration of the ES2N system."</data>
      <data key="d6">9dd8e12c7acbe10cf34817ff14780d24</data>
    </edge>
    <edge source="&quot;ES2N&quot;" target="&quot;LEAKY ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Leaky ESN is a variant of the ES2N system, suggesting a relationship between the two concepts."</data>
      <data key="d6">b49cfd8b041b59aa70d9ef614256eab2</data>
    </edge>
    <edge source="&quot;ES2N&quot;" target="&quot;PROXIMITY PARAMETER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES2N model's performance is being investigated with respect to different Proximity Parameters."</data>
      <data key="d6">24b347e60cb01aea26f46f3067f5a0f0</data>
    </edge>
    <edge source="&quot;ES2N&quot;" target="&quot;PHAS[I]&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"PHAS[i] is mentioned in the context of the ES2N model."</data>
      <data key="d6">97f5d2e9d34b3b50f8e922fc4bb7f824</data>
    </edge>
    <edge source="&quot;ES2N&quot;" target="&quot;ALPHA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ES2N is mentioned in the context of the alpha concept."</data>
      <data key="d6">97f5d2e9d34b3b50f8e922fc4bb7f824</data>
    </edge>
    <edge source="&quot;ES2N&quot;" target="&quot;NOISY_TANH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"noisy_tanh is used as an activation function in the ES2N machine learning model."</data>
      <data key="d6">1298c65a923053e1de35aacddc13832c</data>
    </edge>
    <edge source="&quot;ES2N&quot;" target="&quot;Y_PRED_ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"y_pred_es2n is the predicted output data from the ES2N machine learning model."</data>
      <data key="d6">1298c65a923053e1de35aacddc13832c</data>
    </edge>
    <edge source="&quot;NONLINEAR RESERVOIR&quot;" target="&quot;MECHANICAL NANOOSCILLATORS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Mechanical Nanooscillators are used as a nonlinear reservoir."</data>
      <data key="d6">7767d42e08c8eab856e8e3025c692309</data>
    </edge>
    <edge source="&quot;NONLINEAR RESERVOIR&quot;" target="&quot;POLYMER MIXTURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Polymer Mixtures are used as a nonlinear reservoir."</data>
      <data key="d6">7767d42e08c8eab856e8e3025c692309</data>
    </edge>
    <edge source="&quot;ES&#178;N&quot;" target="&quot;LEAKY ECHO STATE NETWORK (ESN)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ES&#178;N is an extension of the Leaky Echo State Network (ESN) model, as it is based on the equation of the ESN model."</data>
      <data key="d6">ca59dc92d05a69002373e96e0a373215</data>
    </edge>
    <edge source="&quot;ES&#178;N&quot;" target="&quot;ANDREA CENI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Andrea Ceni is a co-author of the paper proposing the ES&#178;N model."</data>
      <data key="d6">ca59dc92d05a69002373e96e0a373215</data>
    </edge>
    <edge source="&quot;ES&#178;N&quot;" target="&quot;CLAUDIO GALLICCHIO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Claudio Gallicchio is a co-author of the paper proposing the ES&#178;N model."</data>
      <data key="d6">ca59dc92d05a69002373e96e0a373215</data>
    </edge>
    <edge source="&quot;NODE CREATION&quot;" target="&quot;INITIALIZE FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Node Creation involves the use of the Initialize Function to set the input and output dimensions of a node and initialize its parameters."</data>
      <data key="d6">81b4162953007160e40ec76b84d045eb</data>
    </edge>
    <edge source="&quot;NODE CREATION&quot;" target="&quot;FORWARD FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Forward Function is used in Node Creation to calculate the output of a node based on its input, activation function, and proximity."</data>
      <data key="d6">81b4162953007160e40ec76b84d045eb</data>
    </edge>
    <edge source="&quot;QR FACTORIZATION&quot;" target="&quot;RANDOM ORTHOGONAL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The QR Factorization method uses the Random Orthogonal Function to generate an orthogonal matrix."</data>
      <data key="d6">81b4162953007160e40ec76b84d045eb</data>
    </edge>
    <edge source="&quot;INITIALIZE FUNCTION&quot;" target="&quot;RANDOM ORTHOGONAL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Initialize Function uses the Random Orthogonal Function to generate a random orthogonal matrix for the Win parameter."</data>
      <data key="d6">81b4162953007160e40ec76b84d045eb</data>
    </edge>
    <edge source="&quot;NODE&quot;" target="&quot;RECURRENT OPERATOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"A Recurrent Operator is a type of Node that maps its internal state and input vector to the next state."</data>
      <data key="d6">dc46bcef51e88747b544f7efb111203a</data>
    </edge>
    <edge source="&quot;ES2&quot;" target="&quot;TANH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES2 model uses the Tanh function as an activation function."</data>
      <data key="d6">4faea507802b94c6fc0f3e4bf8bafb95</data>
    </edge>
    <edge source="&quot;ES2&quot;" target="&quot;KERNEL FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES2 model uses a Kernel Function."</data>
      <data key="d6">4faea507802b94c6fc0f3e4bf8bafb95</data>
    </edge>
    <edge source="&quot;ES2&quot;" target="&quot;JACOBIAN FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES2 model uses a Jacobian Function."</data>
      <data key="d6">4faea507802b94c6fc0f3e4bf8bafb95</data>
    </edge>
    <edge source="&quot;ES2&quot;" target="&quot;EIGENVALUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES2 model's eigenvalues are visualized."</data>
      <data key="d6">4faea507802b94c6fc0f3e4bf8bafb95</data>
    </edge>
    <edge source="&quot;ES2N MODEL&quot;" target="&quot;INPUT SIGNAL U&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES2N Model is used to predict the output signal based on the input signal u."</data>
      <data key="d6">ba1721161cafb25a7f10d8113f419612</data>
    </edge>
    <edge source="&quot;ES2N MODEL&quot;" target="&quot;DELAY K&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES2N Model is evaluated for its memory capacity with respect to different delays k."</data>
      <data key="d6">ba1721161cafb25a7f10d8113f419612</data>
    </edge>
    <edge source="&quot;ES2N MODEL&quot;" target="&quot;MEMORY CAPACITY SCORE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Memory Capacity Score is calculated to evaluate the overall memory capacity of the ES2N Model."</data>
      <data key="d6">ba1721161cafb25a7f10d8113f419612</data>
    </edge>
    <edge source="&quot;PEARSON CORRELATION COEFFICIENT&quot;" target="&quot;MEMORY CAPACITY SCORE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Pearson Correlation Coefficient is used in the calculation of the Memory Capacity Score."</data>
      <data key="d6">ba1721161cafb25a7f10d8113f419612</data>
    </edge>
    <edge source="&quot;KTH_MEMORY_CAPACITY&quot;" target="&quot;MEMORY_CAPACITY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"kth_memory_capacity is used within the memory_capacity function to calculate memory capacities."</data>
      <data key="d6">a614fa3f8de82d4078f220e5f373f03a</data>
    </edge>
    <edge source="&quot;ORTHOGONAL ESN&quot;" target="&quot;RANDOM ORTHOGONAL MATRIX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Orthogonal ESN models use a Random Orthogonal Matrix to initialize their reservoir matrix."</data>
      <data key="d6">c53825a1ab5e01a794a428988435a7a7</data>
    </edge>
    <edge source="&quot;LINEARESN&quot;" target="&quot;MC_ORTHOESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"linearESN and mc_orthoESN are variants of the same memory capacity model, with differences in their reservoir matrix and input connectivity."</data>
      <data key="d6">385037cd24deb1be217b7e1db823ce9f</data>
    </edge>
    <edge source="&quot;LINEARESN&quot;" target="&quot;MC_LINEARSCR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"linearESN and mc_linearSCR are variants of the same memory capacity model, with differences in their reservoir matrix and input connectivity."</data>
      <data key="d6">385037cd24deb1be217b7e1db823ce9f</data>
    </edge>
    <edge source="&quot;LINEARESN&quot;" target="&quot;PLT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"plt is mentioned in the context of plotting data for linearESN."</data>
      <data key="d6">385037cd24deb1be217b7e1db823ce9f</data>
    </edge>
    <edge source="&quot;MC_ORTHOESN&quot;" target="&quot;MC_LINEARSCR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"mc_orthoESN and mc_linearSCR are variants of the same memory capacity model, with differences in their reservoir matrix and input connectivity."</data>
      <data key="d6">385037cd24deb1be217b7e1db823ce9f</data>
    </edge>
    <edge source="&quot;MC_ORTHOESN&quot;" target="&quot;PLT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"plt is mentioned in the context of plotting data for mc_orthoESN."</data>
      <data key="d6">385037cd24deb1be217b7e1db823ce9f</data>
    </edge>
    <edge source="&quot;MC_LINEARSCR&quot;" target="&quot;PLT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"plt is mentioned in the context of plotting data for mc_linearSCR."</data>
      <data key="d6">385037cd24deb1be217b7e1db823ce9f</data>
    </edge>
    <edge source="&quot;MC_ESN&quot;" target="&quot;PLT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"plt is mentioned in the context of plotting data for mc_ESN."</data>
      <data key="d6">385037cd24deb1be217b7e1db823ce9f</data>
    </edge>
    <edge source="&quot;MC_ES2N&quot;" target="&quot;PLT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"plt is mentioned in the context of plotting data for mc_ES2N."</data>
      <data key="d6">385037cd24deb1be217b7e1db823ce9f</data>
    </edge>
    <edge source="&quot;PLT&quot;" target="&quot;Y_PRED_ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"plt is used to visualize y_pred_es2n, which represents the predictions made by the ES^2N model."</data>
      <data key="d6">12d680622df43439e6de83058b734953</data>
    </edge>
    <edge source="&quot;PLT&quot;" target="&quot;Y_PRED_ESN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"plt is used to visualize y_pred_esn, which represents the predictions made by the ESN model."</data>
      <data key="d6">12d680622df43439e6de83058b734953</data>
    </edge>
    <edge source="&quot;PLT&quot;" target="&quot;TEST_STEPS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"plt is used to visualize the data associated with the TEST_STEPS event."</data>
      <data key="d6">b496f2a8e7e239e6d3313a892de8e648</data>
    </edge>
    <edge source="&quot;MATPLOTLIB&quot;" target="&quot;ES^2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Matplotlib is used to create visualizations of the time series prediction results obtained using the ES^2N, as demonstrated in the provided code."</data>
      <data key="d6">d2cc4243ed9b1bb887527f7cc1153033</data>
    </edge>
    <edge source="&quot;MATPLOTLIB&quot;" target="&quot;SINE WAVE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Matplotlib is used to visualize the Sine Wave in the example."</data>
      <data key="d6">71f966d00b6d0eceb580d00b9cb86b1e</data>
    </edge>
    <edge source="&quot;MATPLOTLIB&quot;" target="&quot;PLOTTING FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Plotting Function utilizes Matplotlib to create visualizations of data."</data>
      <data key="d6">c5c29ba06a5cc70a086c2c2c8858e5aa</data>
    </edge>
    <edge source="&quot;MATPLOTLIB&quot;" target="&quot;CHAPTER 2&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Matplotlib is mentioned in Chapter 2 as a library used for data visualization in the context of the generative mode."</data>
      <data key="d6">e396354e3a9be76616392af11f56e671</data>
    </edge>
    <edge source="&quot;ES^2N&quot;" target="&quot;RESEARCHER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Researcher uses ES^2N in their analysis for comparison."</data>
      <data key="d6">f16792dcee6dab8ea8f8c8c6793bbc3d</data>
    </edge>
    <edge source="&quot;ES^2N&quot;" target="&quot;HYPER-PARAMETERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hyper-parameters are used in the analysis of ES^2N."</data>
      <data key="d6">f16792dcee6dab8ea8f8c8c6793bbc3d</data>
    </edge>
    <edge source="&quot;ES^2N&quot;" target="&quot;NOISY_TANH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The ES^2N model uses the noisy hyperbolic tangent activation function (noisy_tanh) as a hyperparameter."</data>
      <data key="d6">6a7bea5f60347ea864c06adc327829dc</data>
    </edge>
    <edge source="&quot;ES^2N&quot;" target="&quot;MULTIPLE SUPERIMPOSED OSCILLATOR TASK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ES^2N is used to analyze and predict the Multiple Superimposed Oscillator Task."</data>
      <data key="d6">4fb25ea25c60216b307931b5edacc5cb</data>
    </edge>
    <edge source="&quot;ES^2N&quot;" target="&quot;Y_PRED_ES2N&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ES^2N is the model used to generate the predictions stored in y_pred_es2n."</data>
      <data key="d6">12d680622df43439e6de83058b734953</data>
    </edge>
    <edge source="&quot;RESEARCHER&quot;" target="&quot;PREDICTION OF Z COORDINATE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The researcher is attempting to predict the z coordinate in the context of the Lorenz System."</data>
      <data key="d6">715720b663e85c5e16cbf8b1ef4ec208</data>
    </edge>
    <edge source="&quot;RNG&quot;" target="&quot;U&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"rng is used to generate the input data array u."</data>
      <data key="d6">f3e58b69b1a93175e3094a2ba65c0429</data>
    </edge>
    <edge source="&quot;RNG&quot;" target="&quot;TAUS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"rng is used to generate the array of time lags TAUS."</data>
      <data key="d6">f3e58b69b1a93175e3094a2ba65c0429</data>
    </edge>
    <edge source="&quot;RNG&quot;" target="&quot;NUS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"rng is used to generate the array of frequencies NUS."</data>
      <data key="d6">f3e58b69b1a93175e3094a2ba65c0429</data>
    </edge>
    <edge source="&quot;U&quot;" target="&quot;RES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The variable u is updated using the model function and then stored in the variable res."</data>
      <data key="d6">7b8e1f350eefb392053be12f35fe7daf</data>
    </edge>
    <edge source="&quot;MULTIPLE SUPERIMPOSED OSCILLATORS&quot;" target="&quot;AUTO-REGRESSIVE NODE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Multiple Superimposed Oscillators are described as a system that interacts with each other in an auto-regressive node."</data>
      <data key="d6">c761a4e63ae52c3953bea0dde6eb3319</data>
    </edge>
    <edge source="&quot;MULTIPLE SUPERIMPOSED OSCILLATORS&quot;" target="&quot;MSO8&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MSO8 is a specific implementation of the Multiple Superimposed Oscillators technique."</data>
      <data key="d6">7cb18067f7d75fd3cd20998c669a1741</data>
    </edge>
    <edge source="&quot;AUTO-REGRESSIVE NODE&quot;" target="&quot;GRAPH THEORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Auto-Regressive Node is a concept used in the context of Graph Theory."</data>
      <data key="d6">7cb18067f7d75fd3cd20998c669a1741</data>
    </edge>
    <edge source="&quot;NOISY TANH&quot;" target="&quot;HYPERBOLIC TANGENT FUNCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Noisy Tanh is a noisy version of the Hyperbolic Tangent Function."</data>
      <data key="d6">7cb18067f7d75fd3cd20998c669a1741</data>
    </edge>
    <edge source="&quot;DOMINEY&quot;" target="&quot;RC NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dominey is a person who contributed to the concept of RC networks in the Neuroscience field."</data>
      <data key="d6">418f92b0dd08e03a20637ffec8193bfc</data>
    </edge>
    <edge source="&quot;HOCHREITER&quot;" target="&quot;LONG SHORT TERM MEMORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hochreiter is a researcher who made significant contributions to the development of Long Short Term Memory (LSTM) cells for training Recurrent Neural Networks."</data>
      <data key="d6">6de297d888d10db4c987b5eafc6398b2</data>
    </edge>
    <edge source="&quot;RC NETWORKS&quot;" target="&quot;MAASS ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Maass et al. is an organization that contributed to the formulation of RC networks."</data>
      <data key="d6">418f92b0dd08e03a20637ffec8193bfc</data>
    </edge>
    <edge source="&quot;RC NETWORKS&quot;" target="&quot;BUONOMANO AND MERZENICH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Buonomano and Merzenich is an organization that contributed to the concept of RC networks in the Neuroscience field."</data>
      <data key="d6">418f92b0dd08e03a20637ffec8193bfc</data>
    </edge>
    <edge source="&quot;DEEP LEARNING FRAMEWORKS&quot;" target="&quot;RNN TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep Learning frameworks, such as TensorFlow and PyTorch, can be used to easily replicate RNN techniques, such as LSTMs."</data>
      <data key="d6">295606b4bc5d12929a913a3c79f93734</data>
    </edge>
    <edge source="&quot;DEEP LEARNING FRAMEWORKS&quot;" target="&quot;RC MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Deep Learning frameworks are not commonly used for RC models, as they are often implemented from scratch due to a lack of common libraries."</data>
      <data key="d6">295606b4bc5d12929a913a3c79f93734</data>
    </edge>
    <edge source="&quot;TROUVAIN AND HINAUT&quot;" target="&quot;RNN TECHNIQUES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Trouvain and Hinaut compare RNN techniques, such as LSTMs, with RC models in their paper."</data>
      <data key="d6">295606b4bc5d12929a913a3c79f93734</data>
    </edge>
    <edge source="&quot;TROUVAIN AND HINAUT&quot;" target="&quot;RC MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Trouvain and Hinaut compare RC models with RNN techniques in their paper."</data>
      <data key="d6">295606b4bc5d12929a913a3c79f93734</data>
    </edge>
    <edge source="&quot;GAUTHIER ET AL.&quot;" target="&quot;NONLINEAR VECTOR AUTO-REGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gauthier et al. introduce the Nonlinear Vector Auto-regressive method in their paper 'Next Generation Reservoir Computing'."</data>
      <data key="d6">244217eb4738aae272df8949bdaaf131</data>
    </edge>
    <edge source="&quot;GAUTHIER ET AL.&quot;" target="&quot;RESERVOIR COMPUTING MACHINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gauthier et al. is the source of the figure that mentions the Reservoir Computing Machine."</data>
      <data key="d6">41ea479401d7ff7c83c9d38c91d76cd9</data>
    </edge>
    <edge source="&quot;GAUTHIER ET AL.&quot;" target="&quot;NEXT GENERATION RESERVOIR COMPUTING MACHINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gauthier et al. is the source of the figure that mentions the Next Generation Reservoir Computing Machine."</data>
      <data key="d6">41ea479401d7ff7c83c9d38c91d76cd9</data>
    </edge>
    <edge source="&quot;PYRCN&quot;" target="&quot;STEINER ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Steiner et al. are the authors of PyRCN, which is compared to Reservoirpy."</data>
      <data key="d6">15e969cdc81fd313d389558850d0c8ec</data>
    </edge>
    <edge source="&quot;PYRCN&quot;" target="&quot;ELM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"PyRCN is a software that allows the training of Extreme Learning Machine (ELM)."</data>
      <data key="d6">a1adb5de4156f0a4a448caf79056e886</data>
    </edge>
    <edge source="&quot;ECHOTORCH&quot;" target="&quot;SCHAETTI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Schaetti is the author of EchoTorch, which is compared to Reservoirpy."</data>
      <data key="d6">15e969cdc81fd313d389558850d0c8ec</data>
    </edge>
    <edge source="&quot;RESERVOIRCOMPUTING.JL&quot;" target="&quot;MARTINUZZI ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Martinuzzi et al. are the authors of ReservoirComputing.jl, which is compared to Reservoirpy."</data>
      <data key="d6">15e969cdc81fd313d389558850d0c8ec</data>
    </edge>
    <edge source="&quot;RESERVOIRCOMPUTING.JL&quot;" target="&quot;UNIVERSITY OF READING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ReservoirComputing.jl is developed by the Reservoir Computing Group at the University of Reading."</data>
      <data key="d6">a1adb5de4156f0a4a448caf79056e886</data>
    </edge>
    <edge source="&quot;DEEPESN&quot;" target="&quot;GALLICCHIO ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gallicchio et al. are the authors of DeepESN, which is compared to Reservoirpy."</data>
      <data key="d6">15e969cdc81fd313d389558850d0c8ec</data>
    </edge>
    <edge source="&quot;LSM&quot;" target="&quot;SPIKING NEURAL NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LSM specializes in handling spiking neural networks."</data>
      <data key="d6">a1adb5de4156f0a4a448caf79056e886</data>
    </edge>
    <edge source="&quot;OGER&quot;" target="&quot;VERSTRAETEN ET AL.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Verstraeten et al. are the authors of Oger, which is compared to Reservoirpy."</data>
      <data key="d6">15e969cdc81fd313d389558850d0c8ec</data>
    </edge>
    <edge source="&quot;OGER&quot;" target="&quot;MODULAR LEARNING ARCHITECTURES FOR LARGE-SCALE SEQUENTIAL PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Oger is the publication source for the research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d6">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </edge>
    <edge source="&quot;NET&quot;" target="&quot;OLEG KOZELSKY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NET is developed by Oleg Kozelsky."</data>
      <data key="d6">a1adb5de4156f0a4a448caf79056e886</data>
    </edge>
    <edge source="&quot;LARS BUITINCK&quot;" target="&quot;API DESIGN FOR MACHINE LEARNING SOFTWARE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Lars Buitinck is an author of a paper on API design for machine learning software, highlighting experiences from the scikit-learn project."</data>
      <data key="d6">82de30f43839f4985de20a981b524af1</data>
    </edge>
    <edge source="&quot;DV BUONOMANO&quot;" target="&quot;TEMPORAL INFORMATION TRANSFORMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"DV Buonomano is an author of a scientific paper on temporal information transformation into a spatial code by a neural network."</data>
      <data key="d6">82de30f43839f4985de20a981b524af1</data>
    </edge>
    <edge source="&quot;DV BUONOMANO&quot;" target="&quot;M. M. MERZENICH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"DV Buonomano and M. M. Merzenich co-authored a scientific paper on transforming temporal information into a spatial code using a neural network."</data>
      <data key="d6">ce7b58ffc7f43f36bc78154597d01903</data>
    </edge>
    <edge source="&quot;M. M. MERZENICH&quot;" target="&quot;TEMPORAL INFORMATION TRANSFORMATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"M. M. Merzenich is an author of a scientific paper on temporal information transformation into a spatial code by a neural network."</data>
      <data key="d6">82de30f43839f4985de20a981b524af1</data>
    </edge>
    <edge source="&quot;P. DOMINEY&quot;" target="&quot;COMPLEX SENSORY-MOTOR SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"P. Dominey is an author of a scientific paper on complex sensory-motor systems."</data>
      <data key="d6">82de30f43839f4985de20a981b524af1</data>
    </edge>
    <edge source="&quot;P. DOMINEY&quot;" target="&quot;C. GALLICCHIO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"P. Dominey and C. Gallicchio are authors of different scientific papers, but their relationship in the context of the text is not explicitly stated."</data>
      <data key="d6">ce7b58ffc7f43f36bc78154597d01903</data>
    </edge>
    <edge source="&quot;C. GALLICCHIO&quot;" target="&quot;A. MICHELI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"C. Gallicchio and A. Micheli co-authored a scientific paper on designing deep echo state networks."</data>
      <data key="d6">ce7b58ffc7f43f36bc78154597d01903</data>
    </edge>
    <edge source="&quot;C. GALLICCHIO&quot;" target="&quot;L. PEDRELLI&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"C. Gallicchio and L. Pedrelli co-authored a scientific paper on designing deep echo state networks."</data>
      <data key="d6">ce7b58ffc7f43f36bc78154597d01903</data>
    </edge>
    <edge source="&quot;SEPP HOCHREITER&quot;" target="&quot;J&#220;RGEN SCHMIDHUBER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sepp Hochreiter and J&#252;rgen Schmidhuber co-authored a scientific paper on long short-term memory."</data>
      <data key="d6">ce7b58ffc7f43f36bc78154597d01903</data>
    </edge>
    <edge source="&quot;SEPP HOCHREITER&quot;" target="&quot;NEURAL COMPUTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sepp Hochreiter is an author of a research paper published in Neural Computation."</data>
      <data key="d6">c8b7bd13cf99920ecce56cb563910cb3</data>
    </edge>
    <edge source="&quot;J&#220;RGEN SCHMIDHUBER&quot;" target="&quot;NEURAL COMPUTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"J&#252;rgen Schmidhuber is an author of a research paper published in Neural Computation."</data>
      <data key="d6">c8b7bd13cf99920ecce56cb563910cb3</data>
    </edge>
    <edge source="&quot;DANIEL J. GAUTHIER&quot;" target="&quot;ERIK BOLLT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Daniel J. Gauthier and Erik Bollt co-authored a scientific paper on next generation reservoir computing."</data>
      <data key="d6">ce7b58ffc7f43f36bc78154597d01903</data>
    </edge>
    <edge source="&quot;DANIEL J. GAUTHIER&quot;" target="&quot;AARON GRIFFITH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Daniel J. Gauthier and Aaron Griffith co-authored a scientific paper on next generation reservoir computing."</data>
      <data key="d6">ce7b58ffc7f43f36bc78154597d01903</data>
    </edge>
    <edge source="&quot;DANIEL J. GAUTHIER&quot;" target="&quot;WENDSON A. S. BARBOSA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Daniel J. Gauthier and Wendson A. S. Barbosa co-authored a scientific paper on next generation reservoir computing."</data>
      <data key="d6">ce7b58ffc7f43f36bc78154597d01903</data>
    </edge>
    <edge source="&quot;DANIEL J. GAUTHIER&quot;" target="&quot;NEXT GENERATION RESERVOIR COMPUTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Daniel J. Gauthier is an author of the paper on Next Generation Reservoir Computing."</data>
      <data key="d6">88b1448c89d0650dad09fe96cca86cee</data>
    </edge>
    <edge source="&quot;ERIK BOLLT&quot;" target="&quot;NEXT GENERATION RESERVOIR COMPUTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Erik Bollt is an author of the paper on Next Generation Reservoir Computing."</data>
      <data key="d6">88b1448c89d0650dad09fe96cca86cee</data>
    </edge>
    <edge source="&quot;AARON GRIFFITH&quot;" target="&quot;NEXT GENERATION RESERVOIR COMPUTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Aaron Grif&#64257;th is an author of the paper on Next Generation Reservoir Computing."</data>
      <data key="d6">88b1448c89d0650dad09fe96cca86cee</data>
    </edge>
    <edge source="&quot;WENDSON A. S. BARBOSA&quot;" target="&quot;NEXT GENERATION RESERVOIR COMPUTING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Wendson A. S. Barbosa is an author of the paper on Next Generation Reservoir Computing."</data>
      <data key="d6">88b1448c89d0650dad09fe96cca86cee</data>
    </edge>
    <edge source="&quot;GREGOR M. HOERZER&quot;" target="&quot;ROBERT LEGENSTEIN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gregor M. Hoerzer and Robert Legenstein co-authored a scientific paper on a specific topic, but the relationship description is not explicitly stated in the text."</data>
      <data key="d6">ce7b58ffc7f43f36bc78154597d01903</data>
    </edge>
    <edge source="&quot;GREGOR M. HOERZER&quot;" target="&quot;NEURAL COMPUTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Gregor M. Hoerzer is an author of a research paper mentioned in the text, but no specific publication is mentioned."</data>
      <data key="d6">c8b7bd13cf99920ecce56cb563910cb3</data>
    </edge>
    <edge source="&quot;ROBERT LEGENSTEIN&quot;" target="&quot;NEURAL COMPUTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Robert Legenstein is an author of a research paper mentioned in the text, but no specific publication is mentioned."</data>
      <data key="d6">c8b7bd13cf99920ecce56cb563910cb3</data>
    </edge>
    <edge source="&quot;S. BARBOSA&quot;" target="&quot;NEURAL COMPUTATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"S. Barbosa is an author of a research paper published in Neural Computation."</data>
      <data key="d6">c8b7bd13cf99920ecce56cb563910cb3</data>
    </edge>
    <edge source="&quot;GUANG-BIN HUANG&quot;" target="&quot;INT. J. MACH. LEARN. &amp; CYBER.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Guang-Bin Huang is an author of a research paper published in Int. J. Mach. Learn. &amp; Cyber."</data>
      <data key="d6">c8b7bd13cf99920ecce56cb563910cb3</data>
    </edge>
    <edge source="&quot;DIAN HUI WANG&quot;" target="&quot;INT. J. MACH. LEARN. &amp; CYBER.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dian Hui Wang is an author of a research paper published in Int. J. Mach. Learn. &amp; Cyber."</data>
      <data key="d6">c8b7bd13cf99920ecce56cb563910cb3</data>
    </edge>
    <edge source="&quot;YUAN LAN&quot;" target="&quot;INT. J. MACH. LEARN. &amp; CYBER.&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Yuan Lan is an author of a research paper published in Int. J. Mach. Learn. &amp; Cyber."</data>
      <data key="d6">c8b7bd13cf99920ecce56cb563910cb3</data>
    </edge>
    <edge source="&quot;HINAUT H. TROUVAIN&quot;" target="&quot;GMD TECH. REPORT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hinaut H. Trouvain is an author of a research paper published in GMD Tech. Report."</data>
      <data key="d6">c8b7bd13cf99920ecce56cb563910cb3</data>
    </edge>
    <edge source="&quot;JACQUES KAISER&quot;" target="&quot;BIOINSPIRATION &amp; BIOMIMETICS, 12(5):055001, SEP 2017&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jacques Kaiser published a paper on scaling up liquid state machines to predict over address events from dynamic vision sensors."</data>
      <data key="d6">546551fd625e354e9afe1245e060bed3</data>
    </edge>
    <edge source="&quot;GERMAN NATIONAL RESEARCH CENTER FOR INFORMATION TECHNOLOGY GMD&quot;" target="&quot;GMD TECH. REPORT, 148:34, 2001&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GMD published a report on the 'echo state' approach for training recurrent neural networks."</data>
      <data key="d6">546551fd625e354e9afe1245e060bed3</data>
    </edge>
    <edge source="&quot;GERMAN NATIONAL RESEARCH CENTER FOR INFORMATION TECHNOLOGY GMD&quot;" target="&quot;BONN, GERMANY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"German National Research Center for Information Technology GMD is based in Bonn, Germany."</data>
      <data key="d6">546551fd625e354e9afe1245e060bed3</data>
    </edge>
    <edge source="&quot;W. MAASS&quot;" target="&quot;NEURAL COMPUTATION, 14(11):2531&#8211;2560, 2002&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"W. Maass published a paper on real-time computing without stable states: A new framework for neural computation based on perturbations."</data>
      <data key="d6">546551fd625e354e9afe1245e060bed3</data>
    </edge>
    <edge source="&quot;W. MAASS, T. NATSCHL&#168;AGER, AND H. MARKRAM&quot;" target="&quot;REAL-TIME COMPUTING WITHOUT STABLE STATES PAPER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The authors contributed to a research paper on real-time computing without stable states."</data>
      <data key="d6">ea62f994886333282704a19ebf0469ea</data>
    </edge>
    <edge source="&quot;FRANCESCO MARTINUZZI, CHRIS RACKAUCKAS, ANAS ABDELREHIM, MIGUEL D. MAHECHA, AND KARIN MORA&quot;" target="&quot;RESERVOIRCOMPUTING.JL LIBRARY&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The authors developed a library for reservoir computing models, published in 2022.""The Reservoircomputing.jl library is developed by a group of authors who published a research paper on reservoir computing models in 2022."</data>
      <data key="d6">ea62f994886333282704a19ebf0469ea</data>
    </edge>
    <edge source="&quot;NILS SCHAETTI&quot;" target="&quot;ECHOTORCH LIBRARY&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Nils Schaetti is the developer of Echotorch, a reservoir computing library with pytorch, published in 2018.""Nils Schaetti developed Echotorch, a reservoir computing library with pytorch, which is mentioned in the text."</data>
      <data key="d6">ea62f994886333282704a19ebf0469ea</data>
    </edge>
    <edge source="&quot;BENJAMIN SCHRAUWEN, MARION WARDERMANN, DAVID VERSTRAETEN, JOCHEN J. STEIL, AND DIRK STROOBANDT&quot;" target="&quot;IMPROVING RESERVOIRS USING INTRINSIC PLASTICITY PAPER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The authors published a research paper on improving reservoirs using intrinsic plasticity in 2008."</data>
      <data key="d6">ea62f994886333282704a19ebf0469ea</data>
    </edge>
    <edge source="&quot;JOCHEN J STEIL&quot;" target="&quot;ONLINE RESERVOIR ADAPTATION PAPER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jochen J Steil is an author who published a research paper on online reservoir adaptation by intrinsic plasticity for backpropagation&#8211;decorrelation and echo state learning in 2007."</data>
      <data key="d6">ea62f994886333282704a19ebf0469ea</data>
    </edge>
    <edge source="&quot;JOCHEN J STEIL&quot;" target="&quot;IMPROVING RESERVOIRS USING INTRINSIC PLASTICITY PAPER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jochen J Steil is an author of the research paper on improving reservoirs using intrinsic plasticity, published in 2008."</data>
      <data key="d6">ea62f994886333282704a19ebf0469ea</data>
    </edge>
    <edge source="&quot;JOCHEN J STEIL&quot;" target="&quot;PYRCN TOOLBOX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jochen J Steil is an author of a research paper related to the Pyrcn Toolbox."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;PETER STEINER, AZARAKHSH JALALVAND, SIMON STONE, AND PETER BIRKHOLZ&quot;" target="&quot;PYRCN TOOLBOX&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The authors developed Pyrcn, a toolbox for exploration and analysis of reservoir computing models.""The Pyrcn toolbox is developed by a group of authors who are mentioned in the text."</data>
      <data key="d6">ea62f994886333282704a19ebf0469ea</data>
    </edge>
    <edge source="&quot;PYRCN TOOLBOX&quot;" target="&quot;PETER STEINER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Peter Steiner is an author of the Pyrcn Toolbox."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;PYRCN TOOLBOX&quot;" target="&quot;AZARAKHSH JALALVAND&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Azarakhsh Jalalvand is an author of the Pyrcn Toolbox."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;PYRCN TOOLBOX&quot;" target="&quot;SIMON STONE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Simon Stone is an author of the Pyrcn Toolbox."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;PYRCN TOOLBOX&quot;" target="&quot;PETER BIRKHOLZ&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Peter Birkholz is an author of the Pyrcn Toolbox."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;DAVID SUSSILLO&quot;" target="&quot;COHERENT PATTERNS RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"David Sussillo is an author of a research paper on generating coherent patterns of activity from chaotic neural networks."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;L. F. ABBOTT&quot;" target="&quot;COHERENT PATTERNS RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"L. F. Abbott is an author of a research paper on generating coherent patterns of activity from chaotic neural networks."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;SANDER DIELEMAN&quot;" target="&quot;OGER RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sander Dieleman is an author of a research paper on oger: modular learning architectures for large-scale sequential processing."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;PHILEMON BRAKEL&quot;" target="&quot;OGER RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Philemon Brakel is an author of a research paper on oger: modular learning architectures for large-scale sequential processing."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;PIETER BUTENEERS&quot;" target="&quot;OGER RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Pieter Buteneers is an author of a research paper on oger: modular learning architectures for large-scale sequential processing."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;DEJAN PECEVSKI&quot;" target="&quot;OGER RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dejan Pecevski is an author of a research paper on oger: modular learning architectures for large-scale sequential processing."</data>
      <data key="d6">7751d435f46a6d4027a1c96edabb9626</data>
    </edge>
    <edge source="&quot;VERSTRAETEN, BENJAMIN&quot;" target="&quot;MODULAR LEARNING ARCHITECTURES FOR LARGE-SCALE SEQUENTIAL PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Verstraeten, Benjamin is an author of the research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d6">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </edge>
    <edge source="&quot;SCHRAUWEN, SANDER&quot;" target="&quot;MODULAR LEARNING ARCHITECTURES FOR LARGE-SCALE SEQUENTIAL PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Schrauwen, Sander is an author of the research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d6">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </edge>
    <edge source="&quot;DIELEMAN, PHILEMON&quot;" target="&quot;MODULAR LEARNING ARCHITECTURES FOR LARGE-SCALE SEQUENTIAL PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dieleman, Philemon is an author of the research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d6">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </edge>
    <edge source="&quot;BRAKEL, PIETER&quot;" target="&quot;MODULAR LEARNING ARCHITECTURES FOR LARGE-SCALE SEQUENTIAL PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Brakel, Pieter is an author of the research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d6">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </edge>
    <edge source="&quot;BUTE-NEERS, AND DEJAN PECEVSKI&quot;" target="&quot;MODULAR LEARNING ARCHITECTURES FOR LARGE-SCALE SEQUENTIAL PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Bute-neers, and Dejan Pecevski are additional authors of the research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d6">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </edge>
    <edge source="&quot;THE JOURNAL OF MACHINE LEARNING RESEARCH&quot;" target="&quot;MODULAR LEARNING ARCHITECTURES FOR LARGE-SCALE SEQUENTIAL PROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Journal of Machine Learning Research is the publication source for the research paper on modular learning architectures for large-scale sequential processing."</data>
      <data key="d6">b8d8b71875a9ccc508b40fe4aad8d796</data>
    </edge>
    <edge source="&quot;DIRECTED ACYCLIC GRAPH&quot;" target="&quot;STRICTLY FEEDFORWARD NEURAL NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Directed Acyclic Graph can be unrolled and replaced with a Strictly Feedforward Neural Network."</data>
      <data key="d6">f59839daadfb1f3832bb9f8d201a7126</data>
    </edge>
    <edge source="&quot;INFINITE IMPULSE RECURRENT NETWORK&quot;" target="&quot;FINITE IMPULSE NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Additional stored states and controlled states can be added to both Infinite Impulse Recurrent Networks and Finite Impulse Networks."</data>
      <data key="d6">f59839daadfb1f3832bb9f8d201a7126</data>
    </edge>
    <edge source="&quot;FEEDBACK NEURAL NETWORK&quot;" target="&quot;LONG SHORT-TERM MEMORY&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Long Short-Term Memory networks are a type of Feedback Neural Network that addresses the vanishing gradient problem.""Feedback Neural Networks are theoretically Turing complete and can incorporate gated states or gated memory, such as Long Short-Term Memory networks."</data>
      <data key="d6">f59839daadfb1f3832bb9f8d201a7126</data>
    </edge>
    <edge source="&quot;FEEDBACK NEURAL NETWORK&quot;" target="&quot;GATED RECURRENT UNITS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Gated Recurrent Units are a type of Feedback Neural Network that incorporates gated states.""Feedback Neural Networks can also incorporate gated states, similar to Gated Recurrent Units."</data>
      <data key="d6">f59839daadfb1f3832bb9f8d201a7126</data>
    </edge>
    <edge source="&quot;LONG SHORT-TERM MEMORY&quot;" target="&quot;SECOND-ORDER RNNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Long Short-Term Memory is an example of second-order RNNs, which use higher order weights and states for direct mapping to a finite-state machine."</data>
      <data key="d6">b888c4ebe914c1dfa26682de69de9de9</data>
    </edge>
    <edge source="&quot;LONG SHORT-TERM MEMORY&quot;" target="&quot;LSTM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LSTM is an abbreviation for Long Short-Term Memory, a type of Recurrent Neural Network that uses a mechanism to retain information over long sequences."</data>
      <data key="d6">2bdd28d9e151597072c8490db69b9941</data>
    </edge>
    <edge source="&quot;HOCHREITER AND SCHMIDHUBER&quot;" target="&quot;LSTM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hochreiter and Schmidhuber invented Long Short-Term Memory (LSTM) networks in 1997."</data>
      <data key="d6">b31ca51b419f7270ee5f4910c90ea331</data>
    </edge>
    <edge source="&quot;LSTM&quot;" target="&quot;GOOGLE&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Google used LSTM networks in its speech recognition and Android systems."
"Google used LSTM for speech recognition and improved machine translation, language modeling, and multilingual language processing."</data>
      <data key="d6">486e4b71bf02f756450ab727db88f821,b31ca51b419f7270ee5f4910c90ea331</data>
    </edge>
    <edge source="&quot;LSTM&quot;" target="&quot;CNN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LSTM was used in combination with CNNs for automatic image captioning."</data>
      <data key="d6">486e4b71bf02f756450ab727db88f821</data>
    </edge>
    <edge source="&quot;LSTM&quot;" target="&quot;CTC&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LSTM is trained using the Connectionist Temporal Classification (CTC) method to find an RNN weight matrix that maximizes the probability of label sequences in a training set, given the corresponding input sequences."</data>
      <data key="d6">b48d5212c060453a846e21cdb98dbd7d</data>
    </edge>
    <edge source="&quot;LSTM&quot;" target="&quot;HMM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LSTM can learn to recognize context-sensitive languages unlike previous models based on Hidden Markov Models (HMM) and similar concepts."</data>
      <data key="d6">b48d5212c060453a846e21cdb98dbd7d</data>
    </edge>
    <edge source="&quot;LSTM&quot;" target="&quot;GRUS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"GRUs are a type of gating mechanism in recurrent neural networks (RNNs) similar to LSTM but with fewer parameters."</data>
      <data key="d6">b48d5212c060453a846e21cdb98dbd7d</data>
    </edge>
    <edge source="&quot;LSTM&quot;" target="&quot;BI-DIRECTIONAL RNNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Bi-directional RNNs can be used in conjunction with LSTM to improve performance in certain applications."</data>
      <data key="d6">b48d5212c060453a846e21cdb98dbd7d</data>
    </edge>
    <edge source="&quot;LSTM&quot;" target="&quot;BPTT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LSTM combines with a BPTT/RTRL hybrid learning method to overcome problems in recurrent neural networks."</data>
      <data key="d6">88405de18768775d8bce062ea467bd7f</data>
    </edge>
    <edge source="&quot;LSTM&quot;" target="&quot;RTRL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LSTM combines with a BPTT/RTRL hybrid learning method to overcome problems in recurrent neural networks."</data>
      <data key="d6">88405de18768775d8bce062ea467bd7f</data>
    </edge>
    <edge source="&quot;LSTM&quot;" target="&quot;INDRNN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IndRNN reduces the context of a neuron to its own past state, which is a concept similar to the approach taken by LSTM."</data>
      <data key="d6">88405de18768775d8bce062ea467bd7f</data>
    </edge>
    <edge source="&quot;BAIDU&quot;" target="&quot;CTC-TRAINED RNNS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5&#8217;00 speech recognition dataset in 2014."</data>
      <data key="d6">b31ca51b419f7270ee5f4910c90ea331</data>
    </edge>
    <edge source="&quot;BAIDU&quot;" target="&quot;2S09 SWITCHBOARD HUB5&#8217;00&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Baidu used the 2S09 Switchboard Hub5&#8217;00 dataset to break a speech recognition benchmark."</data>
      <data key="d6">486e4b71bf02f756450ab727db88f821</data>
    </edge>
    <edge source="&quot;BAIDU&quot;" target="&quot;CTC&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Baidu used CTC-trained RNNs in their machine learning models."</data>
      <data key="d6">486e4b71bf02f756450ab727db88f821</data>
    </edge>
    <edge source="&quot;GOOGLE&quot;" target="&quot;CTC&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Google used CTC-trained LSTM in their machine learning models."</data>
      <data key="d6">486e4b71bf02f756450ab727db88f821</data>
    </edge>
    <edge source="&quot;CTC&quot;" target="&quot;LONG SHORT-TERM MEMORY (LSTM)&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Long short-term memory (LSTM) is often used in applications that train stacks of LSTM RNNs using Connectionist Temporal Classification (CTC)."</data>
      <data key="d6">4470a7f7ad60a2866b31907a2a3ca96e</data>
    </edge>
    <edge source="&quot;ELMAN NETWORKS&quot;" target="&quot;SEQUENCE PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Elman Networks are specifically mentioned as being able to perform tasks such as sequence prediction due to their additional context units."</data>
      <data key="d6">bf4dccb5096a917a6a71f0cc224e4d7c</data>
    </edge>
    <edge source="&quot;ELMAN NETWORK&quot;" target="&quot;JEFF ELMAN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Elman Networks were invented by Jeff Elman."</data>
      <data key="d6">fb7999a4b39733c28630293d3659d7eb</data>
    </edge>
    <edge source="&quot;ELMAN NETWORK&quot;" target="&quot;SIMPLE RECURRENT NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Elman Networks are a type of Simple Recurrent Network."</data>
      <data key="d6">fb7999a4b39733c28630293d3659d7eb</data>
    </edge>
    <edge source="&quot;JORDAN NETWORK&quot;" target="&quot;GEOFFREY HINTON&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jordan Networks are also known as Simple Recurrent Networks, which are a field of study associated with Geoffrey Hinton."</data>
      <data key="d6">fb7999a4b39733c28630293d3659d7eb</data>
    </edge>
    <edge source="&quot;JORDAN NETWORK&quot;" target="&quot;SIMPLE RECURRENT NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Jordan Networks are a type of Simple Recurrent Network."</data>
      <data key="d6">fb7999a4b39733c28630293d3659d7eb</data>
    </edge>
    <edge source="&quot;BIDIRECTIONAL ASSOCIATIVE MEMORY (BAM) NETWORK&quot;" target="&quot;BART KOSKO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Bart Kosko is the author of the Bidirectional Associative Memory (BAM) Network."</data>
      <data key="d6">a8c0edd2cdddb7d6d899284063b541f5</data>
    </edge>
    <edge source="&quot;BIDIRECTIONAL ASSOCIATIVE MEMORY (BAM) NETWORK&quot;" target="&quot;RECENTLY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Bidirectional Associative Memory (BAM) Network has recently been optimized for increased network stability and relevance to real-world applications."</data>
      <data key="d6">a8c0edd2cdddb7d6d899284063b541f5</data>
    </edge>
    <edge source="&quot;INDEPENDENTLY RECURRENT NEURAL NETWORK&quot;" target="&quot;RECURSIVE NEURAL NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Both Independently Recurrent Neural Network and Recursive Neural Network are types of neural networks that address the gradient vanishing and exploding problems in the traditional fully connected RNN, but they differ in their structure and application."</data>
      <data key="d6">423cdb622c47fa8cec25f22eb9f9f01f</data>
    </edge>
    <edge source="&quot;RECURSIVE NEURAL NETWORK&quot;" target="&quot;AUTOMATIC DIFFERENTIATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recursive Neural Networks are typically trained using the reverse mode of automatic differentiation."</data>
      <data key="d6">7b6ff30ef255db2d2c68326d78cf0115</data>
    </edge>
    <edge source="&quot;RECURSIVE NEURAL NETWORK&quot;" target="&quot;DISTRIBUTED REPRESENTATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Recursive Neural Networks can process distributed representations of structure, such as logical terms."</data>
      <data key="d6">7b6ff30ef255db2d2c68326d78cf0115</data>
    </edge>
    <edge source="&quot;CHUNKER&quot;" target="&quot;AUTOMATIZER&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"The Chunker learns to predict and compress inputs that are unpredictable by the Automatizer.""The Automatizer learns to predict or imitate inputs based on the hidden units of the Chunker."</data>
      <data key="d6">b888c4ebe914c1dfa26682de69de9de9</data>
    </edge>
    <edge source="&quot;GENERATIVE MODEL&quot;" target="&quot;VANISHING GRADIENT PROBLEM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Generative Model partially overcame the vanishing gradient problem in neural networks."</data>
      <data key="d6">b888c4ebe914c1dfa26682de69de9de9</data>
    </edge>
    <edge source="&quot;SECOND-ORDER RNNS&quot;" target="&quot;FINITE-STATE MACHINE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Second-order RNNs can be directly mapped to a finite-state machine, allowing both training and representation."</data>
      <data key="d6">4470a7f7ad60a2866b31907a2a3ca96e</data>
    </edge>
    <edge source="&quot;CONTINUOUS-TIME RECURRENT NEURAL NETWORK&quot;" target="&quot;CTRNN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"CTRNN is an abbreviation for Continuous-time Recurrent Neural Network, a type of neural network that uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs."</data>
      <data key="d6">2bdd28d9e151597072c8490db69b9941</data>
    </edge>
    <edge source="&quot;CTRNN&quot;" target="&quot;EVOLUTIONARY ROBOTICS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"CTRNNs have been applied to Evolutionary Robotics where they have been used to address vision, co-operation, and minimal cognitive behaviour."</data>
      <data key="d6">2bdd28d9e151597072c8490db69b9941</data>
    </edge>
    <edge source="&quot;HIERARCHICAL RECURRENT NEURAL NETWORK&quot;" target="&quot;HRNN&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"HRNN is an abbreviation for Hierarchical Recurrent Neural Network, a type of Recurrent Neural Network that uses multiple layers to process data at different levels of abstraction."</data>
      <data key="d6">2bdd28d9e151597072c8490db69b9941</data>
    </edge>
    <edge source="&quot;HIERARCHICAL RECURRENT NEURAL NETWORKS&quot;" target="&quot;CONSUMER PRICE INDEX&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hierarchical recurrent neural networks are used in forecasting disaggregated inflation components of the consumer price index (CPI), demonstrating their application in predicting economic data."</data>
      <data key="d6">9e61c22432d6984a19da5840f64d417d</data>
    </edge>
    <edge source="&quot;US CPI-U INDEX&quot;" target="&quot;HRNN MODEL&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The HRNN model is evaluated using the US CPI-U index dataset."</data>
      <data key="d6">c7ca22e82a3823afda793ad30077348e</data>
    </edge>
    <edge source="&quot;HAWKINS&quot;" target="&quot;MEMORY-PREDICTION THEORY&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Hawkins is known for his work on the memory-prediction theory of brain function."</data>
      <data key="d6">0f59288ce2aaf33e468cdc3877cefd85</data>
    </edge>
    <edge source="&quot;NEURAL TURING MACHINES&quot;" target="&quot;EXTERNAL MEMORY RESOURCES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Neural Turing machines are designed to interact with external memory resources."</data>
      <data key="d6">0f59288ce2aaf33e468cdc3877cefd85</data>
    </edge>
    <edge source="&quot;NEURAL TURING MACHINES&quot;" target="&quot;DIFFERENTIABLE NEURAL COMPUTERS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Differentiable neural computers are an extension of Neural Turing machines."</data>
      <data key="d6">0f59288ce2aaf33e468cdc3877cefd85</data>
    </edge>
    <edge source="&quot;NEURAL TURING MACHINES&quot;" target="&quot;NEURAL NETWORK PUSHDOWN AUTOMATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Neural network pushdown automata are similar to Neural Turing machines."</data>
      <data key="d6">0f59288ce2aaf33e468cdc3877cefd85</data>
    </edge>
    <edge source="&quot;MEMRISTIVE NETWORKS&quot;" target="&quot;CORTICAL NEURAL NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Memristive networks are a system of cortical neural networks."</data>
      <data key="d6">0f59288ce2aaf33e468cdc3877cefd85</data>
    </edge>
    <edge source="&quot;MEMRISTIVE NETWORKS&quot;" target="&quot;GREG SNIDER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Greg Snider describes a system of cortical neural networks that use memristive devices for memory storage and processing."</data>
      <data key="d6">0f59288ce2aaf33e468cdc3877cefd85</data>
    </edge>
    <edge source="&quot;MEMRISTIVE NETWORKS&quot;" target="&quot;HP LABS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"HP Labs is mentioned in the context of developing systems with memristive nanodevices, which are similar to Memristive Networks."</data>
      <data key="d6">671755b49cf8893c9fcf9c9c05777ea6</data>
    </edge>
    <edge source="&quot;MEMRISTIVE NETWORKS&quot;" target="&quot;NEUROMORPHIC ARCHITECTURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Memristive Networks are a particular type of physical neural network that have been described as being used in the development of neuromorphic architectures."</data>
      <data key="d6">671755b49cf8893c9fcf9c9c05777ea6</data>
    </edge>
    <edge source="&quot;MEMRISTIVE NETWORKS&quot;" target="&quot;LITTLE-HOPFIELD NETWORKS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Memristive Networks are compared to Little-Hopfield Networks, sharing similar properties such as continuous dynamics and limited memory capacity."</data>
      <data key="d6">5445391448d4ac43471e2bce5eb41a70</data>
    </edge>
    <edge source="&quot;MEMRISTIVE NETWORKS&quot;" target="&quot;RESISTOR-CAPACITOR NETWORK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Memristive Networks are mentioned to have a more interesting non-linear behavior compared to Resistor-Capacitor Networks."</data>
      <data key="d6">5445391448d4ac43471e2bce5eb41a70</data>
    </edge>
    <edge source="&quot;MEMRISTIVE NETWORKS&quot;" target="&quot;NEUROMORPHIC ENGINEERING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Memristive Networks are mentioned in the context of Neuromorphic Engineering, where the device behavior depends on the circuit wiring or topology."</data>
      <data key="d6">5445391448d4ac43471e2bce5eb41a70</data>
    </edge>
    <edge source="&quot;MEMRISTIVE NETWORKS&quot;" target="&quot;CARAVELLI-TRAVERSA-DI VENTRA EQUATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Caravelli-Traversa-Di Ventra Equation is mentioned as a method used to study the evolution of memristive networks analytically."</data>
      <data key="d6">5445391448d4ac43471e2bce5eb41a70</data>
    </edge>
    <edge source="&quot;GREG SNIDER&quot;" target="&quot;HP LABS&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Greg Snider works at HP Labs."
"Greg Snider is a researcher at HP Labs who has described a system of cortical computing with memristive nanodevices."</data>
      <data key="d6">0f59288ce2aaf33e468cdc3877cefd85,671755b49cf8893c9fcf9c9c05777ea6</data>
    </edge>
    <edge source="&quot;HP LABS&quot;" target="&quot;DARPA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"DARPA has funded projects in collaboration with IBM Research, HP Labs, and the Boston University Department of Cognitive and Neural Systems (CNS) to develop neuromorphic architectures based on memristive systems."</data>
      <data key="d6">671755b49cf8893c9fcf9c9c05777ea6</data>
    </edge>
    <edge source="&quot;DARPA&quot;" target="&quot;IBM RESEARCH&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"DARPA has funded projects in collaboration with IBM Research, HP Labs, and the Boston University Department of Cognitive and Neural Systems (CNS) to develop neuromorphic architectures based on memristive systems."</data>
      <data key="d6">671755b49cf8893c9fcf9c9c05777ea6</data>
    </edge>
    <edge source="&quot;DARPA&quot;" target="&quot;BOSTON UNIVERSITY DEPARTMENT OF COGNITIVE AND NEURAL SYSTEMS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"DARPA has funded projects in collaboration with IBM Research, HP Labs, and the Boston University Department of Cognitive and Neural Systems (CNS) to develop neuromorphic architectures based on memristive systems."</data>
      <data key="d6">671755b49cf8893c9fcf9c9c05777ea6</data>
    </edge>
    <edge source="&quot;BPTT&quot;" target="&quot;CRBP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"CRBP implements and combines BPTT and RTRL paradigms for locally recurrent networks."</data>
      <data key="d6">88405de18768775d8bce062ea467bd7f</data>
    </edge>
    <edge source="&quot;RTRL&quot;" target="&quot;CRBP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"CRBP implements and combines BPTT and RTRL paradigms for locally recurrent networks."</data>
      <data key="d6">88405de18768775d8bce062ea467bd7f</data>
    </edge>
    <edge source="&quot;SIGNAL-FLOW GRAPHS&quot;" target="&quot;LEE&#8217;S THEOREM&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Signal-Flow Graphs is based on Lee&#8217;s Theorem for network sensitivity calculations."</data>
      <data key="d6">88405de18768775d8bce062ea467bd7f</data>
    </edge>
    <edge source="&quot;DYNAMICAL SYSTEMS THEORY&quot;" target="&quot;APPLICATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Dynamical Systems Theory is mentioned in the context of analyzing chaotic behavior in systems, which could be considered an application."</data>
      <data key="d6">2f1161d1f711d264529aa7bddf81959b</data>
    </edge>
    <edge source="&quot;SILENCING MECHANISM&quot;" target="&quot;APPLICATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Silencing Mechanism is mentioned in the context of a more biological-based model for memory-based learning, which could be considered an application."</data>
      <data key="d6">2f1161d1f711d264529aa7bddf81959b</data>
    </edge>
    <edge source="&quot;MACKEY-GLASS EQUATIONS&quot;" target="&quot;PHYSIOLOGICAL SIGNALS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Mackey-Glass Equations are used to describe the temporal behavior of physiological signals."</data>
      <data key="d6">2f4c992d69812866e6fce6dbb52d8612</data>
    </edge>
    <edge source="&quot;TASK 1: 10 TIMESTEPS AHEAD FORECAST&quot;" target="&quot;DATA PREPROCESSING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Data Preprocessing is a necessary step in Task 1 to prepare the Mackey-Glass Time Series data for prediction."</data>
      <data key="d6">fac681bdc38ae5829173c747ee6240fa</data>
    </edge>
    <edge source="&quot;DATA PREPROCESSING&quot;" target="&quot;THE DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Data Preprocessing is the step of loading and processing the data found on Zenodo."</data>
      <data key="d6">8677349b328abac82fa1cfc91c856a6c</data>
    </edge>
    <edge source="&quot;DATA PREPROCESSING&quot;" target="&quot;TESTING DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Data Preprocessing is applied to the Testing Data to clean and prepare it for use in the Echo State Network (ESN) model."</data>
      <data key="d6">1365a36c76afc697ac626fd0f784804a</data>
    </edge>
    <edge source="&quot;GENERATIVE MODE&quot;" target="&quot;CHAPTER 2&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Generative Mode is the main concept discussed in Chapter 2, which involves generating new data based on learned patterns."</data>
      <data key="d6">e396354e3a9be76616392af11f56e671</data>
    </edge>
    <edge source="&quot;GENERATIVE MODE&quot;" target="&quot;WARMUP&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Generative Mode includes a Warmup phase, where initial data is used to prepare the system."</data>
      <data key="d6">70db98fabc82fc96ecf8cc2c023b586b</data>
    </edge>
    <edge source="&quot;ROBOT FALLING&quot;" target="&quot;ZENODO&quot;">
      <data key="d4">2.0</data>
      <data key="d5">"Data for the Robot Falling use case can be found on Zenodo."
"Data for the robot falling use case can be found on Zenodo."</data>
      <data key="d6">af2db1cc5ab6b16acae2c93d3facb668,b483c6bbce54156c724905b340aa2e85</data>
    </edge>
    <edge source="&quot;ZENODO&quot;" target="&quot;CANARY SONG DECODING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Zenodo hosts data for the canary song decoding use case, making it accessible for analysis."</data>
      <data key="d6">d15f6d075c072f0335b5332f11c00299</data>
    </edge>
    <edge source="&quot;ZENODO&quot;" target="&quot;THE DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Data is found on the Zenodo platform."</data>
      <data key="d6">8677349b328abac82fa1cfc91c856a6c</data>
    </edge>
    <edge source="&quot;ROBOT&quot;" target="&quot;FALL INDICATOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The robot's data is being analyzed to predict and monitor the fall indicator, with the goal of preventing falls."</data>
      <data key="d6">90fa1052aec4e6374867e9a2951fb3c4</data>
    </edge>
    <edge source="&quot;ROBOT&quot;" target="&quot;FORCE MAGNITUDE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The robot's data includes measurements of force magnitude, which is being analyzed in the context of the data."</data>
      <data key="d6">90fa1052aec4e6374867e9a2951fb3c4</data>
    </edge>
    <edge source="&quot;CANARY SONG DECODING&quot;" target="&quot;CANARY SONG&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Canary Song Decoding is a process that involves analyzing Canary Song to extract information."</data>
      <data key="d6">e7d249cdab85dc69b631d43ac6b62915</data>
    </edge>
    <edge source="&quot;CANARY SONG DECODING&quot;" target="&quot;CHAPTER 5&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Chapter 5 discusses a use case in the wild involving Canary Song Decoding."</data>
      <data key="d6">1e8ee805d22cd143d2372d300997d253</data>
    </edge>
    <edge source="&quot;LIBRISPEECH DATASET&quot;" target="&quot;MFCC&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"MFCC is used to extract features from the audio files in the Librispeech Dataset."</data>
      <data key="d6">adfade0d7bc85c6420e61ecd1ce7095c</data>
    </edge>
    <edge source="&quot;LIBRISPEECH DATASET&quot;" target="&quot;DELTA-DELTA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Delta-Delta is used to extract features from the audio files in the Librispeech Dataset."</data>
      <data key="d6">adfade0d7bc85c6420e61ecd1ce7095c</data>
    </edge>
    <edge source="&quot;LIBRISPEECH DATASET&quot;" target="&quot;LOAD_DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The load_data function is used to process data from the Librispeech Dataset."</data>
      <data key="d6">adfade0d7bc85c6420e61ecd1ce7095c</data>
    </edge>
    <edge source="&quot;SKLEARN&quot;" target="&quot;DATA SCIENTIST&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The data scientist uses the sklearn library for data analysis and model training."</data>
      <data key="d6">9fdaabd6c7e893a275a3848c10007477</data>
    </edge>
    <edge source="&quot;SKLEARN&quot;" target="&quot;CHAPTER 5&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Sklearn is used in Chapter 5 for data preprocessing and label encoding."</data>
      <data key="d6">1e8ee805d22cd143d2372d300997d253</data>
    </edge>
    <edge source="&quot;ONE_HOT&quot;" target="&quot;NP.VSTACK&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"np.vstack is used in conjunction with one_hot encoding for transforming data into a suitable format for analysis."</data>
      <data key="d6">1db5e6cd356c6066227de5e273de1abe</data>
    </edge>
    <edge source="&quot;VOCAB&quot;" target="&quot;OUTPUTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"vocab is used for mapping between numerical and categorical representations of data in the context of comparing predicted outputs to actual targets."</data>
      <data key="d6">1db5e6cd356c6066227de5e273de1abe</data>
    </edge>
    <edge source="&quot;AVERAGE ACCURACY&quot;" target="&quot;STANDARD DEVIATION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Average Accuracy and Standard Deviation are used together to provide a more comprehensive understanding of a model's performance, with the Standard Deviation indicating the variability in the accuracy scores."</data>
      <data key="d6">eb4cbfc924325a7ec01e566ffac75ac3</data>
    </edge>
    <edge source="&quot;MODEL CREATION&quot;" target="&quot;CONNECTION CHAINING&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Connection chaining is a technique used in model creation to connect nodes in a sequential manner."</data>
      <data key="d6">f1fc6fbc8158d3da070d55544041a2ca</data>
    </edge>
    <edge source="&quot;MODEL CREATION&quot;" target="&quot;CONNECTION MERGE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Connection merge is a technique used in model creation to combine connections from multiple nodes to a single node."</data>
      <data key="d6">f1fc6fbc8158d3da070d55544041a2ca</data>
    </edge>
    <edge source="&quot;INITIALIZER FUNCTIONS&quot;" target="&quot;READOUTS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Initializer functions are used to initialize the parameters of the readout matrices."</data>
      <data key="d6">ff860bc63e3d697a6183c0b850689048</data>
    </edge>
    <edge source="&quot;GENERATIVE TASK&quot;" target="&quot;ESN CALL METHOD&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"ESN Call Method is a method mentioned in the context of performing a generative task using a for-loop."</data>
      <data key="d6">ff860bc63e3d697a6183c0b850689048</data>
    </edge>
    <edge source="&quot;NEXT GENERATION RESERVOIR COMPUTING&quot;" target="&quot;NATURE COMMUNICATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The paper on Next Generation Reservoir Computing was published in Nature Communications."</data>
      <data key="d6">88b1448c89d0650dad09fe96cca86cee</data>
    </edge>
    <edge source="&quot;NEXT GENERATION RESERVOIR COMPUTING&quot;" target="&quot;DOI: 10.1038/S41467-021-25801-2&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"DOI: 10.1038/s41467-021-25801-2 is the Digital Object Identifier for the paper on Next Generation Reservoir Computing."</data>
      <data key="d6">88b1448c89d0650dad09fe96cca86cee</data>
    </edge>
    <edge source="&quot;NVAR MACHINE&quot;" target="&quot;LINEAR FEATURES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NVAR Machine constructs Linear Features as a part of its feature vector."</data>
      <data key="d6">5430aac4404b66ea20503e5cac4d4328</data>
    </edge>
    <edge source="&quot;NVAR MACHINE&quot;" target="&quot;NONLINEAR REPRESENTATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NVAR Machine constructs Nonlinear Representations as a part of its feature vector."</data>
      <data key="d6">5430aac4404b66ea20503e5cac4d4328</data>
    </edge>
    <edge source="&quot;NVAR MACHINE&quot;" target="&quot;FEATURE VECTOR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NVAR Machine constructs the final Feature Vector by gathering Linear Features and Nonlinear Representations."</data>
      <data key="d6">5430aac4404b66ea20503e5cac4d4328</data>
    </edge>
    <edge source="&quot;NVAR&quot;" target="&quot;LINEAR COMPONENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NVAR is a model that combines a linear component to capture linear relationships."</data>
      <data key="d6">ac2eb4232eaa7c1adbf00d4a0be3d799</data>
    </edge>
    <edge source="&quot;NVAR&quot;" target="&quot;NONLINEAR COMPONENT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NVAR is a model that combines a nonlinear component to capture nonlinear relationships."</data>
      <data key="d6">ac2eb4232eaa7c1adbf00d4a0be3d799</data>
    </edge>
    <edge source="&quot;NVAR&quot;" target="&quot;REGULARIZED LINEAR REGRESSION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NVAR is connected to a readout layer using Regularized Linear Regression for training."</data>
      <data key="d6">ac2eb4232eaa7c1adbf00d4a0be3d799</data>
    </edge>
    <edge source="&quot;NVAR&quot;" target="&quot;ONE-STEP-AHEAD PREDICTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"NVAR is trained to perform one-step-ahead prediction on time series data."</data>
      <data key="d6">2035514bf3ab5b7f12ae1321972551f1</data>
    </edge>
    <edge source="&quot;MATHEMATICAL MODEL&quot;" target="&quot;DIFFERENTIAL EQUATIONS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Mathematical Model is a representation of a system using mathematical equations, such as Differential Equations."</data>
      <data key="d6">ac2eb4232eaa7c1adbf00d4a0be3d799</data>
    </edge>
    <edge source="&quot;X-COORDINATE&quot;" target="&quot;RECONSTRUCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The x-coordinate is used as input in the Reconstruction process to predict the z-coordinate."</data>
      <data key="d6">ac3456a3574f6939fcb6d5242c202810</data>
    </edge>
    <edge source="&quot;Y-COORDINATE&quot;" target="&quot;RECONSTRUCTION&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The y-coordinate is used as input in the Reconstruction process to predict the z-coordinate."</data>
      <data key="d6">ac3456a3574f6939fcb6d5242c202810</data>
    </edge>
    <edge source="&quot;NVAR MODEL&quot;" target="&quot;TRAINER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Trainer sets up and trains the NVAR Model to analyze and predict the Time Series data."</data>
      <data key="d6">c838b1b4744bc0f400abf85f791950cf</data>
    </edge>
    <edge source="&quot;SINE WAVE&quot;" target="&quot;INPUT TIMESERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Sine Wave is used as an example of Input Timeseries in the text."</data>
      <data key="d6">5366a81a025c098744b5d6f1432c2fbc</data>
    </edge>
    <edge source="&quot;SINE WAVE&quot;" target="&quot;TARGET TIMESERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Sine Wave is used as an example of Target Timeseries in the text."</data>
      <data key="d6">5366a81a025c098744b5d6f1432c2fbc</data>
    </edge>
    <edge source="&quot;INPUT TIMESERIES&quot;" target="&quot;TARGET TIMESERIES&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Input Timeseries and Target Timeseries are used together to train Ridge to solve a prediction task."</data>
      <data key="d6">5366a81a025c098744b5d6f1432c2fbc</data>
    </edge>
    <edge source="&quot;ESN_MODEL&quot;" target="&quot;RESERVOIRPY.NODES.RESERVOIR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The reservoirpy.nodes.Reservoir organization is used to construct the esn_model."</data>
      <data key="d6">751b176a8d6149a853e597c65a6fe0cf</data>
    </edge>
    <edge source="&quot;ESN_MODEL&quot;" target="&quot;RESERVOIRPY.MAT&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The reservoirpy.mat module is likely used in the construction of the esn_model."</data>
      <data key="d6">751b176a8d6149a853e597c65a6fe0cf</data>
    </edge>
    <edge source="&quot;NORMAL_W&quot;" target="&quot;RESERVOIRPY.NODES.RESERVOIR&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The normal_w function is used to generate weights for the reservoirpy.nodes.Reservoir organization."</data>
      <data key="d6">751b176a8d6149a853e597c65a6fe0cf</data>
    </edge>
    <edge source="&quot;TQDM&quot;" target="&quot;DATA SCIENTIST&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The data scientist uses the tqdm library to monitor the progress of data processing."</data>
      <data key="d6">9fdaabd6c7e893a275a3848c10007477</data>
    </edge>
    <edge source="&quot;ROBOT PERFORMANCE EVALUATION&quot;" target="&quot;ROBOT PERFORMANCE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Robot Performance Evaluation is a process used to assess Robot Performance."</data>
      <data key="d6">e7d249cdab85dc69b631d43ac6b62915</data>
    </edge>
    <edge source="&quot;CHAPTER 5&quot;" target="&quot;IPYTHON&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"IPython is used in Chapter 5 for displaying audio and visualizing data."</data>
      <data key="d6">1e8ee805d22cd143d2372d300997d253</data>
    </edge>
    <edge source="&quot;CHAPTER 5&quot;" target="&quot;PANDAS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Pandas is used in Chapter 5 for data manipulation and analysis."</data>
      <data key="d6">1e8ee805d22cd143d2372d300997d253</data>
    </edge>
    <edge source="&quot;OBJECTIVE FUNCTIONS&quot;" target="&quot;RESERVOIR COMPUTING MODELS&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Objective Functions are used to evaluate the performance of Reservoir Computing Models and guide the search for optimal parameters."</data>
      <data key="d6">0113164912437e96423379cb9c039f56</data>
    </edge>
    <edge source="&quot;LPC (CEPSTRA)&quot;" target="&quot;SPEAKER DATA&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"LPC (cepstra) is used to analyze and extract features from speaker data."</data>
      <data key="d6">688ebc7151bc148ac24dc7e2727d7afe</data>
    </edge>
    <edge source="&quot;LINEAR MODEL&quot;" target="&quot;LASSO&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"Lasso is a type of linear model that falls under the broader category of Linear Models."</data>
      <data key="d6">eebc9d7d2b66e3898b7d068c38fd200f</data>
    </edge>
    <edge source="&quot;RESERVOIR MODEL&quot;" target="&quot;SK RIDGE&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir Model uses SK Ridge as one of its components."</data>
      <data key="d6">0970cd32ce54f6ee1180ab237fdcefe1</data>
    </edge>
    <edge source="&quot;RESERVOIR MODEL&quot;" target="&quot;SK LOGISTIC&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir Model uses SK Logistic as one of its components."</data>
      <data key="d6">0970cd32ce54f6ee1180ab237fdcefe1</data>
    </edge>
    <edge source="&quot;RESERVOIR MODEL&quot;" target="&quot;SK PERCEPTRON&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir Model uses SK Perceptron as one of its components."</data>
      <data key="d6">0970cd32ce54f6ee1180ab237fdcefe1</data>
    </edge>
    <edge source="&quot;RESERVOIR MODEL&quot;" target="&quot;SPEAKER&quot;">
      <data key="d4">1.0</data>
      <data key="d5">"The Reservoir Model is used to predict or classify the Speaker."</data>
      <data key="d6">0970cd32ce54f6ee1180ab237fdcefe1</data>
    </edge>
  </graph>
</graphml>