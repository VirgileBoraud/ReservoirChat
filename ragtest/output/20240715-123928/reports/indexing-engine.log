12:39:28,702 graphrag.config.read_dotenv INFO Loading pipeline .env file
12:39:28,706 graphrag.index.cli INFO using default configuration: {
    "llm": {
        "api_key": "REDACTED, length 9",
        "type": "openai_chat",
        "model": "TechxGenus/Codestral-22B-v0.1-GPTQ",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8000/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 25000,
        "requests_per_minute": 100,
        "max_retries": 5,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "./ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "output/${timestamp}/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "output/${timestamp}/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "REDACTED, length 9",
            "type": "openai_embedding",
            "model": "nomic-ai/nomic-embed-text-v1.5-GGUF",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:1234/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 300,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "REDACTED, length 9",
            "type": "openai_chat",
            "model": "TechxGenus/Codestral-22B-v0.1-GPTQ",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 25000,
            "requests_per_minute": 100,
            "max_retries": 5,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 0,
        "strategy": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "REDACTED, length 9",
            "type": "openai_chat",
            "model": "TechxGenus/Codestral-22B-v0.1-GPTQ",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 25000,
            "requests_per_minute": 100,
            "max_retries": 5,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "REDACTED, length 9",
            "type": "openai_chat",
            "model": "TechxGenus/Codestral-22B-v0.1-GPTQ",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 25000,
            "requests_per_minute": 100,
            "max_retries": 5,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": null,
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "REDACTED, length 9",
            "type": "openai_chat",
            "model": "TechxGenus/Codestral-22B-v0.1-GPTQ",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 25000,
            "requests_per_minute": 100,
            "max_retries": 5,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 0,
        "strategy": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
12:39:28,707 graphrag.index.create_pipeline_config INFO skipping workflows 
12:39:28,710 graphrag.index.run INFO Running pipeline
12:39:28,710 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at ragtest/output/20240715-123928/artifacts
12:39:28,712 graphrag.index.input.load_input INFO loading input from root_dir=input
12:39:28,712 graphrag.index.input.load_input INFO using file storage for input
12:39:28,713 graphrag.index.storage.file_pipeline_storage INFO search ragtest/input for files matching .*\.txt$
12:39:28,714 graphrag.index.input.text INFO found text files from input, found [('Time_Series_Wikipedia.txt', {}), ('ESN_Wikipedia.txt', {}), ('tuto1.txt', {}), ('tuto4.txt', {}), ('Q&A_format.txt', {}), ('tuto6.txt', {}), ('TH2022_ReservoirPy_RC_Tool_Formatted (2).txt', {}), ('ESN_Scholarpedia.txt', {}), ('RNN_Wikipedia.txt', {}), ('tuto3.txt', {}), ('codes.txt', {}), ('tuto2.txt', {}), ('tuto5.txt', {}), ('reservoirPy_intro.txt', {}), ('RNN_Scholarpedia.txt', {})]
12:39:28,723 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
12:39:28,723 graphrag.index.run INFO Final # of rows loaded: 15
12:39:28,826 graphrag.index.run INFO Running workflow: create_base_text_units...
12:39:28,826 graphrag.index.run INFO dependencies for create_base_text_units: []
12:39:28,830 datashaper.workflow.workflow INFO executing verb orderby
12:39:28,832 datashaper.workflow.workflow INFO executing verb zip
12:39:28,836 datashaper.workflow.workflow INFO executing verb aggregate_override
12:39:28,841 datashaper.workflow.workflow INFO executing verb chunk
12:39:29,96 datashaper.workflow.workflow INFO executing verb select
12:39:29,100 datashaper.workflow.workflow INFO executing verb unroll
12:39:29,105 datashaper.workflow.workflow INFO executing verb rename
12:39:29,111 datashaper.workflow.workflow INFO executing verb genid
12:39:29,125 datashaper.workflow.workflow INFO executing verb unzip
12:39:29,131 datashaper.workflow.workflow INFO executing verb copy
12:39:29,136 datashaper.workflow.workflow INFO executing verb filter
12:39:29,150 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
12:39:29,282 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
12:39:29,282 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
12:39:29,282 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
12:39:29,299 datashaper.workflow.workflow INFO executing verb entity_extract
12:39:29,323 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8000/v1
12:39:29,360 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for TechxGenus/Codestral-22B-v0.1-GPTQ: TPM=25000, RPM=100
12:39:29,360 graphrag.index.llm.load_llm INFO create concurrency limiter for TechxGenus/Codestral-22B-v0.1-GPTQ: 25
12:39:30,161 datashaper.workflow.workflow INFO executing verb merge_graphs
12:39:30,427 datashaper.workflow.workflow INFO executing verb snapshot_rows
12:39:30,434 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
12:39:30,570 graphrag.index.run INFO Running workflow: create_summarized_entities...
12:39:30,570 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
12:39:30,571 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
12:39:30,590 datashaper.workflow.workflow INFO executing verb summarize_descriptions
12:39:31,826 datashaper.workflow.workflow INFO executing verb snapshot_rows
12:39:31,835 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
12:39:31,971 graphrag.index.run INFO Running workflow: create_base_entity_graph...
12:39:31,971 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
12:39:31,972 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
12:39:31,991 datashaper.workflow.workflow INFO executing verb cluster_graph
12:39:33,197 datashaper.workflow.workflow INFO executing verb snapshot_rows
12:39:33,215 datashaper.workflow.workflow INFO executing verb snapshot_rows
12:39:33,235 datashaper.workflow.workflow INFO executing verb select
12:39:33,241 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
12:39:33,485 graphrag.index.run INFO Running workflow: create_final_entities...
12:39:33,485 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
12:39:33,486 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
12:39:33,534 datashaper.workflow.workflow INFO executing verb unpack_graph
12:39:34,50 datashaper.workflow.workflow INFO executing verb rename
12:39:34,61 datashaper.workflow.workflow INFO executing verb select
12:39:34,72 datashaper.workflow.workflow INFO executing verb dedupe
12:39:34,83 datashaper.workflow.workflow INFO executing verb rename
12:39:34,94 datashaper.workflow.workflow INFO executing verb filter
12:39:34,142 datashaper.workflow.workflow INFO executing verb text_split
12:39:34,174 datashaper.workflow.workflow INFO executing verb drop
12:39:34,187 datashaper.workflow.workflow INFO executing verb merge
12:39:34,495 datashaper.workflow.workflow INFO executing verb text_embed
12:39:34,497 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:1234/v1
12:39:34,549 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for nomic-ai/nomic-embed-text-v1.5-GGUF: TPM=0, RPM=0
12:39:34,549 graphrag.index.llm.load_llm INFO create concurrency limiter for nomic-ai/nomic-embed-text-v1.5-GGUF: 25
12:39:34,659 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 1577 inputs via 1577 snippets using 99 batches. max_batch_size=16, max_tokens=8191
12:42:34,801 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"ACCURACY_SCORE": The "ACCURACY_SCORE" is a versatile function used in the evaluation of various machine learning models. It is primarily employed to compare the predictions made by these models with the actual values, thereby assessing their performance. This metric is used to evaluate the performance of both trained ESN models and machine learning models in general. It compares the predicted outputs of a model to the actual targets, providing a measure of the model\'s accuracy.', '"DR. STEPHEN GROSSBERG":"Dr. Stephen Grossberg is a researcher at Boston University, MA, who focuses on recurrent neural networks."', '"BOSTON UNIVERSITY":"Boston University is an educational institution where Dr. Stephen Grossberg works on recurrent neural networks."', '"MA":"MA is a geographical location, likely referring to Massachusetts, where Boston University is located."', '"ARTIFICIAL NEURAL NETWORKS (ARNN)":"Artificial Neural Networks (aRNN) are a type of neural network used in technological applications, mentioned in the text."', '"BIOLOGICAL RECURRENT NEURAL NETWORKS (BRNN)":"Biological Recurrent Neural Networks (bRNN) are a type of neural network found in the brain, with feedback signals occurring in neurons within a single processing layer or between multiple processing layers."', '"MCCULLOCH-PITTS MODEL": The McCulloch-Pitts Model is a prominent model in the field of threshold logic systems that describes the interaction of nodes in a network. It was developed by McCulloch and Pitts and has had a significant influence on the field of neural networks and the development of digital computers. This model is a classical representation that provides insights into the functioning of neural networks.', '"BINARY SYSTEMS":"Binary Systems are a type of recurrent neural network that were inspired by neurophysiological observations showing that signals between many neurons are carried by all-or-none spikes."', '"LINEAR SYSTEMS":"Linear Systems are a type of recurrent neural network that use linear operations to combine inputs and produce outputs."', '"CONTINUOUS-NONLINEAR SYSTEMS":"Continuous-Nonlinear Systems are a type of recurrent neural network that use continuous and nonlinear operations to combine inputs and produce outputs."', '"SHORT-TERM MEMORY (STM)":"Short-Term Memory (STM) refers to the activities or traces of nodes in a network, which are used to store and process information."', '"NODES": Nodes are processing units in a network that perform computations and interact with each other. These units are capable of receiving and transmitting data, playing a crucial role in the network\'s functionality.', '"GROSSBERG": Stephen Grossberg is a renowned cognitive scientist and neural network researcher known for his significant contributions to the field. He is particularly recognized for proposing the on-center off-surround network as a solution to the noise-saturation dilemma. Additionally, he is a co-author of a study that introduces the TELOS Model and its components. Grossberg\'s research has also focused on understanding Working Memory and its representation, and he has made notable contributions to the development of neural network learning algorithms such as Outstar Learning and Instar Learning. He is also known for his work on Adaptive Resonance Theory and the development of the Additive Model. Grossberg has been mentioned in the text in relation to various models, including the LAMINART Family, LIST PARSE, and TELOS models. He has also contributed to the understanding of spatial pattern learning and signal transmission between cells. Grossberg\'s research has been cited in the context of Item-and-Order working memory, and his work on working memory networks and list chunking networks is referenced in the text. He has developed the Competitive Learning (CL) and Adaptive Resonance Theory (ART) models and has been mentioned in relation to shunting dynamics. Grossberg\'s work on Self-Organizing Avalanches and mathematical analyses of learning processes is also notable. He has been mentioned as a co-author in multiple references related to the neural network and its components.', '"JOHN VON NEUMANN":"John von Neumann is a mathematician and computer scientist who contributed to the development of the digital computer, influenced by the McCulloch-Pitts Model."', '"FRANK ROSENBLATT":"Frank Rosenblatt is a psychologist and neuroscientist who developed the continuous-time STM equation used in the classical Perceptron model."', '"WARREN MCCULLOCH":"Warren McCulloch is a neurophysiologist who, along with Walter Pitts, developed the McCulloch-Pitts Model, a foundational model in neural networks."']}
12:42:34,802 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"HODGKIN":"Hodgkin is a scientist who developed a mathematical model for the shunting dynamics of individual neurons."', '"STM TRACES": "STM Traces" are components of the Generalized Additive System that represent the activities of the system. These traces are also known as short-term memory traces, which are signals or activities that are bounded within an interval and interact with balanced positive and negative signals. They play a role in representing activities in a neural system.', '"SHUNTING MODEL": The Shunting Model is a mathematical component of the Liapunov function proposed by Cohen and Grossberg. It is a variant of the STM Equation, featuring parameters C and F, and E set to 0 or a specific value. The Shunting Model is also a mathematical model used to describe the behavior of cells in a network and is employed to explain associative learning of temporal order information in serial learning paradigms. It is a type of network mentioned in the text and is included in the Cohen-Grossberg Model systems. The Shunting Model approximates the Additive Model in certain conditions, utilizing a combination of shunting and additive terms.', '"WILSON-COWAN MODEL": The Wilson-Cowan Model is a mathematical model developed by Wilson and Cowan. This model uses a sigmoid of sums that is multiplied by a shunting term, as described in the text. Additionally, the model incorporates a combination of shunting and additive terms, but these are presented in a different formulation compared to the initial description.', '"WILSON":"Wilson is a scientist who developed the Wilson-Cowan model with Cowan."', '"COWAN": Cowan is a distinguished researcher and scientist who has made significant contributions to the field. He is known for his work on experimental data supporting the existence of a four plus or minus one WM capacity limit when LTM and grouping influences are minimized. Additionally, he is credited for developing the Wilson-Cowan model in collaboration with Wilson.', '"MTM": "MTM, or Medium-Term Memory, is a type of memory system mentioned in the text that stores and retrieves information for a moderate duration. It is described as a system or model that includes MTM traces or habituative transmitter gates. Despite not being explicitly mentioned, the term \'MTM\' could refer to Medium-Term Memory based on the context provided."', '"FAST INHIBITION":"Fast Inhibition is a feature of the STM Equation that assumes inhibitory interneurons respond instantaneously to their inputs."', '"SLOWER INHIBITION":"Slower Inhibition is a feature of the STM Equation that considers the temporal evolution of inhibitory interneuronal activities."', '"COHEN AND GROSSBERG": Cohen and Grossberg are renowned researchers who have made significant contributions to various fields. They are best known for their work in deriving a Liapunov function for a generalization of the Additive and Shunting Models, which has been instrumental in proving the existence of global equilibria. Additionally, they have developed the Cohen-Grossberg Liapunov function and have contributed to the study of competitive systems, particularly through the development of the Masking Field model. Their research has also included the proposal of a Liapunov function that encompasses both the Additive Model and the Shunting Model.', '"MEDIUM-TERM MEMORY (MTM)":"Medium-term memory (MTM) is described as activity-dependent habituation, often called habituative transmitter gates, which plays multiple roles in the described system."', '"ADDITIVE AND SHUNTING MODELS": "Additive and Shunting Models are neural network architectures mentioned in the text. These models are also generalized by Cohen and Grossberg, and they are referred to in the research as a specific network structure."\n\nThe provided descriptions all refer to the same entities, "ADDITIVE AND SHUNTING MODELS," which are neural network architectures. The descriptions mention that these models are generalized by Cohen and Grossberg and that they are used in the research as a specific network structure. Therefore, the comprehensive description is that Additive and Shunting Models are neural network architectures that have been generalized by Cohen and Grossberg and are mentioned in the text as a specific network structure used in research.', '"LIAPUNOV FUNCTION": The Liapunov function is a mathematical concept that serves multiple purposes. It is primarily used to prove the global convergence of a system, but it is also employed to analyze the stability of dynamic systems, including the Cohen-Grossberg Model. This mathematical concept plays a significant role in the analysis of system stability, making it a valuable tool in various mathematical and scientific disciplines.', '"COHEN-GROSSBERG MODEL": The Cohen-Grossberg Model is a neural network model developed by Cohen and Grossberg in 1983. It is a system of equations used to study the behavior of networks, with a focus on symmetry in interaction coefficients. The model is also mentioned in the text and includes the Liapunov function. It was adapted by Kosko.', '"MEDIUM-TERM MEMORY":"Medium-term Memory, or activity-dependent habituation, is a concept mentioned in the text that plays multiple roles in neural networks."', '"GATED DIPOLE OPPONENT PROCESSING NETWORK":"Gated Dipole Opponent Processing Network is a type of neural network mentioned in the text that enables reset events and arousal bursts."']}
12:42:34,804 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"SPATIAL PATTERNS":"Spatial patterns refer to the distribution of information across networks of neurons, such as in a picture or a scene."', '"TEMPORAL PATTERNS":"Temporal patterns refer to the sequence of signals over time, such as in speech."', '"NEURONS": Neurons are biological cells that process information and are the individual units in a neural network or reservoir. They are capable of processing variable input intensities and can evolve in time following the evolution of the input timeseries. Neurons are mentioned in the context of biological concepts and are also referred to as the individual units within a reservoir.', '"BRAINS":"Brains are mentioned as having evolved network designs in neurons to process variable input intensities."', '"NOISE-SATURATION DILEMMA": The Noise-Saturation Dilemma is a complex issue that arises when trying to maintain neuronal sensitivity to input patterns while dealing with varying input intensities. This challenge involves balancing the need to be sensitive to relative inputs to avoid saturation, which can lead to loss of information, with the need to maintain neuronal sensitivity to input patterns. This dilemma is characterized by the struggle to maintain sensitivity to input patterns while avoiding saturation when the total input size varies significantly.', '"ON-CENTER OFF-SURROUND NETWORK": The "ON-CENTER OFF-SURROUND NETWORK" is a type of network that obeys the membrane or shunting equations of neurophysiology. This network design is characterized by its ability to enable neurons to retain sensitivity to the relative sizes of their inputs across the network, which contributes to its ubiquity in the brain. This network design helps explain its presence and function in the brain.', '"MEMBRANE":"Membrane, or shunting, is a concept mentioned in the context of neuronal network design."', '"SPATIAL PATTERN":"A spatial pattern of inputs that is processed by a network of cells."', '"NETWORK OF CELLS":"A network of cells that processes a spatial pattern of inputs, with each cell maintaining sensitivity to the relative size of its inputs."', '"RELATIVE SIZE":"The constant relative size, or reflectance, of each input in the spatial pattern."', '"TOTAL INPUT SIZE":"The variable total input size that is the sum of the relative sizes of all inputs in the spatial pattern."', '"CELL (V_I)": "Cell (v_i) is a biological unit that maintains sensitivity to its input size (I_i) and competes with other inputs (I_k) to activate itself. Additionally, each cell in the network of cells is designed to maintain sensitivity to the relative size of its inputs." This description highlights that Cell (v_i) is a biological unit that responds to its input size and competes with other inputs for activation. Furthermore, it is mentioned that each cell in the network is designed to be sensitive to the relative size of its inputs, adding to the understanding of its function within the network.', '"INPUT (I_I)": Each input (I_i) in the spatial pattern is a component that is processed by the network of cells. These inputs contribute to the total input size that each cell (v_i) receives, thereby influencing the sensitivity of the cell. In other words, the size of the input (I_i) determines the level of stimulation each cell receives, which in turn affects its sensitivity and response.', '"INPUT (I_K)":"Input (I_k) are the additional inputs that compete with Input (I_i) to inhibit the activation of cell (v_i)."', '"KUFFLER": Kuffler is a renowned researcher who has made significant contributions to the field of vision research. In 1953, he reported on-center off-surround anatomies in the cat retina. This groundbreaking work has been a cornerstone in understanding the structure and function of the retina.', '"ON-CENTER OFF-SURROUND ANATOMY":"On-Center Off-Surround Anatomy is a neural structure that activates and inhibits cells through time, with each cell having excitable sites that can spontaneously decay."']}
12:42:34,806 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"BRIGHTNESS CONSTANCY":"Brightness Constancy is a property of visual perception that ensures that objects appear the same brightness regardless of their surrounding luminance."', '"BRIGHTNESS CONTRAST":"Brightness Contrast is a visual phenomenon that occurs when increasing the luminance of inputs to the off-surround makes the on-center look darker."', '"NORMALIZATION PROPERTY":"Normalization Property is a property that underlies many properties of limited capacity processing in the brain, such as visual perception and cognition."', '"WORKING MEMORY": Working Memory is a cognitive system that temporarily stores and manipulates information. This system has a limited capacity and activity redistribution when new items are stored. It is responsible for holding and processing information needed for immediate use in cognitive tasks. The model developed by Grossberg (1978a, 1978b) further explains the storage and retrieval of items in short-term memory, taking into account factors such as activity levels and noise. In summary, Working Memory is a limited capacity system in the brain that holds and manipulates information for immediate use, with consideration for activity levels and noise in information processing.', '"SYSTEM":', '"LIMITED CAPACITY PROCESSING":', '"SHUNTING NETWORK":"The Shunting Network is a model used to describe the activity of neurons, with properties such as Weber law processing, adaptation level processing, and edge and spatial frequency processing."', '"NEUROPHYSIOLOGY":"Neurophysiology is the scientific discipline that studies the electrical and chemical properties of neurons and their role in information processing."', '"WERBLIN":"Werblin is a researcher who has studied the retina of the mudpuppy Necturus, finding a shift property similar to that described in the shunting network equations."', '"RETINA":"The Retina is a part of the eye that contains photoreceptor cells, which are sensitive to light and transmit visual information to the brain."', '"MUDPUPPY NECTURUS":"The Mudpuppy Necturus is an aquatic animal whose retina has been studied for its shift property in response to off-surround input."', '"HODGKIN AND HUXLEY": Hodgkin and Huxley are a renowned research team in the field of neurophysiology. They are best known for their groundbreaking work on the membrane equation, which has significantly contributed to our understanding of neurophysiology. Additionally, they are credited with proposing a model of signal propagation in axons, further expanding our knowledge in this area.', '"SHUNTING NETWORK EQUATIONS":"Shunting Network Equations are a mathematical model mentioned in the text, used to generate various properties such as Weber law processing and edge and spatial frequency processing."', '"MEMBRANE EQUATION": The Membrane Equation is a mathematical model that is mentioned in the text. It is primarily used to describe the voltage of a cell. This model was developed by Hodgkin and Huxley and is based on the concept of various conductances and saturation voltages. The descriptions provided are consistent, indicating that the Membrane Equation is a comprehensive mathematical model that describes the voltage of a cell, taking into account different conductances and saturation voltages.', '"SODIUM CHANNEL":"Sodium Channel is a type of ion channel that contributes to the membrane equation, representing the excitatory saturation voltage."', '"POTASSIUM CHANNEL":"Potassium Channel is a type of ion channel that contributes to the membrane equation, representing the inhibitory saturation voltage."']}
12:42:34,807 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"PERIRHINAL CORTEX (PRC)":"Perirhinal Cortex (PRC) is a region of the brain involved in spatial navigation and memory."', '"PARAHIPPOCAMPAL CORTEX (PHC)":"Parahippocampal Cortex (PHC) is a region of the brain involved in spatial navigation and memory."', '"GRIDPLACEMAP":"GridPlaceMap is a model that simulates the formation of a grid cell representation of space."', '"VISUAL CORTICES V1, V2, AND V4":', '"POSTERIOR AND ANTERIOR INFEROTEMPORAL CORTEX (ITA, ITP)":', '"BRAIN REGIONS":', '"NEURONAL LEARNING":"Neuronal Learning is a process described in the text that involves the interaction of STM and LTM."', '"MATHEMATICAL THEOREMS": Mathematical Theorems play a significant role in enhancing the learning capabilities of the Generalized Additive RNNs architecture. They are mentioned as a foundation for the learning process, providing a solid theoretical basis that supports the architecture\'s performance. Overall, Mathematical Theorems are crucial in the development and understanding of the Generalized Additive RNNs architecture.', '"GENERALIZED ADDITIVE RNNS":"Generalized Additive RNNs is a type of architecture that includes interactions between STM and LTM, allowing it to learn from its environments."', '"SPATIAL PATTERN LEARNING": Spatial Pattern Learning is a concept that encompasses the ability of certain anatomies to learn patterns. This concept is further described as the process of identifying and learning patterns in spatial data, such as images or signals. In essence, Spatial Pattern Learning involves the recognition and understanding of patterns within spatial information.', '"SIGNAL TRANSMISSION":"Signal Transmission is a concept mentioned in the text, referring to the process of transmitting signals between cells."', '"KATZ":"Katz is a researcher who contributed to the understanding of signal density and its effect on postsynaptic cells."', '"SIGNAL PROPAGATION":"Signal Propagation refers to the transmission of signals along axons and their effect on postsynaptic cells."', '"AXON":"Axon is a type of nerve cell that transmits signals from the cell body to other cells."', '"SYNAPTIC KNOB":"Synaptic Knob is a structure on the axon where signals interact with postsynaptic cells."', '"IONIC FLUXES":"Ionic Fluxes refer to the movement of ions in response to signals, contributing to signal transmission."']}
12:42:34,809 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"UNCONDITIONED STIMULI (US)":"Unconditioned Stimuli (US) are particular spatial patterns that are correlated with Conditioned Stimuli in the context of the Unbiased Spatial Pattern Learning Theorem."', '"PAVLOVIAN CONDITIONING":"Pavlovian Conditioning is a form of associative learning that involves pairing a stimulus with a response to create a conditioned response."', '"CS AND US":"CS and US are likely abbreviations for Conditioned Stimulus and Unconditioned Stimulus, which are components of Pavlovian Conditioning."', '"GENERALIZED ADDITIVE MODEL":"The Generalized Additive Model is a statistical framework that allows for the modeling of complex relationships between variables."', '"OUTSTAR LEARNING THEOREM": The Outstar Learning Theorem is a learning theory proposed by Stanley Grossberg. This theory suggests how a series of Outstars can learn an arbitrary spatiotemporal pattern. Additionally, it has been mentioned that the Outstar Learning Theorem is a specific case of the Generalized Additive Model. This means that the theory can be understood within the broader context of the Generalized Additive Model, and it adds to the understanding of this model by focusing on a specific application.', '"GROSSBERG AND SOMERS":"Grossberg and Somers is a collaboration between researchers Grossberg and Somers, who have published on the topic of resynchronizing activities in networks."', '"GROSSBERG AND GRUNEWALD":"Grossberg and Grunewald is a collaboration between researchers Grossberg and Grunewald, who have published on the topic of resynchronizing activities in networks."', '"YAZDANBAKHSH AND GROSSBERG":"Yazdanbakhsh and Grossberg is a collaboration between researchers Yazdanbakhsh and Grossberg, who have published on the topic of resynchronizing activities in laminar cortical circuits."', '"STANLEY GROSSBERG":"Stanley Grossberg is a researcher known for his contributions to neural networks and learning theories, including the Outstar Learning Theorem and the Sparse Stable Category Learning Theorem."', '"SPARSE STABLE CATEGORY LEARNING THEOREM":"The Sparse Stable Category Learning Theorem is another learning theory proposed by Stanley Grossberg, which occurs using the dual network to the Outstar, namely the Instar. This theorem involves multiple Instars competing with each other via a RCF to form a Competitive Learning or Self-Organizing Map network."', '"INSTAR":"Instar is the dual network to the Outstar, which competes with other Instars to form a Competitive Learning or Self-Organizing Map network."', '"LEARNING THEORIES":"Learning Theories are theoretical frameworks that explain how systems can learn and adapt to new information or patterns."', '"COMPETITIVE LEARNING": "Competitive Learning" is a method mentioned in the text, primarily a concept or technique used in research. This learning model involves a competitive process where neurons in a network compete to respond to input patterns. It is a type of unsupervised learning, meaning that there is no external guidance or supervision during the learning process. In this model, neurons compete for the right to respond to input patterns, enhancing their ability to learn and adapt.', '"SELF-ORGANIZING MAP": "Self-Organizing Map" is a type of artificial neural network that is designed to learn and represent the structure of input data. It typically accomplishes this by organizing the input data in a lower-dimensional space. Additionally, Self-Organizing Map is a learning model that dynamically organizes input data, allowing it to adapt and learn from the patterns and relationships present in the data.', '"COMPETITIVE LEARNING OR SELF-ORGANIZING MAP NETWORK":"The Competitive Learning or Self-Organizing Map Network is a type of network mentioned in the text, which is formed by multiple Instars competing via a RCF."', '"KOSKO": "Kosko is a researcher who has made significant contributions to the field. He is mentioned in the text for adapting the Cohen-Grossberg Model, and he has also referred to the equation in (39) as the signal Hebb law, although it does not fully obey Hebb\'s property. Additionally, Kosko has adapted the Cohen-Grossberg model and Liapunov function to define a system that combines Short-Term Memory (STM) and Long-Term Memory (LTM)."']}
12:42:34,810 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"RECURRENT CONNECTIONS":"Recurrent Connections are connections in a network that allow for the storage of inputs in short-term memory after the inputs shut off."', '"GROSSBERG AND PEARSON":"Grossberg and Pearson are mentioned as the authors of a model that generalizes Item-and-Order working memories to include rank information, permitting the temporary storage of repeated items in a list."', '"ITEM-ORDER-RANK WORKING MEMORIES":', '"FRONTAL CORTEX": The Frontal Cortex is a brain region that plays a significant role in various cognitive functions, including working memory. Additionally, it is known to modulate song performance in songbirds and higher species, suggesting its involvement in flexible performance. This region\'s role in song production and cognitive functions highlights its complexity and versatility in the brain\'s functional landscape.', '"ITEM-AND-ORDER WORKING MEMORY":"Item-and-Order Working Memory is a cognitive model that stores and recalls lists in a specific order, influenced by primacy, recency, and bowed activation gradients."', '"ITEM-ORDER-RANK WORKING MEMORY":"Item-Order-Rank Working Memory is a variant of Item-and-Order Working Memory that includes positional information, allowing the temporary storage of repeated items in a list."', '"FREE RECALL":"Free Recall is a cognitive process where individuals try to recall a once-heard list in any order, typically showing patterns of primacy and recency."', '"BRADSKI ET AL.": Bradski et al. are a group of researchers who have made significant contributions to the field. They are known for defining the STORE model, a family of Item-and-Order RNNs that exhibits mathematically provable primacy, recency, and bowed gradient properties. Additionally, they have mathematically proven the patterns of activation in an Item-and-Order working memory.', '"BOARDMAN AND BULLOCK":"Boardman and Bullock are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON AND HARTLEY":"Houghton and Hartley are researchers who have contributed to the Item-and-Order working memory design."', '"PAGE AND NORRIS":"Page and Norris are researchers who have developed variants of the Item-and-Order working memory design."', '"RHODES ET AL.":"Rhodes et al. are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON":"Houghton is a researcher who has referred to Item-and-Order models as Competitive Queuing models."', '"FARRELL AND LEWANDOWSKY":"Farrell and Lewandowsky are researchers who have provided experimental support for Item-and-Order working memory properties."', '"AVERBECK ET AL.": Averbeck et al. are a research group known for their contributions to the field of neurophysiology. They have reported neurophysiological evidence supporting the primacy gradient in Item-and-Order working memory, as well as evidence of primacy gradients and inhibition of the most active cell in sequential copying movements. Their research has significantly advanced our understanding of these areas in cognitive science.', '"ITEM-AND-ORDER WM":']}
12:42:36,539 httpx INFO HTTP Request: POST http://localhost:1234/v1/embeddings "HTTP/1.1 200 OK"
12:42:36,602 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 1 retries took 0.36999999999898137. input_tokens=808, output_tokens=0
12:45:35,908 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"BRIGHTNESS CONSTANCY":"Brightness Constancy is a property of visual perception that ensures that objects appear the same brightness regardless of their surrounding luminance."', '"BRIGHTNESS CONTRAST":"Brightness Contrast is a visual phenomenon that occurs when increasing the luminance of inputs to the off-surround makes the on-center look darker."', '"NORMALIZATION PROPERTY":"Normalization Property is a property that underlies many properties of limited capacity processing in the brain, such as visual perception and cognition."', '"WORKING MEMORY": Working Memory is a cognitive system that temporarily stores and manipulates information. This system has a limited capacity and activity redistribution when new items are stored. It is responsible for holding and processing information needed for immediate use in cognitive tasks. The model developed by Grossberg (1978a, 1978b) further explains the storage and retrieval of items in short-term memory, taking into account factors such as activity levels and noise. In summary, Working Memory is a limited capacity system in the brain that holds and manipulates information for immediate use, with consideration for activity levels and noise in information processing.', '"SYSTEM":', '"LIMITED CAPACITY PROCESSING":', '"SHUNTING NETWORK":"The Shunting Network is a model used to describe the activity of neurons, with properties such as Weber law processing, adaptation level processing, and edge and spatial frequency processing."', '"NEUROPHYSIOLOGY":"Neurophysiology is the scientific discipline that studies the electrical and chemical properties of neurons and their role in information processing."', '"WERBLIN":"Werblin is a researcher who has studied the retina of the mudpuppy Necturus, finding a shift property similar to that described in the shunting network equations."', '"RETINA":"The Retina is a part of the eye that contains photoreceptor cells, which are sensitive to light and transmit visual information to the brain."', '"MUDPUPPY NECTURUS":"The Mudpuppy Necturus is an aquatic animal whose retina has been studied for its shift property in response to off-surround input."', '"HODGKIN AND HUXLEY": Hodgkin and Huxley are a renowned research team in the field of neurophysiology. They are best known for their groundbreaking work on the membrane equation, which has significantly contributed to our understanding of neurophysiology. Additionally, they are credited with proposing a model of signal propagation in axons, further expanding our knowledge in this area.', '"SHUNTING NETWORK EQUATIONS":"Shunting Network Equations are a mathematical model mentioned in the text, used to generate various properties such as Weber law processing and edge and spatial frequency processing."', '"MEMBRANE EQUATION": The Membrane Equation is a mathematical model that is mentioned in the text. It is primarily used to describe the voltage of a cell. This model was developed by Hodgkin and Huxley and is based on the concept of various conductances and saturation voltages. The descriptions provided are consistent, indicating that the Membrane Equation is a comprehensive mathematical model that describes the voltage of a cell, taking into account different conductances and saturation voltages.', '"SODIUM CHANNEL":"Sodium Channel is a type of ion channel that contributes to the membrane equation, representing the excitatory saturation voltage."', '"POTASSIUM CHANNEL":"Potassium Channel is a type of ion channel that contributes to the membrane equation, representing the inhibitory saturation voltage."']}
12:45:35,910 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"ACCURACY_SCORE": The "ACCURACY_SCORE" is a versatile function used in the evaluation of various machine learning models. It is primarily employed to compare the predictions made by these models with the actual values, thereby assessing their performance. This metric is used to evaluate the performance of both trained ESN models and machine learning models in general. It compares the predicted outputs of a model to the actual targets, providing a measure of the model\'s accuracy.', '"DR. STEPHEN GROSSBERG":"Dr. Stephen Grossberg is a researcher at Boston University, MA, who focuses on recurrent neural networks."', '"BOSTON UNIVERSITY":"Boston University is an educational institution where Dr. Stephen Grossberg works on recurrent neural networks."', '"MA":"MA is a geographical location, likely referring to Massachusetts, where Boston University is located."', '"ARTIFICIAL NEURAL NETWORKS (ARNN)":"Artificial Neural Networks (aRNN) are a type of neural network used in technological applications, mentioned in the text."', '"BIOLOGICAL RECURRENT NEURAL NETWORKS (BRNN)":"Biological Recurrent Neural Networks (bRNN) are a type of neural network found in the brain, with feedback signals occurring in neurons within a single processing layer or between multiple processing layers."', '"MCCULLOCH-PITTS MODEL": The McCulloch-Pitts Model is a prominent model in the field of threshold logic systems that describes the interaction of nodes in a network. It was developed by McCulloch and Pitts and has had a significant influence on the field of neural networks and the development of digital computers. This model is a classical representation that provides insights into the functioning of neural networks.', '"BINARY SYSTEMS":"Binary Systems are a type of recurrent neural network that were inspired by neurophysiological observations showing that signals between many neurons are carried by all-or-none spikes."', '"LINEAR SYSTEMS":"Linear Systems are a type of recurrent neural network that use linear operations to combine inputs and produce outputs."', '"CONTINUOUS-NONLINEAR SYSTEMS":"Continuous-Nonlinear Systems are a type of recurrent neural network that use continuous and nonlinear operations to combine inputs and produce outputs."', '"SHORT-TERM MEMORY (STM)":"Short-Term Memory (STM) refers to the activities or traces of nodes in a network, which are used to store and process information."', '"NODES": Nodes are processing units in a network that perform computations and interact with each other. These units are capable of receiving and transmitting data, playing a crucial role in the network\'s functionality.', '"GROSSBERG": Stephen Grossberg is a renowned cognitive scientist and neural network researcher known for his significant contributions to the field. He is particularly recognized for proposing the on-center off-surround network as a solution to the noise-saturation dilemma. Additionally, he is a co-author of a study that introduces the TELOS Model and its components. Grossberg\'s research has also focused on understanding Working Memory and its representation, and he has made notable contributions to the development of neural network learning algorithms such as Outstar Learning and Instar Learning. He is also known for his work on Adaptive Resonance Theory and the development of the Additive Model. Grossberg has been mentioned in the text in relation to various models, including the LAMINART Family, LIST PARSE, and TELOS models. He has also contributed to the understanding of spatial pattern learning and signal transmission between cells. Grossberg\'s research has been cited in the context of Item-and-Order working memory, and his work on working memory networks and list chunking networks is referenced in the text. He has developed the Competitive Learning (CL) and Adaptive Resonance Theory (ART) models and has been mentioned in relation to shunting dynamics. Grossberg\'s work on Self-Organizing Avalanches and mathematical analyses of learning processes is also notable. He has been mentioned as a co-author in multiple references related to the neural network and its components.', '"JOHN VON NEUMANN":"John von Neumann is a mathematician and computer scientist who contributed to the development of the digital computer, influenced by the McCulloch-Pitts Model."', '"FRANK ROSENBLATT":"Frank Rosenblatt is a psychologist and neuroscientist who developed the continuous-time STM equation used in the classical Perceptron model."', '"WARREN MCCULLOCH":"Warren McCulloch is a neurophysiologist who, along with Walter Pitts, developed the McCulloch-Pitts Model, a foundational model in neural networks."']}
12:45:36,42 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"RECURRENT CONNECTIONS":"Recurrent Connections are connections in a network that allow for the storage of inputs in short-term memory after the inputs shut off."', '"GROSSBERG AND PEARSON":"Grossberg and Pearson are mentioned as the authors of a model that generalizes Item-and-Order working memories to include rank information, permitting the temporary storage of repeated items in a list."', '"ITEM-ORDER-RANK WORKING MEMORIES":', '"FRONTAL CORTEX": The Frontal Cortex is a brain region that plays a significant role in various cognitive functions, including working memory. Additionally, it is known to modulate song performance in songbirds and higher species, suggesting its involvement in flexible performance. This region\'s role in song production and cognitive functions highlights its complexity and versatility in the brain\'s functional landscape.', '"ITEM-AND-ORDER WORKING MEMORY":"Item-and-Order Working Memory is a cognitive model that stores and recalls lists in a specific order, influenced by primacy, recency, and bowed activation gradients."', '"ITEM-ORDER-RANK WORKING MEMORY":"Item-Order-Rank Working Memory is a variant of Item-and-Order Working Memory that includes positional information, allowing the temporary storage of repeated items in a list."', '"FREE RECALL":"Free Recall is a cognitive process where individuals try to recall a once-heard list in any order, typically showing patterns of primacy and recency."', '"BRADSKI ET AL.": Bradski et al. are a group of researchers who have made significant contributions to the field. They are known for defining the STORE model, a family of Item-and-Order RNNs that exhibits mathematically provable primacy, recency, and bowed gradient properties. Additionally, they have mathematically proven the patterns of activation in an Item-and-Order working memory.', '"BOARDMAN AND BULLOCK":"Boardman and Bullock are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON AND HARTLEY":"Houghton and Hartley are researchers who have contributed to the Item-and-Order working memory design."', '"PAGE AND NORRIS":"Page and Norris are researchers who have developed variants of the Item-and-Order working memory design."', '"RHODES ET AL.":"Rhodes et al. are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON":"Houghton is a researcher who has referred to Item-and-Order models as Competitive Queuing models."', '"FARRELL AND LEWANDOWSKY":"Farrell and Lewandowsky are researchers who have provided experimental support for Item-and-Order working memory properties."', '"AVERBECK ET AL.": Averbeck et al. are a research group known for their contributions to the field of neurophysiology. They have reported neurophysiological evidence supporting the primacy gradient in Item-and-Order working memory, as well as evidence of primacy gradients and inhibition of the most active cell in sequential copying movements. Their research has significantly advanced our understanding of these areas in cognitive science.', '"ITEM-AND-ORDER WM":']}
12:45:36,117 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"HODGKIN":"Hodgkin is a scientist who developed a mathematical model for the shunting dynamics of individual neurons."', '"STM TRACES": "STM Traces" are components of the Generalized Additive System that represent the activities of the system. These traces are also known as short-term memory traces, which are signals or activities that are bounded within an interval and interact with balanced positive and negative signals. They play a role in representing activities in a neural system.', '"SHUNTING MODEL": The Shunting Model is a mathematical component of the Liapunov function proposed by Cohen and Grossberg. It is a variant of the STM Equation, featuring parameters C and F, and E set to 0 or a specific value. The Shunting Model is also a mathematical model used to describe the behavior of cells in a network and is employed to explain associative learning of temporal order information in serial learning paradigms. It is a type of network mentioned in the text and is included in the Cohen-Grossberg Model systems. The Shunting Model approximates the Additive Model in certain conditions, utilizing a combination of shunting and additive terms.', '"WILSON-COWAN MODEL": The Wilson-Cowan Model is a mathematical model developed by Wilson and Cowan. This model uses a sigmoid of sums that is multiplied by a shunting term, as described in the text. Additionally, the model incorporates a combination of shunting and additive terms, but these are presented in a different formulation compared to the initial description.', '"WILSON":"Wilson is a scientist who developed the Wilson-Cowan model with Cowan."', '"COWAN": Cowan is a distinguished researcher and scientist who has made significant contributions to the field. He is known for his work on experimental data supporting the existence of a four plus or minus one WM capacity limit when LTM and grouping influences are minimized. Additionally, he is credited for developing the Wilson-Cowan model in collaboration with Wilson.', '"MTM": "MTM, or Medium-Term Memory, is a type of memory system mentioned in the text that stores and retrieves information for a moderate duration. It is described as a system or model that includes MTM traces or habituative transmitter gates. Despite not being explicitly mentioned, the term \'MTM\' could refer to Medium-Term Memory based on the context provided."', '"FAST INHIBITION":"Fast Inhibition is a feature of the STM Equation that assumes inhibitory interneurons respond instantaneously to their inputs."', '"SLOWER INHIBITION":"Slower Inhibition is a feature of the STM Equation that considers the temporal evolution of inhibitory interneuronal activities."', '"COHEN AND GROSSBERG": Cohen and Grossberg are renowned researchers who have made significant contributions to various fields. They are best known for their work in deriving a Liapunov function for a generalization of the Additive and Shunting Models, which has been instrumental in proving the existence of global equilibria. Additionally, they have developed the Cohen-Grossberg Liapunov function and have contributed to the study of competitive systems, particularly through the development of the Masking Field model. Their research has also included the proposal of a Liapunov function that encompasses both the Additive Model and the Shunting Model.', '"MEDIUM-TERM MEMORY (MTM)":"Medium-term memory (MTM) is described as activity-dependent habituation, often called habituative transmitter gates, which plays multiple roles in the described system."', '"ADDITIVE AND SHUNTING MODELS": "Additive and Shunting Models are neural network architectures mentioned in the text. These models are also generalized by Cohen and Grossberg, and they are referred to in the research as a specific network structure."\n\nThe provided descriptions all refer to the same entities, "ADDITIVE AND SHUNTING MODELS," which are neural network architectures. The descriptions mention that these models are generalized by Cohen and Grossberg and that they are used in the research as a specific network structure. Therefore, the comprehensive description is that Additive and Shunting Models are neural network architectures that have been generalized by Cohen and Grossberg and are mentioned in the text as a specific network structure used in research.', '"LIAPUNOV FUNCTION": The Liapunov function is a mathematical concept that serves multiple purposes. It is primarily used to prove the global convergence of a system, but it is also employed to analyze the stability of dynamic systems, including the Cohen-Grossberg Model. This mathematical concept plays a significant role in the analysis of system stability, making it a valuable tool in various mathematical and scientific disciplines.', '"COHEN-GROSSBERG MODEL": The Cohen-Grossberg Model is a neural network model developed by Cohen and Grossberg in 1983. It is a system of equations used to study the behavior of networks, with a focus on symmetry in interaction coefficients. The model is also mentioned in the text and includes the Liapunov function. It was adapted by Kosko.', '"MEDIUM-TERM MEMORY":"Medium-term Memory, or activity-dependent habituation, is a concept mentioned in the text that plays multiple roles in neural networks."', '"GATED DIPOLE OPPONENT PROCESSING NETWORK":"Gated Dipole Opponent Processing Network is a type of neural network mentioned in the text that enables reset events and arousal bursts."']}
12:45:36,664 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"PERIRHINAL CORTEX (PRC)":"Perirhinal Cortex (PRC) is a region of the brain involved in spatial navigation and memory."', '"PARAHIPPOCAMPAL CORTEX (PHC)":"Parahippocampal Cortex (PHC) is a region of the brain involved in spatial navigation and memory."', '"GRIDPLACEMAP":"GridPlaceMap is a model that simulates the formation of a grid cell representation of space."', '"VISUAL CORTICES V1, V2, AND V4":', '"POSTERIOR AND ANTERIOR INFEROTEMPORAL CORTEX (ITA, ITP)":', '"BRAIN REGIONS":', '"NEURONAL LEARNING":"Neuronal Learning is a process described in the text that involves the interaction of STM and LTM."', '"MATHEMATICAL THEOREMS": Mathematical Theorems play a significant role in enhancing the learning capabilities of the Generalized Additive RNNs architecture. They are mentioned as a foundation for the learning process, providing a solid theoretical basis that supports the architecture\'s performance. Overall, Mathematical Theorems are crucial in the development and understanding of the Generalized Additive RNNs architecture.', '"GENERALIZED ADDITIVE RNNS":"Generalized Additive RNNs is a type of architecture that includes interactions between STM and LTM, allowing it to learn from its environments."', '"SPATIAL PATTERN LEARNING": Spatial Pattern Learning is a concept that encompasses the ability of certain anatomies to learn patterns. This concept is further described as the process of identifying and learning patterns in spatial data, such as images or signals. In essence, Spatial Pattern Learning involves the recognition and understanding of patterns within spatial information.', '"SIGNAL TRANSMISSION":"Signal Transmission is a concept mentioned in the text, referring to the process of transmitting signals between cells."', '"KATZ":"Katz is a researcher who contributed to the understanding of signal density and its effect on postsynaptic cells."', '"SIGNAL PROPAGATION":"Signal Propagation refers to the transmission of signals along axons and their effect on postsynaptic cells."', '"AXON":"Axon is a type of nerve cell that transmits signals from the cell body to other cells."', '"SYNAPTIC KNOB":"Synaptic Knob is a structure on the axon where signals interact with postsynaptic cells."', '"IONIC FLUXES":"Ionic Fluxes refer to the movement of ions in response to signals, contributing to signal transmission."']}
12:45:36,669 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"UNCONDITIONED STIMULI (US)":"Unconditioned Stimuli (US) are particular spatial patterns that are correlated with Conditioned Stimuli in the context of the Unbiased Spatial Pattern Learning Theorem."', '"PAVLOVIAN CONDITIONING":"Pavlovian Conditioning is a form of associative learning that involves pairing a stimulus with a response to create a conditioned response."', '"CS AND US":"CS and US are likely abbreviations for Conditioned Stimulus and Unconditioned Stimulus, which are components of Pavlovian Conditioning."', '"GENERALIZED ADDITIVE MODEL":"The Generalized Additive Model is a statistical framework that allows for the modeling of complex relationships between variables."', '"OUTSTAR LEARNING THEOREM": The Outstar Learning Theorem is a learning theory proposed by Stanley Grossberg. This theory suggests how a series of Outstars can learn an arbitrary spatiotemporal pattern. Additionally, it has been mentioned that the Outstar Learning Theorem is a specific case of the Generalized Additive Model. This means that the theory can be understood within the broader context of the Generalized Additive Model, and it adds to the understanding of this model by focusing on a specific application.', '"GROSSBERG AND SOMERS":"Grossberg and Somers is a collaboration between researchers Grossberg and Somers, who have published on the topic of resynchronizing activities in networks."', '"GROSSBERG AND GRUNEWALD":"Grossberg and Grunewald is a collaboration between researchers Grossberg and Grunewald, who have published on the topic of resynchronizing activities in networks."', '"YAZDANBAKHSH AND GROSSBERG":"Yazdanbakhsh and Grossberg is a collaboration between researchers Yazdanbakhsh and Grossberg, who have published on the topic of resynchronizing activities in laminar cortical circuits."', '"STANLEY GROSSBERG":"Stanley Grossberg is a researcher known for his contributions to neural networks and learning theories, including the Outstar Learning Theorem and the Sparse Stable Category Learning Theorem."', '"SPARSE STABLE CATEGORY LEARNING THEOREM":"The Sparse Stable Category Learning Theorem is another learning theory proposed by Stanley Grossberg, which occurs using the dual network to the Outstar, namely the Instar. This theorem involves multiple Instars competing with each other via a RCF to form a Competitive Learning or Self-Organizing Map network."', '"INSTAR":"Instar is the dual network to the Outstar, which competes with other Instars to form a Competitive Learning or Self-Organizing Map network."', '"LEARNING THEORIES":"Learning Theories are theoretical frameworks that explain how systems can learn and adapt to new information or patterns."', '"COMPETITIVE LEARNING": "Competitive Learning" is a method mentioned in the text, primarily a concept or technique used in research. This learning model involves a competitive process where neurons in a network compete to respond to input patterns. It is a type of unsupervised learning, meaning that there is no external guidance or supervision during the learning process. In this model, neurons compete for the right to respond to input patterns, enhancing their ability to learn and adapt.', '"SELF-ORGANIZING MAP": "Self-Organizing Map" is a type of artificial neural network that is designed to learn and represent the structure of input data. It typically accomplishes this by organizing the input data in a lower-dimensional space. Additionally, Self-Organizing Map is a learning model that dynamically organizes input data, allowing it to adapt and learn from the patterns and relationships present in the data.', '"COMPETITIVE LEARNING OR SELF-ORGANIZING MAP NETWORK":"The Competitive Learning or Self-Organizing Map Network is a type of network mentioned in the text, which is formed by multiple Instars competing via a RCF."', '"KOSKO": "Kosko is a researcher who has made significant contributions to the field. He is mentioned in the text for adapting the Cohen-Grossberg Model, and he has also referred to the equation in (39) as the signal Hebb law, although it does not fully obey Hebb\'s property. Additionally, Kosko has adapted the Cohen-Grossberg model and Liapunov function to define a system that combines Short-Term Memory (STM) and Long-Term Memory (LTM)."']}
12:45:38,265 httpx INFO HTTP Request: POST http://localhost:1234/v1/embeddings "HTTP/1.1 200 OK"
12:45:38,324 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 2 retries took 0.3930000000000291. input_tokens=924, output_tokens=0
12:45:39,182 httpx INFO HTTP Request: POST http://localhost:1234/v1/embeddings "HTTP/1.1 200 OK"
12:45:39,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 2 retries took 0.44800000000032014. input_tokens=1083, output_tokens=0
12:45:39,595 httpx INFO HTTP Request: POST http://localhost:1234/v1/embeddings "HTTP/1.1 200 OK"
12:45:39,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 2 retries took 0.2970000000004802. input_tokens=515, output_tokens=0
12:45:39,976 httpx INFO HTTP Request: POST http://localhost:1234/v1/embeddings "HTTP/1.1 200 OK"
12:45:40,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 2 retries took 0.3780000000006112. input_tokens=897, output_tokens=0
12:48:38,475 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"BRIGHTNESS CONSTANCY":"Brightness Constancy is a property of visual perception that ensures that objects appear the same brightness regardless of their surrounding luminance."', '"BRIGHTNESS CONTRAST":"Brightness Contrast is a visual phenomenon that occurs when increasing the luminance of inputs to the off-surround makes the on-center look darker."', '"NORMALIZATION PROPERTY":"Normalization Property is a property that underlies many properties of limited capacity processing in the brain, such as visual perception and cognition."', '"WORKING MEMORY": Working Memory is a cognitive system that temporarily stores and manipulates information. This system has a limited capacity and activity redistribution when new items are stored. It is responsible for holding and processing information needed for immediate use in cognitive tasks. The model developed by Grossberg (1978a, 1978b) further explains the storage and retrieval of items in short-term memory, taking into account factors such as activity levels and noise. In summary, Working Memory is a limited capacity system in the brain that holds and manipulates information for immediate use, with consideration for activity levels and noise in information processing.', '"SYSTEM":', '"LIMITED CAPACITY PROCESSING":', '"SHUNTING NETWORK":"The Shunting Network is a model used to describe the activity of neurons, with properties such as Weber law processing, adaptation level processing, and edge and spatial frequency processing."', '"NEUROPHYSIOLOGY":"Neurophysiology is the scientific discipline that studies the electrical and chemical properties of neurons and their role in information processing."', '"WERBLIN":"Werblin is a researcher who has studied the retina of the mudpuppy Necturus, finding a shift property similar to that described in the shunting network equations."', '"RETINA":"The Retina is a part of the eye that contains photoreceptor cells, which are sensitive to light and transmit visual information to the brain."', '"MUDPUPPY NECTURUS":"The Mudpuppy Necturus is an aquatic animal whose retina has been studied for its shift property in response to off-surround input."', '"HODGKIN AND HUXLEY": Hodgkin and Huxley are a renowned research team in the field of neurophysiology. They are best known for their groundbreaking work on the membrane equation, which has significantly contributed to our understanding of neurophysiology. Additionally, they are credited with proposing a model of signal propagation in axons, further expanding our knowledge in this area.', '"SHUNTING NETWORK EQUATIONS":"Shunting Network Equations are a mathematical model mentioned in the text, used to generate various properties such as Weber law processing and edge and spatial frequency processing."', '"MEMBRANE EQUATION": The Membrane Equation is a mathematical model that is mentioned in the text. It is primarily used to describe the voltage of a cell. This model was developed by Hodgkin and Huxley and is based on the concept of various conductances and saturation voltages. The descriptions provided are consistent, indicating that the Membrane Equation is a comprehensive mathematical model that describes the voltage of a cell, taking into account different conductances and saturation voltages.', '"SODIUM CHANNEL":"Sodium Channel is a type of ion channel that contributes to the membrane equation, representing the excitatory saturation voltage."', '"POTASSIUM CHANNEL":"Potassium Channel is a type of ion channel that contributes to the membrane equation, representing the inhibitory saturation voltage."']}
12:48:38,655 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"RECURRENT CONNECTIONS":"Recurrent Connections are connections in a network that allow for the storage of inputs in short-term memory after the inputs shut off."', '"GROSSBERG AND PEARSON":"Grossberg and Pearson are mentioned as the authors of a model that generalizes Item-and-Order working memories to include rank information, permitting the temporary storage of repeated items in a list."', '"ITEM-ORDER-RANK WORKING MEMORIES":', '"FRONTAL CORTEX": The Frontal Cortex is a brain region that plays a significant role in various cognitive functions, including working memory. Additionally, it is known to modulate song performance in songbirds and higher species, suggesting its involvement in flexible performance. This region\'s role in song production and cognitive functions highlights its complexity and versatility in the brain\'s functional landscape.', '"ITEM-AND-ORDER WORKING MEMORY":"Item-and-Order Working Memory is a cognitive model that stores and recalls lists in a specific order, influenced by primacy, recency, and bowed activation gradients."', '"ITEM-ORDER-RANK WORKING MEMORY":"Item-Order-Rank Working Memory is a variant of Item-and-Order Working Memory that includes positional information, allowing the temporary storage of repeated items in a list."', '"FREE RECALL":"Free Recall is a cognitive process where individuals try to recall a once-heard list in any order, typically showing patterns of primacy and recency."', '"BRADSKI ET AL.": Bradski et al. are a group of researchers who have made significant contributions to the field. They are known for defining the STORE model, a family of Item-and-Order RNNs that exhibits mathematically provable primacy, recency, and bowed gradient properties. Additionally, they have mathematically proven the patterns of activation in an Item-and-Order working memory.', '"BOARDMAN AND BULLOCK":"Boardman and Bullock are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON AND HARTLEY":"Houghton and Hartley are researchers who have contributed to the Item-and-Order working memory design."', '"PAGE AND NORRIS":"Page and Norris are researchers who have developed variants of the Item-and-Order working memory design."', '"RHODES ET AL.":"Rhodes et al. are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON":"Houghton is a researcher who has referred to Item-and-Order models as Competitive Queuing models."', '"FARRELL AND LEWANDOWSKY":"Farrell and Lewandowsky are researchers who have provided experimental support for Item-and-Order working memory properties."', '"AVERBECK ET AL.": Averbeck et al. are a research group known for their contributions to the field of neurophysiology. They have reported neurophysiological evidence supporting the primacy gradient in Item-and-Order working memory, as well as evidence of primacy gradients and inhibition of the most active cell in sequential copying movements. Their research has significantly advanced our understanding of these areas in cognitive science.', '"ITEM-AND-ORDER WM":']}
12:51:43,3 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"BRIGHTNESS CONSTANCY":"Brightness Constancy is a property of visual perception that ensures that objects appear the same brightness regardless of their surrounding luminance."', '"BRIGHTNESS CONTRAST":"Brightness Contrast is a visual phenomenon that occurs when increasing the luminance of inputs to the off-surround makes the on-center look darker."', '"NORMALIZATION PROPERTY":"Normalization Property is a property that underlies many properties of limited capacity processing in the brain, such as visual perception and cognition."', '"WORKING MEMORY": Working Memory is a cognitive system that temporarily stores and manipulates information. This system has a limited capacity and activity redistribution when new items are stored. It is responsible for holding and processing information needed for immediate use in cognitive tasks. The model developed by Grossberg (1978a, 1978b) further explains the storage and retrieval of items in short-term memory, taking into account factors such as activity levels and noise. In summary, Working Memory is a limited capacity system in the brain that holds and manipulates information for immediate use, with consideration for activity levels and noise in information processing.', '"SYSTEM":', '"LIMITED CAPACITY PROCESSING":', '"SHUNTING NETWORK":"The Shunting Network is a model used to describe the activity of neurons, with properties such as Weber law processing, adaptation level processing, and edge and spatial frequency processing."', '"NEUROPHYSIOLOGY":"Neurophysiology is the scientific discipline that studies the electrical and chemical properties of neurons and their role in information processing."', '"WERBLIN":"Werblin is a researcher who has studied the retina of the mudpuppy Necturus, finding a shift property similar to that described in the shunting network equations."', '"RETINA":"The Retina is a part of the eye that contains photoreceptor cells, which are sensitive to light and transmit visual information to the brain."', '"MUDPUPPY NECTURUS":"The Mudpuppy Necturus is an aquatic animal whose retina has been studied for its shift property in response to off-surround input."', '"HODGKIN AND HUXLEY": Hodgkin and Huxley are a renowned research team in the field of neurophysiology. They are best known for their groundbreaking work on the membrane equation, which has significantly contributed to our understanding of neurophysiology. Additionally, they are credited with proposing a model of signal propagation in axons, further expanding our knowledge in this area.', '"SHUNTING NETWORK EQUATIONS":"Shunting Network Equations are a mathematical model mentioned in the text, used to generate various properties such as Weber law processing and edge and spatial frequency processing."', '"MEMBRANE EQUATION": The Membrane Equation is a mathematical model that is mentioned in the text. It is primarily used to describe the voltage of a cell. This model was developed by Hodgkin and Huxley and is based on the concept of various conductances and saturation voltages. The descriptions provided are consistent, indicating that the Membrane Equation is a comprehensive mathematical model that describes the voltage of a cell, taking into account different conductances and saturation voltages.', '"SODIUM CHANNEL":"Sodium Channel is a type of ion channel that contributes to the membrane equation, representing the excitatory saturation voltage."', '"POTASSIUM CHANNEL":"Potassium Channel is a type of ion channel that contributes to the membrane equation, representing the inhibitory saturation voltage."']}
12:51:43,89 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"RECURRENT CONNECTIONS":"Recurrent Connections are connections in a network that allow for the storage of inputs in short-term memory after the inputs shut off."', '"GROSSBERG AND PEARSON":"Grossberg and Pearson are mentioned as the authors of a model that generalizes Item-and-Order working memories to include rank information, permitting the temporary storage of repeated items in a list."', '"ITEM-ORDER-RANK WORKING MEMORIES":', '"FRONTAL CORTEX": The Frontal Cortex is a brain region that plays a significant role in various cognitive functions, including working memory. Additionally, it is known to modulate song performance in songbirds and higher species, suggesting its involvement in flexible performance. This region\'s role in song production and cognitive functions highlights its complexity and versatility in the brain\'s functional landscape.', '"ITEM-AND-ORDER WORKING MEMORY":"Item-and-Order Working Memory is a cognitive model that stores and recalls lists in a specific order, influenced by primacy, recency, and bowed activation gradients."', '"ITEM-ORDER-RANK WORKING MEMORY":"Item-Order-Rank Working Memory is a variant of Item-and-Order Working Memory that includes positional information, allowing the temporary storage of repeated items in a list."', '"FREE RECALL":"Free Recall is a cognitive process where individuals try to recall a once-heard list in any order, typically showing patterns of primacy and recency."', '"BRADSKI ET AL.": Bradski et al. are a group of researchers who have made significant contributions to the field. They are known for defining the STORE model, a family of Item-and-Order RNNs that exhibits mathematically provable primacy, recency, and bowed gradient properties. Additionally, they have mathematically proven the patterns of activation in an Item-and-Order working memory.', '"BOARDMAN AND BULLOCK":"Boardman and Bullock are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON AND HARTLEY":"Houghton and Hartley are researchers who have contributed to the Item-and-Order working memory design."', '"PAGE AND NORRIS":"Page and Norris are researchers who have developed variants of the Item-and-Order working memory design."', '"RHODES ET AL.":"Rhodes et al. are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON":"Houghton is a researcher who has referred to Item-and-Order models as Competitive Queuing models."', '"FARRELL AND LEWANDOWSKY":"Farrell and Lewandowsky are researchers who have provided experimental support for Item-and-Order working memory properties."', '"AVERBECK ET AL.": Averbeck et al. are a research group known for their contributions to the field of neurophysiology. They have reported neurophysiological evidence supporting the primacy gradient in Item-and-Order working memory, as well as evidence of primacy gradients and inhibition of the most active cell in sequential copying movements. Their research has significantly advanced our understanding of these areas in cognitive science.', '"ITEM-AND-ORDER WM":']}
12:54:51,697 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"RECURRENT CONNECTIONS":"Recurrent Connections are connections in a network that allow for the storage of inputs in short-term memory after the inputs shut off."', '"GROSSBERG AND PEARSON":"Grossberg and Pearson are mentioned as the authors of a model that generalizes Item-and-Order working memories to include rank information, permitting the temporary storage of repeated items in a list."', '"ITEM-ORDER-RANK WORKING MEMORIES":', '"FRONTAL CORTEX": The Frontal Cortex is a brain region that plays a significant role in various cognitive functions, including working memory. Additionally, it is known to modulate song performance in songbirds and higher species, suggesting its involvement in flexible performance. This region\'s role in song production and cognitive functions highlights its complexity and versatility in the brain\'s functional landscape.', '"ITEM-AND-ORDER WORKING MEMORY":"Item-and-Order Working Memory is a cognitive model that stores and recalls lists in a specific order, influenced by primacy, recency, and bowed activation gradients."', '"ITEM-ORDER-RANK WORKING MEMORY":"Item-Order-Rank Working Memory is a variant of Item-and-Order Working Memory that includes positional information, allowing the temporary storage of repeated items in a list."', '"FREE RECALL":"Free Recall is a cognitive process where individuals try to recall a once-heard list in any order, typically showing patterns of primacy and recency."', '"BRADSKI ET AL.": Bradski et al. are a group of researchers who have made significant contributions to the field. They are known for defining the STORE model, a family of Item-and-Order RNNs that exhibits mathematically provable primacy, recency, and bowed gradient properties. Additionally, they have mathematically proven the patterns of activation in an Item-and-Order working memory.', '"BOARDMAN AND BULLOCK":"Boardman and Bullock are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON AND HARTLEY":"Houghton and Hartley are researchers who have contributed to the Item-and-Order working memory design."', '"PAGE AND NORRIS":"Page and Norris are researchers who have developed variants of the Item-and-Order working memory design."', '"RHODES ET AL.":"Rhodes et al. are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON":"Houghton is a researcher who has referred to Item-and-Order models as Competitive Queuing models."', '"FARRELL AND LEWANDOWSKY":"Farrell and Lewandowsky are researchers who have provided experimental support for Item-and-Order working memory properties."', '"AVERBECK ET AL.": Averbeck et al. are a research group known for their contributions to the field of neurophysiology. They have reported neurophysiological evidence supporting the primacy gradient in Item-and-Order working memory, as well as evidence of primacy gradients and inhibition of the most active cell in sequential copying movements. Their research has significantly advanced our understanding of these areas in cognitive science.', '"ITEM-AND-ORDER WM":']}
12:54:51,837 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"BRIGHTNESS CONSTANCY":"Brightness Constancy is a property of visual perception that ensures that objects appear the same brightness regardless of their surrounding luminance."', '"BRIGHTNESS CONTRAST":"Brightness Contrast is a visual phenomenon that occurs when increasing the luminance of inputs to the off-surround makes the on-center look darker."', '"NORMALIZATION PROPERTY":"Normalization Property is a property that underlies many properties of limited capacity processing in the brain, such as visual perception and cognition."', '"WORKING MEMORY": Working Memory is a cognitive system that temporarily stores and manipulates information. This system has a limited capacity and activity redistribution when new items are stored. It is responsible for holding and processing information needed for immediate use in cognitive tasks. The model developed by Grossberg (1978a, 1978b) further explains the storage and retrieval of items in short-term memory, taking into account factors such as activity levels and noise. In summary, Working Memory is a limited capacity system in the brain that holds and manipulates information for immediate use, with consideration for activity levels and noise in information processing.', '"SYSTEM":', '"LIMITED CAPACITY PROCESSING":', '"SHUNTING NETWORK":"The Shunting Network is a model used to describe the activity of neurons, with properties such as Weber law processing, adaptation level processing, and edge and spatial frequency processing."', '"NEUROPHYSIOLOGY":"Neurophysiology is the scientific discipline that studies the electrical and chemical properties of neurons and their role in information processing."', '"WERBLIN":"Werblin is a researcher who has studied the retina of the mudpuppy Necturus, finding a shift property similar to that described in the shunting network equations."', '"RETINA":"The Retina is a part of the eye that contains photoreceptor cells, which are sensitive to light and transmit visual information to the brain."', '"MUDPUPPY NECTURUS":"The Mudpuppy Necturus is an aquatic animal whose retina has been studied for its shift property in response to off-surround input."', '"HODGKIN AND HUXLEY": Hodgkin and Huxley are a renowned research team in the field of neurophysiology. They are best known for their groundbreaking work on the membrane equation, which has significantly contributed to our understanding of neurophysiology. Additionally, they are credited with proposing a model of signal propagation in axons, further expanding our knowledge in this area.', '"SHUNTING NETWORK EQUATIONS":"Shunting Network Equations are a mathematical model mentioned in the text, used to generate various properties such as Weber law processing and edge and spatial frequency processing."', '"MEMBRANE EQUATION": The Membrane Equation is a mathematical model that is mentioned in the text. It is primarily used to describe the voltage of a cell. This model was developed by Hodgkin and Huxley and is based on the concept of various conductances and saturation voltages. The descriptions provided are consistent, indicating that the Membrane Equation is a comprehensive mathematical model that describes the voltage of a cell, taking into account different conductances and saturation voltages.', '"SODIUM CHANNEL":"Sodium Channel is a type of ion channel that contributes to the membrane equation, representing the excitatory saturation voltage."', '"POTASSIUM CHANNEL":"Potassium Channel is a type of ion channel that contributes to the membrane equation, representing the inhibitory saturation voltage."']}
12:58:01,811 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"RECURRENT CONNECTIONS":"Recurrent Connections are connections in a network that allow for the storage of inputs in short-term memory after the inputs shut off."', '"GROSSBERG AND PEARSON":"Grossberg and Pearson are mentioned as the authors of a model that generalizes Item-and-Order working memories to include rank information, permitting the temporary storage of repeated items in a list."', '"ITEM-ORDER-RANK WORKING MEMORIES":', '"FRONTAL CORTEX": The Frontal Cortex is a brain region that plays a significant role in various cognitive functions, including working memory. Additionally, it is known to modulate song performance in songbirds and higher species, suggesting its involvement in flexible performance. This region\'s role in song production and cognitive functions highlights its complexity and versatility in the brain\'s functional landscape.', '"ITEM-AND-ORDER WORKING MEMORY":"Item-and-Order Working Memory is a cognitive model that stores and recalls lists in a specific order, influenced by primacy, recency, and bowed activation gradients."', '"ITEM-ORDER-RANK WORKING MEMORY":"Item-Order-Rank Working Memory is a variant of Item-and-Order Working Memory that includes positional information, allowing the temporary storage of repeated items in a list."', '"FREE RECALL":"Free Recall is a cognitive process where individuals try to recall a once-heard list in any order, typically showing patterns of primacy and recency."', '"BRADSKI ET AL.": Bradski et al. are a group of researchers who have made significant contributions to the field. They are known for defining the STORE model, a family of Item-and-Order RNNs that exhibits mathematically provable primacy, recency, and bowed gradient properties. Additionally, they have mathematically proven the patterns of activation in an Item-and-Order working memory.', '"BOARDMAN AND BULLOCK":"Boardman and Bullock are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON AND HARTLEY":"Houghton and Hartley are researchers who have contributed to the Item-and-Order working memory design."', '"PAGE AND NORRIS":"Page and Norris are researchers who have developed variants of the Item-and-Order working memory design."', '"RHODES ET AL.":"Rhodes et al. are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON":"Houghton is a researcher who has referred to Item-and-Order models as Competitive Queuing models."', '"FARRELL AND LEWANDOWSKY":"Farrell and Lewandowsky are researchers who have provided experimental support for Item-and-Order working memory properties."', '"AVERBECK ET AL.": Averbeck et al. are a research group known for their contributions to the field of neurophysiology. They have reported neurophysiological evidence supporting the primacy gradient in Item-and-Order working memory, as well as evidence of primacy gradients and inhibition of the most active cell in sequential copying movements. Their research has significantly advanced our understanding of these areas in cognitive science.', '"ITEM-AND-ORDER WM":']}
12:58:01,844 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"BRIGHTNESS CONSTANCY":"Brightness Constancy is a property of visual perception that ensures that objects appear the same brightness regardless of their surrounding luminance."', '"BRIGHTNESS CONTRAST":"Brightness Contrast is a visual phenomenon that occurs when increasing the luminance of inputs to the off-surround makes the on-center look darker."', '"NORMALIZATION PROPERTY":"Normalization Property is a property that underlies many properties of limited capacity processing in the brain, such as visual perception and cognition."', '"WORKING MEMORY": Working Memory is a cognitive system that temporarily stores and manipulates information. This system has a limited capacity and activity redistribution when new items are stored. It is responsible for holding and processing information needed for immediate use in cognitive tasks. The model developed by Grossberg (1978a, 1978b) further explains the storage and retrieval of items in short-term memory, taking into account factors such as activity levels and noise. In summary, Working Memory is a limited capacity system in the brain that holds and manipulates information for immediate use, with consideration for activity levels and noise in information processing.', '"SYSTEM":', '"LIMITED CAPACITY PROCESSING":', '"SHUNTING NETWORK":"The Shunting Network is a model used to describe the activity of neurons, with properties such as Weber law processing, adaptation level processing, and edge and spatial frequency processing."', '"NEUROPHYSIOLOGY":"Neurophysiology is the scientific discipline that studies the electrical and chemical properties of neurons and their role in information processing."', '"WERBLIN":"Werblin is a researcher who has studied the retina of the mudpuppy Necturus, finding a shift property similar to that described in the shunting network equations."', '"RETINA":"The Retina is a part of the eye that contains photoreceptor cells, which are sensitive to light and transmit visual information to the brain."', '"MUDPUPPY NECTURUS":"The Mudpuppy Necturus is an aquatic animal whose retina has been studied for its shift property in response to off-surround input."', '"HODGKIN AND HUXLEY": Hodgkin and Huxley are a renowned research team in the field of neurophysiology. They are best known for their groundbreaking work on the membrane equation, which has significantly contributed to our understanding of neurophysiology. Additionally, they are credited with proposing a model of signal propagation in axons, further expanding our knowledge in this area.', '"SHUNTING NETWORK EQUATIONS":"Shunting Network Equations are a mathematical model mentioned in the text, used to generate various properties such as Weber law processing and edge and spatial frequency processing."', '"MEMBRANE EQUATION": The Membrane Equation is a mathematical model that is mentioned in the text. It is primarily used to describe the voltage of a cell. This model was developed by Hodgkin and Huxley and is based on the concept of various conductances and saturation voltages. The descriptions provided are consistent, indicating that the Membrane Equation is a comprehensive mathematical model that describes the voltage of a cell, taking into account different conductances and saturation voltages.', '"SODIUM CHANNEL":"Sodium Channel is a type of ion channel that contributes to the membrane equation, representing the excitatory saturation voltage."', '"POTASSIUM CHANNEL":"Potassium Channel is a type of ion channel that contributes to the membrane equation, representing the inhibitory saturation voltage."']}
13:01:11,934 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"RECURRENT CONNECTIONS":"Recurrent Connections are connections in a network that allow for the storage of inputs in short-term memory after the inputs shut off."', '"GROSSBERG AND PEARSON":"Grossberg and Pearson are mentioned as the authors of a model that generalizes Item-and-Order working memories to include rank information, permitting the temporary storage of repeated items in a list."', '"ITEM-ORDER-RANK WORKING MEMORIES":', '"FRONTAL CORTEX": The Frontal Cortex is a brain region that plays a significant role in various cognitive functions, including working memory. Additionally, it is known to modulate song performance in songbirds and higher species, suggesting its involvement in flexible performance. This region\'s role in song production and cognitive functions highlights its complexity and versatility in the brain\'s functional landscape.', '"ITEM-AND-ORDER WORKING MEMORY":"Item-and-Order Working Memory is a cognitive model that stores and recalls lists in a specific order, influenced by primacy, recency, and bowed activation gradients."', '"ITEM-ORDER-RANK WORKING MEMORY":"Item-Order-Rank Working Memory is a variant of Item-and-Order Working Memory that includes positional information, allowing the temporary storage of repeated items in a list."', '"FREE RECALL":"Free Recall is a cognitive process where individuals try to recall a once-heard list in any order, typically showing patterns of primacy and recency."', '"BRADSKI ET AL.": Bradski et al. are a group of researchers who have made significant contributions to the field. They are known for defining the STORE model, a family of Item-and-Order RNNs that exhibits mathematically provable primacy, recency, and bowed gradient properties. Additionally, they have mathematically proven the patterns of activation in an Item-and-Order working memory.', '"BOARDMAN AND BULLOCK":"Boardman and Bullock are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON AND HARTLEY":"Houghton and Hartley are researchers who have contributed to the Item-and-Order working memory design."', '"PAGE AND NORRIS":"Page and Norris are researchers who have developed variants of the Item-and-Order working memory design."', '"RHODES ET AL.":"Rhodes et al. are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON":"Houghton is a researcher who has referred to Item-and-Order models as Competitive Queuing models."', '"FARRELL AND LEWANDOWSKY":"Farrell and Lewandowsky are researchers who have provided experimental support for Item-and-Order working memory properties."', '"AVERBECK ET AL.": Averbeck et al. are a research group known for their contributions to the field of neurophysiology. They have reported neurophysiological evidence supporting the primacy gradient in Item-and-Order working memory, as well as evidence of primacy gradients and inhibition of the most active cell in sequential copying movements. Their research has significantly advanced our understanding of these areas in cognitive science.', '"ITEM-AND-ORDER WM":']}
13:01:11,937 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"BRIGHTNESS CONSTANCY":"Brightness Constancy is a property of visual perception that ensures that objects appear the same brightness regardless of their surrounding luminance."', '"BRIGHTNESS CONTRAST":"Brightness Contrast is a visual phenomenon that occurs when increasing the luminance of inputs to the off-surround makes the on-center look darker."', '"NORMALIZATION PROPERTY":"Normalization Property is a property that underlies many properties of limited capacity processing in the brain, such as visual perception and cognition."', '"WORKING MEMORY": Working Memory is a cognitive system that temporarily stores and manipulates information. This system has a limited capacity and activity redistribution when new items are stored. It is responsible for holding and processing information needed for immediate use in cognitive tasks. The model developed by Grossberg (1978a, 1978b) further explains the storage and retrieval of items in short-term memory, taking into account factors such as activity levels and noise. In summary, Working Memory is a limited capacity system in the brain that holds and manipulates information for immediate use, with consideration for activity levels and noise in information processing.', '"SYSTEM":', '"LIMITED CAPACITY PROCESSING":', '"SHUNTING NETWORK":"The Shunting Network is a model used to describe the activity of neurons, with properties such as Weber law processing, adaptation level processing, and edge and spatial frequency processing."', '"NEUROPHYSIOLOGY":"Neurophysiology is the scientific discipline that studies the electrical and chemical properties of neurons and their role in information processing."', '"WERBLIN":"Werblin is a researcher who has studied the retina of the mudpuppy Necturus, finding a shift property similar to that described in the shunting network equations."', '"RETINA":"The Retina is a part of the eye that contains photoreceptor cells, which are sensitive to light and transmit visual information to the brain."', '"MUDPUPPY NECTURUS":"The Mudpuppy Necturus is an aquatic animal whose retina has been studied for its shift property in response to off-surround input."', '"HODGKIN AND HUXLEY": Hodgkin and Huxley are a renowned research team in the field of neurophysiology. They are best known for their groundbreaking work on the membrane equation, which has significantly contributed to our understanding of neurophysiology. Additionally, they are credited with proposing a model of signal propagation in axons, further expanding our knowledge in this area.', '"SHUNTING NETWORK EQUATIONS":"Shunting Network Equations are a mathematical model mentioned in the text, used to generate various properties such as Weber law processing and edge and spatial frequency processing."', '"MEMBRANE EQUATION": The Membrane Equation is a mathematical model that is mentioned in the text. It is primarily used to describe the voltage of a cell. This model was developed by Hodgkin and Huxley and is based on the concept of various conductances and saturation voltages. The descriptions provided are consistent, indicating that the Membrane Equation is a comprehensive mathematical model that describes the voltage of a cell, taking into account different conductances and saturation voltages.', '"SODIUM CHANNEL":"Sodium Channel is a type of ion channel that contributes to the membrane equation, representing the excitatory saturation voltage."', '"POTASSIUM CHANNEL":"Potassium Channel is a type of ion channel that contributes to the membrane equation, representing the inhibitory saturation voltage."']}
13:04:22,21 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"RECURRENT CONNECTIONS":"Recurrent Connections are connections in a network that allow for the storage of inputs in short-term memory after the inputs shut off."', '"GROSSBERG AND PEARSON":"Grossberg and Pearson are mentioned as the authors of a model that generalizes Item-and-Order working memories to include rank information, permitting the temporary storage of repeated items in a list."', '"ITEM-ORDER-RANK WORKING MEMORIES":', '"FRONTAL CORTEX": The Frontal Cortex is a brain region that plays a significant role in various cognitive functions, including working memory. Additionally, it is known to modulate song performance in songbirds and higher species, suggesting its involvement in flexible performance. This region\'s role in song production and cognitive functions highlights its complexity and versatility in the brain\'s functional landscape.', '"ITEM-AND-ORDER WORKING MEMORY":"Item-and-Order Working Memory is a cognitive model that stores and recalls lists in a specific order, influenced by primacy, recency, and bowed activation gradients."', '"ITEM-ORDER-RANK WORKING MEMORY":"Item-Order-Rank Working Memory is a variant of Item-and-Order Working Memory that includes positional information, allowing the temporary storage of repeated items in a list."', '"FREE RECALL":"Free Recall is a cognitive process where individuals try to recall a once-heard list in any order, typically showing patterns of primacy and recency."', '"BRADSKI ET AL.": Bradski et al. are a group of researchers who have made significant contributions to the field. They are known for defining the STORE model, a family of Item-and-Order RNNs that exhibits mathematically provable primacy, recency, and bowed gradient properties. Additionally, they have mathematically proven the patterns of activation in an Item-and-Order working memory.', '"BOARDMAN AND BULLOCK":"Boardman and Bullock are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON AND HARTLEY":"Houghton and Hartley are researchers who have contributed to the Item-and-Order working memory design."', '"PAGE AND NORRIS":"Page and Norris are researchers who have developed variants of the Item-and-Order working memory design."', '"RHODES ET AL.":"Rhodes et al. are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON":"Houghton is a researcher who has referred to Item-and-Order models as Competitive Queuing models."', '"FARRELL AND LEWANDOWSKY":"Farrell and Lewandowsky are researchers who have provided experimental support for Item-and-Order working memory properties."', '"AVERBECK ET AL.": Averbeck et al. are a research group known for their contributions to the field of neurophysiology. They have reported neurophysiological evidence supporting the primacy gradient in Item-and-Order working memory, as well as evidence of primacy gradients and inhibition of the most active cell in sequential copying movements. Their research has significantly advanced our understanding of these areas in cognitive science.', '"ITEM-AND-ORDER WM":']}
13:04:22,22 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"BRIGHTNESS CONSTANCY":"Brightness Constancy is a property of visual perception that ensures that objects appear the same brightness regardless of their surrounding luminance."', '"BRIGHTNESS CONTRAST":"Brightness Contrast is a visual phenomenon that occurs when increasing the luminance of inputs to the off-surround makes the on-center look darker."', '"NORMALIZATION PROPERTY":"Normalization Property is a property that underlies many properties of limited capacity processing in the brain, such as visual perception and cognition."', '"WORKING MEMORY": Working Memory is a cognitive system that temporarily stores and manipulates information. This system has a limited capacity and activity redistribution when new items are stored. It is responsible for holding and processing information needed for immediate use in cognitive tasks. The model developed by Grossberg (1978a, 1978b) further explains the storage and retrieval of items in short-term memory, taking into account factors such as activity levels and noise. In summary, Working Memory is a limited capacity system in the brain that holds and manipulates information for immediate use, with consideration for activity levels and noise in information processing.', '"SYSTEM":', '"LIMITED CAPACITY PROCESSING":', '"SHUNTING NETWORK":"The Shunting Network is a model used to describe the activity of neurons, with properties such as Weber law processing, adaptation level processing, and edge and spatial frequency processing."', '"NEUROPHYSIOLOGY":"Neurophysiology is the scientific discipline that studies the electrical and chemical properties of neurons and their role in information processing."', '"WERBLIN":"Werblin is a researcher who has studied the retina of the mudpuppy Necturus, finding a shift property similar to that described in the shunting network equations."', '"RETINA":"The Retina is a part of the eye that contains photoreceptor cells, which are sensitive to light and transmit visual information to the brain."', '"MUDPUPPY NECTURUS":"The Mudpuppy Necturus is an aquatic animal whose retina has been studied for its shift property in response to off-surround input."', '"HODGKIN AND HUXLEY": Hodgkin and Huxley are a renowned research team in the field of neurophysiology. They are best known for their groundbreaking work on the membrane equation, which has significantly contributed to our understanding of neurophysiology. Additionally, they are credited with proposing a model of signal propagation in axons, further expanding our knowledge in this area.', '"SHUNTING NETWORK EQUATIONS":"Shunting Network Equations are a mathematical model mentioned in the text, used to generate various properties such as Weber law processing and edge and spatial frequency processing."', '"MEMBRANE EQUATION": The Membrane Equation is a mathematical model that is mentioned in the text. It is primarily used to describe the voltage of a cell. This model was developed by Hodgkin and Huxley and is based on the concept of various conductances and saturation voltages. The descriptions provided are consistent, indicating that the Membrane Equation is a comprehensive mathematical model that describes the voltage of a cell, taking into account different conductances and saturation voltages.', '"SODIUM CHANNEL":"Sodium Channel is a type of ion channel that contributes to the membrane equation, representing the excitatory saturation voltage."', '"POTASSIUM CHANNEL":"Potassium Channel is a type of ion channel that contributes to the membrane equation, representing the inhibitory saturation voltage."']}
13:07:32,147 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"RECURRENT CONNECTIONS":"Recurrent Connections are connections in a network that allow for the storage of inputs in short-term memory after the inputs shut off."', '"GROSSBERG AND PEARSON":"Grossberg and Pearson are mentioned as the authors of a model that generalizes Item-and-Order working memories to include rank information, permitting the temporary storage of repeated items in a list."', '"ITEM-ORDER-RANK WORKING MEMORIES":', '"FRONTAL CORTEX": The Frontal Cortex is a brain region that plays a significant role in various cognitive functions, including working memory. Additionally, it is known to modulate song performance in songbirds and higher species, suggesting its involvement in flexible performance. This region\'s role in song production and cognitive functions highlights its complexity and versatility in the brain\'s functional landscape.', '"ITEM-AND-ORDER WORKING MEMORY":"Item-and-Order Working Memory is a cognitive model that stores and recalls lists in a specific order, influenced by primacy, recency, and bowed activation gradients."', '"ITEM-ORDER-RANK WORKING MEMORY":"Item-Order-Rank Working Memory is a variant of Item-and-Order Working Memory that includes positional information, allowing the temporary storage of repeated items in a list."', '"FREE RECALL":"Free Recall is a cognitive process where individuals try to recall a once-heard list in any order, typically showing patterns of primacy and recency."', '"BRADSKI ET AL.": Bradski et al. are a group of researchers who have made significant contributions to the field. They are known for defining the STORE model, a family of Item-and-Order RNNs that exhibits mathematically provable primacy, recency, and bowed gradient properties. Additionally, they have mathematically proven the patterns of activation in an Item-and-Order working memory.', '"BOARDMAN AND BULLOCK":"Boardman and Bullock are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON AND HARTLEY":"Houghton and Hartley are researchers who have contributed to the Item-and-Order working memory design."', '"PAGE AND NORRIS":"Page and Norris are researchers who have developed variants of the Item-and-Order working memory design."', '"RHODES ET AL.":"Rhodes et al. are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON":"Houghton is a researcher who has referred to Item-and-Order models as Competitive Queuing models."', '"FARRELL AND LEWANDOWSKY":"Farrell and Lewandowsky are researchers who have provided experimental support for Item-and-Order working memory properties."', '"AVERBECK ET AL.": Averbeck et al. are a research group known for their contributions to the field of neurophysiology. They have reported neurophysiological evidence supporting the primacy gradient in Item-and-Order working memory, as well as evidence of primacy gradients and inhibition of the most active cell in sequential copying movements. Their research has significantly advanced our understanding of these areas in cognitive science.', '"ITEM-AND-ORDER WM":']}
13:07:32,149 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"BRIGHTNESS CONSTANCY":"Brightness Constancy is a property of visual perception that ensures that objects appear the same brightness regardless of their surrounding luminance."', '"BRIGHTNESS CONTRAST":"Brightness Contrast is a visual phenomenon that occurs when increasing the luminance of inputs to the off-surround makes the on-center look darker."', '"NORMALIZATION PROPERTY":"Normalization Property is a property that underlies many properties of limited capacity processing in the brain, such as visual perception and cognition."', '"WORKING MEMORY": Working Memory is a cognitive system that temporarily stores and manipulates information. This system has a limited capacity and activity redistribution when new items are stored. It is responsible for holding and processing information needed for immediate use in cognitive tasks. The model developed by Grossberg (1978a, 1978b) further explains the storage and retrieval of items in short-term memory, taking into account factors such as activity levels and noise. In summary, Working Memory is a limited capacity system in the brain that holds and manipulates information for immediate use, with consideration for activity levels and noise in information processing.', '"SYSTEM":', '"LIMITED CAPACITY PROCESSING":', '"SHUNTING NETWORK":"The Shunting Network is a model used to describe the activity of neurons, with properties such as Weber law processing, adaptation level processing, and edge and spatial frequency processing."', '"NEUROPHYSIOLOGY":"Neurophysiology is the scientific discipline that studies the electrical and chemical properties of neurons and their role in information processing."', '"WERBLIN":"Werblin is a researcher who has studied the retina of the mudpuppy Necturus, finding a shift property similar to that described in the shunting network equations."', '"RETINA":"The Retina is a part of the eye that contains photoreceptor cells, which are sensitive to light and transmit visual information to the brain."', '"MUDPUPPY NECTURUS":"The Mudpuppy Necturus is an aquatic animal whose retina has been studied for its shift property in response to off-surround input."', '"HODGKIN AND HUXLEY": Hodgkin and Huxley are a renowned research team in the field of neurophysiology. They are best known for their groundbreaking work on the membrane equation, which has significantly contributed to our understanding of neurophysiology. Additionally, they are credited with proposing a model of signal propagation in axons, further expanding our knowledge in this area.', '"SHUNTING NETWORK EQUATIONS":"Shunting Network Equations are a mathematical model mentioned in the text, used to generate various properties such as Weber law processing and edge and spatial frequency processing."', '"MEMBRANE EQUATION": The Membrane Equation is a mathematical model that is mentioned in the text. It is primarily used to describe the voltage of a cell. This model was developed by Hodgkin and Huxley and is based on the concept of various conductances and saturation voltages. The descriptions provided are consistent, indicating that the Membrane Equation is a comprehensive mathematical model that describes the voltage of a cell, taking into account different conductances and saturation voltages.', '"SODIUM CHANNEL":"Sodium Channel is a type of ion channel that contributes to the membrane equation, representing the excitatory saturation voltage."', '"POTASSIUM CHANNEL":"Potassium Channel is a type of ion channel that contributes to the membrane equation, representing the inhibitory saturation voltage."']}
13:10:42,291 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"RECURRENT CONNECTIONS":"Recurrent Connections are connections in a network that allow for the storage of inputs in short-term memory after the inputs shut off."', '"GROSSBERG AND PEARSON":"Grossberg and Pearson are mentioned as the authors of a model that generalizes Item-and-Order working memories to include rank information, permitting the temporary storage of repeated items in a list."', '"ITEM-ORDER-RANK WORKING MEMORIES":', '"FRONTAL CORTEX": The Frontal Cortex is a brain region that plays a significant role in various cognitive functions, including working memory. Additionally, it is known to modulate song performance in songbirds and higher species, suggesting its involvement in flexible performance. This region\'s role in song production and cognitive functions highlights its complexity and versatility in the brain\'s functional landscape.', '"ITEM-AND-ORDER WORKING MEMORY":"Item-and-Order Working Memory is a cognitive model that stores and recalls lists in a specific order, influenced by primacy, recency, and bowed activation gradients."', '"ITEM-ORDER-RANK WORKING MEMORY":"Item-Order-Rank Working Memory is a variant of Item-and-Order Working Memory that includes positional information, allowing the temporary storage of repeated items in a list."', '"FREE RECALL":"Free Recall is a cognitive process where individuals try to recall a once-heard list in any order, typically showing patterns of primacy and recency."', '"BRADSKI ET AL.": Bradski et al. are a group of researchers who have made significant contributions to the field. They are known for defining the STORE model, a family of Item-and-Order RNNs that exhibits mathematically provable primacy, recency, and bowed gradient properties. Additionally, they have mathematically proven the patterns of activation in an Item-and-Order working memory.', '"BOARDMAN AND BULLOCK":"Boardman and Bullock are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON AND HARTLEY":"Houghton and Hartley are researchers who have contributed to the Item-and-Order working memory design."', '"PAGE AND NORRIS":"Page and Norris are researchers who have developed variants of the Item-and-Order working memory design."', '"RHODES ET AL.":"Rhodes et al. are researchers who have developed variants of the Item-and-Order working memory design."', '"HOUGHTON":"Houghton is a researcher who has referred to Item-and-Order models as Competitive Queuing models."', '"FARRELL AND LEWANDOWSKY":"Farrell and Lewandowsky are researchers who have provided experimental support for Item-and-Order working memory properties."', '"AVERBECK ET AL.": Averbeck et al. are a research group known for their contributions to the field of neurophysiology. They have reported neurophysiological evidence supporting the primacy gradient in Item-and-Order working memory, as well as evidence of primacy gradients and inhibition of the most active cell in sequential copying movements. Their research has significantly advanced our understanding of these areas in cognitive science.', '"ITEM-AND-ORDER WM":']}
13:10:42,294 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"BRIGHTNESS CONSTANCY":"Brightness Constancy is a property of visual perception that ensures that objects appear the same brightness regardless of their surrounding luminance."', '"BRIGHTNESS CONTRAST":"Brightness Contrast is a visual phenomenon that occurs when increasing the luminance of inputs to the off-surround makes the on-center look darker."', '"NORMALIZATION PROPERTY":"Normalization Property is a property that underlies many properties of limited capacity processing in the brain, such as visual perception and cognition."', '"WORKING MEMORY": Working Memory is a cognitive system that temporarily stores and manipulates information. This system has a limited capacity and activity redistribution when new items are stored. It is responsible for holding and processing information needed for immediate use in cognitive tasks. The model developed by Grossberg (1978a, 1978b) further explains the storage and retrieval of items in short-term memory, taking into account factors such as activity levels and noise. In summary, Working Memory is a limited capacity system in the brain that holds and manipulates information for immediate use, with consideration for activity levels and noise in information processing.', '"SYSTEM":', '"LIMITED CAPACITY PROCESSING":', '"SHUNTING NETWORK":"The Shunting Network is a model used to describe the activity of neurons, with properties such as Weber law processing, adaptation level processing, and edge and spatial frequency processing."', '"NEUROPHYSIOLOGY":"Neurophysiology is the scientific discipline that studies the electrical and chemical properties of neurons and their role in information processing."', '"WERBLIN":"Werblin is a researcher who has studied the retina of the mudpuppy Necturus, finding a shift property similar to that described in the shunting network equations."', '"RETINA":"The Retina is a part of the eye that contains photoreceptor cells, which are sensitive to light and transmit visual information to the brain."', '"MUDPUPPY NECTURUS":"The Mudpuppy Necturus is an aquatic animal whose retina has been studied for its shift property in response to off-surround input."', '"HODGKIN AND HUXLEY": Hodgkin and Huxley are a renowned research team in the field of neurophysiology. They are best known for their groundbreaking work on the membrane equation, which has significantly contributed to our understanding of neurophysiology. Additionally, they are credited with proposing a model of signal propagation in axons, further expanding our knowledge in this area.', '"SHUNTING NETWORK EQUATIONS":"Shunting Network Equations are a mathematical model mentioned in the text, used to generate various properties such as Weber law processing and edge and spatial frequency processing."', '"MEMBRANE EQUATION": The Membrane Equation is a mathematical model that is mentioned in the text. It is primarily used to describe the voltage of a cell. This model was developed by Hodgkin and Huxley and is based on the concept of various conductances and saturation voltages. The descriptions provided are consistent, indicating that the Membrane Equation is a comprehensive mathematical model that describes the voltage of a cell, taking into account different conductances and saturation voltages.', '"SODIUM CHANNEL":"Sodium Channel is a type of ion channel that contributes to the membrane equation, representing the excitatory saturation voltage."', '"POTASSIUM CHANNEL":"Potassium Channel is a type of ion channel that contributes to the membrane equation, representing the inhibitory saturation voltage."']}
13:10:42,295 datashaper.workflow.workflow ERROR Error executing verb "text_embed" in create_final_entities: Request timed out.
Traceback (most recent call last):
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_transports/default.py", line 69, in map_httpcore_exceptions
    yield
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_transports/default.py", line 373, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/connection_pool.py", line 216, in handle_async_request
    raise exc from None
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/connection_pool.py", line 196, in handle_async_request
    response = await connection.handle_async_request(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/connection.py", line 101, in handle_async_request
    return await self._connection.handle_async_request(request)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/http11.py", line 143, in handle_async_request
    raise exc
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/http11.py", line 113, in handle_async_request
    ) = await self._receive_response_headers(**kwargs)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/http11.py", line 186, in _receive_response_headers
    event = await self._receive_event(timeout=timeout)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/http11.py", line 224, in _receive_event
    data = await self._network_stream.read(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_backends/anyio.py", line 32, in read
    with map_exceptions(exc_map):
  File "/usr/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ReadTimeout

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/openai/_base_client.py", line 1558, in _request
    response = await self._client.send(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_client.py", line 1661, in send
    response = await self._send_handling_auth(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_client.py", line 1689, in _send_handling_auth
    response = await self._send_handling_redirects(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_client.py", line 1726, in _send_handling_redirects
    response = await self._send_single_request(request)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_client.py", line 1763, in _send_single_request
    response = await transport.handle_async_request(request)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_transports/default.py", line 372, in handle_async_request
    with map_httpcore_exceptions():
  File "/usr/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_transports/default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/index/verbs/text/embed/text_embed.py", line 105, in text_embed
    return await _text_embed_in_memory(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/index/verbs/text/embed/text_embed.py", line 130, in _text_embed_in_memory
    result = await strategy_exec(texts, callbacks, cache, strategy_args)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/index/verbs/text/embed/strategies/openai.py", line 61, in run
    embeddings = await _execute(llm, text_batches, ticker, semaphore)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/index/verbs/text/embed/strategies/openai.py", line 105, in _execute
    results = await asyncio.gather(*futures)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/index/verbs/text/embed/strategies/openai.py", line 99, in embed
    chunk_embeddings = await llm(chunk)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 104, in __call__
    result = await self._delegate(input, **kwargs)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/openai/openai_embeddings_llm.py", line 36, in _execute_llm
    embedding = await self.client.embeddings.create(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/openai/resources/embeddings.py", line 215, in create
    return await self._post(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/openai/_base_client.py", line 1826, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/openai/_base_client.py", line 1519, in request
    return await self._request(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/openai/_base_client.py", line 1577, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
13:10:42,300 graphrag.index.reporting.file_workflow_callbacks INFO Error executing verb "text_embed" in create_final_entities: Request timed out. details=None
13:10:42,333 graphrag.index.run ERROR error running workflow create_final_entities
Traceback (most recent call last):
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_transports/default.py", line 69, in map_httpcore_exceptions
    yield
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_transports/default.py", line 373, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/connection_pool.py", line 216, in handle_async_request
    raise exc from None
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/connection_pool.py", line 196, in handle_async_request
    response = await connection.handle_async_request(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/connection.py", line 101, in handle_async_request
    return await self._connection.handle_async_request(request)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/http11.py", line 143, in handle_async_request
    raise exc
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/http11.py", line 113, in handle_async_request
    ) = await self._receive_response_headers(**kwargs)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/http11.py", line 186, in _receive_response_headers
    event = await self._receive_event(timeout=timeout)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_async/http11.py", line 224, in _receive_event
    data = await self._network_stream.read(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_backends/anyio.py", line 32, in read
    with map_exceptions(exc_map):
  File "/usr/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ReadTimeout

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/openai/_base_client.py", line 1558, in _request
    response = await self._client.send(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_client.py", line 1661, in send
    response = await self._send_handling_auth(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_client.py", line 1689, in _send_handling_auth
    response = await self._send_handling_redirects(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_client.py", line 1726, in _send_handling_redirects
    response = await self._send_single_request(request)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_client.py", line 1763, in _send_single_request
    response = await transport.handle_async_request(request)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_transports/default.py", line 372, in handle_async_request
    with map_httpcore_exceptions():
  File "/usr/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/httpx/_transports/default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/index/run.py", line 323, in run_pipeline
    result = await workflow.run(context, callbacks)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/index/verbs/text/embed/text_embed.py", line 105, in text_embed
    return await _text_embed_in_memory(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/index/verbs/text/embed/text_embed.py", line 130, in _text_embed_in_memory
    result = await strategy_exec(texts, callbacks, cache, strategy_args)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/index/verbs/text/embed/strategies/openai.py", line 61, in run
    embeddings = await _execute(llm, text_batches, ticker, semaphore)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/index/verbs/text/embed/strategies/openai.py", line 105, in _execute
    results = await asyncio.gather(*futures)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/index/verbs/text/embed/strategies/openai.py", line 99, in embed
    chunk_embeddings = await llm(chunk)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 104, in __call__
    result = await self._delegate(input, **kwargs)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/graphrag/llm/openai/openai_embeddings_llm.py", line 36, in _execute_llm
    embedding = await self.client.embeddings.create(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/openai/resources/embeddings.py", line 215, in create
    return await self._post(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/openai/_base_client.py", line 1826, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/openai/_base_client.py", line 1519, in request
    return await self._request(
  File "/home/l-t7-unknown/.local/lib/python3.10/site-packages/openai/_base_client.py", line 1577, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
13:10:42,334 graphrag.index.reporting.file_workflow_callbacks INFO Error running pipeline! details=None
