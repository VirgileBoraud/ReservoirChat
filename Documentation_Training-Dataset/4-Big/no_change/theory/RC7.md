Echo state networks are universal
Lyudmila Grigoryeva1 and Juan-Pablo Ortega2,3
Abstract
This paper shows that echo state networks are universal uniform approximants in the context of
discrete-time fading memory ï¬lters with uniformly bounded inputs deï¬ned on negative inï¬nite times.
The proof uses newly introduced internal approximation results for ï¬lters associated to reservoir
computing systems, as well as the external approximation properties of state aï¬ƒne systems proved
in a previous work.
Key Words: reservoir computing, universality, echo state networks, ESN, state-aï¬ƒne systems,
SAS, machine learning, fading memory property, echo state property, linear training, uniform system
approximation.
1
Introduction
Many recently introduced machine learning techniques in the context of dynamical problems have much
in common with system identiï¬cation procedures developed in the last decades for applications in signal
treatment, circuit theory and, in general, systems theory.
In these problems, system knowledge is
only available in the form of input-output observations and the task consists in ï¬nding or learning a
model that approximates it for mainly forecasting or classiï¬cation purposes. An important goal in that
context is to ï¬nd a family of transformations that is both computationally feasible and versatile enough
to reproduce a rich number of patterns just by modifying a limited number of procedural parameters.
This feature is usually referred to as universality.
A ï¬rst solution to this problem was pioneered in the works of FrÂ´echet [Frec 10] and Volterra [Volt 30]
one century ago when they proved that ï¬nite Volterra series can be used to uniformly approximate
continuous functionals deï¬ned on compact sets of continuous functions.
These results were further
extended in the 1950s by the MIT school lead by N. Wiener [Wien 58, Bril 58, Geor 59] but always
under compactness assumptions on the input space and the time interval in which inputs are deï¬ned.
A major breakthrough was the generalization to inï¬nite time intervals carried out by Boyd and Chua
in [Boyd 85] using the so called fading memory property.
In this paper we address that problem for transformations or ï¬lters of discrete time signals of inï¬nite
length that have the fading memory property.
The approximating set that we use is generated by
nonlinear state-space transformations and that is referred to as reservoir computers (RC) [Jaeg 10,
Jaeg 04, Maas 02, Maas 11, Croo 07, Vers 07, Luko 09] or reservoir systems. These are special types
of recurrent neural network determined by two maps, namely a reservoir F : RN Ã— Rn âˆ’â†’RN,
1Department of Mathematics and Statistics.
UniversitÂ¨at Konstanz.
Box 146.
D-78457 Konstanz.
Germany.
Lyudmila.Grigoryeva@uni-konstanz.de
2UniversitÂ¨at Sankt Gallen. Faculty of Mathematics and Statistics. Bodanstrasse 6. CH-9000 Sankt Gallen. Switzer-
land. Juan-Pablo.Ortega@unisg.ch
3Centre National de la Recherche Scientiï¬que (CNRS). France.
1
Echo state networks are universal
2
n, N âˆˆN, and a readout map h : RN â†’Rd that under certain hypotheses transform (or ï¬lter) an
inï¬nite discrete-time input z = (. . . , zâˆ’1, z0, z1, . . .) âˆˆ(Rn)Z into an output signal y âˆˆ(Rd)Z of the
same type using the state-space transformation given by:
xt = F(xtâˆ’1, zt),
yt = h(xt),
(1.1)
(1.2)
where t âˆˆZ and the dimension N âˆˆN of the state vectors xt âˆˆRN is referred to as the number of
virtual neurons of the system. When a RC system has a uniquely determined ï¬lter associated to it,
we refer to it as the RC ï¬lter.
An important advantage of the RC approach is that, under certain hypotheses, intrinsically inï¬nite
dimensional problems regarding ï¬lters can be translated into analogous questions related to the reser-
voir and readout maps that generate them and that are deï¬ned on much simpler ï¬nite dimensional
spaces. This strategy has already been used in the literature in relation to the universality question
in, for instance, [Sand 91a, Sand 91b, Matt 92, Matt 93, Perr 96, Stub 97]. The universal approxima-
tion properties of feedforward neural networks [Kolm 56, Arno 57, Spre 65, Spre 96, Spre 97, Cybe 89,
Horn 89, Horn 90, Horn 91, Horn 93, Rusc 98] was used in those works to ï¬nd neural networks-based
families of ï¬lters that are dense in the set of approximately ï¬nite memory ï¬lters with inputs deï¬ned in
the positive half real line. Other works in connection with the universality problem in the RC context
are [Maas 00, Maas 02, Maas 04, Maas 07]. Another strand of interesting literature that we will not
explore in this work has to with the Turing computability capabilities of the systems of the type that
we just introduced; recent relevant works in this direction are [Kili 96, Sieg 97, Cabe 15, Cabe 16], and
references therein.
The main contribution of this paper is showing that a particularly simple type of RC systems called
echo state networks (ESNs) can be used as universal approximants in the context of discrete-time
fading memory ï¬lters with uniformly bounded inputs deï¬ned on negative inï¬nite times. ESNs are RC
systems of the form (1.1)-(1.2) given by:
xt = Ïƒ (Axtâˆ’1 + Czt + Î¶) ,
yt = Wxt.
(1.3)
(1.4)
In these equations, C âˆˆMN,n is called the input mask, Î¶ âˆˆRN is the input shift, and A âˆˆMN,N
is referred to as the reservoir matrix. The map Ïƒ in the state-space equation (1.3) is constructed by
componentwise application of a sigmoid function (like the hyperbolic tangent or the logistic function)
and is called the activation function. Finally, the readout map is linear in this case and implemented
via the readout matrix W âˆˆMd,N.
ESNs already appear in [Matt 92, Matt 93] under the name
of recurrent networks but it was only more recently, in the works of H. Jaeger [Jaeg 04], that their
outstanding performance in machine learning applications was demonstrated.
The strategy that we follow to prove that statement is a combination of what the literature refers
to as internal and external approximation. External approximation is the construction of a RC
ï¬lter that approximates a given (not necessarily RC) ï¬lter. In the internal approximation problem, one
is given a RC ï¬lter and builds another RC ï¬lter that approximates it by ï¬nding reservoir and readout
maps that are close to those of the given one. In the external part of our proof we use a previous
work [Grig 17] where we constructed a family of RC systems with linear readouts that we called non-
homogeneous state aï¬ƒne systems (SAS). We showed in that paper that the RC ï¬lters associated
to SAS systems uniformly approximate any discrete-time fading memory ï¬lter with uniformly bounded
inputs deï¬ned on negative inï¬nite times. Regarding the internal approximation, we show that any RC
ï¬lter, in particular SAS ï¬lters, can be approximated by ESN ï¬lters using the universal approximation
property of neural networks. These two facts put together allow us to conclude that ESN ï¬lters are
Echo state networks are universal
3
capable of uniformly approximating any discrete-time fading memory ï¬lter with uniformly bounded
inputs.
The paper is structured in three sections:
â€¢ Section 2 introduces the notation that we use all along the paper and, more importantly, speciï¬es
the topologies and Banach space structures that we need in order to talk about continuity and
diï¬€erentiability in the context of discrete-time ï¬lters. It is worth mentioning that we characterize
the fading memory property as a continuity condition of the ï¬lters that have it with respect to
the product topology in the input space. On other words, the fading memory property is not a
metric property, as it is usually presented in the literature, but a topological one. An important
conceptual consequence of this fact is that the fading memory property does not contain any
information about the rate at which systems that have it â€œforgetâ€ inputs. Several corollaries can
be formulated as a consequence of this fact that are very instrumental in the developments in the
paper.
â€¢ Section 3 contains a collection of general results in relation with the properties of the RC systems
generated by continuous reservoir maps. In particular, we provide conditions that guarantee that
a unique reservoir ï¬lter can be associated to them (the so called echo state property) and we
identify situations in which those ï¬lters are themselves continuous (they satisfy automatically
the fading memory property). We also point out large classes of RC systems for which internal
approximation is possible, that is, if the RC systems are close then so are the associated reservoir
ï¬lters.
â€¢ Section 4 shows that echo state networks are universal uniform approximants in the category of
discrete-time fading memory ï¬lters with uniformly bounded inputs.
2
Continuous and fading memory ï¬lters
This section introduces the notation of the paper as well as general facts about ï¬lters and functionals
needed in the developments that follow. The new results are contained in Section 2.3, where we charac-
terize the fading memory property as a continuity condition when the sequence spaces where inputs and
outputs are deï¬ned are uniformly bounded and are endowed with the product topology. This feature
makes this property independent of the weighting sequences that are usually introduced to deï¬ne it.
2.1
Notation
Vectors and matrices.
A column vector is denoted by a bold lower case symbol like r and râŠ¤
indicates its transpose. Given a vector v âˆˆRn, we denote its entries by vi, with i âˆˆ{1, . . . , n}; we also
write v = (vi)iâˆˆ{1,...,n}. We denote by Mn,m the space of real n Ã— m matrices with m, n âˆˆN. When
n = m, we use the symbol Mn to refer to the space of square matrices of order n. Given a matrix
A âˆˆMn,m, we denote its components by Aij and we write A = (Aij), with i âˆˆ{1, . . . , n}, j âˆˆ{1, . . . m}.
Given a vector v âˆˆRn, the symbol âˆ¥vâˆ¥stands for any norm in Rn (they are all equivalent) and is not
necessarily the Euclidean one, unless it is explicitly mentioned. The open balls with respect to a given
norm âˆ¥Â·âˆ¥, center v âˆˆRn, and radius r > 0 will be denoted by Bâˆ¥Â·âˆ¥(v, r); their closures by Bâˆ¥Â·âˆ¥(v, r).
For any A âˆˆMn,m, âˆ¥Aâˆ¥2 denotes its matrix norm induced by the Euclidean norms in Rm and Rn, and
satisï¬es [Horn 13, Example 5.6.6] that âˆ¥Aâˆ¥2 = Ïƒmax(A), with Ïƒmax(A) the largest singular value of A.
âˆ¥Aâˆ¥2 is sometimes referred to as the spectral norm of A. The symbol |||Â·||| is reserved for the norms of
operators or functionals deï¬ned on inï¬nite dimensional spaces.
Echo state networks are universal
4
Sequence spaces.
N denotes the set of natural numbers with the zero element included.
Z (re-
spectively, Z+ and Zâˆ’) are the integers (respectively, the positive and the negative integers).
The
symbol (Rn)Z denotes the set of inï¬nite real sequences of the form z = (. . . , zâˆ’1, z0, z1, . . .), zi âˆˆRn,
i âˆˆZ; (Rn)Zâˆ’and (Rn)Z+ are the subspaces consisting of, respectively, left and right inï¬nite sequences:
(Rn)Zâˆ’= {z = (. . . , zâˆ’2, zâˆ’1, z0) | zi âˆˆRn, i âˆˆZâˆ’}, (Rn)Z+ = {z = (z0, z1, z2, . . .) | zi âˆˆRn, i âˆˆZ+}.
Analogously, (Dn)Z, (Dn)Zâˆ’, and (Dn)Z+ stand for (semi-)inï¬nite sequences with elements in the sub-
set Dn âŠ‚Rn. In most cases we endow these inï¬nite product spaces with the Banach space structures
associated to one of the following two norms:
â€¢ The supremum norm: deï¬ne âˆ¥zâˆ¥âˆ:= suptâˆˆZ {âˆ¥ztâˆ¥}. The symbols â„“âˆ(Rn) and â„“âˆ
Â± (Rn) are
used to denote the Banach spaces formed by the elements in the corresponding inï¬nite product
spaces that have a ï¬nite supremum norm.
â€¢ The weighted norm: let w : N âˆ’â†’(0, 1] be a decreasing sequence with zero limit. We deï¬ne the
associated weighted norm
âˆ¥Â· âˆ¥w on (Rn)Zâˆ’associated to the weighting sequence w as the
map:
âˆ¥Â· âˆ¥w :
(Rn)Zâˆ’
âˆ’â†’
R+
z
7âˆ’â†’
âˆ¥zâˆ¥w := suptâˆˆZâˆ’âˆ¥ztwâˆ’tâˆ¥.
The Proposition 5.2 in Appendix 5.10 shows that the space
â„“w
âˆ’(Rn) :=
n
z âˆˆ(Rn)Zâˆ’| âˆ¥zâˆ¥w < âˆ
o
,
endowed with weighted norm âˆ¥Â· âˆ¥w forms also a Banach space.
It is very easy to show that âˆ¥zâˆ¥w â‰¤âˆ¥zâˆ¥âˆ, for all v âˆˆ(Rn)Zâˆ’. This implies that â„“âˆ
âˆ’(Rn) âŠ‚â„“w
âˆ’(Rn) and
that the inclusion map (â„“âˆ
âˆ’(Rn), âˆ¥Â·âˆ¥âˆ) ,â†’(â„“w
âˆ’(Rn, âˆ¥Â·âˆ¥w) is continuous.
2.2
Filters and systems
Filters.
Let Dn âŠ‚Rn and DN âŠ‚RN. We refer to the maps of the type U : (Dn)Z âˆ’â†’(DN)Z as ï¬lters
or operators and to those like H : (Dn)Z âˆ’â†’DN (or H : (Dn)ZÂ± âˆ’â†’DN) as RN-valued functionals.
A ï¬lter U : (Dn)Z âˆ’â†’(DN)Z is called causal when for any two elements z, w âˆˆ(Dn)Z that satisfy
that zÏ„ = wÏ„ for any Ï„ â‰¤t, for a given t âˆˆZ, we have that U(z)t = U(w)t. Let TÏ„ : (Dn)Z âˆ’â†’(Dn)Z
be the time delay operator deï¬ned by TÏ„(z)t := ztâˆ’Ï„. The ï¬lter U is called time-invariant when it
commutes with the time delay operator, that is, TÏ„ â—¦U = U â—¦TÏ„, for any Ï„ âˆˆZ (in this expression, the
two operators TÏ„ have to be understood as deï¬ned in the appropriate sequence spaces).
We recall (see for instance [Boyd 85]) that there is a bijection between causal time-invariant ï¬lters
and functionals on (Dn)Zâˆ’. Indeed, consider the sets F(Dn)Zâˆ’,(DN)Zâˆ’and H(Dn)Zâˆ’,(DN)Zâˆ’
F(Dn)Zâˆ’,(DN)Zâˆ’
=

U : (Dn)Z âˆ’â†’(DN)Z | U is causal and time-invariant
	
,
(2.1)
H(Dn)Zâˆ’,(DN)Zâˆ’
=

H : (Dn)Zâˆ’âˆ’â†’DN
	
.
(2.2)
Then, given a time-invariant ï¬lter U, we can associate to it a functional HU via the assignment HU(z) :=
U(ze)0, where ze âˆˆ(Rn)Z is an arbitrary extension of z âˆˆ(Dn)Zâˆ’to (Dn)Z. Let Î¨ : F(Dn)Zâˆ’,(DN)Zâˆ’âˆ’â†’
H(Dn)Zâˆ’,(DN)Zâˆ’be the map such that Î¨(U) := HU. Conversely, for any functional H, we can deï¬ne a
time-invariant causal ï¬lter UH by UH(z)t := H((PZâˆ’â—¦Tâˆ’t)(z)), where Tâˆ’t is the (âˆ’t)-time delay operator
Echo state networks are universal
5
and PZâˆ’: (Rn)Z âˆ’â†’(Rn)Zâˆ’is the natural projection. Let Î¦ : H(Dn)Zâˆ’,(DN)Zâˆ’âˆ’â†’F(Dn)Zâˆ’,(DN)Zâˆ’be
the map such that Î¦(H) := UH. It is easy to verify that:
Î¨ â—¦Î¦
=
IH
or, equivalently,
HUH = H,
for any functional
H : (Dn)Zâˆ’âˆ’â†’DN,
Î¦ â—¦Î¨
=
IF
or, equivalently,
UHU = U,
for any causal time-invariant ï¬lter
U : (Dn)Z âˆ’â†’(DN)Z,
that is, Î¨ and Î¦ are inverse of each other and hence are both bijections. Additionally, we note that
the sets F(Dn)Zâˆ’,(DN)Zâˆ’and H(Dn)Zâˆ’,(DN)Zâˆ’are vector spaces with naturally deï¬ned operations and
that Î¨ and Î¦ are linear maps between them, which allows us to conclude that F(Dn)Zâˆ’,(DN)Zâˆ’and
H(Dn)Zâˆ’,(DN)Zâˆ’are linear isomorphic.
When a ï¬lter is causal and time-invariant, we work in many situations just with the restriction U :
(Rn)Zâˆ’âˆ’â†’(RN)Zâˆ’instead of the original ï¬lter U : (Rn)Z âˆ’â†’(RN)Z without making the distinction,
since the former uniquely determines the latter. Indeed, by deï¬nition, for any z âˆˆ(Dn)Z and t âˆˆZ:
U(z)t = (Tâˆ’t (U(z)))0 = (U (Tâˆ’t(z)))0 ,
(2.3)
where the second equality holds by the time-invariance of U and the value in the right-hand side depends
only on PZâˆ’(Tâˆ’t(z)) âˆˆ(Rn)Zâˆ’, by causality.
Reservoir systems and ï¬lters.
Consider now the RC system determined by (1.1)â€“(1.2) with reser-
voir map deï¬ned on subsets DN, Dâ€²
N âŠ‚RN and Dn âŠ‚Rn, that is, F : DN Ã— Dn âˆ’â†’Dâ€²
N and
h : Dâ€²
N â†’Rd. There are two properties of reservoir systems that will be crucial in what follows:
â€¢ Existence of solutions property: this property holds when for each z âˆˆ(Dn)Z there exists an
element x âˆˆ(DN)Z that satisï¬es the relation (1.1) for each t âˆˆZ.
â€¢ Uniqueness of solutions or echo state property (ESP): it holds when the system has the
existence of solutions property and, additionally, these solutions are unique.
The echo state property has deserved much attention in the context of echo state networks [Jaeg 10,
Jaeg 04, Bueh 06, Yild 12, Bai 12, Wain 16, Manj 13, Gall 17]. We emphasize that these two properties
are genuine conditions that are not automatically satisï¬ed by all RC systems. Later on in the paper,
Theorem 3.1 speciï¬es suï¬ƒcient conditions for them to hold.
The combination of the existence of solutions with the axiom of choice allows us to associate ï¬lters
U F : (Dn)Z âˆ’â†’(DN)Z to each RC system with that property via the reservoir map and (1.1), that is,
U F (z)t := xt âˆˆRN, for all t âˆˆZ. We will denote by U F
h : (Dn)Z âˆ’â†’(Dd)Z the corresponding ï¬lter
determined by the entire reservoir system, that is, U F
h (z)t = h
 U F (z)t

:= yt âˆˆRd. U F
h is said to be
a reservoir ï¬lter or a response map associated to the RC system (1.1)â€“(1.2). The ï¬lters U F and
U F
h are causal by construction. A unique reservoir ï¬lter can be associated to a reservoir system when
the echo state property holds. We warn the reader that reservoir ï¬lters appear in the literature only
in the presence of the ESP; that is why we sometimes make the distinction between those that come
from reservoir systems that do and do not satisfy the ESP by referring to them as reservoir ï¬lters
and generalized reservoir ï¬lters, respectively.
In the systems theory literature, the RC equations (1.1)â€“(1.2) are referred to as the state-variable
or the internal representation point of view and associated ï¬lters as the external representation
of the system.
The next proposition shows that in the presence of the ESP reservoir ï¬lters are not only causal
but also time-invariant.
In that situation we can hence associate to U F
h a reservoir functional
HF
h : (Dn)Zâˆ’âˆ’â†’Rd determined by HF
h := HU F
h .
Echo state networks are universal
6
Proposition 2.1 Let DN âŠ‚RN, Dn âŠ‚Rn, and F : DN Ã— Dn âˆ’â†’DN be a reservoir map that satisï¬es
the echo state property for all the elements in (Dn)Z. Then, the corresponding ï¬lter U F : (Dn)Z âˆ’â†’
(DN)Z is causal and time-invariant.
We emphasize that it is the autonomous character of the reservoir map that guarantees time-
invariance in the previous proposition. An explicit time dependence on time in that map would spoil
that conclusion.
Reservoir system morphisms.
Let N1, N2, n, d âˆˆN and let F1 : DN1 Ã—Dn âˆ’â†’DN1, h1 : DN1 â†’Rd
and F2 : DN2 Ã— Dn âˆ’â†’DN2, h2 : DN2 â†’Rd be two reservoir systems. We say that a map f : DN1 âˆ’â†’
DN2 is a morphism between the two systems when it satisï¬es the following two properties:
(i) Reservoir equivariance: f(F1(x1, z)) = F2(f(x1), z), for all x1 âˆˆDN1, and z âˆˆDn.
(ii) Readout invariance: h1(x1) = h2(f(x1)), for all x1 âˆˆDN1.
When the map f has an inverse and it is also a morphism between the systems determined by the
pairs (F2, h2) and (F1, h1) we say that f is a system isomorphism and that (F1, h1) and (F2, h2) are
isomorphic. Given a system F1 : DN1 Ã—Dn âˆ’â†’DN1, h1 : DN1 â†’Rd and a bijection f : DN1 âˆ’â†’DN2,
the map f is a system isomorphism with respect to the system F2 : DN2 Ã— Dn âˆ’â†’DN2, h2 : DN2 â†’Rd
deï¬ned by
F2(x2, z)
:=
f
 F1(f âˆ’1(x2), z)

,
for all
x2 âˆˆDN2, z âˆˆDn,
(2.4)
h2(x2)
:=
h1(f âˆ’1(x2))),
for all
x2 âˆˆDN2.
(2.5)
The proof of the following statement is a straightforward consequence of the deï¬nitions.
Proposition 2.2 Let F : DN1Ã—Dn âˆ’â†’DN1, h : DN1 â†’Rd and F : DN2Ã—Dn âˆ’â†’DN2, h : DN2 â†’Rd
be two reservoir systems. Let f : DN1 âˆ’â†’DN2 be a morphism between them. Then:
(i) If x1 âˆˆ(DN1)Z is a solution for the reservoir map F1 associated to the input z âˆˆ(Dn)Z, then the
sequence x2 âˆˆ(DN2)Z deï¬ned by x2
t := f
 x1
t

, t âˆˆZ, is a solution for the reservoir map F2
associated to the same input.
(ii) If U F1
h1 is a reservoir ï¬lter for the system determined by the pair (F1, h1) then it is also a reservoir
ï¬lter for the system (F2, h2).
(iii) If f is a system isomorphism then the implications in the previous two points are equivalences.
2.3
Continuity and the fading memory property
In agreement with the notation introduced in the previous section, in the following paragraphs the
symbol U : (Dn)Zâˆ’âˆ’â†’(DN)Zâˆ’stands for a causal and time-invariant ï¬lter or, strictly speaking, for
the restriction of U : (Dn)Z âˆ’â†’(DN)Z to Zâˆ’, see (2.3); HU : (Dn)Zâˆ’âˆ’â†’DN is the associated
functional, for some DN âŠ‚RN and Dn âŠ‚Rn. Analogously, UH is the ï¬lter associated to a given
functional H.
Deï¬nition 2.3 (Continuous ï¬lters and functionals) Let DN âŠ‚RN and Dn âŠ‚Rn be such that
(Dn)Zâˆ’âŠ‚â„“âˆ
âˆ’(Rn) and (DN)Zâˆ’âŠ‚â„“âˆ
âˆ’(RN). A causal and time-invariant ï¬lter U : (Dn)Zâˆ’âˆ’â†’(DN)Zâˆ’
is called continuous when it is a continuous map between the metric spaces

(Dn)Zâˆ’, âˆ¥Â·âˆ¥âˆ

and

(DN)Zâˆ’, âˆ¥Â·âˆ¥âˆ

.
An analogous prescription can be used to deï¬ne continuous functionals H :
Echo state networks are universal
7

(Dn)Zâˆ’, âˆ¥Â·âˆ¥âˆ

âˆ’â†’(DN, âˆ¥Â·âˆ¥). We denote by Fâˆ
(Dn)Zâˆ’,(DN)Zâˆ’and Hâˆ
(Dn)Zâˆ’,(DN)Zâˆ’the set of continuous
ï¬lters and functionals, respectively.
The following proposition shows that when ï¬lters are causal and time-invariant, their continuity can
be read out of their corresponding functionals and viceversa.
Proposition 2.4 Let Dn âŠ‚Rn and DN âŠ‚RN be such that (Dn)Zâˆ’âŠ‚â„“âˆ
âˆ’(Rn) and (DN)Zâˆ’âŠ‚â„“âˆ
âˆ’(RN).
Let U : (Dn)Zâˆ’âˆ’â†’(DN)Zâˆ’be a causal and time-invariant ï¬lter, H : (Dn)Zâˆ’âˆ’â†’DN a functional, and
let Î¦ and Î¨ be the maps deï¬ned in the previous section. Then, if the ï¬lter U is continuous then so is the
associated functional Î¨(U) =: HU. Conversely, if H is continuous then so is Î¦(H) =: UH. This implies
that the maps Î¨ : Fâˆ
(Dn)Zâˆ’,(DN)Zâˆ’âˆ’â†’Hâˆ
(Dn)Zâˆ’,(DN)Zâˆ’and Î¦ : Hâˆ
(Dn)Zâˆ’,(DN)Zâˆ’âˆ’â†’Fâˆ
(Dn)Zâˆ’,(DN)Zâˆ’are
linear isomorphisms and are inverses of each other.
Deï¬nition 2.5 (Fading memory ï¬lters and functionals) Let w : N âˆ’â†’(0, 1] be a weighting se-
quence and let DN âŠ‚RN and Dn âŠ‚Rn be such that (Dn)Zâˆ’âŠ‚â„“w
âˆ’(Rn) and (DN)Zâˆ’âŠ‚â„“w
âˆ’(RN).
We say that a causal and time-invariant ï¬lter U : (Dn)Zâˆ’âˆ’â†’(DN)Zâˆ’(respectively, a functional
H : (Dn)Zâˆ’âˆ’â†’DN) satisï¬es the fading memory property (FMP) with respect to the sequence w
when it is a continuous map between the metric spaces

(Dn)Zâˆ’, âˆ¥Â·âˆ¥w

and

(DN)Zâˆ’, âˆ¥Â·âˆ¥w

(respec-
tively,

(Dn)Zâˆ’, âˆ¥Â·âˆ¥w

and (DN, âˆ¥Â·âˆ¥)). We denote by Fw
(Dn)Zâˆ’,(DN)Zâˆ’and Hw
(Dn)Zâˆ’,(DN)Zâˆ’the set of
FMP ï¬lters and functionals, respectively. If the weighting sequence w is such that wt = Î»t, for some
Î» âˆˆ(0, 1) and all t âˆˆN, then U is said to have the Î»-exponential fading memory property.
A very important part of the results that follow concern uniformly bounded families of sequences,
that is, subsets of (Rn)Zâˆ’of the form
KM :=
n
z âˆˆ(Rn)Zâˆ’| âˆ¥ztâˆ¥â‰¤M
for all
t âˆˆZâˆ’
o
,
for some M > 0.
(2.6)
It is straightforward to show that KM âŠ‚â„“âˆ
âˆ’(Rn) âŠ‚â„“w
âˆ’(Rn), for all M > 0. A very useful fact is that
the relative topology induced by (â„“w
âˆ’(Rn), âˆ¥Â·âˆ¥w) in KM coincides with the one induced by the product
topology in (Rn)Zâˆ’.
This is a consequence of the following result that is a slight generalization of
[Munk 14, Theorem 20.5]. A proof is provided in Appendix 5.3 for the sake of completeness.
Theorem 2.6 Let âˆ¥Â·âˆ¥: Rn âˆ’â†’[0, âˆ) be a norm in Rn, M > 0, and let w : N âˆ’â†’(0, 1] be a weighting
sequence. Let dM(a, b) := min {âˆ¥a âˆ’bâˆ¥, M}, a, b âˆˆRn, be a bounded metric on Rn and deï¬ne the
w-weighted metric DM
w on (Rn)Zâˆ’as
DM
w (x, y) := sup
tâˆˆZâˆ’

dM(xt, yt)wâˆ’t
	
,
x, y âˆˆ(Rn)Zâˆ’.
(2.7)
Then DM
w
is a metric that induces the product topology on (Rn)Zâˆ’.
The space (Rn)Zâˆ’is complete
relative to this metric.
An important consequence that can be drawn from this theorem is that all the weighted norms induce
the same topology on the subspaces formed by uniformly bounded sequences. An obvious consequence
of this fact is that continuity with respect to this topology can be deï¬ned without the help of weighting
sequences or, equivalently, ï¬lters or functionals with uniformly bounded inputs that have the fading
memory with respect to a weighting sequence, have the same feature with respect to any other weighting
sequence. We make this more speciï¬c in the following statements.
Echo state networks are universal
8
Corollary 2.7 Let M > 0 and let KM :=
n
z âˆˆ(Rn)Zâˆ’| âˆ¥ztâˆ¥â‰¤M
for all
t âˆˆZâˆ’
o
be a subset of
(Rn)Zâˆ’formed by uniformly bounded sequences. Let w : N âˆ’â†’(0, 1] be an arbitrary weighting sequence.
Then, the metric induced by the weighted norm âˆ¥Â·âˆ¥w on KM coincides with D2M
w . Moreover, since D2M
w
induces the product topology on KM =

Bâˆ¥Â·âˆ¥(0, M)
Zâˆ’
, we can conclude that all the weighted norms
induce the same topology on KM. The same conclusion holds when instead of KM we consider the set
(Dn)Zâˆ’, with Dn a compact subset of Rn.
Theorem 2.6 can also be used to give a quick alternative proof in discrete time to an important
compactness result originally formulated in Boyd and Chua in [Boyd 85, Lemma 1] for continuous time
and, later on, in [Grig 17] for discrete time. The next corollary contains an additional completeness
statement.
Corollary 2.8 Let KM be the set of uniformly bounded sequences, deï¬ned as in (2.6), and let w :
N âˆ’â†’(0, 1] be a weighting sequence. Then, (KM, âˆ¥Â·âˆ¥w) is a compact, complete, and convex subset of the
Banach space (â„“w
âˆ’(Rn), âˆ¥Â·âˆ¥w). The compactness and the completeness statements also hold when instead
of KM we consider the set (Dn)Zâˆ’, with Dn a compact subset of Rn; if Dn is additionally convex then
the convexity of (Dn)Zâˆ’is also guaranteed.
It is important to point out that the coincidence between the product topology and the topologies
induced by weighted norms that we described in Corollary 2.7 only occurs for uniformly bounded sets
of the type introduced in (2.6). As we state in the next result, the norm topology in â„“w
âˆ’(Rn) is strictly
ï¬ner than the one induced by the product topology in (Rn)Zâˆ’.
Proposition 2.9 Let w : N âˆ’â†’(0, 1] be a weighting sequence and let (â„“w
âˆ’(Rn), âˆ¥Â·âˆ¥w) be the Banach
space constructed using the corresponding weighted norm on the space of left inï¬nite sequences with
elements in Rn. The norm topology in â„“w
âˆ’(Rn) is strictly ï¬ner than the subspace topology induced by the
product topology in (Rn)Zâˆ’on â„“w
âˆ’(Rn) âŠ‚(Rn)Zâˆ’.
The next proposition spells out how the fading memory property is independent of the weighting
sequence that is used to deï¬ne it, which shows its intrinsically topological nature. A conceptual conse-
quence of this fact is that the fading memory property does not contain any information about the rate
at which systems that have it â€œforgetâ€ inputs. A similar statement in the continuous time setup has
been formulated in [Sand 03]. Additionally, there is a bijection between FMP ï¬lters and functionals.
Proposition 2.10 Let KM âŠ‚(Rn)Zâˆ’and KL âŠ‚
 RNZâˆ’be subsets of uniformly bounded sequences
deï¬ned as in (2.6) and let w : N âˆ’â†’(0, 1] be a weighting sequence. Let U : KM âˆ’â†’KL be a causal and
time-invariant ï¬lter and let H : KM âˆ’â†’Bâˆ¥Â·âˆ¥(0, L) be a functional. Then:
(i) If U (respectively H) has the fading memory property with respect to the weighting sequence w, then
it has the same property with respect to any other weighting sequence. In particular, this implies
that
Fw
KM,KL = Fwâ€²
KM,KL
and
Hw
KM,KL = Hwâ€²
KM,KL,
for any weighting sequence wâ€².
In what follows we just say that U (respectively H) has the fading memory property and denote
FFMP
KM,KL := Fw
KM,KL
and
HFMP
KM,KL := Hw
KM,KL,
for any weighting sequence w.
(ii) Let Î¦ and Î¨ be the maps deï¬ned in the previous section. Then if the ï¬lter U has the fading memory
property then so does the associated functional Î¨(U) =: HU. Analogously, if H has the fading
memory property then so does Î¦(H) =: UH. This implies that the maps Î¨ : FFMP
KM,KL âˆ’â†’HFMP
KM,KL
and Î¦ : HFMP
KM,KL âˆ’â†’FFMP
KM,KL are linear isomorphisms and are inverses of each other.
Echo state networks are universal
9
The same statements can be formulated when instead of KM and KL we consider the sets (Dn)Zâˆ’and
(DN)Zâˆ’, with Dn and DN compact subsets of Rn and RN, respectively.
In the conditions of the previous proposition, the vector spaces FFMP
KM,KL and HFMP
KM,KL can be endowed
with a norm. More speciï¬cally, let U : KM âˆ’â†’KL be a ï¬lter and let H : KM âˆ’â†’Bâˆ¥Â·âˆ¥(0, L) be a
functional that have the FMP. Deï¬ne:
|||U|||âˆ
:=
sup
zâˆˆKM
{âˆ¥U(z)âˆ¥âˆ} = sup
zâˆˆKM
(
sup
tâˆˆZâˆ’
{âˆ¥U(z)tâˆ¥}
)
,
(2.8)
|||H|||âˆ
:=
sup
zâˆˆKM
{âˆ¥H(z)âˆ¥} .
(2.9)
The compactness of (KM, âˆ¥Â·âˆ¥w) guaranteed by Corollary 2.8 and the fact that U and H map into KL and
Bâˆ¥Â·âˆ¥(0, L), respectively, ensures that the values in (2.8) and (2.9) are ï¬nite which makes
 FFMP
KM,KL, |||Â·|||âˆ

and
 HFMP
KM,KL, |||Â·|||âˆ

into normed spaces that, as the next result shows, are linearly homeomorphic.
Proposition 2.11 The maps Î¨ :
 FFMP
KM,KL, |||Â·|||âˆ

âˆ’â†’
 HFMP
KM,KL, |||Â·|||âˆ

and its inverse Î¦ satisfy that
|||Î¨(U1) âˆ’Î¨(U2)|||âˆ
â‰¤
|||U1 âˆ’U2|||âˆ,
for any
U1, U2 âˆˆFFMP
KM,KL,
(2.10)
|||Î¦(H1) âˆ’Î¦(H2)|||âˆ
â‰¤
|||H1 âˆ’H2|||âˆ,
for any
H1, H2 âˆˆHFMP
KM,KL.
(2.11)
These inequalities imply that these two maps are continuous linear bijections and hence the spaces
 FFMP
KM,KL, |||Â·|||âˆ

and
 HFMP
KM,KL, |||Â·|||âˆ

are homeomorphic.
The same statements can be formulated
when instead of KM and KL we consider the sets (Dn)Zâˆ’and (DN)Zâˆ’, with Dn and DN compact
subsets of Rn and RN, respectively.
3
Internal approximation of reservoir ï¬lters
This section characterizes situations under which reservoir ï¬lters can be uniformly approximated by
ï¬nding uniform approximants for the corresponding reservoir systems. Such a statement is part of the
next theorem that also identiï¬es criteria for the availability of the echo state and the fading memory
properties. As it was already mentioned, a reservoir system has the ESP when it has a unique semi-
inï¬nite solution for each semi-inï¬nite input. We also recall that, as it was shown in Section 2.3, the FMP
amounts to the continuity of a reservoir ï¬lter with respect to the product topologies on the input and
output spaces. The completeness and compactness of those spaces established in Corollary 2.8 allows
us to use various ï¬xed point theorems to show that solutions for reservoir systems exist under very
weak hypotheses and that for contracting and continuous reservoir maps (we deï¬ne this below) these
solutions are unique and depend continuously on the inputs. Said diï¬€erently, contracting continuous
reservoir maps induce reservoir ï¬lters that automatically have the echo state and the fading memory
properties.
Theorem 3.1 Let KM âŠ‚(Rn)Zâˆ’and KL âŠ‚
 RNZâˆ’be subsets of uniformly bounded sequences deï¬ned
as in (2.6) and let F : Bâˆ¥Â·âˆ¥(0, L) Ã— Bâˆ¥Â·âˆ¥(0, M) âˆ’â†’Bâˆ¥Â·âˆ¥(0, L) be a continuous reservoir map.
(i) Existence of solutions: for each z âˆˆKM there exists a x âˆˆKL (not necessarily unique) that
solves the reservoir equation associated to F, that is,
xt = F(xtâˆ’1, zt),
for all t âˆˆZâˆ’.
Echo state networks are universal
10
(ii) Uniqueness and continuity of solutions (ESP and FMP): suppose that the reservoir map F
is a contraction, that is, there exists 0 < r < 1 such that for all u, v âˆˆBâˆ¥Â·âˆ¥(0, L), z âˆˆBâˆ¥Â·âˆ¥(0, M),
one has
âˆ¥F(u, z) âˆ’F(v, z)âˆ¥â‰¤r âˆ¥u âˆ’vâˆ¥.
Then, the reservoir system associated to F has the echo state property. Moreover, this system has
a unique associated causal and time-invariant ï¬lter UF : KM âˆ’â†’KL that has the fading memory
property, that is, UF âˆˆFFMP
KM,KL. The set UF (KM) of accessible states of the ï¬lter UF is compact.
(iii) Internal approximation property: let F1, F2 : Bâˆ¥Â·âˆ¥(0, L) Ã— Bâˆ¥Â·âˆ¥(0, M) âˆ’â†’Bâˆ¥Â·âˆ¥(0, L) be two
continuous reservoir maps such that F1 is a contraction with constant 0 < r < 1 and F2 has the
existence of solutions property. Let UF1, UF2 : KM âˆ’â†’KL be the corresponding ï¬lters (if F2 does
not have the ESP, then UF2 is just a generalized ï¬lter). Then, for any Ïµ > 0, we have that
âˆ¥F1 âˆ’F2âˆ¥âˆ< Î´(Ïµ) := (1 âˆ’r)Ïµ
implies that
|||UF1 âˆ’UF2|||âˆ< Ïµ.
(3.1)
Part (i) also holds true when instead of KM and KL we consider the sets (Dn)Zâˆ’and (DN)Zâˆ’, with Dn
and DN compact and convex subsets of Rn and RN, respectively, that replace the closed balls Bâˆ¥Â·âˆ¥(0, M)
and Bâˆ¥Â·âˆ¥(0, L). The same applies to parts (ii) and (iii) but, this time, the convexity hypothesis is not
needed.
Deï¬ne the set KKM,KL :=
n
F : Bâˆ¥Â·âˆ¥(0, L) Ã— Bâˆ¥Â·âˆ¥(0, M) âˆ’â†’Bâˆ¥Â·âˆ¥(0, L) | F is a continuous contraction
o
.
Using the notation introduced in the previous section, the statement in (3.1) and part (ii) of the theorem
automatically imply that the map
Î :
(KKM,KL, âˆ¥Â·âˆ¥âˆ)
âˆ’â†’
 FFMP
KM,KL, |||Â·|||âˆ

F
7âˆ’â†’
UF
is continuous and by Proposition 2.11, the map that associates to each F âˆˆKKM,KL the corresponding
functional HF , that is,
Î¨ â—¦Î :
(KKM,KL, âˆ¥Â·âˆ¥âˆ)
âˆ’â†’
 HFMP
KM,KL, |||Â·|||âˆ

F
7âˆ’â†’
HF ,
is also continuous.
Proof of the theorem.
(i) We start by deï¬ning, for each z âˆˆKM, the map given by
Fz :
KL
âˆ’â†’
KL
x
7âˆ’â†’
(Fz(x))t := F(xtâˆ’1, zt).
We show ï¬rst that Fz can be written as a product of continuous functions. Indeed:
Fz =
Y
tâˆˆZâˆ’
F(Â·, zt) â—¦ptâˆ’1(x),
(3.2)
where the projections pt : KL âˆ’â†’Bâˆ¥Â·âˆ¥(0, L) are given by pt(x) = xt. These projections are continuous
when we consider in KL the product topology. Additionally, the continuity of the reservoir F implies
that Fz is a product of continuous functions, which ensures that Fz is itself continuous [Munk 14,
Theorem 19.6]. Moreover, by the corollaries 2.7 and 2.8, the space KL is a compact and convex subset
of the Banach space
 â„“w
âˆ’(Rn), âˆ¥Â· âˆ¥w

(see Proposition 5.2), for any weighting sequence w. Schauderâ€™s
Echo state networks are universal
11
Fixed Point Theorem (see [Shap 16, Theorem 7.1, page 75]) guarantees then that Fz has at least a ï¬xed
point, that is, a point x âˆˆKL that satisï¬es Fz(x) = x or, equivalently,
xt = F(xtâˆ’1, zt),
for all t âˆˆZâˆ’,
which implies that x is a solution of F for z, as required.
Proof of part (ii) The main tool in the proof of this part is a parameter dependent version of the
Contraction Fixed Point Theorem, that we include here for the sake of completeness and whose proof
can be found in [Ster 10, Theorem 6.4.1, page 137].
Theorem 3.2 Let (X, dX) be a complete metric space and let Z be a metric space. Let K : XÃ—Z âˆ’â†’X
be a continuous map such that for each z âˆˆZ, the map Kz : X âˆ’â†’X given by Kz(x) := K(x, z) is a
contraction with a constant 0 < r < 1 (independent of z), that is, dX(K(x, z), K(y, z)) â‰¤rd(x, y), for
all x, y âˆˆX and all z âˆˆZ. Then:
(i) For each z âˆˆZ, the map Kz has a unique ï¬xed point in X.
(ii) The map UK : Z âˆ’â†’X that associates to each point z âˆˆZ the unique ï¬xed point of Kz is
continuous.
Consider now the map
F :
KL Ã— KM
âˆ’â†’
KL
(x, z)
7âˆ’â†’
(F(x, z))t := F(xtâˆ’1, zt).
First, as we did in (3.2), it is easy to show that F is continuous with respect to the product topologies
in KM and KL, by writing it down as the product of the composition of continuous functions. Second,
we show that the map F is a contraction. Indeed, since by Corollary 2.7 we can choose an arbitrary
weighting sequence to generate the product topologies in KM and KL, we select w : N âˆ’â†’(0, 1] given
by wt := Î»t, with t âˆˆN and Î» > 0 that satisï¬es 0 < r < Î» < 1. Then, for any x, y âˆˆKL and any
z âˆˆKM, we have
âˆ¥F(x, z) âˆ’F(y, z)âˆ¥w = sup
tâˆˆZâˆ’

âˆ¥F(xtâˆ’1, zt) âˆ’F(ytâˆ’1, zt)âˆ¥Î»âˆ’t	
â‰¤sup
tâˆˆZâˆ’

âˆ¥xtâˆ’1 âˆ’ytâˆ’1âˆ¥rÎ»âˆ’t	
,
where we used that F is a contraction. Now, since 0 < r < Î» < 1 and hence r/Î» < 1, we have
sup
tâˆˆZâˆ’

âˆ¥xtâˆ’1 âˆ’ytâˆ’1âˆ¥rÎ»âˆ’t	
= sup
tâˆˆZâˆ’
n
âˆ¥xtâˆ’1 âˆ’ytâˆ’1âˆ¥Î»âˆ’(tâˆ’1) r
Î»
o
â‰¤r
Î» âˆ¥x âˆ’yâˆ¥w .
This shows that F is a family of contractions with constant r/Î» < 1 that is continuously parametrized
by the elements in KM. Theorem 3.2 implies the existence of a continuous map UF : (KM, âˆ¥Â·âˆ¥w) âˆ’â†’
(KL, âˆ¥Â·âˆ¥w) that is uniquely determined by the identity
F (UF (z), z) = UF (z),
for all z âˆˆKM.
Proposition 2.1 implies that UF is causal and time-invariant. The set UF (KM) of accessible states of
the ï¬lter UF is compact because it is the image of a compact set (see Corollary 2.8) by a continuous
map (see [Munk 14, Theorem 26.5, page 166]).
Proof of part (iii) Let z âˆˆKM and let UF1(z) be the unique solution for z of the reservoir systems
associated to F1 available by the part (ii) of the theorem that we just proved. Additionally, let UF2(z)
Echo state networks are universal
12
be the value of a generalized ï¬lter associated to F2 that exist by hypothesis. Then, for any t âˆˆZâˆ’, we
have:
âˆ¥UF1(z)t âˆ’UF2(z)tâˆ¥= âˆ¥F1(UF1(z)tâˆ’1, zt) âˆ’F2(UF2(z)tâˆ’1, zt)âˆ¥
= âˆ¥F1(UF1(z)tâˆ’1, zt) âˆ’F1(UF2(z)tâˆ’1, zt) + F1(UF2(z)tâˆ’1, zt) âˆ’F2(UF2(z)tâˆ’1, zt)âˆ¥
â‰¤âˆ¥F1(UF1(z)tâˆ’1, zt) âˆ’F1(UF2(z)tâˆ’1, zt)âˆ¥+ âˆ¥F1(UF2(z)tâˆ’1, zt) âˆ’F2(UF2(z)tâˆ’1, zt)âˆ¥
â‰¤r âˆ¥UF1(z)tâˆ’1 âˆ’UF2(z)tâˆ’1âˆ¥+ âˆ¥F1(UF2(z)tâˆ’1, zt) âˆ’F2(UF2(z)tâˆ’1, zt)âˆ¥.
If we now recursively apply n times the same procedure to the ï¬rst summand of this expression, we
obtain that
âˆ¥UF1(z)t âˆ’UF2(z)tâˆ¥â‰¤rnâˆ¥UF1(z)tâˆ’n âˆ’UF2(z)tâˆ’nâˆ¥+ âˆ¥F1(UF2(z)tâˆ’1, zt) âˆ’F2(UF2(z)tâˆ’1, zt)âˆ¥
+ r âˆ¥F1(UF2(z)tâˆ’2, ztâˆ’1) âˆ’F2(UF2(z)tâˆ’2, ztâˆ’1)âˆ¥
+ Â· Â· Â· + rnâˆ’1 F1(UF2(z)tâˆ’n, ztâˆ’(n+1)) âˆ’F2(UF2(z)tâˆ’n, ztâˆ’(n+1))

(3.3)
If we combine the inequality (3.3) with the hypothesis
âˆ¥F1 âˆ’F2âˆ¥âˆ=
sup
xâˆˆBâˆ¥Â·âˆ¥(0,L), zâˆˆBâˆ¥Â·âˆ¥(0,M)
{âˆ¥F1(x, z) âˆ’F2(x, z)âˆ¥} < Î´(Ïµ) := (1 âˆ’r)Ïµ,
we obtain
âˆ¥UF1(z) âˆ’UF2(z)âˆ¥âˆ= sup
tâˆˆZâˆ’
{âˆ¥UF1(z)t âˆ’UF2(z)tâˆ¥}
â‰¤2Lrn + (1 + Â· Â· Â· + rnâˆ’1)Î´(Ïµ) = 2Lrn + 1 âˆ’rn
1 âˆ’r Î´(Ïµ)
(3.4)
Since this inequality is valid for any n âˆˆN, we can take the limit n âˆ’â†’âˆand we obtain that
âˆ¥UF1(z) âˆ’UF2(z)âˆ¥âˆâ‰¤Î´(Ïµ)
1 âˆ’r = Ïµ.
Additionally, as this relation is valid for any z âˆˆKM, we can conclude that
|||UF1 âˆ’UF2|||âˆ= sup
zâˆˆKM
{âˆ¥UF1(z) âˆ’UF2(z)âˆ¥âˆ} â‰¤Ïµ,
as required.
â– 
As a straightforward corollary of the ï¬rst part of the previous theorem, it is easy to show that echo
state networks always have (generalized) reservoir ï¬lters associated as well as to formulate conditions
that ensure simultaneously the echo state and the fading memory properties.
We recall that a map Ïƒ : R âˆ’â†’[âˆ’1, 1] is a squashing function if it is non-decreasing, limxâ†’âˆ’âˆÏƒ(x) =
âˆ’1, and limxâ†’âˆÏƒ(x) = 1.
Corollary 3.3 Consider echo state network given by
xt = Ïƒ (Axtâˆ’1 + Czt + Î¶) ,
yt = Wxt,
(3.5)
(3.6)
where C âˆˆMN,n for some N âˆˆN, Î¶ âˆˆRN, A âˆˆMN,N, W âˆˆMd,N, and the input signal z âˆˆ(Dn)Z,
with Dn âŠ‚Rn a compact and convex subset. The function Ïƒ : RN âˆ’â†’[âˆ’1, 1]N in (3.5) is constructed
by componentwise application of a squashing function that we also call Ïƒ. Then:
Echo state networks are universal
13
(i) If the squashing function Ïƒ is continuous, then the reservoir equation (3.5) has the existence of
solutions property and we can hence associate to the system (3.5)-(3.6) a generalized reservoir
ï¬lter.
(ii) If the squashing function Ïƒ is diï¬€erentiable with Lipschitz constant LÏƒ := supxâˆˆR |Ïƒâ€²(x)| < âˆ
and the matrix A is such that âˆ¥Aâˆ¥2 LÏƒ = Ïƒmax(A)LÏƒ < 1, then the reservoir system (3.5)-(3.6)
has the echo state and the fading memory properties and we can hence associate to it a unique
time-invariant reservoir ï¬lter.
The statement in part (i) remains valid when [âˆ’1, 1]N is replaced by a compact and convex subset
DN âŠ‚[âˆ’1, 1]N that is left invariant by the reservoir equation (3.5), that is, Ïƒ (Ax + Cz + Î¶) âˆˆDN for
any x âˆˆDN and any z âˆˆDn. The same applies to part (ii) but only the compactness hypothesis is
necessary.
Remark 3.4 The hypothesis âˆ¥Aâˆ¥2 LÏƒ < 1 appears in the literature as a suï¬ƒcient condition to ensure
the echo state property, which has been extensively studied in the ESN literature [Jaeg 10, Jaeg 04,
Bueh 06, Bai 12, Yild 12, Wain 16, Manj 13]. Our result shows that this condition implies automati-
cally the fading memory property. Nevertheless, that condition is far from being sharp and has been
signiï¬cantly improved in [Bueh 06, Yild 12]. We point out that the enhanced suï¬ƒcient conditions for
the echo state property contained in those references also imply the fading memory property via part
(ii) of Theorem 3.1.
4
Echo state networks as universal uniform approximants
The internal approximation property that we introduced in part (ii) of Theorem 3.1 tells us that we
can approximate any reservoir ï¬lter by ï¬nding an approximant for the reservoir system that generates
it. This reduces the problem of proving a density statement in a space of operators between inï¬nite-
dimensional spaces to a space of functions with ï¬nite dimensional variables and values. This topic is
the subject of many results in approximation theory, some of which we mentioned in the introduction.
This strategy allows one to ï¬nd simple approximating reservoir ï¬lters for any reservoir system that has
the fading memory property. In the next result we use as approximating family the echo state networks
that we presented in the introduction and that, as we see later on, are the natural generalizations of
neural networks in a dynamic learning setup, with the important feature that they are constructed using
linear readouts. The combination of this approach with a previously obtained result [Grig 17] on the
density of reservoir ï¬lters on the fading memory category allows us to prove in the next theorem that
echo state networks can approximate any fading memory ï¬lter. On other words, echo state networks
are universal.
All along this section, we use the Euclidean norm for the ï¬nite dimensional spaces, that is, for
each x âˆˆRn, we write âˆ¥xâˆ¥:=
 Pn
i=1 x2
i
1/2. For any M > 0, the symbol Bâˆ¥Â·âˆ¥(0, M) (respectively
Bâˆ¥Â·âˆ¥(0, M)) denotes here the open (respectively closed) balls with respect to that norm. Additionally,
we set In := Bâˆ¥Â·âˆ¥(0, 1).
Theorem 4.1 Let U : IZâˆ’
n
âˆ’â†’
 RdZâˆ’be a causal and time-invariant ï¬lter that has the fading memory
ï¬lter. Then, for any Ïµ > 0 and any weighting sequence w, there is an echo state network
xt = Ïƒ (Axtâˆ’1 + Czt + Î¶) ,
yt = Wxt.
(4.1)
(4.2)
whose associated generalized ï¬lters UESN : IZâˆ’
n
âˆ’â†’
 RdZâˆ’satisfy that
|||U âˆ’UESN|||âˆ< Ïµ.
(4.3)
Echo state networks are universal
14
In these expressions C âˆˆMN,n for some N âˆˆN, Î¶ âˆˆRN, A âˆˆMN,N, and W âˆˆMd,N. The function
Ïƒ : RN âˆ’â†’[âˆ’1, 1]N in (4.1) is constructed by componentwise application of a continuous squashing
function Ïƒ : R âˆ’â†’[âˆ’1, 1] that we denote with the same symbol.
When the approximating echo state network (4.1)-(4.2) satisï¬es the echo state property, then it has a
unique ï¬lter UESN associated which is necessarily time-invariant. The corresponding reservoir functional
HESN : IZâˆ’
n
âˆ’â†’Rd satisï¬es that
|||HU âˆ’HESN|||âˆ< Ïµ.
(4.4)
Proof.
As we already explained, we proceed by ï¬rst approximating the ï¬lter U by one of the non-
homogeneous state-aï¬ƒne system (SAS) reservoir ï¬lters introduced in [Grig 17], and we later on show
that we can approximate that reservoir ï¬lter by an echo state network like the one in (4.1)-(4.2).
We start by recalling that a non-homogeneous state-aï¬ƒne system is a reservoir system determined
by the state-space transformation:
xt = p(zt)xtâˆ’1 + q(zt),
yt = W1xt,
(4.5)
(4.6)
where the inputs zt âˆˆIn := Bâˆ¥Â·âˆ¥(0, 1), the states xt âˆˆRN1, for some N1 âˆˆN, and W1 âˆˆMd,N1. The
symbols p(zt) and q(zt) stand for polynomials with matrix coeï¬ƒcients and degrees r and s, respectively,
of the form:
p(z)
=
X
i1,...,inâˆˆ{0,...,r}
i1+Â·Â·Â·+inâ‰¤r
zi1
1 Â· Â· Â· zin
n Ai1,...,in,
Ai1,...,in âˆˆMN1,
z âˆˆIn
q(z)
=
X
i1,...,inâˆˆ{0,...,s}
i1+Â·Â·Â·+inâ‰¤s
zi1
1 Â· Â· Â· zin
n Bi1,...,in,
Bi1,...,in âˆˆMN1,1,
z âˆˆIn.
Let L > 0 and choose a real number K such that
0 < K <
L
L + 1 < 1.
(4.7)
Consider now SAS ï¬lters that satisfy that maxzâˆˆIn Ïƒmax(p(z)) < K and maxzâˆˆIn Ïƒmax(p(z)) < K.
It can be shown [Grig 17, Proposition 3.7] that under those hypotheses, the reservoir system (4.5)-
(4.6) has the echo state property and deï¬nes a unique causal, time-invariant, and fading memory ï¬lter
U p,q
W1 : IZâˆ’
n
âˆ’â†’(Rd)Zâˆ’. Moreover, Theorem 3.12 in [Grig 17] shows that for any Ïµ1 > 0, there exists a
SAS ï¬lter U p,q
W1 satisfying the hypotheses that we just discussed, for which
HU âˆ’Hp,q
W1

âˆ< Ïµ1,
(4.8)
where HU and Hp,q
W1 are the reservoir functionals associated to U and U p,q
W1, respectively. Proposition
2.11 together with this inequality imply that
U âˆ’U p,q
W1

âˆ< Ïµ1.
(4.9)
We now show that the SAS ï¬lter U p,q
W1 can be approximated by the ï¬lters generated by an echo state
network. Deï¬ne the map
FSAS :
Bâˆ¥Â·âˆ¥(0, L) Ã— In
âˆ’â†’
RN1
(x, z)
7âˆ’â†’
p(z)x + q(z),
(4.10)
with Bâˆ¥Â·âˆ¥(0, L) âŠ‚RN1 and p and q the polynomials associated to the approximating SAS ï¬lter U p,q
W1 in
(4.9).
Echo state networks are universal
15
The prescription on the choice of the constant K in (4.7) has two main consequences. Firstly, the
map FSAS is a contraction. Indeed, for any (x, z), (y, z) âˆˆBâˆ¥Â·âˆ¥(0, L) Ã— In:
âˆ¥FSAS(x, z) âˆ’FSAS(y, z)âˆ¥â‰¤âˆ¥p(z)x âˆ’p(z)yâˆ¥â‰¤âˆ¥p(z)âˆ¥2 âˆ¥x âˆ’yâˆ¥â‰¤K âˆ¥x âˆ’yâˆ¥.
(4.11)
The map FSAS is hence a contraction since K < 1 by hypothesis. Second, âˆ¥FSASâˆ¥âˆ< L because by
(4.7)
âˆ¥FSASâˆ¥âˆ=
sup
(x,z)âˆˆBâˆ¥Â·âˆ¥(0,L)Ã—In
âˆ¥p(z)x + q(z)âˆ¥â‰¤
sup
(x,z)âˆˆBâˆ¥Â·âˆ¥(0,L)Ã—In
âˆ¥p(z)âˆ¥2 âˆ¥xâˆ¥+ âˆ¥q(z)âˆ¥â‰¤KL + K < L.
This implies, in particular, that the map FSAS maps into Bâˆ¥Â·âˆ¥(0, L) and hence (4.10) can be rewritten
as
FSAS : Bâˆ¥Â·âˆ¥(0, L) Ã— In âˆ’â†’Bâˆ¥Â·âˆ¥(0, L).
Additionally, we set
L1 := âˆ¥FSASâˆ¥âˆ< L.
(4.12)
The uniform density on compacta of the family of feedforward neural networks with one hidden
layer proved in [Cybe 89, Horn 89] guarantees that for any Ïµ2 > 0, there exists N âˆˆN, G âˆˆMN,N1,
C âˆˆMN,n, E âˆˆMN1,N, and Î¶ âˆˆRN, such that the map deï¬ned by
FNN :
Bâˆ¥Â·âˆ¥(0, L) Ã— In
âˆ’â†’
RN1
(x, z)
7âˆ’â†’
EÏƒ (Gx + Cz + Î¶) ,
(4.13)
satisï¬es that
âˆ¥FNN âˆ’FSASâˆ¥âˆ=
sup
xâˆˆBâˆ¥Â·âˆ¥(0,L), zâˆˆIn
{âˆ¥FNN(x, z) âˆ’FSAS(x, z)âˆ¥} < Ïµ2.
(4.14)
The combination of (4.14) with the reverse triangle inequality implies that âˆ¥FNNâˆ¥âˆâˆ’âˆ¥FSASâˆ¥âˆ< Ïµ2
or, equivalently,
âˆ¥FNNâˆ¥âˆ< âˆ¥FSASâˆ¥âˆ+ Ïµ2.
(4.15)
Given that âˆ¥FSASâˆ¥âˆ= L1 < L, if we choose Ïµ2 > 0 small enough so that L1 + Ïµ2 < L or, equivalently,
Ïµ2 < L âˆ’L1,
(4.16)
then (4.15) guarantees that âˆ¥FNNâˆ¥âˆ< L, which shows that FNN maps into Bâˆ¥Â·âˆ¥(0, L), that is, we can
write that
FNN : Bâˆ¥Â·âˆ¥(0, L) Ã— In âˆ’â†’Bâˆ¥Â·âˆ¥(0, L).
(4.17)
The continuity of the map FNN and the ï¬rst part of Theorem 3.1 imply that the corresponding reservoir
equation has the existence of solutions property and that we can hence associate to it a (generalized)
ï¬lter UFNN. At the same time, as we proved in (4.11), the map FSAS is a contraction with constant
K < 1. These facts, together with (4.14) and the internal approximation property in Theorem 3.1 allow
us to conclude that the (unique) reservoir ï¬lter UFSAS associated to the reservoir map FSAS is such that
|||UFNN âˆ’UFSAS|||âˆ< Ïµ2/(1 âˆ’K).
(4.18)
Consider now the readout map hW1 : RN1 âˆ’â†’Rd given by hW1(x) := W1x and let U
hW1
FNN : (In)Zâˆ’âˆ’â†’
(Rd)Zâˆ’be the ï¬lter given by U
hW1
FNN (z)t := W1UFNN(z)t, t âˆˆZâˆ’. Analogously, deï¬ne U
hW1
FSAS : (In)Zâˆ’âˆ’â†’
(Rd)Zâˆ’and notice that U
hW1
FSAS = U p,q
W1. Using these observations and (4.18) we have proved that for any
Ïµ2 > 0 we can ï¬nd a ï¬lter of the type U
hW1
FNN that satisï¬es that


U p,q
W1 âˆ’U
hW1
FNN



âˆâ‰¤âˆ¥W1âˆ¥2 |||UFSAS âˆ’UFNN|||âˆ< âˆ¥W1âˆ¥2 Ïµ2/(1 âˆ’K).
(4.19)
Echo state networks are universal
16
Consequently, for any Ïµ > 0, if we ï¬rst set Ïµ1 = Ïµ/2 in (4.8) and we then choose
Ïµ2 := min
Ïµ(1 âˆ’K)
2 âˆ¥W1âˆ¥2
, L âˆ’L1
2

,
(4.20)
in view of (4.16) and (4.19), we can guarantee using (4.9) and (4.19) that


U âˆ’U
hW1
FNN



âˆâ‰¤
U âˆ’U p,q
W1

âˆ+


U p,q
W1 âˆ’U
hW1
FNN



âˆâ‰¤Ïµ
2 + Ïµ
2 = Ïµ.
(4.21)
In order to conclude the proof it suï¬ƒces to show that the ï¬lter U
hW1
FNN can be realized as the reservoir
ï¬lter associated to an echo state network of the type presented in the statement. We carry that out
by using the elements that appeared in the construction of the reservoir FNN in (4.13) to deï¬ne a new
reservoir map FESN with the architecture of an echo state network. Let A := GE âˆˆMN and deï¬ne
FESN :
DN Ã— In
âˆ’â†’
RN
(x, z)
7âˆ’â†’
Ïƒ (Ax + Cz + Î¶) .
(4.22)
The set DN in the domain of FESN is given by
DN := [âˆ’1, 1]N âˆ©Eâˆ’1(Bâˆ¥Â·âˆ¥(0, L)),
(4.23)
where Eâˆ’1(Bâˆ¥Â·âˆ¥(0, L)) denotes the preimage of the set Bâˆ¥Â·âˆ¥(0, L) âŠ‚RN1 by the linear map E : RN âˆ’â†’
RN1 associated to the matrix E âˆˆMN1,N. This set is compact as Eâˆ’1(Bâˆ¥Â·âˆ¥(0, L)) is closed and [âˆ’1, 1]N
is compact and hence DN is a closed subspace of a compact space which is always compact [Munk 14,
Theorem 26.2]. Additionally, DN is also convex because [âˆ’1, 1]N is convex and Eâˆ’1(Bâˆ¥Â·âˆ¥(0, L)) is also
convex because it is the preimage of a convex set by a linear map, which is always convex.
We note now that the image of FESN is contained in DN. First, as the squashing function maps into
the interval [âˆ’1, 1], it is clear that
FESN (DN, In) âŠ‚[âˆ’1, 1]N.
(4.24)
Second, for any x âˆˆDN we have by construction that x âˆˆEâˆ’1(Bâˆ¥Â·âˆ¥(0, L)) and hence Ex âˆˆBâˆ¥Â·âˆ¥(0, L).
Since by (4.17) FNN maps into Bâˆ¥Â·âˆ¥(0, L), we can ensure that for any z âˆˆIn, the image FNN(Ex, z) =
EÏƒ (GEx + Cz + Î¶) = EÏƒ (Ax + Cz + Î¶) âˆˆBâˆ¥Â·âˆ¥(0, L) or, equivalently,
FESN(x, z) = Ïƒ (Ax + Cz + Î¶) âˆˆEâˆ’1(Bâˆ¥Â·âˆ¥(0, L)).
(4.25)
The relations (4.24) and (4.25) imply that
FESN (DN, In) âŠ‚DN,
(4.26)
and hence, we can rewrite (4.22) as
FESN : DN Ã— In âˆ’â†’DN.
The continuity of the map FESN and the compactness and convexity of the set DN âŠ‚RN that we
established above allow us to use the ï¬rst part of Theorem 3.1 to conclude that the corresponding
reservoir equation has the existence of solutions property and that we can hence associate to it a
(generalized) ï¬lter UFESN. Let W := W1E âˆˆMd,n and deï¬ne the readout map hESN : DN âˆ’â†’Rd
by hESN(x) := Wx = W1Ex. Denote by UESN any generalized reservoir ï¬lter associated to the echo
state network system (FESN, hESN) that, by construction, satisï¬es UESN(z)t := hESN(UFESN(z)t) =
WUFESN(z)t, for any z âˆˆIn and t âˆˆZâˆ’.
Echo state networks are universal
17
We next show that the map f : DN = [âˆ’1, 1]N âˆ©Eâˆ’1(Bâˆ¥Â·âˆ¥(0, L)) âˆ’â†’Bâˆ¥Â·âˆ¥(0, L) given by f(x) := Ex
is a morphism between the echo state network system (FESN, hESN) and the reservoir system (FNN, hW1).
Indeed, the reservoir equivariance property holds because, for any (x, z) âˆˆDN Ã—In, the deï¬nitions (4.13)
and (4.22) ensure that
f(FESN(x, z)) = EÏƒ (Ax + Cz + Î¶) = EÏƒ (GEx + Cz + Î¶) = FNN(Ex, z) = FNN(f(x), z).
The readout invariance is obvious. This fact and the second part in Proposition 2.2 imply that all the
generalized ï¬lters UESN associated to the echo state network are actually ï¬lters generated by the system
(FNN, hW1). This means that for each generalized ï¬lter UESN there exists a generalized ï¬lter of the type
U
hW1
FNN such that UESN = U
hW1
FNN . The inequality (4.21) proves then (4.3) in the statement of the theorem.
The last claim in the theorem is a straightforward consequence of Propositions 2.1 and 2.11.
â– 
5
Appendices
5.1
Proof of Proposition 2.1
Let Ï„ âˆˆN and let T n
Ï„ : (Dn)Z âˆ’â†’(Dn)Z and T N
Ï„
: (DN)Z âˆ’â†’(DN)Z be the corresponding operators.
For any z âˆˆ(Dn)Z, let x âˆˆ(DN)Z be the unique solution of the reservoir system determined by F, that
is,
x := U F (z).
(5.1)
Then, for any t âˆˆZ,
 T N
Ï„ â—¦U F (z)

= xtâˆ’Ï„.
(5.2)
Analogously, let ex âˆˆ(DN)Z be the unique solution of F associated to the input T n
Ï„ (z), that is,
ext =
 U F â—¦T n
Ï„ (z)

t ,
for any
t âˆˆZ.
(5.3)
By construction, the sequence ex satisï¬es that
ext = F (extâˆ’1, T n
Ï„ (z)t) = F (extâˆ’1, ztâˆ’Ï„) ,
for any
t âˆˆZ.
It we set s := t âˆ’Ï„, this expression can be rewritten as
exs+Ï„ = F (exs+Ï„âˆ’1, zs) ,
for any
s âˆˆZ,
(5.4)
and if we deï¬ne bxs := exs+Ï„, the equality (5.4) becomes
bxs = F (bxsâˆ’1, zs) ,
for any
s âˆˆZ,
which shows that bx âˆˆ(DN)Z is a solution of F determined by the input z âˆˆ(Dn)Z. Since the sequence
x âˆˆ(DN)Z in (5.1) is also a solution of F for the same input, the echo state property hypothesis on
the systems determined by F implies that x = bx, necessarily. This implies that xtâˆ’Ï„ = bxtâˆ’Ï„ for all
t âˆˆZ, which is equivalent to ext = xtâˆ’Ï„. This equality guarantees that (5.2) and (5.3) are equal and
since bz âˆˆ(Dn)Z is arbitrary, we have that
T N
Ï„ â—¦U F = U F â—¦T n
Ï„ ,
as required.
â– 
Echo state networks are universal
18
5.2
Proof of Proposition 2.4
Suppose ï¬rst that U is continuous. This implies the existence of a positive function Î´U(Ïµ) such that if
u, v âˆˆ(Dn)Zâˆ’are such that âˆ¥u âˆ’vâˆ¥âˆ< Î´U(Ïµ), then âˆ¥U(u) âˆ’U(v)âˆ¥âˆ< Ïµ. Under that hypothesis, it
is clear that:
âˆ¥HU(u) âˆ’HU(v)âˆ¥= âˆ¥U(u)0 âˆ’U(v)0âˆ¥â‰¤sup
tâˆˆZâˆ’
{âˆ¥U(u)t âˆ’U(v)tâˆ¥} = âˆ¥U(u) âˆ’U(v)âˆ¥âˆ< Ïµ,
which shows the continuity of HU :

(Dn)Zâˆ’, âˆ¥Â·âˆ¥âˆ

âˆ’â†’(DN, âˆ¥Â·âˆ¥).
Conversely, suppose that H :

(Dn)Zâˆ’, âˆ¥Â·âˆ¥âˆ

âˆ’â†’(DN, âˆ¥Â·âˆ¥) is continuous and let Î´H(Ïµ) > 0 be such
that if âˆ¥u âˆ’vâˆ¥âˆ< Î´H(Ïµ) then âˆ¥H(u) âˆ’H(v)âˆ¥< Ïµ. Then, for any t âˆˆZâˆ’,
âˆ¥UH(u)t âˆ’UH(v)tâˆ¥=
H((PZâˆ’â—¦Tâˆ’t)(u)) âˆ’H((PZâˆ’â—¦Tâˆ’t)(v))
 < Ïµ,
(5.5)
which proves the continuity of UH. The inequality follows from the fact that for any u âˆˆ(Dn)Zâˆ’, the
components of the sequence (PZâˆ’â—¦Tâˆ’t)(u) are included in those of u and hence supsâˆˆZâˆ’
(PZâˆ’â—¦Tâˆ’t(u))s
	
â‰¤
supsâˆˆZâˆ’{âˆ¥usâˆ¥} or, equivalently,
(PZâˆ’â—¦Tâˆ’t)(u)

âˆâ‰¤âˆ¥uâˆ¥âˆ. This implies that if âˆ¥u âˆ’vâˆ¥âˆ< Î´H(Ïµ)
then âˆ¥Tâˆ’t(u) âˆ’Tâˆ’t(v)âˆ¥âˆ< Î´H(Ïµ) and hence (5.5) holds.
â– 
5.3
Proof of Theorem 2.6
We ï¬rst show that the map DM
w
: (Rn)Zâˆ’Ã— (Rn)Zâˆ’âˆ’â†’[0, âˆ) deï¬ned in (2.7) is indeed a met-
ric.
It is clear that DM
w (x, y) â‰¥0 and that DM
w (x, x) = 0, for any x, y âˆˆ(Rn)Zâˆ’.
Conversely, if
DM
w (x, y) = 0, this implies that dM(xt, yt)wâˆ’t â‰¤suptâˆˆZâˆ’

dM(xt, yt)wâˆ’t
	
= DM
w (x, y) = 0, which
ensures that dM(xt, yt) = 0, for any t âˆˆZâˆ’, and hence x = y necessarily since the map dM is a metric
in Rn [Munk 14, Chapter 2, Â§20]. It is also obvious that DM
w (x, y) = DM
w (y, x). Regarding the triangle
inequality, notice that for any x, y, z âˆˆ(Rn)Zâˆ’and t âˆˆZâˆ’:
dM(xt, zt)wâˆ’t â‰¤dM(xt, yt)wâˆ’t + dM(yt, zt)wâˆ’t â‰¤DM
w (x, y) + DM
w (y, z),
which implies that
DM
w (x, z) = sup
tâˆˆZâˆ’

dM(xt, zt)wâˆ’t
	
â‰¤DM
w (x, y) + DM
w (y, z).
We now show that the metric topology on (R)Zâˆ’associated to DM
w
coincides with the product
topology. Let x âˆˆ(Rn)Zâˆ’and let BDM
w (x, Ïµ) be an Ïµ-ball around it with respect to the metric DM
w . Let
now N âˆˆN be large enough so that wN < Ïµ/M. We then show that the basis element V for the product
topology in (Rn)Zâˆ’given by
V := Â· Â· Â· Ã— Rn Ã— Rn Ã— BdM (xâˆ’N, Ïµ) Ã— Â· Â· Â· BdM (xâˆ’1, Ïµ) Ã— BdM (x0, Ïµ)
and that obviously contains the element x âˆˆ(Rn)Zâˆ’is such that V âŠ‚BDM
w (x, Ïµ). Indeed, since for any
y âˆˆ(Rn)Zâˆ’and any t âˆˆZâˆ’we have that dM(xt, yt) â‰¤M, we can conclude that
dM(xt, yt)wâˆ’t â‰¤MwN,
for all
t â‰¤âˆ’N.
Therefore, DM
w (x, y) â‰¤max

Mwâˆ’N, dM(xâˆ’N, yâˆ’N)wN, . . . , dM(xâˆ’1, yâˆ’1)w1, dM(x0, y0)w0
	
and hence
if y âˆˆV this expression is smaller than Ïµ which allows us to conclude the desired inclusion V âŠ‚
BDM
w (x, Ïµ).
Echo state networks are universal
19
Conversely, consider a basis element of the product topology given by U = Q
tâˆˆZâˆ’Ut where Ut =
BdM (xt, Ïµt) for a ï¬nite set of indices t âˆˆ{Î±1, . . . , Î±r}, Ïµt â‰¤1, and Ut = Rn for the rest.
Let
Ïµ := mintâˆˆ{Î±1,...,Î±r} {Ïµtwâˆ’t}.
We now show that BDM
w (x, Ïµ) âŠ‚U.
Indeed, if y âˆˆBDM
w (x, Ïµ) then
dM(xt, yt)wâˆ’t â‰¤DM
w (x, y) < Ïµ, for all t âˆˆZâˆ’.
It t âˆˆ{Î±1, . . . , Î±r} then Ïµ < Ïµtwâˆ’t and hence
dM(xt, yt)wâˆ’t < Ïµtwâˆ’t, which ensures that dM(xt, yt) < Ïµt and hence y âˆˆU, as desired.
We conclude by showing that
 (Rn)Zâˆ’, DM
w

is a complete metric space. First, notice that since for
any x, y âˆˆ(Rn)Zâˆ’and any given t âˆˆZâˆ’we have that
dM(xt, yt) â‰¤DM
w (x, y)
wâˆ’t
,
we can conclude that if {x(i)}iâˆˆN is a Cauchy sequence in (Rn)Zâˆ’, then so are the sequences {xt(i)}iâˆˆN
in Rn, for any t âˆˆZâˆ’, with respect to the bounded metric dM. Since the completeness with respect
to the bounded metric dM and the Euclidean metric are equivalent [Munk 14, Chapter 7, Â§43] we can
ensure that {xt(i)}iâˆˆN converges to an element at âˆˆRn with respect to the Euclidean metric for any
t âˆˆZâˆ’. We now show that {x(i)}iâˆˆN converges to a := (at)tâˆˆZâˆ’âˆˆ(Rn)Zâˆ’, with respect to the metric
DM
w , which proves the completeness statement.
Indeed, since the metric DM
w generates the product topology, let U = Q
âˆˆâˆˆZâˆ’Ut be a basis element
such that a âˆˆU and, as before, Ut = BdM (at, Ïµt) for a ï¬nite set of indices t âˆˆ{Î±1, . . . , Î±r}, Ïµt â‰¤1,
and Ut = Rn for the rest. Let Ïµ = min {ÏµÎ±1, . . . , ÏµÎ±r}. Since for each t âˆˆZâˆ’the sequence xt(i)
iâ†’âˆ
âˆ’â†’at,
then there exists Nt âˆˆN such that for any k > Nt we have that âˆ¥xt(k) âˆ’atâˆ¥< Ïµ. If we take NÏµ =
max {NÎ±1, . . . , NÎ±r} then it is clear that x(i) âˆˆU, for all i > NÏµ, as required.
â– 
5.4
Proof of Corollary 2.7
Notice ï¬rst that for any x, y âˆˆKM, we have that âˆ¥xt âˆ’ytâˆ¥< 2M, t âˆˆZâˆ’, and hence
D2M
w (x, y) := sup
tâˆˆZâˆ’

d2M(xt, yt)wâˆ’t
	
= sup
tâˆˆZâˆ’
{âˆ¥xt âˆ’ytâˆ¥wâˆ’t} = âˆ¥x âˆ’yâˆ¥w .
Hence, the topology induced by the weighted norm âˆ¥Â·âˆ¥w on KM coincides with the metric topology
induced by the restricted metric D2M
w |KMÃ—KM which, by Theorem 2.6, is the subspace topology induced
by the product topology on (Rn)Zâˆ’on KM (see [Munk 14, Exercise 1, page 133]), as well as the product
topology on the product KM =

Bâˆ¥Â·âˆ¥(0, M)
Zâˆ’
(see [Munk 14, Theorem 19.3, page 116])).
â– 
5.5
Proof of Corollary 2.8
First, since KM =

Bâˆ¥Â·âˆ¥(0, M)
Zâˆ’
, it is clearly the product of compact spaces. By Tychonoï¬€â€™s Theorem
([Munk 14, Chapter 5]) KM is compact when endowed with the product topology which, by Corollary
2.7, coincides with the topology associated to the restriction of the norm âˆ¥Â·âˆ¥w to KM, as well as with
the metric topology given by D2M
w |KMÃ—KM .
Second, since (KM, âˆ¥Â·âˆ¥w) is metrizable it is a Hausdorï¬€space. This implies (see [Munk 14, Theorem
26.3]) that as KM is a compact subspace of the Banach space (â„“w
âˆ’(Rn), âˆ¥Â·âˆ¥w) (see Proposition 5.2)
then it is necessarily closed. This in turn implies ([Simm 63, Theorem B, page 72]) that (KM, âˆ¥Â·âˆ¥w) is
complete.
Finally, the convexity statement follows from the fact that the product of convex sets is always
convex.
â– 
Echo state networks are universal
20
5.6
Proof of Proposition 2.9
Let dw be the metric on â„“w
âˆ’(Rn) induced by the weighted norm âˆ¥Â·âˆ¥w and let Dw := D1
w be the w-
weighted metric on (Rn)Zâˆ’with constant M = 1 introduced in Theorem 2.6 and deï¬ned using the same
underlying norm in Rn as the one associated to âˆ¥Â·âˆ¥w. As we saw in that theorem, the metric Dw induces
the product topology on (Rn)Zâˆ’.
Let now u âˆˆâ„“w
âˆ’(Rn) and let Ïµ > 0. Let now v âˆˆâ„“w
âˆ’(Rn) be such that dw(u, v) < Ïµ. By deï¬nition,
we have that
Dw(u, v) = sup
tâˆˆZâˆ’

d1(xt, yt)wâˆ’t
	
= sup
tâˆˆZâˆ’
{(min{âˆ¥xt âˆ’ytâˆ¥, 1})wâˆ’t} â‰¤sup
tâˆˆZâˆ’
{âˆ¥xt âˆ’ytâˆ¥wâˆ’t} = dw(u, v) < Ïµ,
which shows that Bdw(u, Ïµ) âŠ‚BDw(u, Ïµ) and allows us to conclude that the norm topology in â„“w
âˆ’(Rn)
is ï¬ner than the subspace topology induced by the product topology in (Rn)Zâˆ’.
We now show that this inclusion is strict. Since the weighting sequence w converges to zero, there
exists an element t0 âˆˆZâˆ’such that wâˆ’t0 < Ïµ/2. Let Î» > 0 arbitrary and deï¬ne the element vÎ» âˆˆ(Rn)Zâˆ’
by setting vÎ»
t0 := Î»ut0 and vÎ»
t := Î»ut when t Ì¸= t0. We now show that vÎ» âˆˆBDw(u, Ïµ) for any Î» > 0.
Indeed,
Dw(u, vÎ») = min{|Î» âˆ’1|âˆ¥ut0âˆ¥, 1}wâˆ’t0 â‰¤1 Â· wâˆ’t0 < Ïµ/2 < Ïµ.
At the same time, by deï¬nition,
dw(u, vÎ») = |Î» âˆ’1|âˆ¥ut0âˆ¥wâˆ’t0 < âˆ,
which shows that vÎ» âˆˆâ„“w
âˆ’(Rn). However, since |Î» âˆ’1|âˆ¥ut0âˆ¥wâˆ’t0 can be made as large as desired by
choosing Î» big enough, we have proved that for any ball Bdw(u, Ïµâ€²), with Ïµâ€² > 0 arbitrary, the ball
BDw(u, Ïµ) contains always an element in â„“w
âˆ’(Rn) that is not included in Bdw(u, Ïµâ€²). This argument
allows us to conclude that the norm topology in â„“w
âˆ’(Rn) is strictly ï¬ner than the subspace topology
induced by the product topology.
â– 
5.7
Proof of Proposition 2.10
Proof of part (i) The FMP of U with respect to the sequence w is, by deï¬nition, equivalent to the con-
tinuity of the map U : (KM, âˆ¥Â·âˆ¥w) âˆ’â†’(KL, âˆ¥Â·âˆ¥w) (respectively, H : (KM, âˆ¥Â·âˆ¥w) âˆ’â†’(Bâˆ¥Â·âˆ¥(0, L), âˆ¥Â·âˆ¥)).
By Corollary 2.8, this is equivalent to the continuity of these maps when KM and KL are endowed with
the product topology which is, by the same result, generated by any arbitrary weighting sequence.
The proof of the second part of the proposition requires the following lemma.
Lemma 5.1 The operator PZâˆ’â—¦Tâˆ’t : (KM, âˆ¥Â·âˆ¥w) âˆ’â†’(KM, âˆ¥Â·âˆ¥w) is a continuous map, for any t âˆˆZâˆ’
and for any weighting sequence w.
Proof of the lemma.
We show that this statement is true by characterizing PZâˆ’â—¦Tâˆ’t as a Cartesian
product of continuous maps between two product spaces endowed with the product topologies by using
Corollary 2.7. Indeed, notice ï¬rst that the projections pi : KM âˆ’â†’Bâˆ¥Â·âˆ¥(0, M) given by pi(z) = zi
are continuous. Since PZâˆ’â—¦Tâˆ’t can be written as the inï¬nite Cartesian product of continuous maps
PZâˆ’â—¦Tâˆ’t = Qâˆ’âˆ
i=t pi = (. . . , ptâˆ’2, ptâˆ’1, pt) it is hence continuous using the product topology induced
by âˆ¥Â·âˆ¥w (see [Munk 14, Theorem 19.6]). â–¼
Proof of part (ii) First, if H has the FMP then the map H : (KM, âˆ¥Â·âˆ¥w) âˆ’â†’(Bâˆ¥Â·âˆ¥(0, L), âˆ¥Â·âˆ¥) is
continuous. This implies that UH = Qâˆ’âˆ
t=0 H â—¦
 PZâˆ’â—¦Tâˆ’t

is also continuous, since by Lemma 5.1 it is
a composition of continuous functions and hence has the FMP. Conversely, if U has the FMP, so is the
case with HU = p0 â—¦U.
â– 
Echo state networks are universal
21
5.8
Proof of Proposition 2.11
We start by proving the continuity of Î¨ by proving the inequality (2.10). Let U1, U2 âˆˆFFMP
KM,KL. By
deï¬nition
|||Î¨(U1) âˆ’Î¨(U2)|||âˆ= sup
zâˆˆKM
{âˆ¥Î¨(U1)(z) âˆ’Î¨(U2)(z)âˆ¥} = sup
zâˆˆKM
{âˆ¥U1(z)0 âˆ’U2(z)0âˆ¥} .
(5.6)
Since we have that
sup
zâˆˆKM
{âˆ¥U1(z)0 âˆ’U2(z)0âˆ¥} â‰¤sup
zâˆˆKM
{ sup
tâˆˆZâˆ’
{âˆ¥U1(z)t âˆ’U2(z)tâˆ¥}} = sup
zâˆˆKM
{âˆ¥U1(z) âˆ’U2(z)âˆ¥âˆ} = |||U1 âˆ’U2|||âˆ,
this shows, together with (5.6), that
|||Î¨(U1) âˆ’Î¨(U2)|||âˆâ‰¤|||U1 âˆ’U2|||âˆ,
which implies the continuity of Î¨. Conversely, let H1, H2 âˆˆHFMP
KM,KL. We have:
|||Î¦(H1) âˆ’Î¦(H2)|||âˆ= sup
zâˆˆKM
(
sup
tâˆˆZâˆ’
{âˆ¥Î¦(H1)(z)t âˆ’Î¦(H2)(z)tâˆ¥}
)
= sup
zâˆˆKM
(
sup
tâˆˆZâˆ’
H1((PZâˆ’â—¦Tâˆ’t)(z)) âˆ’H2((PZâˆ’â—¦Tâˆ’t)(z))
	
)
â‰¤sup
zâˆˆKM
{âˆ¥H1(z) âˆ’H2(z)âˆ¥âˆ} = |||H1 âˆ’H2|||âˆ,
which proves the continuity of Î¦. The inequality is a consequence of the fact that the sequence (PZâˆ’â—¦
Tâˆ’t)(z) âˆˆKM.
â– 
5.9
Proof of Corollary 3.3
(i) Consider the reservoir map FESN : [âˆ’1, 1]N Ã— Dn âˆ’â†’[âˆ’1, 1]N given by FESN := Ïƒ (Ax + Cz + Î¶)
The statement is a direct consequence of the continuity of FESN, the compactness and convexity of
[âˆ’1, 1]N and Dn, and of part (i) of Theorem 3.1.
(ii) The result follows from part (ii) of Theorem 3.1 since the hypotheses in the statement imply that
the reservoir map FESN is in those circumstances a contraction. Indeed, let x, y âˆˆ[âˆ’1, 1]N and let
z âˆˆDn, then
âˆ¥FESN(x, z) âˆ’FESN(y, z)âˆ¥= âˆ¥Ïƒ (Ax + Cz + Î¶) âˆ’Ïƒ (Ay + Cz + Î¶)âˆ¥â‰¤LÏƒ âˆ¥Aâˆ¥2 âˆ¥x âˆ’yâˆ¥.
Since by hypothesis LÏƒ âˆ¥Aâˆ¥2 < 1, we can conclude that FESN is a contraction, as required. The norm
âˆ¥Â·âˆ¥in the previous expression is the Euclidean norm in RN. The time invariance of the resulting unique
fading memory reservoir ï¬lter is a consequence of Proposition 2.1.
â– 
5.10
 â„“w
âˆ’(Rn), âˆ¥Â· âˆ¥w

is a Banach space
Proposition 5.2 Let w : N âˆ’â†’(0, 1] be a weighting sequence and let âˆ¥Â· âˆ¥w : (Rn)Zâˆ’âˆ’â†’R+ be the
corresponding weighted norm. Then, the space
 â„“w
âˆ’(Rn), âˆ¥Â· âˆ¥w

deï¬ned by
â„“w
âˆ’(Rn) :=
n
z âˆˆ(Rn)Zâˆ’| âˆ¥zâˆ¥w < âˆ
o
,
endowed with weighted norm âˆ¥Â· âˆ¥w is a Banach space.
Echo state networks are universal
22
Proof.
We ï¬rst show that â„“w
âˆ’(Rn) is a linear subspace of (Rn)Zâˆ’. Let u, v âˆˆâ„“w
âˆ’(Rn) and let Î» âˆˆR.
Then,
âˆ¥u + Î»vâˆ¥w = sup
tâˆˆZâˆ’
{âˆ¥ut + Î»vtâˆ¥wâˆ’t} â‰¤sup
tâˆˆZâˆ’
{âˆ¥utâˆ¥wâˆ’t + Î» âˆ¥vtâˆ¥wâˆ’t}
â‰¤sup
tâˆˆZâˆ’
{âˆ¥utâˆ¥wâˆ’t} + Î» sup
tâˆˆZâˆ’
{âˆ¥vtâˆ¥wâˆ’t} = âˆ¥uâˆ¥w + Î» âˆ¥vâˆ¥w .
We now show that this space is complete. Let {u(n)}nâˆˆN âŠ‚â„“w
âˆ’(Rn) be a Cauchy sequence. This implies
that for any Ïµ > 0, there exists N(Ïµ) âˆˆN such that for all m, n > N(Ïµ) we have âˆ¥u(n) âˆ’u(m)âˆ¥w < Ïµ.
Hence, for any t âˆˆZâˆ’,
âˆ¥ut(n) âˆ’ut(m)âˆ¥wâˆ’t â‰¤sup
tâˆˆZâˆ’
{âˆ¥ut(n) âˆ’ut(m)âˆ¥wâˆ’t} = âˆ¥u(n) âˆ’u(m)âˆ¥w < Ïµ.
(5.7)
This implies that taking for each ï¬xed t âˆˆZâˆ’the value N(Ïµwâˆ’t), the sequences {ut(n)}nâˆˆN in Rn are
Cauchy and hence convergent to values ut âˆˆRn. We now show that {u(n)}nâˆˆN converges to u âˆˆ(Rn)Zâˆ’.
Using (5.7), take N(Ïµ/2) so that for all m, n > N(Ïµ/2) and any t âˆˆZâˆ’one has âˆ¥ut(n) âˆ’ut(n)âˆ¥wâˆ’t â‰¤
Ïµ/2. If we take the limit m â†’âˆin this inequality, we obtain
âˆ¥ut(n) âˆ’utâˆ¥wâˆ’t â‰¤Ïµ/2,
for all t âˆˆZâˆ’.
This implies that
âˆ¥u(n) âˆ’uâˆ¥w = sup
tâˆˆZâˆ’
{âˆ¥ut(n) âˆ’utâˆ¥wâˆ’t} â‰¤Ïµ/2 < Ïµ,
which proves that {u(n)}nâˆˆN converges to u, as required. It remains to be shown that u âˆˆâ„“w
âˆ’(Rn), that
is, that âˆ¥uâˆ¥< âˆ. In order to show that this is indeed the case, let n âˆˆN be such that âˆ¥u âˆ’u(n)âˆ¥< Ïµ.
This implies that
âˆ¥uâˆ¥w âˆ’âˆ¥u(n)âˆ¥w < | âˆ¥uâˆ¥w âˆ’âˆ¥u(n)âˆ¥w | < âˆ¥u âˆ’u(n)âˆ¥w < Ïµ,
and hence âˆ¥uâˆ¥w < âˆ¥u(n)âˆ¥w + Ïµ < âˆ, as required.
â– 
Acknowledgments: The authors acknowledge partial ï¬nancial support of the French ANR â€œBIPHO-
PROCâ€ project (ANR-14-OHRI-0002-02) as well as the hospitality of the Centre Interfacultaire Bernoulli
of the Ecole Polytechnique FÂ´edÂ´erale de Lausanne during the program â€œStochastic Dynamical Models
in Mathematical Finance, Econometrics, and Actuarial Sciencesâ€ that made possible the collaboration
that lead to some of the results included in this paper. LG acknowledges partial ï¬nancial support of the
Graduate School of Decision Sciences and the Young Scholar Fund AFF of the UniversitÂ¨at Konstanz.
JPO acknowledges partial ï¬nancial support coming from the Research Commission of the UniversitÂ¨at
Sankt Gallen and the Swiss National Science Foundation (grant number 200021 175801/1).
References
[Arno 57]
V. I. Arnold.
â€œOn functions of three variablesâ€.
Proceedings of the USSR Academy of
Sciences, Vol. 114, pp. 679â€“681, 1957.
[Bai 12]
Bai Zhang, D. J. Miller, and Yue Wang. â€œNonlinear system modeling with random matri-
ces: echo state networks revisitedâ€. IEEE Transactions on Neural Networks and Learning
Systems, Vol. 23, No. 1, pp. 175â€“182, jan 2012.
Echo state networks are universal
23
[Boyd 85]
S. Boyd and L. Chua. â€œFading memory and the problem of approximating nonlinear oper-
ators with Volterra seriesâ€. IEEE Transactions on Circuits and Systems, Vol. 32, No. 11,
pp. 1150â€“1161, nov 1985.
[Bril 58]
M. B. Brilliant. â€œTheory of the analysis of nonlinear systemsâ€. Tech. Rep., Massachusetts
Institute of Technology, Research Laboratory of Electronics, 1958.
[Bueh 06]
M. Buehner and P. Young. â€œA tighter bound for the echo state propertyâ€. IEEE Transactions
on Neural Networks, Vol. 17, No. 3, pp. 820â€“824, 2006.
[Cabe 15]
J. Cabessa and A. E. Villa. â€œComputational capabilities of recurrent neural networks based
on their attractor dynamicsâ€. In: 2015 International Joint Conference on Neural Networks
(IJCNN), pp. 1â€“8, IEEE, jul 2015.
[Cabe 16]
J. Cabessa and A. E. Villa.
â€œExpressive power of ï¬rst-order recurrent neural networks
determined by their attractor dynamicsâ€. Journal of Computer and System Sciences, Vol. 82,
No. 8, pp. 1232â€“1250, 2016.
[Croo 07]
N. Crook. â€œNonlinear transient computationâ€. Neurocomputing, Vol. 70, pp. 1167â€“1176,
2007.
[Cybe 89]
G. Cybenko. â€œApproximation by superpositions of a sigmoidal functionâ€. Mathematics of
Control, Signals, and Systems, Vol. 2, No. 4, pp. 303â€“314, dec 1989.
[Frec 10]
M. FrÂ´echet. â€œSur les fonctionnelles continuesâ€. Annales scientiï¬ques de lâ€™Ecole Normale
SupÂ´erieure. 3`eme sÂ´erie., Vol. 27, pp. 193â€“216, 1910.
[Gall 17]
C. Gallicchio and A. Micheli. â€œEcho state property of deep reservoir computing networksâ€.
Cognitive Computation, Vol. 9, 2017.
[Geor 59]
D. A. George.
â€œContinuous nonlinear systemsâ€.
Tech. Rep., Massachusetts Institute of
Technology, Research Laboratory of Electronics, 1959.
[Grig 17]
L. Grigoryeva and J.-P. Ortega. â€œUniversal discrete-time reservoir computers with stochastic
inputs and linear readouts using non-homogeneous state-aï¬ƒne systemsâ€. Preprint, dec 2017.
[Horn 13]
R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, second Ed.,
2013.
[Horn 89]
K. Hornik, M. Stinchcombe, and H. White. â€œMultilayer feedforward networks are universal
approximatorsâ€. Neural Networks, Vol. 2, No. 5, pp. 359â€“366, 1989.
[Horn 90]
K. Hornik, M. Stinchcombe, and H. White. â€œUniversal approximation of an unknown map-
ping and its derivatives using multilayer feedforward networksâ€. Neural Networks, Vol. 3,
No. 5, pp. 551â€“560, 1990.
[Horn 91]
K. Hornik.
â€œApproximation Capabilities of Muitilayer Feedforward Networksâ€.
Neural
Networks, Vol. 4, No. 1989, pp. 251â€“257, 1991.
[Horn 93]
K. Hornik. â€œSome new results on neural network approximationâ€. Neural Networks, Vol. 6,
No. 8, pp. 1069â€“1072, 1993.
[Jaeg 04]
H. Jaeger and H. Haas. â€œHarnessing Nonlinearity: Predicting Chaotic Systems and Saving
Energy in Wireless Communicationâ€. Science, Vol. 304, No. 5667, pp. 78â€“80, 2004.
Echo state networks are universal
24
[Jaeg 10]
H. Jaeger. â€œThe â€™echo stateâ€™ approach to analysing and training recurrent neural networks
with an erratum noteâ€.
Tech. Rep., German National Research Center for Information
Technology, 2010.
[Kili 96]
J. Kilian and H. T. Siegelmann. â€œThe dynamic universality of sigmoidal neural networksâ€.
Information and Computation, Vol. 128, No. 1, pp. 48â€“56, 1996.
[Kolm 56]
A. N. Kolmogorov. â€œOn the representation of continuous functions of several variables as
superpositions of functions of smaller number of variablesâ€. Soviet Math. Dokl, Vol. 108,
pp. 179â€“182, 1956.
[Luko 09]
M. LukoË‡seviË‡cius and H. Jaeger. â€œReservoir computing approaches to recurrent neural net-
work trainingâ€. Computer Science Review, Vol. 3, No. 3, pp. 127â€“149, 2009.
[Maas 00]
W. Maass and E. D. Sontag. â€œNeural Systems as Nonlinear Filtersâ€. Neural Computation,
Vol. 12, No. 8, pp. 1743â€“1772, aug 2000.
[Maas 02]
W. Maass, T. NatschlÂ¨ager, and H. Markram. â€œReal-time computing without stable states:
a new framework for neural computation based on perturbationsâ€. Neural Computation,
Vol. 14, pp. 2531â€“2560, 2002.
[Maas 04]
W. Maass, T. NatschlÂ¨ager, and H. Markram. â€œFading memory and kernel properties of
generic cortical microcircuit modelsâ€. Journal of Physiology Paris, Vol. 98, No. 4-6 SPEC.
ISS., pp. 315â€“330, 2004.
[Maas 07]
W. Maass, P. Joshi, and E. D. Sontag.
â€œComputational aspects of feedback in neural
circuitsâ€. PLoS Computational Biology, Vol. 3, No. 1, p. e165, 2007.
[Maas 11]
W. Maass. â€œLiquid state machines: motivation, theory, and applicationsâ€. In: S. S. Barry
Cooper and A. Sorbi, Eds., Computability In Context: Computation and Logic in the Real
World, Chap. 8, pp. 275â€“296, 2011.
[Manj 13]
G. Manjunath and H. Jaeger. â€œEcho state property linked to an input: exploring a funda-
mental characteristic of recurrent neural networksâ€. Neural Computation, Vol. 25, No. 3,
pp. 671â€“696, 2013.
[Matt 92]
M. B. Matthews.
On the Uniform Approximation of Nonlinear Discrete-Time Fading-
Memory Systems Using Neural Network Models. PhD thesis, ETH ZÂ¨urich, 1992.
[Matt 93]
M. B. Matthews. â€œApproximating nonlinear fading-memory operators using neural network
modelsâ€. Circuits, Systems, and Signal Processing, Vol. 12, No. 2, pp. 279â€“307, jun 1993.
[Munk 14] J. Munkres. Topology. Pearson, second Ed., 2014.
[Perr 96]
P. C. Perryman. Approximation Theory for Deterministic and Stochastic Nonlinear Systems.
PhD thesis, University of California, Irvine, 1996.
[Rusc 98]
L. RÂ¨uschendorf and W. Thomsen. â€œClosedness of sum spaces and the generalized schrÂ¨odinger
Problemâ€. Theory of Probability & Its Applications, Vol. 42, No. 3, pp. 483â€“494, jan 1998.
[Sand 03]
I. W. Sandberg. â€œNotes of fading-memory conditionsâ€. Circuits, Systems, and Signal Pro-
cessing, Vol. 22, No. 1, pp. 43â€“55, 2003.
[Sand 91a] I. W. Sandberg. â€œApproximation Theorems for Discrete-Time Systemsâ€. IEEE Transactions
on Circuits and Systems, Vol. 38, No. 5, pp. 564â€“566, 1991.
Echo state networks are universal
25
[Sand 91b] I. W. Sandberg. â€œStructure theorems for nonlinear systemsâ€. Multidimensional Systems and
Signal Processing, Vol. 2, pp. 267â€“286, 1991.
[Shap 16]
J. H. Shapiro. A Fixed-Point Farrago. Springer International Publishing Switzerland, 2016.
[Sieg 97]
H. Siegelmann, B. Horne, and C. Giles. â€œComputational capabilities of recurrent NARX neu-
ral networksâ€. IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics),
Vol. 27, No. 2, pp. 208â€“215, apr 1997.
[Simm 63] G. F. Simmons. Topology and Modern Analysis. McGraw-Hill, 1963.
[Spre 65]
D. A. Sprecher. â€œA representation theorem for continuous functions of several variablesâ€.
Proceedings of the American Mathematical Society, Vol. 16, No. 2, p. 200, apr 1965.
[Spre 96]
D. A. Sprecher.
â€œA numerical implementation of Kolmogorovâ€™s superpositionsâ€. Neural
Networks, Vol. 9, No. 5, pp. 765â€“772, 1996.
[Spre 97]
D. A. Sprecher. â€œA numerical implementation of Kolmogorovâ€™s superpositions IIâ€. Neural
Networks, Vol. 10, No. 3, pp. 447â€“457, 1997.
[Ster 10]
S. Sternberg. Dynamical Systems. Dover, 2010.
[Stub 97]
A. Stubberud and P. Perryman. â€œCurrent state of system approximation for deterministic
and stochastic systemsâ€. In: Conference Record of The Thirtieth Asilomar Conference on
Signals, Systems and Computers, pp. 141â€“145, IEEE Comput. Soc. Press, 1997.
[Vers 07]
D. Verstraeten, B. Schrauwen, M. Dâ€™Haene, and D. Stroobandt. â€œAn experimental uniï¬ca-
tion of reservoir computing methodsâ€. Neural Networks, Vol. 20, pp. 391â€“403, 2007.
[Volt 30]
V. Volterra.
Theory of Functionals and of Integral and Integro-Diï¬€erential Equations.
Blackie & Son Limited, Glasgow, 1930.
[Wain 16]
G. Wainrib and M. N. Galtier. â€œA local echo state property through the largest Lyapunov
exponentâ€. Neural Networks, Vol. 76, pp. 39â€“45, apr 2016.
[Wien 58]
N. Wiener. Nonlinear Problems in Random Theory. The Technology Press of MIT, 1958.
[Yild 12]
I. B. Yildiz, H. Jaeger, and S. J. Kiebel. â€œRe-visiting the echo state property.â€. Neural
networks : the oï¬ƒcial journal of the International Neural Network Society, Vol. 35, pp. 1â€“9,
nov 2012.
