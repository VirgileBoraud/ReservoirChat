Echo state networks are universal
Lyudmila Grigoryeva1 and Juan-Pablo Ortega2,3
Abstract
This paper shows that echo state networks are universal uniform approximants in the context of
discrete-time fading memory ﬁlters with uniformly bounded inputs deﬁned on negative inﬁnite times.
The proof uses newly introduced internal approximation results for ﬁlters associated to reservoir
computing systems, as well as the external approximation properties of state aﬃne systems proved
in a previous work.
Key Words: reservoir computing, universality, echo state networks, ESN, state-aﬃne systems,
SAS, machine learning, fading memory property, echo state property, linear training, uniform system
approximation.
1
Introduction
Many recently introduced machine learning techniques in the context of dynamical problems have much
in common with system identiﬁcation procedures developed in the last decades for applications in signal
treatment, circuit theory and, in general, systems theory.
In these problems, system knowledge is
only available in the form of input-output observations and the task consists in ﬁnding or learning a
model that approximates it for mainly forecasting or classiﬁcation purposes. An important goal in that
context is to ﬁnd a family of transformations that is both computationally feasible and versatile enough
to reproduce a rich number of patterns just by modifying a limited number of procedural parameters.
This feature is usually referred to as universality.
A ﬁrst solution to this problem was pioneered in the works of Fr´echet [Frec 10] and Volterra [Volt 30]
one century ago when they proved that ﬁnite Volterra series can be used to uniformly approximate
continuous functionals deﬁned on compact sets of continuous functions.
These results were further
extended in the 1950s by the MIT school lead by N. Wiener [Wien 58, Bril 58, Geor 59] but always
under compactness assumptions on the input space and the time interval in which inputs are deﬁned.
A major breakthrough was the generalization to inﬁnite time intervals carried out by Boyd and Chua
in [Boyd 85] using the so called fading memory property.
In this paper we address that problem for transformations or ﬁlters of discrete time signals of inﬁnite
length that have the fading memory property.
The approximating set that we use is generated by
nonlinear state-space transformations and that is referred to as reservoir computers (RC) [Jaeg 10,
Jaeg 04, Maas 02, Maas 11, Croo 07, Vers 07, Luko 09] or reservoir systems. These are special types
of recurrent neural network determined by two maps, namely a reservoir F : RN × Rn −→RN,
1Department of Mathematics and Statistics.
Universit¨at Konstanz.
Box 146.
D-78457 Konstanz.
Germany.
Lyudmila.Grigoryeva@uni-konstanz.de
2Universit¨at Sankt Gallen. Faculty of Mathematics and Statistics. Bodanstrasse 6. CH-9000 Sankt Gallen. Switzer-
land. Juan-Pablo.Ortega@unisg.ch
3Centre National de la Recherche Scientiﬁque (CNRS). France.
1
Echo state networks are universal
2
n, N ∈N, and a readout map h : RN →Rd that under certain hypotheses transform (or ﬁlter) an
inﬁnite discrete-time input z = (. . . , z−1, z0, z1, . . .) ∈(Rn)Z into an output signal y ∈(Rd)Z of the
same type using the state-space transformation given by:
xt = F(xt−1, zt),
yt = h(xt),
(1.1)
(1.2)
where t ∈Z and the dimension N ∈N of the state vectors xt ∈RN is referred to as the number of
virtual neurons of the system. When a RC system has a uniquely determined ﬁlter associated to it,
we refer to it as the RC ﬁlter.
An important advantage of the RC approach is that, under certain hypotheses, intrinsically inﬁnite
dimensional problems regarding ﬁlters can be translated into analogous questions related to the reser-
voir and readout maps that generate them and that are deﬁned on much simpler ﬁnite dimensional
spaces. This strategy has already been used in the literature in relation to the universality question
in, for instance, [Sand 91a, Sand 91b, Matt 92, Matt 93, Perr 96, Stub 97]. The universal approxima-
tion properties of feedforward neural networks [Kolm 56, Arno 57, Spre 65, Spre 96, Spre 97, Cybe 89,
Horn 89, Horn 90, Horn 91, Horn 93, Rusc 98] was used in those works to ﬁnd neural networks-based
families of ﬁlters that are dense in the set of approximately ﬁnite memory ﬁlters with inputs deﬁned in
the positive half real line. Other works in connection with the universality problem in the RC context
are [Maas 00, Maas 02, Maas 04, Maas 07]. Another strand of interesting literature that we will not
explore in this work has to with the Turing computability capabilities of the systems of the type that
we just introduced; recent relevant works in this direction are [Kili 96, Sieg 97, Cabe 15, Cabe 16], and
references therein.
The main contribution of this paper is showing that a particularly simple type of RC systems called
echo state networks (ESNs) can be used as universal approximants in the context of discrete-time
fading memory ﬁlters with uniformly bounded inputs deﬁned on negative inﬁnite times. ESNs are RC
systems of the form (1.1)-(1.2) given by:
xt = σ (Axt−1 + Czt + ζ) ,
yt = Wxt.
(1.3)
(1.4)
In these equations, C ∈MN,n is called the input mask, ζ ∈RN is the input shift, and A ∈MN,N
is referred to as the reservoir matrix. The map σ in the state-space equation (1.3) is constructed by
componentwise application of a sigmoid function (like the hyperbolic tangent or the logistic function)
and is called the activation function. Finally, the readout map is linear in this case and implemented
via the readout matrix W ∈Md,N.
ESNs already appear in [Matt 92, Matt 93] under the name
of recurrent networks but it was only more recently, in the works of H. Jaeger [Jaeg 04], that their
outstanding performance in machine learning applications was demonstrated.
The strategy that we follow to prove that statement is a combination of what the literature refers
to as internal and external approximation. External approximation is the construction of a RC
ﬁlter that approximates a given (not necessarily RC) ﬁlter. In the internal approximation problem, one
is given a RC ﬁlter and builds another RC ﬁlter that approximates it by ﬁnding reservoir and readout
maps that are close to those of the given one. In the external part of our proof we use a previous
work [Grig 17] where we constructed a family of RC systems with linear readouts that we called non-
homogeneous state aﬃne systems (SAS). We showed in that paper that the RC ﬁlters associated
to SAS systems uniformly approximate any discrete-time fading memory ﬁlter with uniformly bounded
inputs deﬁned on negative inﬁnite times. Regarding the internal approximation, we show that any RC
ﬁlter, in particular SAS ﬁlters, can be approximated by ESN ﬁlters using the universal approximation
property of neural networks. These two facts put together allow us to conclude that ESN ﬁlters are
Echo state networks are universal
3
capable of uniformly approximating any discrete-time fading memory ﬁlter with uniformly bounded
inputs.
The paper is structured in three sections:
• Section 2 introduces the notation that we use all along the paper and, more importantly, speciﬁes
the topologies and Banach space structures that we need in order to talk about continuity and
diﬀerentiability in the context of discrete-time ﬁlters. It is worth mentioning that we characterize
the fading memory property as a continuity condition of the ﬁlters that have it with respect to
the product topology in the input space. On other words, the fading memory property is not a
metric property, as it is usually presented in the literature, but a topological one. An important
conceptual consequence of this fact is that the fading memory property does not contain any
information about the rate at which systems that have it “forget” inputs. Several corollaries can
be formulated as a consequence of this fact that are very instrumental in the developments in the
paper.
• Section 3 contains a collection of general results in relation with the properties of the RC systems
generated by continuous reservoir maps. In particular, we provide conditions that guarantee that
a unique reservoir ﬁlter can be associated to them (the so called echo state property) and we
identify situations in which those ﬁlters are themselves continuous (they satisfy automatically
the fading memory property). We also point out large classes of RC systems for which internal
approximation is possible, that is, if the RC systems are close then so are the associated reservoir
ﬁlters.
• Section 4 shows that echo state networks are universal uniform approximants in the category of
discrete-time fading memory ﬁlters with uniformly bounded inputs.
2
Continuous and fading memory ﬁlters
This section introduces the notation of the paper as well as general facts about ﬁlters and functionals
needed in the developments that follow. The new results are contained in Section 2.3, where we charac-
terize the fading memory property as a continuity condition when the sequence spaces where inputs and
outputs are deﬁned are uniformly bounded and are endowed with the product topology. This feature
makes this property independent of the weighting sequences that are usually introduced to deﬁne it.
2.1
Notation
Vectors and matrices.
A column vector is denoted by a bold lower case symbol like r and r⊤
indicates its transpose. Given a vector v ∈Rn, we denote its entries by vi, with i ∈{1, . . . , n}; we also
write v = (vi)i∈{1,...,n}. We denote by Mn,m the space of real n × m matrices with m, n ∈N. When
n = m, we use the symbol Mn to refer to the space of square matrices of order n. Given a matrix
A ∈Mn,m, we denote its components by Aij and we write A = (Aij), with i ∈{1, . . . , n}, j ∈{1, . . . m}.
Given a vector v ∈Rn, the symbol ∥v∥stands for any norm in Rn (they are all equivalent) and is not
necessarily the Euclidean one, unless it is explicitly mentioned. The open balls with respect to a given
norm ∥·∥, center v ∈Rn, and radius r > 0 will be denoted by B∥·∥(v, r); their closures by B∥·∥(v, r).
For any A ∈Mn,m, ∥A∥2 denotes its matrix norm induced by the Euclidean norms in Rm and Rn, and
satisﬁes [Horn 13, Example 5.6.6] that ∥A∥2 = σmax(A), with σmax(A) the largest singular value of A.
∥A∥2 is sometimes referred to as the spectral norm of A. The symbol |||·||| is reserved for the norms of
operators or functionals deﬁned on inﬁnite dimensional spaces.
Echo state networks are universal
4
Sequence spaces.
N denotes the set of natural numbers with the zero element included.
Z (re-
spectively, Z+ and Z−) are the integers (respectively, the positive and the negative integers).
The
symbol (Rn)Z denotes the set of inﬁnite real sequences of the form z = (. . . , z−1, z0, z1, . . .), zi ∈Rn,
i ∈Z; (Rn)Z−and (Rn)Z+ are the subspaces consisting of, respectively, left and right inﬁnite sequences:
(Rn)Z−= {z = (. . . , z−2, z−1, z0) | zi ∈Rn, i ∈Z−}, (Rn)Z+ = {z = (z0, z1, z2, . . .) | zi ∈Rn, i ∈Z+}.
Analogously, (Dn)Z, (Dn)Z−, and (Dn)Z+ stand for (semi-)inﬁnite sequences with elements in the sub-
set Dn ⊂Rn. In most cases we endow these inﬁnite product spaces with the Banach space structures
associated to one of the following two norms:
• The supremum norm: deﬁne ∥z∥∞:= supt∈Z {∥zt∥}. The symbols ℓ∞(Rn) and ℓ∞
± (Rn) are
used to denote the Banach spaces formed by the elements in the corresponding inﬁnite product
spaces that have a ﬁnite supremum norm.
• The weighted norm: let w : N −→(0, 1] be a decreasing sequence with zero limit. We deﬁne the
associated weighted norm
∥· ∥w on (Rn)Z−associated to the weighting sequence w as the
map:
∥· ∥w :
(Rn)Z−
−→
R+
z
7−→
∥z∥w := supt∈Z−∥ztw−t∥.
The Proposition 5.2 in Appendix 5.10 shows that the space
ℓw
−(Rn) :=
n
z ∈(Rn)Z−| ∥z∥w < ∞
o
,
endowed with weighted norm ∥· ∥w forms also a Banach space.
It is very easy to show that ∥z∥w ≤∥z∥∞, for all v ∈(Rn)Z−. This implies that ℓ∞
−(Rn) ⊂ℓw
−(Rn) and
that the inclusion map (ℓ∞
−(Rn), ∥·∥∞) ,→(ℓw
−(Rn, ∥·∥w) is continuous.
2.2
Filters and systems
Filters.
Let Dn ⊂Rn and DN ⊂RN. We refer to the maps of the type U : (Dn)Z −→(DN)Z as ﬁlters
or operators and to those like H : (Dn)Z −→DN (or H : (Dn)Z± −→DN) as RN-valued functionals.
A ﬁlter U : (Dn)Z −→(DN)Z is called causal when for any two elements z, w ∈(Dn)Z that satisfy
that zτ = wτ for any τ ≤t, for a given t ∈Z, we have that U(z)t = U(w)t. Let Tτ : (Dn)Z −→(Dn)Z
be the time delay operator deﬁned by Tτ(z)t := zt−τ. The ﬁlter U is called time-invariant when it
commutes with the time delay operator, that is, Tτ ◦U = U ◦Tτ, for any τ ∈Z (in this expression, the
two operators Tτ have to be understood as deﬁned in the appropriate sequence spaces).
We recall (see for instance [Boyd 85]) that there is a bijection between causal time-invariant ﬁlters
and functionals on (Dn)Z−. Indeed, consider the sets F(Dn)Z−,(DN)Z−and H(Dn)Z−,(DN)Z−
F(Dn)Z−,(DN)Z−
=

U : (Dn)Z −→(DN)Z | U is causal and time-invariant
	
,
(2.1)
H(Dn)Z−,(DN)Z−
=

H : (Dn)Z−−→DN
	
.
(2.2)
Then, given a time-invariant ﬁlter U, we can associate to it a functional HU via the assignment HU(z) :=
U(ze)0, where ze ∈(Rn)Z is an arbitrary extension of z ∈(Dn)Z−to (Dn)Z. Let Ψ : F(Dn)Z−,(DN)Z−−→
H(Dn)Z−,(DN)Z−be the map such that Ψ(U) := HU. Conversely, for any functional H, we can deﬁne a
time-invariant causal ﬁlter UH by UH(z)t := H((PZ−◦T−t)(z)), where T−t is the (−t)-time delay operator
Echo state networks are universal
5
and PZ−: (Rn)Z −→(Rn)Z−is the natural projection. Let Φ : H(Dn)Z−,(DN)Z−−→F(Dn)Z−,(DN)Z−be
the map such that Φ(H) := UH. It is easy to verify that:
Ψ ◦Φ
=
IH
or, equivalently,
HUH = H,
for any functional
H : (Dn)Z−−→DN,
Φ ◦Ψ
=
IF
or, equivalently,
UHU = U,
for any causal time-invariant ﬁlter
U : (Dn)Z −→(DN)Z,
that is, Ψ and Φ are inverse of each other and hence are both bijections. Additionally, we note that
the sets F(Dn)Z−,(DN)Z−and H(Dn)Z−,(DN)Z−are vector spaces with naturally deﬁned operations and
that Ψ and Φ are linear maps between them, which allows us to conclude that F(Dn)Z−,(DN)Z−and
H(Dn)Z−,(DN)Z−are linear isomorphic.
When a ﬁlter is causal and time-invariant, we work in many situations just with the restriction U :
(Rn)Z−−→(RN)Z−instead of the original ﬁlter U : (Rn)Z −→(RN)Z without making the distinction,
since the former uniquely determines the latter. Indeed, by deﬁnition, for any z ∈(Dn)Z and t ∈Z:
U(z)t = (T−t (U(z)))0 = (U (T−t(z)))0 ,
(2.3)
where the second equality holds by the time-invariance of U and the value in the right-hand side depends
only on PZ−(T−t(z)) ∈(Rn)Z−, by causality.
Reservoir systems and ﬁlters.
Consider now the RC system determined by (1.1)–(1.2) with reser-
voir map deﬁned on subsets DN, D′
N ⊂RN and Dn ⊂Rn, that is, F : DN × Dn −→D′
N and
h : D′
N →Rd. There are two properties of reservoir systems that will be crucial in what follows:
• Existence of solutions property: this property holds when for each z ∈(Dn)Z there exists an
element x ∈(DN)Z that satisﬁes the relation (1.1) for each t ∈Z.
• Uniqueness of solutions or echo state property (ESP): it holds when the system has the
existence of solutions property and, additionally, these solutions are unique.
The echo state property has deserved much attention in the context of echo state networks [Jaeg 10,
Jaeg 04, Bueh 06, Yild 12, Bai 12, Wain 16, Manj 13, Gall 17]. We emphasize that these two properties
are genuine conditions that are not automatically satisﬁed by all RC systems. Later on in the paper,
Theorem 3.1 speciﬁes suﬃcient conditions for them to hold.
The combination of the existence of solutions with the axiom of choice allows us to associate ﬁlters
U F : (Dn)Z −→(DN)Z to each RC system with that property via the reservoir map and (1.1), that is,
U F (z)t := xt ∈RN, for all t ∈Z. We will denote by U F
h : (Dn)Z −→(Dd)Z the corresponding ﬁlter
determined by the entire reservoir system, that is, U F
h (z)t = h
 U F (z)t

:= yt ∈Rd. U F
h is said to be
a reservoir ﬁlter or a response map associated to the RC system (1.1)–(1.2). The ﬁlters U F and
U F
h are causal by construction. A unique reservoir ﬁlter can be associated to a reservoir system when
the echo state property holds. We warn the reader that reservoir ﬁlters appear in the literature only
in the presence of the ESP; that is why we sometimes make the distinction between those that come
from reservoir systems that do and do not satisfy the ESP by referring to them as reservoir ﬁlters
and generalized reservoir ﬁlters, respectively.
In the systems theory literature, the RC equations (1.1)–(1.2) are referred to as the state-variable
or the internal representation point of view and associated ﬁlters as the external representation
of the system.
The next proposition shows that in the presence of the ESP reservoir ﬁlters are not only causal
but also time-invariant.
In that situation we can hence associate to U F
h a reservoir functional
HF
h : (Dn)Z−−→Rd determined by HF
h := HU F
h .
Echo state networks are universal
6
Proposition 2.1 Let DN ⊂RN, Dn ⊂Rn, and F : DN × Dn −→DN be a reservoir map that satisﬁes
the echo state property for all the elements in (Dn)Z. Then, the corresponding ﬁlter U F : (Dn)Z −→
(DN)Z is causal and time-invariant.
We emphasize that it is the autonomous character of the reservoir map that guarantees time-
invariance in the previous proposition. An explicit time dependence on time in that map would spoil
that conclusion.
Reservoir system morphisms.
Let N1, N2, n, d ∈N and let F1 : DN1 ×Dn −→DN1, h1 : DN1 →Rd
and F2 : DN2 × Dn −→DN2, h2 : DN2 →Rd be two reservoir systems. We say that a map f : DN1 −→
DN2 is a morphism between the two systems when it satisﬁes the following two properties:
(i) Reservoir equivariance: f(F1(x1, z)) = F2(f(x1), z), for all x1 ∈DN1, and z ∈Dn.
(ii) Readout invariance: h1(x1) = h2(f(x1)), for all x1 ∈DN1.
When the map f has an inverse and it is also a morphism between the systems determined by the
pairs (F2, h2) and (F1, h1) we say that f is a system isomorphism and that (F1, h1) and (F2, h2) are
isomorphic. Given a system F1 : DN1 ×Dn −→DN1, h1 : DN1 →Rd and a bijection f : DN1 −→DN2,
the map f is a system isomorphism with respect to the system F2 : DN2 × Dn −→DN2, h2 : DN2 →Rd
deﬁned by
F2(x2, z)
:=
f
 F1(f −1(x2), z)

,
for all
x2 ∈DN2, z ∈Dn,
(2.4)
h2(x2)
:=
h1(f −1(x2))),
for all
x2 ∈DN2.
(2.5)
The proof of the following statement is a straightforward consequence of the deﬁnitions.
Proposition 2.2 Let F : DN1×Dn −→DN1, h : DN1 →Rd and F : DN2×Dn −→DN2, h : DN2 →Rd
be two reservoir systems. Let f : DN1 −→DN2 be a morphism between them. Then:
(i) If x1 ∈(DN1)Z is a solution for the reservoir map F1 associated to the input z ∈(Dn)Z, then the
sequence x2 ∈(DN2)Z deﬁned by x2
t := f
 x1
t

, t ∈Z, is a solution for the reservoir map F2
associated to the same input.
(ii) If U F1
h1 is a reservoir ﬁlter for the system determined by the pair (F1, h1) then it is also a reservoir
ﬁlter for the system (F2, h2).
(iii) If f is a system isomorphism then the implications in the previous two points are equivalences.
2.3
Continuity and the fading memory property
In agreement with the notation introduced in the previous section, in the following paragraphs the
symbol U : (Dn)Z−−→(DN)Z−stands for a causal and time-invariant ﬁlter or, strictly speaking, for
the restriction of U : (Dn)Z −→(DN)Z to Z−, see (2.3); HU : (Dn)Z−−→DN is the associated
functional, for some DN ⊂RN and Dn ⊂Rn. Analogously, UH is the ﬁlter associated to a given
functional H.
Deﬁnition 2.3 (Continuous ﬁlters and functionals) Let DN ⊂RN and Dn ⊂Rn be such that
(Dn)Z−⊂ℓ∞
−(Rn) and (DN)Z−⊂ℓ∞
−(RN). A causal and time-invariant ﬁlter U : (Dn)Z−−→(DN)Z−
is called continuous when it is a continuous map between the metric spaces

(Dn)Z−, ∥·∥∞

and

(DN)Z−, ∥·∥∞

.
An analogous prescription can be used to deﬁne continuous functionals H :
Echo state networks are universal
7

(Dn)Z−, ∥·∥∞

−→(DN, ∥·∥). We denote by F∞
(Dn)Z−,(DN)Z−and H∞
(Dn)Z−,(DN)Z−the set of continuous
ﬁlters and functionals, respectively.
The following proposition shows that when ﬁlters are causal and time-invariant, their continuity can
be read out of their corresponding functionals and viceversa.
Proposition 2.4 Let Dn ⊂Rn and DN ⊂RN be such that (Dn)Z−⊂ℓ∞
−(Rn) and (DN)Z−⊂ℓ∞
−(RN).
Let U : (Dn)Z−−→(DN)Z−be a causal and time-invariant ﬁlter, H : (Dn)Z−−→DN a functional, and
let Φ and Ψ be the maps deﬁned in the previous section. Then, if the ﬁlter U is continuous then so is the
associated functional Ψ(U) =: HU. Conversely, if H is continuous then so is Φ(H) =: UH. This implies
that the maps Ψ : F∞
(Dn)Z−,(DN)Z−−→H∞
(Dn)Z−,(DN)Z−and Φ : H∞
(Dn)Z−,(DN)Z−−→F∞
(Dn)Z−,(DN)Z−are
linear isomorphisms and are inverses of each other.
Deﬁnition 2.5 (Fading memory ﬁlters and functionals) Let w : N −→(0, 1] be a weighting se-
quence and let DN ⊂RN and Dn ⊂Rn be such that (Dn)Z−⊂ℓw
−(Rn) and (DN)Z−⊂ℓw
−(RN).
We say that a causal and time-invariant ﬁlter U : (Dn)Z−−→(DN)Z−(respectively, a functional
H : (Dn)Z−−→DN) satisﬁes the fading memory property (FMP) with respect to the sequence w
when it is a continuous map between the metric spaces

(Dn)Z−, ∥·∥w

and

(DN)Z−, ∥·∥w

(respec-
tively,

(Dn)Z−, ∥·∥w

and (DN, ∥·∥)). We denote by Fw
(Dn)Z−,(DN)Z−and Hw
(Dn)Z−,(DN)Z−the set of
FMP ﬁlters and functionals, respectively. If the weighting sequence w is such that wt = λt, for some
λ ∈(0, 1) and all t ∈N, then U is said to have the λ-exponential fading memory property.
A very important part of the results that follow concern uniformly bounded families of sequences,
that is, subsets of (Rn)Z−of the form
KM :=
n
z ∈(Rn)Z−| ∥zt∥≤M
for all
t ∈Z−
o
,
for some M > 0.
(2.6)
It is straightforward to show that KM ⊂ℓ∞
−(Rn) ⊂ℓw
−(Rn), for all M > 0. A very useful fact is that
the relative topology induced by (ℓw
−(Rn), ∥·∥w) in KM coincides with the one induced by the product
topology in (Rn)Z−.
This is a consequence of the following result that is a slight generalization of
[Munk 14, Theorem 20.5]. A proof is provided in Appendix 5.3 for the sake of completeness.
Theorem 2.6 Let ∥·∥: Rn −→[0, ∞) be a norm in Rn, M > 0, and let w : N −→(0, 1] be a weighting
sequence. Let dM(a, b) := min {∥a −b∥, M}, a, b ∈Rn, be a bounded metric on Rn and deﬁne the
w-weighted metric DM
w on (Rn)Z−as
DM
w (x, y) := sup
t∈Z−

dM(xt, yt)w−t
	
,
x, y ∈(Rn)Z−.
(2.7)
Then DM
w
is a metric that induces the product topology on (Rn)Z−.
The space (Rn)Z−is complete
relative to this metric.
An important consequence that can be drawn from this theorem is that all the weighted norms induce
the same topology on the subspaces formed by uniformly bounded sequences. An obvious consequence
of this fact is that continuity with respect to this topology can be deﬁned without the help of weighting
sequences or, equivalently, ﬁlters or functionals with uniformly bounded inputs that have the fading
memory with respect to a weighting sequence, have the same feature with respect to any other weighting
sequence. We make this more speciﬁc in the following statements.
Echo state networks are universal
8
Corollary 2.7 Let M > 0 and let KM :=
n
z ∈(Rn)Z−| ∥zt∥≤M
for all
t ∈Z−
o
be a subset of
(Rn)Z−formed by uniformly bounded sequences. Let w : N −→(0, 1] be an arbitrary weighting sequence.
Then, the metric induced by the weighted norm ∥·∥w on KM coincides with D2M
w . Moreover, since D2M
w
induces the product topology on KM =

B∥·∥(0, M)
Z−
, we can conclude that all the weighted norms
induce the same topology on KM. The same conclusion holds when instead of KM we consider the set
(Dn)Z−, with Dn a compact subset of Rn.
Theorem 2.6 can also be used to give a quick alternative proof in discrete time to an important
compactness result originally formulated in Boyd and Chua in [Boyd 85, Lemma 1] for continuous time
and, later on, in [Grig 17] for discrete time. The next corollary contains an additional completeness
statement.
Corollary 2.8 Let KM be the set of uniformly bounded sequences, deﬁned as in (2.6), and let w :
N −→(0, 1] be a weighting sequence. Then, (KM, ∥·∥w) is a compact, complete, and convex subset of the
Banach space (ℓw
−(Rn), ∥·∥w). The compactness and the completeness statements also hold when instead
of KM we consider the set (Dn)Z−, with Dn a compact subset of Rn; if Dn is additionally convex then
the convexity of (Dn)Z−is also guaranteed.
It is important to point out that the coincidence between the product topology and the topologies
induced by weighted norms that we described in Corollary 2.7 only occurs for uniformly bounded sets
of the type introduced in (2.6). As we state in the next result, the norm topology in ℓw
−(Rn) is strictly
ﬁner than the one induced by the product topology in (Rn)Z−.
Proposition 2.9 Let w : N −→(0, 1] be a weighting sequence and let (ℓw
−(Rn), ∥·∥w) be the Banach
space constructed using the corresponding weighted norm on the space of left inﬁnite sequences with
elements in Rn. The norm topology in ℓw
−(Rn) is strictly ﬁner than the subspace topology induced by the
product topology in (Rn)Z−on ℓw
−(Rn) ⊂(Rn)Z−.
The next proposition spells out how the fading memory property is independent of the weighting
sequence that is used to deﬁne it, which shows its intrinsically topological nature. A conceptual conse-
quence of this fact is that the fading memory property does not contain any information about the rate
at which systems that have it “forget” inputs. A similar statement in the continuous time setup has
been formulated in [Sand 03]. Additionally, there is a bijection between FMP ﬁlters and functionals.
Proposition 2.10 Let KM ⊂(Rn)Z−and KL ⊂
 RNZ−be subsets of uniformly bounded sequences
deﬁned as in (2.6) and let w : N −→(0, 1] be a weighting sequence. Let U : KM −→KL be a causal and
time-invariant ﬁlter and let H : KM −→B∥·∥(0, L) be a functional. Then:
(i) If U (respectively H) has the fading memory property with respect to the weighting sequence w, then
it has the same property with respect to any other weighting sequence. In particular, this implies
that
Fw
KM,KL = Fw′
KM,KL
and
Hw
KM,KL = Hw′
KM,KL,
for any weighting sequence w′.
In what follows we just say that U (respectively H) has the fading memory property and denote
FFMP
KM,KL := Fw
KM,KL
and
HFMP
KM,KL := Hw
KM,KL,
for any weighting sequence w.
(ii) Let Φ and Ψ be the maps deﬁned in the previous section. Then if the ﬁlter U has the fading memory
property then so does the associated functional Ψ(U) =: HU. Analogously, if H has the fading
memory property then so does Φ(H) =: UH. This implies that the maps Ψ : FFMP
KM,KL −→HFMP
KM,KL
and Φ : HFMP
KM,KL −→FFMP
KM,KL are linear isomorphisms and are inverses of each other.
Echo state networks are universal
9
The same statements can be formulated when instead of KM and KL we consider the sets (Dn)Z−and
(DN)Z−, with Dn and DN compact subsets of Rn and RN, respectively.
In the conditions of the previous proposition, the vector spaces FFMP
KM,KL and HFMP
KM,KL can be endowed
with a norm. More speciﬁcally, let U : KM −→KL be a ﬁlter and let H : KM −→B∥·∥(0, L) be a
functional that have the FMP. Deﬁne:
|||U|||∞
:=
sup
z∈KM
{∥U(z)∥∞} = sup
z∈KM
(
sup
t∈Z−
{∥U(z)t∥}
)
,
(2.8)
|||H|||∞
:=
sup
z∈KM
{∥H(z)∥} .
(2.9)
The compactness of (KM, ∥·∥w) guaranteed by Corollary 2.8 and the fact that U and H map into KL and
B∥·∥(0, L), respectively, ensures that the values in (2.8) and (2.9) are ﬁnite which makes
 FFMP
KM,KL, |||·|||∞

and
 HFMP
KM,KL, |||·|||∞

into normed spaces that, as the next result shows, are linearly homeomorphic.
Proposition 2.11 The maps Ψ :
 FFMP
KM,KL, |||·|||∞

−→
 HFMP
KM,KL, |||·|||∞

and its inverse Φ satisfy that
|||Ψ(U1) −Ψ(U2)|||∞
≤
|||U1 −U2|||∞,
for any
U1, U2 ∈FFMP
KM,KL,
(2.10)
|||Φ(H1) −Φ(H2)|||∞
≤
|||H1 −H2|||∞,
for any
H1, H2 ∈HFMP
KM,KL.
(2.11)
These inequalities imply that these two maps are continuous linear bijections and hence the spaces
 FFMP
KM,KL, |||·|||∞

and
 HFMP
KM,KL, |||·|||∞

are homeomorphic.
The same statements can be formulated
when instead of KM and KL we consider the sets (Dn)Z−and (DN)Z−, with Dn and DN compact
subsets of Rn and RN, respectively.
3
Internal approximation of reservoir ﬁlters
This section characterizes situations under which reservoir ﬁlters can be uniformly approximated by
ﬁnding uniform approximants for the corresponding reservoir systems. Such a statement is part of the
next theorem that also identiﬁes criteria for the availability of the echo state and the fading memory
properties. As it was already mentioned, a reservoir system has the ESP when it has a unique semi-
inﬁnite solution for each semi-inﬁnite input. We also recall that, as it was shown in Section 2.3, the FMP
amounts to the continuity of a reservoir ﬁlter with respect to the product topologies on the input and
output spaces. The completeness and compactness of those spaces established in Corollary 2.8 allows
us to use various ﬁxed point theorems to show that solutions for reservoir systems exist under very
weak hypotheses and that for contracting and continuous reservoir maps (we deﬁne this below) these
solutions are unique and depend continuously on the inputs. Said diﬀerently, contracting continuous
reservoir maps induce reservoir ﬁlters that automatically have the echo state and the fading memory
properties.
Theorem 3.1 Let KM ⊂(Rn)Z−and KL ⊂
 RNZ−be subsets of uniformly bounded sequences deﬁned
as in (2.6) and let F : B∥·∥(0, L) × B∥·∥(0, M) −→B∥·∥(0, L) be a continuous reservoir map.
(i) Existence of solutions: for each z ∈KM there exists a x ∈KL (not necessarily unique) that
solves the reservoir equation associated to F, that is,
xt = F(xt−1, zt),
for all t ∈Z−.
Echo state networks are universal
10
(ii) Uniqueness and continuity of solutions (ESP and FMP): suppose that the reservoir map F
is a contraction, that is, there exists 0 < r < 1 such that for all u, v ∈B∥·∥(0, L), z ∈B∥·∥(0, M),
one has
∥F(u, z) −F(v, z)∥≤r ∥u −v∥.
Then, the reservoir system associated to F has the echo state property. Moreover, this system has
a unique associated causal and time-invariant ﬁlter UF : KM −→KL that has the fading memory
property, that is, UF ∈FFMP
KM,KL. The set UF (KM) of accessible states of the ﬁlter UF is compact.
(iii) Internal approximation property: let F1, F2 : B∥·∥(0, L) × B∥·∥(0, M) −→B∥·∥(0, L) be two
continuous reservoir maps such that F1 is a contraction with constant 0 < r < 1 and F2 has the
existence of solutions property. Let UF1, UF2 : KM −→KL be the corresponding ﬁlters (if F2 does
not have the ESP, then UF2 is just a generalized ﬁlter). Then, for any ϵ > 0, we have that
∥F1 −F2∥∞< δ(ϵ) := (1 −r)ϵ
implies that
|||UF1 −UF2|||∞< ϵ.
(3.1)
Part (i) also holds true when instead of KM and KL we consider the sets (Dn)Z−and (DN)Z−, with Dn
and DN compact and convex subsets of Rn and RN, respectively, that replace the closed balls B∥·∥(0, M)
and B∥·∥(0, L). The same applies to parts (ii) and (iii) but, this time, the convexity hypothesis is not
needed.
Deﬁne the set KKM,KL :=
n
F : B∥·∥(0, L) × B∥·∥(0, M) −→B∥·∥(0, L) | F is a continuous contraction
o
.
Using the notation introduced in the previous section, the statement in (3.1) and part (ii) of the theorem
automatically imply that the map
Ξ :
(KKM,KL, ∥·∥∞)
−→
 FFMP
KM,KL, |||·|||∞

F
7−→
UF
is continuous and by Proposition 2.11, the map that associates to each F ∈KKM,KL the corresponding
functional HF , that is,
Ψ ◦Ξ :
(KKM,KL, ∥·∥∞)
−→
 HFMP
KM,KL, |||·|||∞

F
7−→
HF ,
is also continuous.
Proof of the theorem.
(i) We start by deﬁning, for each z ∈KM, the map given by
Fz :
KL
−→
KL
x
7−→
(Fz(x))t := F(xt−1, zt).
We show ﬁrst that Fz can be written as a product of continuous functions. Indeed:
Fz =
Y
t∈Z−
F(·, zt) ◦pt−1(x),
(3.2)
where the projections pt : KL −→B∥·∥(0, L) are given by pt(x) = xt. These projections are continuous
when we consider in KL the product topology. Additionally, the continuity of the reservoir F implies
that Fz is a product of continuous functions, which ensures that Fz is itself continuous [Munk 14,
Theorem 19.6]. Moreover, by the corollaries 2.7 and 2.8, the space KL is a compact and convex subset
of the Banach space
 ℓw
−(Rn), ∥· ∥w

(see Proposition 5.2), for any weighting sequence w. Schauder’s
Echo state networks are universal
11
Fixed Point Theorem (see [Shap 16, Theorem 7.1, page 75]) guarantees then that Fz has at least a ﬁxed
point, that is, a point x ∈KL that satisﬁes Fz(x) = x or, equivalently,
xt = F(xt−1, zt),
for all t ∈Z−,
which implies that x is a solution of F for z, as required.
Proof of part (ii) The main tool in the proof of this part is a parameter dependent version of the
Contraction Fixed Point Theorem, that we include here for the sake of completeness and whose proof
can be found in [Ster 10, Theorem 6.4.1, page 137].
Theorem 3.2 Let (X, dX) be a complete metric space and let Z be a metric space. Let K : X×Z −→X
be a continuous map such that for each z ∈Z, the map Kz : X −→X given by Kz(x) := K(x, z) is a
contraction with a constant 0 < r < 1 (independent of z), that is, dX(K(x, z), K(y, z)) ≤rd(x, y), for
all x, y ∈X and all z ∈Z. Then:
(i) For each z ∈Z, the map Kz has a unique ﬁxed point in X.
(ii) The map UK : Z −→X that associates to each point z ∈Z the unique ﬁxed point of Kz is
continuous.
Consider now the map
F :
KL × KM
−→
KL
(x, z)
7−→
(F(x, z))t := F(xt−1, zt).
First, as we did in (3.2), it is easy to show that F is continuous with respect to the product topologies
in KM and KL, by writing it down as the product of the composition of continuous functions. Second,
we show that the map F is a contraction. Indeed, since by Corollary 2.7 we can choose an arbitrary
weighting sequence to generate the product topologies in KM and KL, we select w : N −→(0, 1] given
by wt := λt, with t ∈N and λ > 0 that satisﬁes 0 < r < λ < 1. Then, for any x, y ∈KL and any
z ∈KM, we have
∥F(x, z) −F(y, z)∥w = sup
t∈Z−

∥F(xt−1, zt) −F(yt−1, zt)∥λ−t	
≤sup
t∈Z−

∥xt−1 −yt−1∥rλ−t	
,
where we used that F is a contraction. Now, since 0 < r < λ < 1 and hence r/λ < 1, we have
sup
t∈Z−

∥xt−1 −yt−1∥rλ−t	
= sup
t∈Z−
n
∥xt−1 −yt−1∥λ−(t−1) r
λ
o
≤r
λ ∥x −y∥w .
This shows that F is a family of contractions with constant r/λ < 1 that is continuously parametrized
by the elements in KM. Theorem 3.2 implies the existence of a continuous map UF : (KM, ∥·∥w) −→
(KL, ∥·∥w) that is uniquely determined by the identity
F (UF (z), z) = UF (z),
for all z ∈KM.
Proposition 2.1 implies that UF is causal and time-invariant. The set UF (KM) of accessible states of
the ﬁlter UF is compact because it is the image of a compact set (see Corollary 2.8) by a continuous
map (see [Munk 14, Theorem 26.5, page 166]).
Proof of part (iii) Let z ∈KM and let UF1(z) be the unique solution for z of the reservoir systems
associated to F1 available by the part (ii) of the theorem that we just proved. Additionally, let UF2(z)
Echo state networks are universal
12
be the value of a generalized ﬁlter associated to F2 that exist by hypothesis. Then, for any t ∈Z−, we
have:
∥UF1(z)t −UF2(z)t∥= ∥F1(UF1(z)t−1, zt) −F2(UF2(z)t−1, zt)∥
= ∥F1(UF1(z)t−1, zt) −F1(UF2(z)t−1, zt) + F1(UF2(z)t−1, zt) −F2(UF2(z)t−1, zt)∥
≤∥F1(UF1(z)t−1, zt) −F1(UF2(z)t−1, zt)∥+ ∥F1(UF2(z)t−1, zt) −F2(UF2(z)t−1, zt)∥
≤r ∥UF1(z)t−1 −UF2(z)t−1∥+ ∥F1(UF2(z)t−1, zt) −F2(UF2(z)t−1, zt)∥.
If we now recursively apply n times the same procedure to the ﬁrst summand of this expression, we
obtain that
∥UF1(z)t −UF2(z)t∥≤rn∥UF1(z)t−n −UF2(z)t−n∥+ ∥F1(UF2(z)t−1, zt) −F2(UF2(z)t−1, zt)∥
+ r ∥F1(UF2(z)t−2, zt−1) −F2(UF2(z)t−2, zt−1)∥
+ · · · + rn−1 F1(UF2(z)t−n, zt−(n+1)) −F2(UF2(z)t−n, zt−(n+1))

(3.3)
If we combine the inequality (3.3) with the hypothesis
∥F1 −F2∥∞=
sup
x∈B∥·∥(0,L), z∈B∥·∥(0,M)
{∥F1(x, z) −F2(x, z)∥} < δ(ϵ) := (1 −r)ϵ,
we obtain
∥UF1(z) −UF2(z)∥∞= sup
t∈Z−
{∥UF1(z)t −UF2(z)t∥}
≤2Lrn + (1 + · · · + rn−1)δ(ϵ) = 2Lrn + 1 −rn
1 −r δ(ϵ)
(3.4)
Since this inequality is valid for any n ∈N, we can take the limit n −→∞and we obtain that
∥UF1(z) −UF2(z)∥∞≤δ(ϵ)
1 −r = ϵ.
Additionally, as this relation is valid for any z ∈KM, we can conclude that
|||UF1 −UF2|||∞= sup
z∈KM
{∥UF1(z) −UF2(z)∥∞} ≤ϵ,
as required.
■
As a straightforward corollary of the ﬁrst part of the previous theorem, it is easy to show that echo
state networks always have (generalized) reservoir ﬁlters associated as well as to formulate conditions
that ensure simultaneously the echo state and the fading memory properties.
We recall that a map σ : R −→[−1, 1] is a squashing function if it is non-decreasing, limx→−∞σ(x) =
−1, and limx→∞σ(x) = 1.
Corollary 3.3 Consider echo state network given by
xt = σ (Axt−1 + Czt + ζ) ,
yt = Wxt,
(3.5)
(3.6)
where C ∈MN,n for some N ∈N, ζ ∈RN, A ∈MN,N, W ∈Md,N, and the input signal z ∈(Dn)Z,
with Dn ⊂Rn a compact and convex subset. The function σ : RN −→[−1, 1]N in (3.5) is constructed
by componentwise application of a squashing function that we also call σ. Then:
Echo state networks are universal
13
(i) If the squashing function σ is continuous, then the reservoir equation (3.5) has the existence of
solutions property and we can hence associate to the system (3.5)-(3.6) a generalized reservoir
ﬁlter.
(ii) If the squashing function σ is diﬀerentiable with Lipschitz constant Lσ := supx∈R |σ′(x)| < ∞
and the matrix A is such that ∥A∥2 Lσ = σmax(A)Lσ < 1, then the reservoir system (3.5)-(3.6)
has the echo state and the fading memory properties and we can hence associate to it a unique
time-invariant reservoir ﬁlter.
The statement in part (i) remains valid when [−1, 1]N is replaced by a compact and convex subset
DN ⊂[−1, 1]N that is left invariant by the reservoir equation (3.5), that is, σ (Ax + Cz + ζ) ∈DN for
any x ∈DN and any z ∈Dn. The same applies to part (ii) but only the compactness hypothesis is
necessary.
Remark 3.4 The hypothesis ∥A∥2 Lσ < 1 appears in the literature as a suﬃcient condition to ensure
the echo state property, which has been extensively studied in the ESN literature [Jaeg 10, Jaeg 04,
Bueh 06, Bai 12, Yild 12, Wain 16, Manj 13]. Our result shows that this condition implies automati-
cally the fading memory property. Nevertheless, that condition is far from being sharp and has been
signiﬁcantly improved in [Bueh 06, Yild 12]. We point out that the enhanced suﬃcient conditions for
the echo state property contained in those references also imply the fading memory property via part
(ii) of Theorem 3.1.
4
Echo state networks as universal uniform approximants
The internal approximation property that we introduced in part (ii) of Theorem 3.1 tells us that we
can approximate any reservoir ﬁlter by ﬁnding an approximant for the reservoir system that generates
it. This reduces the problem of proving a density statement in a space of operators between inﬁnite-
dimensional spaces to a space of functions with ﬁnite dimensional variables and values. This topic is
the subject of many results in approximation theory, some of which we mentioned in the introduction.
This strategy allows one to ﬁnd simple approximating reservoir ﬁlters for any reservoir system that has
the fading memory property. In the next result we use as approximating family the echo state networks
that we presented in the introduction and that, as we see later on, are the natural generalizations of
neural networks in a dynamic learning setup, with the important feature that they are constructed using
linear readouts. The combination of this approach with a previously obtained result [Grig 17] on the
density of reservoir ﬁlters on the fading memory category allows us to prove in the next theorem that
echo state networks can approximate any fading memory ﬁlter. On other words, echo state networks
are universal.
All along this section, we use the Euclidean norm for the ﬁnite dimensional spaces, that is, for
each x ∈Rn, we write ∥x∥:=
 Pn
i=1 x2
i
1/2. For any M > 0, the symbol B∥·∥(0, M) (respectively
B∥·∥(0, M)) denotes here the open (respectively closed) balls with respect to that norm. Additionally,
we set In := B∥·∥(0, 1).
Theorem 4.1 Let U : IZ−
n
−→
 RdZ−be a causal and time-invariant ﬁlter that has the fading memory
ﬁlter. Then, for any ϵ > 0 and any weighting sequence w, there is an echo state network
xt = σ (Axt−1 + Czt + ζ) ,
yt = Wxt.
(4.1)
(4.2)
whose associated generalized ﬁlters UESN : IZ−
n
−→
 RdZ−satisfy that
|||U −UESN|||∞< ϵ.
(4.3)
Echo state networks are universal
14
In these expressions C ∈MN,n for some N ∈N, ζ ∈RN, A ∈MN,N, and W ∈Md,N. The function
σ : RN −→[−1, 1]N in (4.1) is constructed by componentwise application of a continuous squashing
function σ : R −→[−1, 1] that we denote with the same symbol.
When the approximating echo state network (4.1)-(4.2) satisﬁes the echo state property, then it has a
unique ﬁlter UESN associated which is necessarily time-invariant. The corresponding reservoir functional
HESN : IZ−
n
−→Rd satisﬁes that
|||HU −HESN|||∞< ϵ.
(4.4)
Proof.
As we already explained, we proceed by ﬁrst approximating the ﬁlter U by one of the non-
homogeneous state-aﬃne system (SAS) reservoir ﬁlters introduced in [Grig 17], and we later on show
that we can approximate that reservoir ﬁlter by an echo state network like the one in (4.1)-(4.2).
We start by recalling that a non-homogeneous state-aﬃne system is a reservoir system determined
by the state-space transformation:
xt = p(zt)xt−1 + q(zt),
yt = W1xt,
(4.5)
(4.6)
where the inputs zt ∈In := B∥·∥(0, 1), the states xt ∈RN1, for some N1 ∈N, and W1 ∈Md,N1. The
symbols p(zt) and q(zt) stand for polynomials with matrix coeﬃcients and degrees r and s, respectively,
of the form:
p(z)
=
X
i1,...,in∈{0,...,r}
i1+···+in≤r
zi1
1 · · · zin
n Ai1,...,in,
Ai1,...,in ∈MN1,
z ∈In
q(z)
=
X
i1,...,in∈{0,...,s}
i1+···+in≤s
zi1
1 · · · zin
n Bi1,...,in,
Bi1,...,in ∈MN1,1,
z ∈In.
Let L > 0 and choose a real number K such that
0 < K <
L
L + 1 < 1.
(4.7)
Consider now SAS ﬁlters that satisfy that maxz∈In σmax(p(z)) < K and maxz∈In σmax(p(z)) < K.
It can be shown [Grig 17, Proposition 3.7] that under those hypotheses, the reservoir system (4.5)-
(4.6) has the echo state property and deﬁnes a unique causal, time-invariant, and fading memory ﬁlter
U p,q
W1 : IZ−
n
−→(Rd)Z−. Moreover, Theorem 3.12 in [Grig 17] shows that for any ϵ1 > 0, there exists a
SAS ﬁlter U p,q
W1 satisfying the hypotheses that we just discussed, for which
HU −Hp,q
W1

∞< ϵ1,
(4.8)
where HU and Hp,q
W1 are the reservoir functionals associated to U and U p,q
W1, respectively. Proposition
2.11 together with this inequality imply that
U −U p,q
W1

∞< ϵ1.
(4.9)
We now show that the SAS ﬁlter U p,q
W1 can be approximated by the ﬁlters generated by an echo state
network. Deﬁne the map
FSAS :
B∥·∥(0, L) × In
−→
RN1
(x, z)
7−→
p(z)x + q(z),
(4.10)
with B∥·∥(0, L) ⊂RN1 and p and q the polynomials associated to the approximating SAS ﬁlter U p,q
W1 in
(4.9).
Echo state networks are universal
15
The prescription on the choice of the constant K in (4.7) has two main consequences. Firstly, the
map FSAS is a contraction. Indeed, for any (x, z), (y, z) ∈B∥·∥(0, L) × In:
∥FSAS(x, z) −FSAS(y, z)∥≤∥p(z)x −p(z)y∥≤∥p(z)∥2 ∥x −y∥≤K ∥x −y∥.
(4.11)
The map FSAS is hence a contraction since K < 1 by hypothesis. Second, ∥FSAS∥∞< L because by
(4.7)
∥FSAS∥∞=
sup
(x,z)∈B∥·∥(0,L)×In
∥p(z)x + q(z)∥≤
sup
(x,z)∈B∥·∥(0,L)×In
∥p(z)∥2 ∥x∥+ ∥q(z)∥≤KL + K < L.
This implies, in particular, that the map FSAS maps into B∥·∥(0, L) and hence (4.10) can be rewritten
as
FSAS : B∥·∥(0, L) × In −→B∥·∥(0, L).
Additionally, we set
L1 := ∥FSAS∥∞< L.
(4.12)
The uniform density on compacta of the family of feedforward neural networks with one hidden
layer proved in [Cybe 89, Horn 89] guarantees that for any ϵ2 > 0, there exists N ∈N, G ∈MN,N1,
C ∈MN,n, E ∈MN1,N, and ζ ∈RN, such that the map deﬁned by
FNN :
B∥·∥(0, L) × In
−→
RN1
(x, z)
7−→
Eσ (Gx + Cz + ζ) ,
(4.13)
satisﬁes that
∥FNN −FSAS∥∞=
sup
x∈B∥·∥(0,L), z∈In
{∥FNN(x, z) −FSAS(x, z)∥} < ϵ2.
(4.14)
The combination of (4.14) with the reverse triangle inequality implies that ∥FNN∥∞−∥FSAS∥∞< ϵ2
or, equivalently,
∥FNN∥∞< ∥FSAS∥∞+ ϵ2.
(4.15)
Given that ∥FSAS∥∞= L1 < L, if we choose ϵ2 > 0 small enough so that L1 + ϵ2 < L or, equivalently,
ϵ2 < L −L1,
(4.16)
then (4.15) guarantees that ∥FNN∥∞< L, which shows that FNN maps into B∥·∥(0, L), that is, we can
write that
FNN : B∥·∥(0, L) × In −→B∥·∥(0, L).
(4.17)
The continuity of the map FNN and the ﬁrst part of Theorem 3.1 imply that the corresponding reservoir
equation has the existence of solutions property and that we can hence associate to it a (generalized)
ﬁlter UFNN. At the same time, as we proved in (4.11), the map FSAS is a contraction with constant
K < 1. These facts, together with (4.14) and the internal approximation property in Theorem 3.1 allow
us to conclude that the (unique) reservoir ﬁlter UFSAS associated to the reservoir map FSAS is such that
|||UFNN −UFSAS|||∞< ϵ2/(1 −K).
(4.18)
Consider now the readout map hW1 : RN1 −→Rd given by hW1(x) := W1x and let U
hW1
FNN : (In)Z−−→
(Rd)Z−be the ﬁlter given by U
hW1
FNN (z)t := W1UFNN(z)t, t ∈Z−. Analogously, deﬁne U
hW1
FSAS : (In)Z−−→
(Rd)Z−and notice that U
hW1
FSAS = U p,q
W1. Using these observations and (4.18) we have proved that for any
ϵ2 > 0 we can ﬁnd a ﬁlter of the type U
hW1
FNN that satisﬁes that


U p,q
W1 −U
hW1
FNN



∞≤∥W1∥2 |||UFSAS −UFNN|||∞< ∥W1∥2 ϵ2/(1 −K).
(4.19)
Echo state networks are universal
16
Consequently, for any ϵ > 0, if we ﬁrst set ϵ1 = ϵ/2 in (4.8) and we then choose
ϵ2 := min
ϵ(1 −K)
2 ∥W1∥2
, L −L1
2

,
(4.20)
in view of (4.16) and (4.19), we can guarantee using (4.9) and (4.19) that


U −U
hW1
FNN



∞≤
U −U p,q
W1

∞+


U p,q
W1 −U
hW1
FNN



∞≤ϵ
2 + ϵ
2 = ϵ.
(4.21)
In order to conclude the proof it suﬃces to show that the ﬁlter U
hW1
FNN can be realized as the reservoir
ﬁlter associated to an echo state network of the type presented in the statement. We carry that out
by using the elements that appeared in the construction of the reservoir FNN in (4.13) to deﬁne a new
reservoir map FESN with the architecture of an echo state network. Let A := GE ∈MN and deﬁne
FESN :
DN × In
−→
RN
(x, z)
7−→
σ (Ax + Cz + ζ) .
(4.22)
The set DN in the domain of FESN is given by
DN := [−1, 1]N ∩E−1(B∥·∥(0, L)),
(4.23)
where E−1(B∥·∥(0, L)) denotes the preimage of the set B∥·∥(0, L) ⊂RN1 by the linear map E : RN −→
RN1 associated to the matrix E ∈MN1,N. This set is compact as E−1(B∥·∥(0, L)) is closed and [−1, 1]N
is compact and hence DN is a closed subspace of a compact space which is always compact [Munk 14,
Theorem 26.2]. Additionally, DN is also convex because [−1, 1]N is convex and E−1(B∥·∥(0, L)) is also
convex because it is the preimage of a convex set by a linear map, which is always convex.
We note now that the image of FESN is contained in DN. First, as the squashing function maps into
the interval [−1, 1], it is clear that
FESN (DN, In) ⊂[−1, 1]N.
(4.24)
Second, for any x ∈DN we have by construction that x ∈E−1(B∥·∥(0, L)) and hence Ex ∈B∥·∥(0, L).
Since by (4.17) FNN maps into B∥·∥(0, L), we can ensure that for any z ∈In, the image FNN(Ex, z) =
Eσ (GEx + Cz + ζ) = Eσ (Ax + Cz + ζ) ∈B∥·∥(0, L) or, equivalently,
FESN(x, z) = σ (Ax + Cz + ζ) ∈E−1(B∥·∥(0, L)).
(4.25)
The relations (4.24) and (4.25) imply that
FESN (DN, In) ⊂DN,
(4.26)
and hence, we can rewrite (4.22) as
FESN : DN × In −→DN.
The continuity of the map FESN and the compactness and convexity of the set DN ⊂RN that we
established above allow us to use the ﬁrst part of Theorem 3.1 to conclude that the corresponding
reservoir equation has the existence of solutions property and that we can hence associate to it a
(generalized) ﬁlter UFESN. Let W := W1E ∈Md,n and deﬁne the readout map hESN : DN −→Rd
by hESN(x) := Wx = W1Ex. Denote by UESN any generalized reservoir ﬁlter associated to the echo
state network system (FESN, hESN) that, by construction, satisﬁes UESN(z)t := hESN(UFESN(z)t) =
WUFESN(z)t, for any z ∈In and t ∈Z−.
Echo state networks are universal
17
We next show that the map f : DN = [−1, 1]N ∩E−1(B∥·∥(0, L)) −→B∥·∥(0, L) given by f(x) := Ex
is a morphism between the echo state network system (FESN, hESN) and the reservoir system (FNN, hW1).
Indeed, the reservoir equivariance property holds because, for any (x, z) ∈DN ×In, the deﬁnitions (4.13)
and (4.22) ensure that
f(FESN(x, z)) = Eσ (Ax + Cz + ζ) = Eσ (GEx + Cz + ζ) = FNN(Ex, z) = FNN(f(x), z).
The readout invariance is obvious. This fact and the second part in Proposition 2.2 imply that all the
generalized ﬁlters UESN associated to the echo state network are actually ﬁlters generated by the system
(FNN, hW1). This means that for each generalized ﬁlter UESN there exists a generalized ﬁlter of the type
U
hW1
FNN such that UESN = U
hW1
FNN . The inequality (4.21) proves then (4.3) in the statement of the theorem.
The last claim in the theorem is a straightforward consequence of Propositions 2.1 and 2.11.
■
5
Appendices
5.1
Proof of Proposition 2.1
Let τ ∈N and let T n
τ : (Dn)Z −→(Dn)Z and T N
τ
: (DN)Z −→(DN)Z be the corresponding operators.
For any z ∈(Dn)Z, let x ∈(DN)Z be the unique solution of the reservoir system determined by F, that
is,
x := U F (z).
(5.1)
Then, for any t ∈Z,
 T N
τ ◦U F (z)

= xt−τ.
(5.2)
Analogously, let ex ∈(DN)Z be the unique solution of F associated to the input T n
τ (z), that is,
ext =
 U F ◦T n
τ (z)

t ,
for any
t ∈Z.
(5.3)
By construction, the sequence ex satisﬁes that
ext = F (ext−1, T n
τ (z)t) = F (ext−1, zt−τ) ,
for any
t ∈Z.
It we set s := t −τ, this expression can be rewritten as
exs+τ = F (exs+τ−1, zs) ,
for any
s ∈Z,
(5.4)
and if we deﬁne bxs := exs+τ, the equality (5.4) becomes
bxs = F (bxs−1, zs) ,
for any
s ∈Z,
which shows that bx ∈(DN)Z is a solution of F determined by the input z ∈(Dn)Z. Since the sequence
x ∈(DN)Z in (5.1) is also a solution of F for the same input, the echo state property hypothesis on
the systems determined by F implies that x = bx, necessarily. This implies that xt−τ = bxt−τ for all
t ∈Z, which is equivalent to ext = xt−τ. This equality guarantees that (5.2) and (5.3) are equal and
since bz ∈(Dn)Z is arbitrary, we have that
T N
τ ◦U F = U F ◦T n
τ ,
as required.
■
Echo state networks are universal
18
5.2
Proof of Proposition 2.4
Suppose ﬁrst that U is continuous. This implies the existence of a positive function δU(ϵ) such that if
u, v ∈(Dn)Z−are such that ∥u −v∥∞< δU(ϵ), then ∥U(u) −U(v)∥∞< ϵ. Under that hypothesis, it
is clear that:
∥HU(u) −HU(v)∥= ∥U(u)0 −U(v)0∥≤sup
t∈Z−
{∥U(u)t −U(v)t∥} = ∥U(u) −U(v)∥∞< ϵ,
which shows the continuity of HU :

(Dn)Z−, ∥·∥∞

−→(DN, ∥·∥).
Conversely, suppose that H :

(Dn)Z−, ∥·∥∞

−→(DN, ∥·∥) is continuous and let δH(ϵ) > 0 be such
that if ∥u −v∥∞< δH(ϵ) then ∥H(u) −H(v)∥< ϵ. Then, for any t ∈Z−,
∥UH(u)t −UH(v)t∥=
H((PZ−◦T−t)(u)) −H((PZ−◦T−t)(v))
 < ϵ,
(5.5)
which proves the continuity of UH. The inequality follows from the fact that for any u ∈(Dn)Z−, the
components of the sequence (PZ−◦T−t)(u) are included in those of u and hence sups∈Z−
(PZ−◦T−t(u))s
	
≤
sups∈Z−{∥us∥} or, equivalently,
(PZ−◦T−t)(u)

∞≤∥u∥∞. This implies that if ∥u −v∥∞< δH(ϵ)
then ∥T−t(u) −T−t(v)∥∞< δH(ϵ) and hence (5.5) holds.
■
5.3
Proof of Theorem 2.6
We ﬁrst show that the map DM
w
: (Rn)Z−× (Rn)Z−−→[0, ∞) deﬁned in (2.7) is indeed a met-
ric.
It is clear that DM
w (x, y) ≥0 and that DM
w (x, x) = 0, for any x, y ∈(Rn)Z−.
Conversely, if
DM
w (x, y) = 0, this implies that dM(xt, yt)w−t ≤supt∈Z−

dM(xt, yt)w−t
	
= DM
w (x, y) = 0, which
ensures that dM(xt, yt) = 0, for any t ∈Z−, and hence x = y necessarily since the map dM is a metric
in Rn [Munk 14, Chapter 2, §20]. It is also obvious that DM
w (x, y) = DM
w (y, x). Regarding the triangle
inequality, notice that for any x, y, z ∈(Rn)Z−and t ∈Z−:
dM(xt, zt)w−t ≤dM(xt, yt)w−t + dM(yt, zt)w−t ≤DM
w (x, y) + DM
w (y, z),
which implies that
DM
w (x, z) = sup
t∈Z−

dM(xt, zt)w−t
	
≤DM
w (x, y) + DM
w (y, z).
We now show that the metric topology on (R)Z−associated to DM
w
coincides with the product
topology. Let x ∈(Rn)Z−and let BDM
w (x, ϵ) be an ϵ-ball around it with respect to the metric DM
w . Let
now N ∈N be large enough so that wN < ϵ/M. We then show that the basis element V for the product
topology in (Rn)Z−given by
V := · · · × Rn × Rn × BdM (x−N, ϵ) × · · · BdM (x−1, ϵ) × BdM (x0, ϵ)
and that obviously contains the element x ∈(Rn)Z−is such that V ⊂BDM
w (x, ϵ). Indeed, since for any
y ∈(Rn)Z−and any t ∈Z−we have that dM(xt, yt) ≤M, we can conclude that
dM(xt, yt)w−t ≤MwN,
for all
t ≤−N.
Therefore, DM
w (x, y) ≤max

Mw−N, dM(x−N, y−N)wN, . . . , dM(x−1, y−1)w1, dM(x0, y0)w0
	
and hence
if y ∈V this expression is smaller than ϵ which allows us to conclude the desired inclusion V ⊂
BDM
w (x, ϵ).
Echo state networks are universal
19
Conversely, consider a basis element of the product topology given by U = Q
t∈Z−Ut where Ut =
BdM (xt, ϵt) for a ﬁnite set of indices t ∈{α1, . . . , αr}, ϵt ≤1, and Ut = Rn for the rest.
Let
ϵ := mint∈{α1,...,αr} {ϵtw−t}.
We now show that BDM
w (x, ϵ) ⊂U.
Indeed, if y ∈BDM
w (x, ϵ) then
dM(xt, yt)w−t ≤DM
w (x, y) < ϵ, for all t ∈Z−.
It t ∈{α1, . . . , αr} then ϵ < ϵtw−t and hence
dM(xt, yt)w−t < ϵtw−t, which ensures that dM(xt, yt) < ϵt and hence y ∈U, as desired.
We conclude by showing that
 (Rn)Z−, DM
w

is a complete metric space. First, notice that since for
any x, y ∈(Rn)Z−and any given t ∈Z−we have that
dM(xt, yt) ≤DM
w (x, y)
w−t
,
we can conclude that if {x(i)}i∈N is a Cauchy sequence in (Rn)Z−, then so are the sequences {xt(i)}i∈N
in Rn, for any t ∈Z−, with respect to the bounded metric dM. Since the completeness with respect
to the bounded metric dM and the Euclidean metric are equivalent [Munk 14, Chapter 7, §43] we can
ensure that {xt(i)}i∈N converges to an element at ∈Rn with respect to the Euclidean metric for any
t ∈Z−. We now show that {x(i)}i∈N converges to a := (at)t∈Z−∈(Rn)Z−, with respect to the metric
DM
w , which proves the completeness statement.
Indeed, since the metric DM
w generates the product topology, let U = Q
∈∈Z−Ut be a basis element
such that a ∈U and, as before, Ut = BdM (at, ϵt) for a ﬁnite set of indices t ∈{α1, . . . , αr}, ϵt ≤1,
and Ut = Rn for the rest. Let ϵ = min {ϵα1, . . . , ϵαr}. Since for each t ∈Z−the sequence xt(i)
i→∞
−→at,
then there exists Nt ∈N such that for any k > Nt we have that ∥xt(k) −at∥< ϵ. If we take Nϵ =
max {Nα1, . . . , Nαr} then it is clear that x(i) ∈U, for all i > Nϵ, as required.
■
5.4
Proof of Corollary 2.7
Notice ﬁrst that for any x, y ∈KM, we have that ∥xt −yt∥< 2M, t ∈Z−, and hence
D2M
w (x, y) := sup
t∈Z−

d2M(xt, yt)w−t
	
= sup
t∈Z−
{∥xt −yt∥w−t} = ∥x −y∥w .
Hence, the topology induced by the weighted norm ∥·∥w on KM coincides with the metric topology
induced by the restricted metric D2M
w |KM×KM which, by Theorem 2.6, is the subspace topology induced
by the product topology on (Rn)Z−on KM (see [Munk 14, Exercise 1, page 133]), as well as the product
topology on the product KM =

B∥·∥(0, M)
Z−
(see [Munk 14, Theorem 19.3, page 116])).
■
5.5
Proof of Corollary 2.8
First, since KM =

B∥·∥(0, M)
Z−
, it is clearly the product of compact spaces. By Tychonoﬀ’s Theorem
([Munk 14, Chapter 5]) KM is compact when endowed with the product topology which, by Corollary
2.7, coincides with the topology associated to the restriction of the norm ∥·∥w to KM, as well as with
the metric topology given by D2M
w |KM×KM .
Second, since (KM, ∥·∥w) is metrizable it is a Hausdorﬀspace. This implies (see [Munk 14, Theorem
26.3]) that as KM is a compact subspace of the Banach space (ℓw
−(Rn), ∥·∥w) (see Proposition 5.2)
then it is necessarily closed. This in turn implies ([Simm 63, Theorem B, page 72]) that (KM, ∥·∥w) is
complete.
Finally, the convexity statement follows from the fact that the product of convex sets is always
convex.
■
Echo state networks are universal
20
5.6
Proof of Proposition 2.9
Let dw be the metric on ℓw
−(Rn) induced by the weighted norm ∥·∥w and let Dw := D1
w be the w-
weighted metric on (Rn)Z−with constant M = 1 introduced in Theorem 2.6 and deﬁned using the same
underlying norm in Rn as the one associated to ∥·∥w. As we saw in that theorem, the metric Dw induces
the product topology on (Rn)Z−.
Let now u ∈ℓw
−(Rn) and let ϵ > 0. Let now v ∈ℓw
−(Rn) be such that dw(u, v) < ϵ. By deﬁnition,
we have that
Dw(u, v) = sup
t∈Z−

d1(xt, yt)w−t
	
= sup
t∈Z−
{(min{∥xt −yt∥, 1})w−t} ≤sup
t∈Z−
{∥xt −yt∥w−t} = dw(u, v) < ϵ,
which shows that Bdw(u, ϵ) ⊂BDw(u, ϵ) and allows us to conclude that the norm topology in ℓw
−(Rn)
is ﬁner than the subspace topology induced by the product topology in (Rn)Z−.
We now show that this inclusion is strict. Since the weighting sequence w converges to zero, there
exists an element t0 ∈Z−such that w−t0 < ϵ/2. Let λ > 0 arbitrary and deﬁne the element vλ ∈(Rn)Z−
by setting vλ
t0 := λut0 and vλ
t := λut when t ̸= t0. We now show that vλ ∈BDw(u, ϵ) for any λ > 0.
Indeed,
Dw(u, vλ) = min{|λ −1|∥ut0∥, 1}w−t0 ≤1 · w−t0 < ϵ/2 < ϵ.
At the same time, by deﬁnition,
dw(u, vλ) = |λ −1|∥ut0∥w−t0 < ∞,
which shows that vλ ∈ℓw
−(Rn). However, since |λ −1|∥ut0∥w−t0 can be made as large as desired by
choosing λ big enough, we have proved that for any ball Bdw(u, ϵ′), with ϵ′ > 0 arbitrary, the ball
BDw(u, ϵ) contains always an element in ℓw
−(Rn) that is not included in Bdw(u, ϵ′). This argument
allows us to conclude that the norm topology in ℓw
−(Rn) is strictly ﬁner than the subspace topology
induced by the product topology.
■
5.7
Proof of Proposition 2.10
Proof of part (i) The FMP of U with respect to the sequence w is, by deﬁnition, equivalent to the con-
tinuity of the map U : (KM, ∥·∥w) −→(KL, ∥·∥w) (respectively, H : (KM, ∥·∥w) −→(B∥·∥(0, L), ∥·∥)).
By Corollary 2.8, this is equivalent to the continuity of these maps when KM and KL are endowed with
the product topology which is, by the same result, generated by any arbitrary weighting sequence.
The proof of the second part of the proposition requires the following lemma.
Lemma 5.1 The operator PZ−◦T−t : (KM, ∥·∥w) −→(KM, ∥·∥w) is a continuous map, for any t ∈Z−
and for any weighting sequence w.
Proof of the lemma.
We show that this statement is true by characterizing PZ−◦T−t as a Cartesian
product of continuous maps between two product spaces endowed with the product topologies by using
Corollary 2.7. Indeed, notice ﬁrst that the projections pi : KM −→B∥·∥(0, M) given by pi(z) = zi
are continuous. Since PZ−◦T−t can be written as the inﬁnite Cartesian product of continuous maps
PZ−◦T−t = Q−∞
i=t pi = (. . . , pt−2, pt−1, pt) it is hence continuous using the product topology induced
by ∥·∥w (see [Munk 14, Theorem 19.6]). ▼
Proof of part (ii) First, if H has the FMP then the map H : (KM, ∥·∥w) −→(B∥·∥(0, L), ∥·∥) is
continuous. This implies that UH = Q−∞
t=0 H ◦
 PZ−◦T−t

is also continuous, since by Lemma 5.1 it is
a composition of continuous functions and hence has the FMP. Conversely, if U has the FMP, so is the
case with HU = p0 ◦U.
■
Echo state networks are universal
21
5.8
Proof of Proposition 2.11
We start by proving the continuity of Ψ by proving the inequality (2.10). Let U1, U2 ∈FFMP
KM,KL. By
deﬁnition
|||Ψ(U1) −Ψ(U2)|||∞= sup
z∈KM
{∥Ψ(U1)(z) −Ψ(U2)(z)∥} = sup
z∈KM
{∥U1(z)0 −U2(z)0∥} .
(5.6)
Since we have that
sup
z∈KM
{∥U1(z)0 −U2(z)0∥} ≤sup
z∈KM
{ sup
t∈Z−
{∥U1(z)t −U2(z)t∥}} = sup
z∈KM
{∥U1(z) −U2(z)∥∞} = |||U1 −U2|||∞,
this shows, together with (5.6), that
|||Ψ(U1) −Ψ(U2)|||∞≤|||U1 −U2|||∞,
which implies the continuity of Ψ. Conversely, let H1, H2 ∈HFMP
KM,KL. We have:
|||Φ(H1) −Φ(H2)|||∞= sup
z∈KM
(
sup
t∈Z−
{∥Φ(H1)(z)t −Φ(H2)(z)t∥}
)
= sup
z∈KM
(
sup
t∈Z−
H1((PZ−◦T−t)(z)) −H2((PZ−◦T−t)(z))
	
)
≤sup
z∈KM
{∥H1(z) −H2(z)∥∞} = |||H1 −H2|||∞,
which proves the continuity of Φ. The inequality is a consequence of the fact that the sequence (PZ−◦
T−t)(z) ∈KM.
■
5.9
Proof of Corollary 3.3
(i) Consider the reservoir map FESN : [−1, 1]N × Dn −→[−1, 1]N given by FESN := σ (Ax + Cz + ζ)
The statement is a direct consequence of the continuity of FESN, the compactness and convexity of
[−1, 1]N and Dn, and of part (i) of Theorem 3.1.
(ii) The result follows from part (ii) of Theorem 3.1 since the hypotheses in the statement imply that
the reservoir map FESN is in those circumstances a contraction. Indeed, let x, y ∈[−1, 1]N and let
z ∈Dn, then
∥FESN(x, z) −FESN(y, z)∥= ∥σ (Ax + Cz + ζ) −σ (Ay + Cz + ζ)∥≤Lσ ∥A∥2 ∥x −y∥.
Since by hypothesis Lσ ∥A∥2 < 1, we can conclude that FESN is a contraction, as required. The norm
∥·∥in the previous expression is the Euclidean norm in RN. The time invariance of the resulting unique
fading memory reservoir ﬁlter is a consequence of Proposition 2.1.
■
5.10
 ℓw
−(Rn), ∥· ∥w

is a Banach space
Proposition 5.2 Let w : N −→(0, 1] be a weighting sequence and let ∥· ∥w : (Rn)Z−−→R+ be the
corresponding weighted norm. Then, the space
 ℓw
−(Rn), ∥· ∥w

deﬁned by
ℓw
−(Rn) :=
n
z ∈(Rn)Z−| ∥z∥w < ∞
o
,
endowed with weighted norm ∥· ∥w is a Banach space.
Echo state networks are universal
22
Proof.
We ﬁrst show that ℓw
−(Rn) is a linear subspace of (Rn)Z−. Let u, v ∈ℓw
−(Rn) and let λ ∈R.
Then,
∥u + λv∥w = sup
t∈Z−
{∥ut + λvt∥w−t} ≤sup
t∈Z−
{∥ut∥w−t + λ ∥vt∥w−t}
≤sup
t∈Z−
{∥ut∥w−t} + λ sup
t∈Z−
{∥vt∥w−t} = ∥u∥w + λ ∥v∥w .
We now show that this space is complete. Let {u(n)}n∈N ⊂ℓw
−(Rn) be a Cauchy sequence. This implies
that for any ϵ > 0, there exists N(ϵ) ∈N such that for all m, n > N(ϵ) we have ∥u(n) −u(m)∥w < ϵ.
Hence, for any t ∈Z−,
∥ut(n) −ut(m)∥w−t ≤sup
t∈Z−
{∥ut(n) −ut(m)∥w−t} = ∥u(n) −u(m)∥w < ϵ.
(5.7)
This implies that taking for each ﬁxed t ∈Z−the value N(ϵw−t), the sequences {ut(n)}n∈N in Rn are
Cauchy and hence convergent to values ut ∈Rn. We now show that {u(n)}n∈N converges to u ∈(Rn)Z−.
Using (5.7), take N(ϵ/2) so that for all m, n > N(ϵ/2) and any t ∈Z−one has ∥ut(n) −ut(n)∥w−t ≤
ϵ/2. If we take the limit m →∞in this inequality, we obtain
∥ut(n) −ut∥w−t ≤ϵ/2,
for all t ∈Z−.
This implies that
∥u(n) −u∥w = sup
t∈Z−
{∥ut(n) −ut∥w−t} ≤ϵ/2 < ϵ,
which proves that {u(n)}n∈N converges to u, as required. It remains to be shown that u ∈ℓw
−(Rn), that
is, that ∥u∥< ∞. In order to show that this is indeed the case, let n ∈N be such that ∥u −u(n)∥< ϵ.
This implies that
∥u∥w −∥u(n)∥w < | ∥u∥w −∥u(n)∥w | < ∥u −u(n)∥w < ϵ,
and hence ∥u∥w < ∥u(n)∥w + ϵ < ∞, as required.
■
Acknowledgments: The authors acknowledge partial ﬁnancial support of the French ANR “BIPHO-
PROC” project (ANR-14-OHRI-0002-02) as well as the hospitality of the Centre Interfacultaire Bernoulli
of the Ecole Polytechnique F´ed´erale de Lausanne during the program “Stochastic Dynamical Models
in Mathematical Finance, Econometrics, and Actuarial Sciences” that made possible the collaboration
that lead to some of the results included in this paper. LG acknowledges partial ﬁnancial support of the
Graduate School of Decision Sciences and the Young Scholar Fund AFF of the Universit¨at Konstanz.
JPO acknowledges partial ﬁnancial support coming from the Research Commission of the Universit¨at
Sankt Gallen and the Swiss National Science Foundation (grant number 200021 175801/1).
References
[Arno 57]
V. I. Arnold.
“On functions of three variables”.
Proceedings of the USSR Academy of
Sciences, Vol. 114, pp. 679–681, 1957.
[Bai 12]
Bai Zhang, D. J. Miller, and Yue Wang. “Nonlinear system modeling with random matri-
ces: echo state networks revisited”. IEEE Transactions on Neural Networks and Learning
Systems, Vol. 23, No. 1, pp. 175–182, jan 2012.
Echo state networks are universal
23
[Boyd 85]
S. Boyd and L. Chua. “Fading memory and the problem of approximating nonlinear oper-
ators with Volterra series”. IEEE Transactions on Circuits and Systems, Vol. 32, No. 11,
pp. 1150–1161, nov 1985.
[Bril 58]
M. B. Brilliant. “Theory of the analysis of nonlinear systems”. Tech. Rep., Massachusetts
Institute of Technology, Research Laboratory of Electronics, 1958.
[Bueh 06]
M. Buehner and P. Young. “A tighter bound for the echo state property”. IEEE Transactions
on Neural Networks, Vol. 17, No. 3, pp. 820–824, 2006.
[Cabe 15]
J. Cabessa and A. E. Villa. “Computational capabilities of recurrent neural networks based
on their attractor dynamics”. In: 2015 International Joint Conference on Neural Networks
(IJCNN), pp. 1–8, IEEE, jul 2015.
[Cabe 16]
J. Cabessa and A. E. Villa.
“Expressive power of ﬁrst-order recurrent neural networks
determined by their attractor dynamics”. Journal of Computer and System Sciences, Vol. 82,
No. 8, pp. 1232–1250, 2016.
[Croo 07]
N. Crook. “Nonlinear transient computation”. Neurocomputing, Vol. 70, pp. 1167–1176,
2007.
[Cybe 89]
G. Cybenko. “Approximation by superpositions of a sigmoidal function”. Mathematics of
Control, Signals, and Systems, Vol. 2, No. 4, pp. 303–314, dec 1989.
[Frec 10]
M. Fr´echet. “Sur les fonctionnelles continues”. Annales scientiﬁques de l’Ecole Normale
Sup´erieure. 3`eme s´erie., Vol. 27, pp. 193–216, 1910.
[Gall 17]
C. Gallicchio and A. Micheli. “Echo state property of deep reservoir computing networks”.
Cognitive Computation, Vol. 9, 2017.
[Geor 59]
D. A. George.
“Continuous nonlinear systems”.
Tech. Rep., Massachusetts Institute of
Technology, Research Laboratory of Electronics, 1959.
[Grig 17]
L. Grigoryeva and J.-P. Ortega. “Universal discrete-time reservoir computers with stochastic
inputs and linear readouts using non-homogeneous state-aﬃne systems”. Preprint, dec 2017.
[Horn 13]
R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, second Ed.,
2013.
[Horn 89]
K. Hornik, M. Stinchcombe, and H. White. “Multilayer feedforward networks are universal
approximators”. Neural Networks, Vol. 2, No. 5, pp. 359–366, 1989.
[Horn 90]
K. Hornik, M. Stinchcombe, and H. White. “Universal approximation of an unknown map-
ping and its derivatives using multilayer feedforward networks”. Neural Networks, Vol. 3,
No. 5, pp. 551–560, 1990.
[Horn 91]
K. Hornik.
“Approximation Capabilities of Muitilayer Feedforward Networks”.
Neural
Networks, Vol. 4, No. 1989, pp. 251–257, 1991.
[Horn 93]
K. Hornik. “Some new results on neural network approximation”. Neural Networks, Vol. 6,
No. 8, pp. 1069–1072, 1993.
[Jaeg 04]
H. Jaeger and H. Haas. “Harnessing Nonlinearity: Predicting Chaotic Systems and Saving
Energy in Wireless Communication”. Science, Vol. 304, No. 5667, pp. 78–80, 2004.
Echo state networks are universal
24
[Jaeg 10]
H. Jaeger. “The ’echo state’ approach to analysing and training recurrent neural networks
with an erratum note”.
Tech. Rep., German National Research Center for Information
Technology, 2010.
[Kili 96]
J. Kilian and H. T. Siegelmann. “The dynamic universality of sigmoidal neural networks”.
Information and Computation, Vol. 128, No. 1, pp. 48–56, 1996.
[Kolm 56]
A. N. Kolmogorov. “On the representation of continuous functions of several variables as
superpositions of functions of smaller number of variables”. Soviet Math. Dokl, Vol. 108,
pp. 179–182, 1956.
[Luko 09]
M. Lukoˇseviˇcius and H. Jaeger. “Reservoir computing approaches to recurrent neural net-
work training”. Computer Science Review, Vol. 3, No. 3, pp. 127–149, 2009.
[Maas 00]
W. Maass and E. D. Sontag. “Neural Systems as Nonlinear Filters”. Neural Computation,
Vol. 12, No. 8, pp. 1743–1772, aug 2000.
[Maas 02]
W. Maass, T. Natschl¨ager, and H. Markram. “Real-time computing without stable states:
a new framework for neural computation based on perturbations”. Neural Computation,
Vol. 14, pp. 2531–2560, 2002.
[Maas 04]
W. Maass, T. Natschl¨ager, and H. Markram. “Fading memory and kernel properties of
generic cortical microcircuit models”. Journal of Physiology Paris, Vol. 98, No. 4-6 SPEC.
ISS., pp. 315–330, 2004.
[Maas 07]
W. Maass, P. Joshi, and E. D. Sontag.
“Computational aspects of feedback in neural
circuits”. PLoS Computational Biology, Vol. 3, No. 1, p. e165, 2007.
[Maas 11]
W. Maass. “Liquid state machines: motivation, theory, and applications”. In: S. S. Barry
Cooper and A. Sorbi, Eds., Computability In Context: Computation and Logic in the Real
World, Chap. 8, pp. 275–296, 2011.
[Manj 13]
G. Manjunath and H. Jaeger. “Echo state property linked to an input: exploring a funda-
mental characteristic of recurrent neural networks”. Neural Computation, Vol. 25, No. 3,
pp. 671–696, 2013.
[Matt 92]
M. B. Matthews.
On the Uniform Approximation of Nonlinear Discrete-Time Fading-
Memory Systems Using Neural Network Models. PhD thesis, ETH Z¨urich, 1992.
[Matt 93]
M. B. Matthews. “Approximating nonlinear fading-memory operators using neural network
models”. Circuits, Systems, and Signal Processing, Vol. 12, No. 2, pp. 279–307, jun 1993.
[Munk 14] J. Munkres. Topology. Pearson, second Ed., 2014.
[Perr 96]
P. C. Perryman. Approximation Theory for Deterministic and Stochastic Nonlinear Systems.
PhD thesis, University of California, Irvine, 1996.
[Rusc 98]
L. R¨uschendorf and W. Thomsen. “Closedness of sum spaces and the generalized schr¨odinger
Problem”. Theory of Probability & Its Applications, Vol. 42, No. 3, pp. 483–494, jan 1998.
[Sand 03]
I. W. Sandberg. “Notes of fading-memory conditions”. Circuits, Systems, and Signal Pro-
cessing, Vol. 22, No. 1, pp. 43–55, 2003.
[Sand 91a] I. W. Sandberg. “Approximation Theorems for Discrete-Time Systems”. IEEE Transactions
on Circuits and Systems, Vol. 38, No. 5, pp. 564–566, 1991.
Echo state networks are universal
25
[Sand 91b] I. W. Sandberg. “Structure theorems for nonlinear systems”. Multidimensional Systems and
Signal Processing, Vol. 2, pp. 267–286, 1991.
[Shap 16]
J. H. Shapiro. A Fixed-Point Farrago. Springer International Publishing Switzerland, 2016.
[Sieg 97]
H. Siegelmann, B. Horne, and C. Giles. “Computational capabilities of recurrent NARX neu-
ral networks”. IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics),
Vol. 27, No. 2, pp. 208–215, apr 1997.
[Simm 63] G. F. Simmons. Topology and Modern Analysis. McGraw-Hill, 1963.
[Spre 65]
D. A. Sprecher. “A representation theorem for continuous functions of several variables”.
Proceedings of the American Mathematical Society, Vol. 16, No. 2, p. 200, apr 1965.
[Spre 96]
D. A. Sprecher.
“A numerical implementation of Kolmogorov’s superpositions”. Neural
Networks, Vol. 9, No. 5, pp. 765–772, 1996.
[Spre 97]
D. A. Sprecher. “A numerical implementation of Kolmogorov’s superpositions II”. Neural
Networks, Vol. 10, No. 3, pp. 447–457, 1997.
[Ster 10]
S. Sternberg. Dynamical Systems. Dover, 2010.
[Stub 97]
A. Stubberud and P. Perryman. “Current state of system approximation for deterministic
and stochastic systems”. In: Conference Record of The Thirtieth Asilomar Conference on
Signals, Systems and Computers, pp. 141–145, IEEE Comput. Soc. Press, 1997.
[Vers 07]
D. Verstraeten, B. Schrauwen, M. D’Haene, and D. Stroobandt. “An experimental uniﬁca-
tion of reservoir computing methods”. Neural Networks, Vol. 20, pp. 391–403, 2007.
[Volt 30]
V. Volterra.
Theory of Functionals and of Integral and Integro-Diﬀerential Equations.
Blackie & Son Limited, Glasgow, 1930.
[Wain 16]
G. Wainrib and M. N. Galtier. “A local echo state property through the largest Lyapunov
exponent”. Neural Networks, Vol. 76, pp. 39–45, apr 2016.
[Wien 58]
N. Wiener. Nonlinear Problems in Random Theory. The Technology Press of MIT, 1958.
[Yild 12]
I. B. Yildiz, H. Jaeger, and S. J. Kiebel. “Re-visiting the echo state property.”. Neural
networks : the oﬃcial journal of the International Neural Network Society, Vol. 35, pp. 1–9,
nov 2012.
